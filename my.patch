diff --git a/.gitignore b/.gitignore
index a1a9709..b75f96b 100755
--- a/.gitignore
+++ b/.gitignore
@@ -1,28 +1,79 @@
+<<<<<<< HEAD
+# Compiled
+*.elc
+
+# Packaging
+.cask
+
+# pytest
+.pytest_cache/
+
+
+# developing
+.developing/
+cursor-memory-bank/
+
+# venv
+.venv/
+
+# Backup files
+=======
 *.elc
 *.pyc
 .cask
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 *~
-#*#
-.#*
+\#*\#
+.\#*
 *.bak
+<<<<<<< HEAD
+
+# Undo-tree save-files
+*.~undo-tree
+
+# Auto-save files
+auto-save-list
+tramp
+.\#*
+
+# etc
+.DS_Store 
+
+# Visual Studio Code
+.vscode/
+.aider*
+
+
+# log files
+=======
 *.~undo-tree
 auto-save-list
 tramp
-.#*
+.\#*
 .DS_Store 
 .aider*
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 *.log
 *.log.*
 *.log.*.*
 *.log.*.*.*
 *.log.*.*.*.*
 *.log.*.*.*.*.*
+<<<<<<< HEAD
+
+# test files
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 *.test
 *.test.*
 *.test.*.*
 *.test.*.*.*
 *.test.*.*.*.*
 *.test.*.*.*.*.*
+<<<<<<< HEAD
+
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 test_*.*
 test_*.*.*
 test_*.*.*.*
@@ -33,6 +84,11 @@ test_*.*.*.*.*.*.*.*
 test_*.*.*.*.*.*.*.*.*
 test_*.*.*.*.*.*.*.*.*.*
 test_*.*.*.*.*.*.*.*.*.*.*
+<<<<<<< HEAD
+
+# directory
+developing/
+=======
 *.el.bak
 
 # directory
@@ -52,22 +108,4 @@ simtag/python-bridge/
 simtag/nano-graphrag/
 .cursor/
 .test/
-.GEMINI.md
-.cursorules
-.pytest_cache/
-test/
-commands/
-
-.ruff_cache/
-IMPROVEMENT_PLAN.md
-
-# YoYo AI version control directory
-.yoyo/
-
-# Claude Code guidance file
-CLAUDE.md
-.claude/
-commands/
-docs/
-hooks/
-logs/
\ No newline at end of file
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
diff --git a/CHANGELOG.org b/CHANGELOG.org
index d790c22..eecb7f9 100755
--- a/CHANGELOG.org
+++ b/CHANGELOG.org
@@ -1,13 +1,7 @@
 * Changelog
-* [3.0.5] - 2025-06-25
-** Features
-*** Decouple metadata and adopt a database-first architecture
-- Fields are no longer synchronized with the Org :PROPERTIES: drawer. All field data is now read from and written directly to the database.
-- Support for headline :TAGS: has been completely removed. The system now exclusively uses org-supertag's inline-tag format for visual representation in files, while the actual tag relationships are managed solely by the database.
-- This view is now the central and sole interface for viewing and editing all metadata associated with a node. Users can modify field values directly within this view, creating a closed loop for data manipulation.
-- The :tag field type has been significantly upgraded to natively support one or more tag values (e.g., a movie with multiple directors). The UI accepts comma-separated input and formats the display cleanly (e.g., TagA / TagB / TagC).
-- The legacy List and Range field types have been deprecated to streamline the data model and reduce complexity.
 
+<<<<<<< HEAD
+=======
 * [3.0.4] - 2025-05-30
 ** Bug Fixes
 *** Fixed database cleanup path matching issues
@@ -27,6 +21,7 @@
 - Added detailed logging for sync scope changes and node removal
 - Ensured proper cleanup when modifying sync directories
 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 * [3.0.2] - 2025-05-24
 ** Improvements
 *** Enhanced synchronization stability with comprehensive data protection
@@ -41,6 +36,8 @@
 - Improved vector database synchronization reliability
 - Enhanced tag similarity computation stability
 
+<<<<<<< HEAD
+=======
 *** Fixed AI auto-tagging system freezing issues
 - Resolved `org-supertag-sim-auto-tag-node` causing Emacs to freeze
 - Added comprehensive timeout protection for all AI operations
@@ -48,6 +45,7 @@
 - Added initialization state validation to prevent infinite loops
 - Enhanced error handling with automatic recovery mechanisms
 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ** Features
 *** Added comprehensive database recovery system
 - Introduced `org-supertag-recovery` methods for database restoration
@@ -55,12 +53,15 @@
 - Added diagnostic tools for database integrity checking
 - Enhanced backup and restore capabilities
 
+<<<<<<< HEAD
+=======
 *** Added AI system diagnostic and emergency tools
 - Added `org-supertag-sim-diagnose` for comprehensive system health checking
 - Added `org-supertag-sim-emergency-reset` for complete system recovery
 - Added `org-supertag-sim-safe-auto-tag-node` as a safer alternative to auto-tagging
 - Provided detailed troubleshooting guidance and emergency commands
 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ** Maintenance
 *** Codebase cleanup and organization
 - Removed deprecated protection scope checking utilities
diff --git a/README.org b/README.org
index 4eadb1b..86ff19b 100755
--- a/README.org
+++ b/README.org
@@ -457,9 +457,13 @@ For details, see [[./CHANGELOG.org][CHANGELOG]]
 - 2025-05-24 3.0.2 release
   - Enhanced synchronization stability with comprehensive data protection
   - Fixed tag vector database update mechanism
+<<<<<<< HEAD
+  - Added comprehensive database recovery system
+=======
   - Fixed AI auto-tagging system freezing issues
   - Added comprehensive database recovery system
   - Added AI system diagnostic and emergency tools
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   - Codebase cleanup and organization improvements
 
 - 2025-04-05 3.0.0 release
diff --git a/README_CN.org b/README_CN.org
index 031abf8..70545ab 100755
--- a/README_CN.org
+++ b/README_CN.org
@@ -472,9 +472,13 @@ Field 略等于 org-properties，和 Node 一样，它代表着被 org-supertag
 - 2025-05-24 3.0.2 release
   - 增强同步稳定性，提供全面的数据保护机制
   - 修复标签向量数据库更新机制
+<<<<<<< HEAD
+  - 新增完善的数据库恢复系统
+=======
   - 修复AI自动标签系统卡死问题
   - 新增完善的数据库恢复系统
   - 新增AI系统诊断和紧急修复工具
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   - 代码库清理和组织优化
 
 - 2025-04-05 3.0.0 release
diff --git a/build/lib/simtag/__init__.py b/build/lib/simtag/__init__.py
new file mode 100644
index 0000000..6cae781
--- /dev/null
+++ b/build/lib/simtag/__init__.py
@@ -0,0 +1,12 @@
+"""
+SimTag Main Package
+Provides tag vector similarity search, entity extraction, and tag generation functionality
+"""
+
+from .config import Config
+from .entity_extractor import EntityExtractor
+from .ollama_bridge import OllamaBridge
+from .tag_vectors import TagVectorEngine
+
+__version__ = "1.0.0"
+__all__ = ['Config', 'EntityExtractor', 'OllamaBridge', 'TagVectorEngine'] 
\ No newline at end of file
diff --git a/build/lib/simtag/config.py b/build/lib/simtag/config.py
new file mode 100644
index 0000000..db67ebc
--- /dev/null
+++ b/build/lib/simtag/config.py
@@ -0,0 +1,227 @@
+"""
+SimTag Configuration Management Module
+Handles all SimTag related configuration options and environment settings
+"""
+
+import os
+import sys
+import logging
+import json
+import subprocess
+from typing import Dict, Any, Optional
+from .utils.logging import setup_logging
+
+def check_dependencies() -> bool:
+    """Check if necessary dependencies are installed
+    
+    Returns:
+        bool: Whether the dependencies are met
+    """
+    try:
+        import torch
+        import sentence_transformers
+        import requests
+        import epc
+        import numpy
+        import urllib3
+        
+        # Log dependency version information
+        logging.info(f"Python version: {sys.version.split()[0]}")
+        logging.info(f"PyTorch version: {torch.__version__}")
+        logging.info(f"Sentence-Transformers version: {sentence_transformers.__version__}")
+        logging.info(f"Urllib3 version: {urllib3.__version__}")
+        
+        return True
+    except ImportError as e:
+        logging.error(f"Missing necessary dependencies: {e}")
+        return False
+
+def check_environment() -> bool:
+    """Check if the running environment meets the requirements
+    
+    Returns:
+        bool: Whether it is in the correct environment
+    """
+    # Check Python version
+    if sys.version_info < (3, 9):  # Relax version requirement to 3.9
+        logging.warning(f"Current Python version {sys.version_info.major}.{sys.version_info.minor} may be too low")
+        return False
+        
+    # Check dependencies
+    return check_dependencies()
+
+def ensure_environment():
+    """Ensure that the running environment meets the requirements"""
+    if not check_environment():
+        msg = """
+The running environment does not meet the requirements:
+1. Requires Python 3.9 or higher
+2. Requires the installation of the following dependencies:
+   - torch
+   - sentence-transformers
+   - requests
+   - epc
+   - numpy
+   - urllib3
+
+If there is a lack of dependencies, they can be installed using the following command:
+uv pip install torch sentence-transformers requests epc numpy urllib3
+"""
+        raise RuntimeError(msg)
+
+class Config:
+    """SimTag Configuration Management Class"""
+    
+    DEFAULT_MODEL_NAME = "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"
+    
+    def __init__(self, 
+                 vector_file: str = None,
+                 db_file: str = None,
+                 model_name: str = DEFAULT_MODEL_NAME,
+                 debug: bool = False,
+                 log_file: str = None,
+                 host: str = '127.0.0.1',
+                 port: int = 0):
+        """Initialize the configuration
+        
+        Args:
+            vector_file: Vector file path (specified by org-supertag-sim-epc-vector-file)
+            db_file: Database file path (specified by org-supertag-db-file)
+            model_name: Ollama model name
+            debug: Whether to enable debug mode
+            log_file: Log file path
+            host: Server address
+            port: Server port
+        """
+        # Check the environment
+        ensure_environment()
+        
+        # Use the file paths passed directly
+        self.vector_file = vector_file
+        self.db_file = db_file
+        
+        # Validate the file paths
+        if not self.vector_file:
+            raise ValueError("Vector file path not specified")
+        if not self.db_file:
+            raise ValueError("Database file path not specified")
+            
+        # Log file path - use the directory where the vector file is located
+        log_dir = os.path.dirname(self.vector_file)
+        self.log_file = log_file or os.path.join(log_dir, "simtag_epc.log")
+        
+        # Other settings
+        self.model_name = model_name
+        self.debug = debug
+        self.log_level = logging.DEBUG if debug else logging.INFO
+        self.host = host
+        self.port = port
+        
+        # Create the log directory
+        if self.log_file:
+            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+            
+        # Log the path information
+        logging.info(f"Configuration initialized:")
+        logging.info(f"Vector file: {self.vector_file}")
+        logging.info(f"Database file: {self.db_file}")
+        logging.info(f"Log file: {self.log_file}")
+
+    def ensure_ollama(self) -> bool:
+        """Ensure Ollama is available"""
+        try:
+            # Simple check if the ollama command is available
+            subprocess.run(["ollama", "--version"], capture_output=True, check=True)
+            return True
+        except (subprocess.CalledProcessError, FileNotFoundError):
+            logging.error("Ollama is not installed or not available")
+            return False
+
+    def initialize_server(self) -> bool:
+        """Initialize the server environment"""
+        try:
+            # Ensure the environment meets the requirements
+            ensure_environment()
+            
+            # Ensure Ollama is available
+            if not self.ensure_ollama():
+                raise RuntimeError("Ollama is not installed or not available")
+            
+            # Create the necessary directories
+            os.makedirs(os.path.dirname(self.vector_file), exist_ok=True)
+            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+            
+            # Set up the logging, passing specific parameters instead of self
+            setup_logging(
+                log_file=self.log_file,
+                log_level=self.log_level,
+                debug=self.debug
+            )
+            
+            return True
+            
+        except Exception as e:
+            logging.error(f"Failed to initialize the server environment: {e}")
+            return False
+
+    def setup(self):
+        """Set up the running environment"""
+        # Apply environment variables
+        for key, value in self.env_vars.items():
+            if key == "PYTHONPATH":
+                # For PYTHONPATH, we need to append instead of overwrite
+                current_path = os.environ.get("PYTHONPATH", "")
+                if current_path:
+                    os.environ["PYTHONPATH"] = f"{value}:{current_path}"
+                else:
+                    os.environ["PYTHONPATH"] = value
+            else:
+                os.environ[key] = value
+        
+        # Create the necessary directories
+        os.makedirs(os.path.dirname(self.vector_file), exist_ok=True)
+        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+        
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert the configuration to a dictionary"""
+        return {
+            "vector_file": self.vector_file,
+            "db_file": self.db_file,
+            "model_name": self.model_name,
+            "debug": self.debug,
+            "log_file": self.log_file,
+            "is_initialized": self.is_initialized,
+            "env_vars": self.env_vars,
+            "host": self.host,
+            "port": self.port
+        }
+        
+    def save(self, filepath: str) -> None:
+        """Save the configuration to a file"""
+        with open(filepath, 'w') as f:
+            json.dump(self.to_dict(), f, indent=2)
+            
+    @classmethod
+    def load(cls, filepath: str) -> 'Config':
+        """Load the configuration from a file"""
+        if not os.path.exists(filepath):
+            return cls()
+            
+        with open(filepath, 'r') as f:
+            config_dict = json.load(f)
+            
+        config = cls(
+            vector_file=config_dict.get("vector_file"),
+            db_file=config_dict.get("db_file"),
+            model_name=config_dict.get("model_name"),
+            debug=config_dict.get("debug", False),
+            log_file=config_dict.get("log_file"),
+            host=config_dict.get("host", '127.0.0.1'),
+            port=config_dict.get("port", 0)
+        )
+        
+        config.is_initialized = config_dict.get("is_initialized", False)
+        if "env_vars" in config_dict:
+            config.env_vars.update(config_dict["env_vars"])
+            
+        return config
\ No newline at end of file
diff --git a/build/lib/simtag/entity_extractor.py b/build/lib/simtag/entity_extractor.py
new file mode 100644
index 0000000..d30762b
--- /dev/null
+++ b/build/lib/simtag/entity_extractor.py
@@ -0,0 +1,105 @@
+"""
+SimTag 实体提取模块
+提供文本中实体的识别和分类功能
+"""
+
+import logging
+import json
+import traceback
+from typing import List, Dict, Any, Optional
+
+# 全局单例实例
+_extractor_instance = None
+
+def extract_entities(text: str) -> List[Dict[str, Any]]:
+    """从文本中提取实体的全局函数，供EPC服务器调用
+    
+    Args:
+        text: 要分析的文本
+        
+    Returns:
+        实体列表，每个实体包含:
+            - entity: 实体文本
+            - type: 实体类型
+            - start: 开始位置
+            - end: 结束位置
+    """
+    global _extractor_instance
+    
+    if _extractor_instance is None:
+        _extractor_instance = EntityExtractor()
+        
+    return _extractor_instance.extract(text)
+
+class EntityExtractor:
+    """实体提取器类"""
+    
+    def __init__(self, ollama_bridge: Any = None):
+        """初始化实体提取器
+        
+        Args:
+            ollama_bridge: Ollama桥接对象，用于实体提取
+        """
+        self.logger = logging.getLogger("simtag.entity_extractor")
+        self.ollama = ollama_bridge
+        
+    def extract(self, text: str) -> List[Dict[str, Any]]:
+        """从文本中提取实体
+        
+        Args:
+            text: 待分析文本
+            
+        Returns:
+            实体列表，每个实体包含:
+                - entity: 实体文本
+                - type: 实体类型
+                - start: 开始位置
+                - end: 结束位置
+        """
+        if not self.ollama:
+            self.logger.error("未提供Ollama实例，无法提取实体")
+            return []
+            
+        system = """You are an expert in Named Entity Recognition (NER). Your task is to identify and classify named entities in the given text. Focus on these entity types:
+
+1. PERSON - Names of people
+2. ORG - Organizations, companies, institutions
+3. PRODUCT - Products, software, technologies
+4. CONCEPT - Technical concepts, methodologies
+5. TECH - Programming languages, frameworks, tools
+
+For each entity found:
+1. Extract the exact text as it appears
+2. Classify its type from the above categories
+3. Find the start and end position in the text
+
+Return your result as a valid JSON array of entity objects:
+[
+  {"entity": "entity_text", "type": "ENTITY_TYPE", "start": start_pos, "end": end_pos},
+  ...
+]
+
+The start and end positions should be character indices where the entity appears in the text."""
+
+        prompt = f"""Extract all named entities from this text:
+
+{text}
+
+Return ONLY a valid JSON array of entities with no comments or explanations."""
+
+        try:
+            response = self.ollama.run(prompt)
+            entities = json.loads(response)
+            
+            # 验证和清理实体
+            valid_entities = []
+            for entity in entities:
+                if isinstance(entity, dict) and 'entity' in entity and 'type' in entity:
+                    valid_entities.append(entity)
+                    
+            return valid_entities
+            
+        except Exception as e:
+            self.logger.error(f"提取实体过程出错: {e}")
+            self.logger.error(traceback.format_exc())
+            return []
diff --git a/build/lib/simtag/epc_server.py b/build/lib/simtag/epc_server.py
new file mode 100644
index 0000000..0ce33a5
--- /dev/null
+++ b/build/lib/simtag/epc_server.py
@@ -0,0 +1,522 @@
+"""
+SimTag EPC Server Module
+Provides a unified EPC interface to connect Emacs with Python backend functionality
+"""
+
+import os
+import sys
+import json
+import logging
+import traceback
+import argparse
+import subprocess
+from typing import List, Dict, Any, Optional, Tuple
+
+from epc.server import EPCServer
+from .config import Config
+from .entity_extractor import EntityExtractor
+from .ollama_bridge import OllamaBridge
+from .tag_vectors import TagVectorEngine
+from .utils.logging import setup_logging
+from .utils.serialization import normalize_response
+from .tag_generator import TagGenerator
+from .tag_relation_analyzer import TagRelationAnalyzer, analyze_tag_relations
+
+logger = logging.getLogger("simtag.epc_server")
+
+class SimTagServer:
+    """SimTag EPC Server Class"""
+    
+    def __init__(self, config: Config):
+        """Initialize server
+        
+        Args:
+            config: Configuration object
+        """
+        self.logger = logging.getLogger("simtag.epc_server")
+        self.config = config
+        self._initialized = False  # Add initialization flag
+        
+        # Initialize base components as None
+        self.ollama = None
+        self.tag_generator = None
+        self.entity_extractor = None
+        self.vector_engine = None
+        
+        # Initialize EPC server
+        self.server = EPCServer((self.config.host, self.config.port))
+        self._register_methods()
+        
+    def _register_methods(self):
+        """Register EPC methods"""
+        methods = [
+            ('echo', self.echo),
+            ('status', self.status),
+            ('initialize', self.initialize),
+            ('find_similar', self.find_similar),
+            ('suggest_tags', self.suggest_tags),
+            ('suggest_tags_json', self.suggest_tags_json),
+            ('extract_entities', self.extract_entities),
+            ('check_imports', self.check_imports),
+            ('get_config', self.get_config),
+            ('test_engine', self.test_engine),
+            ('analyze_tag_relations', self.analyze_tag_relations),
+            ('run_ollama', self.run_ollama),
+        ]
+        
+        for name, method in methods:
+            self.server.register_function(method)
+            
+    def start(self):
+        """Start server"""
+        try:
+            port = self.server.server_address[1]
+            self.logger.info(f"Server port obtained: {port}")
+            
+            # Ensure clean stdout
+            sys.stdout.flush()  # Clear buffer
+            
+            # Important: Output port number on a separate line
+            print(f"{port}", flush=True)
+            self.logger.info(f"Port number output to stdout: {port}")
+            
+            # Start server
+            self.logger.info("Starting serve_forever()...")
+            self.server.serve_forever()
+        except Exception as e:
+            self.logger.error(f"Server startup failed: {e}")
+            raise
+        
+    def echo(self, message: str) -> str:
+        """Echo test method"""
+        self.logger.info(f"Echo test: {message}")
+        return f"Echo: {message}"
+        
+    def status(self) -> Dict[str, Any]:
+        """Get server status
+        
+        Returns:
+            Status information dictionary
+        """
+        status = {
+            "server": {
+                "running": True,
+                "port": self.server.server_address[1]
+            },
+            "components": {
+                "vector_engine": self.vector_engine.status() if self.vector_engine else None,
+                "ollama": self.ollama.status() if self.ollama else None
+            },
+            "config": self.config.to_dict()
+        }
+        return normalize_response(status)
+
+    """def initialize initialization function do not break update"""   
+    def initialize(self, vector_file: str = None, db_file: str = None) -> Dict[str, Any]:
+        """Initialize server components"""
+        try:
+            self.logger.info("Initializing server components...")
+            
+            # Update and validate file paths
+            if vector_file:
+                self.logger.info(f"Using specified vector file: {vector_file}")
+                if not os.path.exists(vector_file):
+                    self.logger.error(f"Specified vector file does not exist: {vector_file}")
+                    return normalize_response(None, "error", f"Vector file does not exist: {vector_file}")
+                self.config.vector_file = vector_file
+            
+            if db_file:
+                self.logger.info(f"Using specified database file: {db_file}")
+                if not os.path.exists(db_file):
+                    self.logger.error(f"Specified database file does not exist: {db_file}")
+                    return normalize_response(None, "error", f"Database file does not exist: {db_file}")
+                self.config.db_file = db_file
+            
+            # Ensure Ollama is available
+            if not self.config.ensure_ollama():
+                raise Exception("Ollama is not installed or not available")
+            
+            # 1. Initialize Ollama
+            self.logger.info("Initializing Ollama...")
+            self.ollama = OllamaBridge(model=self.config.model_name)
+            
+            # 2. Initialize tag generator
+            self.logger.info("Initializing tag generator...")
+            self.tag_generator = TagGenerator(self.ollama)
+            
+            # 3. Initialize other components
+            self.logger.info("Initializing other components...")
+            self.entity_extractor = EntityExtractor(self.ollama)
+            self.vector_engine = TagVectorEngine(vector_file=self.config.vector_file)
+            
+            # Mark initialization complete
+            self._initialized = True
+            self.logger.info("All components initialized")
+            
+            return normalize_response({
+                "status": "success",
+                "vector_file": self.config.vector_file,
+                "db_file": self.config.db_file,
+                "model": self.config.model_name
+            })
+            
+        except Exception as e:
+            self._initialized = False  # Ensure marked as uninitialized on failure
+            self.logger.error(f"Initialization failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+            
+    def find_similar(self, tag_name: str, content: str = "", top_k: int = 5) -> Dict[str, Any]:
+        """Find similar tags
+        
+        Args:
+            tag_name: Tag name
+            content: Related content
+            top_k: Number of results to return
+            
+        Returns:
+            List of similar tags
+        """
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+            
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+            
+            # Use hybrid search
+            self.logger.info(f"Finding tags similar to '{tag_name}'...")
+            results = self.vector_engine.find_similar(tag_name, top_k)
+            
+            return normalize_response(results)
+            
+        except Exception as e:
+            error_msg = f"Failed to find similar tags: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def suggest_tags(self, text: str, limit: int = 5) -> Dict[str, Any]:
+        """Generate tag suggestions"""
+        try:
+            # If not initialized, try auto-initialization
+            if not self._initialized:
+                self.logger.info("Service not initialized, attempting auto-initialization...")
+                init_result = self.initialize()
+                if init_result.get("status") != "success":
+                    self.logger.error("Auto-initialization failed")
+                    return normalize_response(None, "error", "Service initialization failed")
+            
+            if not self.tag_generator:
+                self.logger.error("Tag generator not initialized")
+                return normalize_response(None, "error", "Tag generator not initialized")
+            
+            # Get tag list
+            self.logger.info("Starting tag generation...")
+            self.logger.debug(f"Input text preview: {text[:100]}...")  # Add input text logging
+            
+            tags = self.tag_generator.suggest_tags(text)
+            
+            # Validate tag list
+            if not tags:
+                self.logger.warning("No tags generated")
+                return normalize_response([])  # Return empty list instead of None
+            
+            if not isinstance(tags, list):
+                self.logger.error(f"Tag generator returned non-list type: {type(tags)}")
+                return normalize_response(None, "error", "Invalid tag format")
+            
+            # Ensure all tags are strings
+            valid_tags = [str(tag).strip() for tag in tags if tag]
+            
+            self.logger.info(f"Successfully generated {len(valid_tags)} tags: {valid_tags}")
+            
+            # Return using normalize_response
+            return normalize_response(valid_tags)
+            
+        except Exception as e:
+            self.logger.error(f"Tag generation failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+            
+    def extract_entities(self, text: str) -> Dict[str, Any]:
+        """Extract entities (full version)
+        
+        Args:
+            text: Text content
+            
+        Returns:
+            List of entities
+        """
+        try:
+            if not self.entity_extractor:
+                raise Exception("Entity extractor not initialized")
+                
+            entities = self.entity_extractor.extract(text)
+            return normalize_response(entities)
+            
+        except Exception as e:
+            error_msg = f"Failed to extract entities: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+            
+
+    def check_imports(self):
+        """Check if required modules are properly imported."""
+        try:
+            import numpy
+            import torch
+            import sentence_transformers
+            import requests
+            return {
+                "status": "success",
+                "imports": {
+                    "numpy": numpy.__version__,
+                    "torch": torch.__version__,
+                    "sentence_transformers": sentence_transformers.__version__,
+                    "requests": requests.__version__
+                }
+            }
+        except ImportError as e:
+            return {
+                "status": "error",
+                "message": str(e)
+            }
+
+    def get_config(self):
+        """Return current configuration information."""
+        return {
+            "vector_file": self.config.vector_file,
+            "db_file": self.config.db_file,
+            "model_name": self.config.model_name,
+            "debug": self.config.debug
+        }
+
+    def test_engine(self, test_text: str) -> Dict[str, Any]:
+        """Test text vector engine functionality
+        
+        Args:
+            test_text: Test text
+            
+        Returns:
+            Vector data
+        """
+        try:
+            if not self.vector_engine:
+                raise Exception("Vector engine not initialized")
+                
+            # Generate text vector
+            self.logger.info(f"Starting text vector generation: {test_text}")
+            vector = self.vector_engine.model.encode(test_text)
+            
+            # Record vector details
+            self.logger.info(f"Vector type: {type(vector)}")
+            self.logger.info(f"Vector shape: {vector.shape if hasattr(vector, 'shape') else len(vector)}")
+            
+            # Get vector data
+            vector_data = vector.tolist() if hasattr(vector, 'tolist') else vector
+            self.logger.info(f"Vector data length: {len(vector_data)}")
+            
+            # Return result
+            result = {
+                "vector": vector_data,
+                "dimensions": len(vector_data),
+                "model": self.vector_engine.model_name if hasattr(self.vector_engine, 'model_name') else None
+            }
+            
+            return normalize_response(result)
+            
+        except Exception as e:
+            error_msg = f"Engine test failed: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def analyze_tag_relations(self, tag: str, tags: list) -> Dict[str, Any]:
+        """Analyze tag relationships
+        
+        Args:
+            tag: Target tag
+            tags: List of tags to analyze
+            
+        Returns:
+            List of tag relationships
+        """
+        try:
+            relations = self.tag_analyzer.analyze_relations(tag, tags)
+            return {
+                "status": "success",
+                "result": relations
+            }
+        except Exception as e:
+            return {
+                "status": "error",
+                "message": f"Failed to analyze tag relationships: {str(e)}"
+            }
+
+    def run_ollama(self, prompt, system=None):
+        """Send message to Ollama and get response
+        
+        Args:
+            prompt: User prompt text
+            system: Optional system prompt text
+            
+        Returns:
+            Dict: Dictionary containing processing results
+        """
+        self.logger.info(f"Received Ollama interaction request, prompt length: {len(prompt)}")
+        
+        # Check initialization status
+        if not self._initialized:
+            self.logger.error("Attempting to use Ollama before initialization")
+            return normalize_response(None, "error", "Service not initialized, please call initialize first")
+        
+        # Check Ollama instance
+        if not self.ollama:
+            self.logger.error("Ollama instance not initialized")
+            return normalize_response(None, "error", "Ollama instance not initialized")
+        
+        try:
+            # Record additional information, avoiding logging long prompts
+            prompt_preview = prompt[:100] + "..." if len(prompt) > 100 else prompt
+            self.logger.info(f"Sending request to Ollama, prompt preview: {prompt_preview}")
+            
+            # Call Ollama
+            response = self.ollama.run(prompt, system=system)
+            
+            # Check response
+            if not response:
+                return normalize_response(None, "error", "Ollama returned empty response")
+            
+            # Record response (partial to avoid large logs)
+            response_preview = response[:100] + "..." if len(response) > 100 else response
+            self.logger.info(f"Received Ollama response, length: {len(response)}, preview: {response_preview}")
+            
+            # Return success response
+            return normalize_response(response, "success")
+            
+        except Exception as e:
+            # Catch and log exception
+            error_message = f"Ollama interaction error: {str(e)}"
+            trace = traceback.format_exc()
+            self.logger.error(f"{error_message}\n{trace}")
+            return normalize_response(None, "error", error_message)
+            
+    def suggest_tags_json(self, json_data: str, limit: int = 5) -> Dict[str, Any]:
+        """Process tag generation request using JSON format
+        
+        Args:
+            json_data: JSON format request data containing text content to analyze
+            limit: Result count limit
+            
+        Returns:
+            List of tags
+        """
+        try:
+            # Record received JSON data length
+            self.logger.info(f"Received JSON format request, length: {len(json_data)}")
+            
+            # Parse JSON data
+            try:
+                import json
+                request = json.loads(json_data)
+                
+                # Ensure JSON format is correct, contains content field
+                if not isinstance(request, dict):
+                    self.logger.error(f"JSON data is not dictionary format: {type(request)}")
+                    return normalize_response(None, "error", "Invalid request format, should be JSON object")
+                
+                text = request.get("content")
+                
+                if not text:
+                    self.logger.error("Request missing content field or empty")
+                    return normalize_response(None, "error", "Request missing text content")
+                
+                self.logger.info(f"Text length extracted from JSON: {len(text)}")
+                text_preview = text[:100] + "..." if len(text) > 100 else text
+                self.logger.info(f"Text preview: {text_preview}")
+                
+            except json.JSONDecodeError as e:
+                self.logger.error(f"JSON parsing failed: {e}")
+                self.logger.error(f"Received JSON data: {json_data[:200]}..." if len(json_data) > 200 else json_data)
+                return normalize_response(None, "error", f"JSON parsing failed: {e}")
+            
+            # Process text using standard suggest_tags method
+            return self.suggest_tags(text, limit)
+            
+        except Exception as e:
+            self.logger.error(f"JSON request processing failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+
+def run_ollama_model(text, model_name="gemma-3b-it"):
+    """Run ollama command directly"""
+    try:
+        cmd = ["ollama", "run", model_name, text]
+        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
+        return result.stdout.strip()
+    except subprocess.CalledProcessError as e:
+        logger.error(f"Failed to run ollama command: {e}")
+        return None
+
+def main(config: Config):
+    """Main function"""
+    try:
+        # Initialize logging
+        log_level = logging.DEBUG if config.debug else logging.INFO
+        setup_logging(config.log_file, log_level)
+        
+        # Record configuration information
+        logger.info("SimTag EPC Server Configuration:")
+        logger.info(f"Vector file: {config.vector_file}")
+        logger.info(f"Database file: {config.db_file}")
+        logger.info(f"Log file: {config.log_file}")
+        logger.info(f"Debug mode: {config.debug}")
+        
+        # Create server instance
+        server = SimTagServer(config)
+        
+        # Start server
+        server.start()
+        
+    except Exception as e:
+        logger.error(f"Server startup failed: {e}")
+        logger.error(traceback.format_exc())
+        sys.exit(1)
+
+if __name__ == "__main__":
+    # Parse command line arguments
+    parser = argparse.ArgumentParser(description='SimTag EPC Server')
+    parser.add_argument('--vector-file', help='Vector file path')
+    parser.add_argument('--db-file', help='Database file path')
+    parser.add_argument('--model', help='Model name')
+    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
+    parser.add_argument('--log-file', help='Log file path')
+    parser.add_argument('--host', default='127.0.0.1', help='Server address')
+    parser = argparse.ArgumentParser(description='SimTag EPC Server')
+    parser.add_argument('--vector-file', help='Vector file path')
+    parser.add_argument('--db-file', help='Database file path')
+    parser.add_argument('--model', help='Model name')
+    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
+    parser.add_argument('--log-file', help='Log file path')
+    parser.add_argument('--host', default='127.0.0.1', help='Server address')
+    parser.add_argument('--port', type=int, default=0, help='Server port')
+    args = parser.parse_args()
+
+    # Create configuration object
+    config = Config(
+        vector_file=args.vector_file,
+        db_file=args.db_file,
+        model_name=args.model,
+        debug=args.debug,
+        log_file=args.log_file,
+        host=args.host,
+        port=args.port
+    )
+    
+    main(config) 
\ No newline at end of file
diff --git a/build/lib/simtag/ollama_bridge.py b/build/lib/simtag/ollama_bridge.py
new file mode 100644
index 0000000..69a8b4a
--- /dev/null
+++ b/build/lib/simtag/ollama_bridge.py
@@ -0,0 +1,222 @@
+"""
+SimTag Ollama Bridge Module - Provides interaction with the Ollama model
+"""
+
+import logging
+from typing import Any, Optional, Dict
+import subprocess
+import traceback
+import requests
+import json
+import sys
+
+class OllamaBridge:
+    """Ollama API integration, providing basic LLM call functionality"""
+    
+    def __init__(self, model: str = "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"):
+        """Initialize the Ollama client
+        
+        Args:
+            model: The name of the model to use
+        """
+        self.logger = logging.getLogger("simtag.ollama_bridge")
+        if not model:
+            model = "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"  # Ensure a default value
+        self.model = str(model)  # Ensure it's a string type
+        self.logger.info(f"Initialized OllamaBridge, using model: {self.model}")
+
+    def run(self, prompt: str, system: str = None) -> str:
+        """Run Ollama command
+        
+        Args:
+            prompt: Prompt text
+            system: System prompt
+            
+        Returns:
+            Model output text
+        """
+        try:
+            self.logger.info("Preparing to call Ollama API")
+            
+            # Ensure the model name is valid
+            if not self.model:
+                self.logger.error("Model name not set")
+                raise Exception("Model name not set")
+            
+            # Ensure the prompt is a valid UTF-8 encoded string
+            if not isinstance(prompt, str):
+                prompt = str(prompt)
+            
+            # Ensure the system prompt is also a valid string
+            if system and not isinstance(system, str):
+                system = str(system)
+                
+            self.logger.debug(f"Using model: {self.model}")
+            self.logger.debug(f"System prompt: {system}")
+            self.logger.debug(f"User prompt: {prompt[:100]}...")
+            
+            # Build the request data
+            data = {
+                "model": self.model,
+                "prompt": prompt,
+                "stream": False,  # Do not use streaming response
+                "options": {
+                    "temperature": 0.7,  # Control the randomness of the output
+                    "num_predict": 1024,  # Maximum output length
+                    "stop": []  # Stop markers
+                }
+            }
+            
+            # Add system prompt
+            if system:
+                data["system"] = system
+            
+            # Log the generated request data (excluding sensitive content)
+            self.logger.info(f"Sending API request to model: {self.model}")
+            
+            # Send the request
+            try:
+                response = requests.post(
+                    "http://127.0.0.1:11434/api/generate",
+                    json=data,
+                    headers={"Content-Type": "application/json"},
+                    timeout=60  # Add timeout setting
+                )
+            except requests.RequestException as e:
+                self.logger.error(f"Request exception: {e}")
+                raise Exception(f"Request exception: {e}")
+            
+            # Check the response status code
+            if response.status_code == 200:
+                try:
+                    response_data = response.json()
+                    result = response_data.get('response', '').strip()
+                    
+                    # Log the generation statistics
+                    if 'eval_duration' in response_data:
+                        eval_duration = response_data['eval_duration']
+                        eval_count = response_data.get('eval_count', 0)
+                        tokens_per_second = eval_count / (eval_duration / 1e9) if eval_duration > 0 else 0
+                        self.logger.info(f"Generation speed: {tokens_per_second:.2f} tokens/s")
+                    
+                    if not result:
+                        self.logger.warning("Ollama returned an empty response")
+                        
+                    self.logger.info("Ollama API call successful")
+                    self.logger.debug(f"Response result: {result[:100]}..." if len(result) > 100 else f"Response result: {result}")
+                    return result
+                except json.JSONDecodeError as e:
+                    self.logger.error(f"Failed to parse JSON response: {e}")
+                    self.logger.error(f"Original response content: {response.text[:200]}...")
+                    raise Exception(f"Failed to parse JSON response: {e}")
+            else:
+                error_msg = f"Ollama API call failed: HTTP {response.status_code} - {response.text}"
+                self.logger.error(error_msg)
+                raise Exception(error_msg)
+                
+        except requests.exceptions.ConnectionError as e:
+            error_msg = f"Failed to connect to Ollama service: {str(e)}"
+            self.logger.error(error_msg)
+            raise Exception(error_msg)
+            
+        except Exception as e:
+            error_msg = f"Ollama execution exception: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            raise Exception(error_msg)
+
+    def status(self) -> Dict[str, Any]:
+        """Get Ollama status"""
+        try:
+            self.logger.info("Checking Ollama status")
+            result = subprocess.run(
+                ["ollama", "list"],
+                capture_output=True,
+                text=True
+            )
+            
+            if result.returncode == 0:
+                self.logger.info("Ollama status check successful")
+                return {
+                    "available": True,
+                    "model": self.model,
+                    "models": result.stdout.strip()
+                }
+            else:
+                self.logger.error(f"Ollama status check failed: {result.stderr}")
+                return {
+                    "available": False,
+                    "error": result.stderr
+                }
+                
+        except Exception as e:
+            error_msg = f"Ollama status check exception: {str(e)}"
+            self.logger.error(error_msg)
+            return {
+                "available": False,
+                "error": error_msg
+            }
+
+def _test():
+    """Test Ollama Bridge functionality"""
+    # Set up logging
+    logging.basicConfig(
+        level=logging.DEBUG,
+        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+        datefmt='%Y-%m-%d %H:%M:%S'
+    )
+    logger = logging.getLogger("ollama_bridge_test")
+    
+    try:
+        logger.info("Starting Ollama Bridge test")
+        
+        # 1. Test initialization
+        bridge = OllamaBridge()
+        logger.info(f"OllamaBridge instance created successfully, using model: {bridge.model}")
+        
+        # 2. Test status check
+        logger.info("Testing status check...")
+        status = bridge.status()
+        logger.info(f"Ollama status: {status}")
+        
+        # 3. Test simple dialogue
+        logger.info("Testing simple dialogue...")
+        prompt = "Hello, please introduce yourself in one sentence."
+        response = bridge.run(prompt)
+        logger.info(f"Simple dialogue response: {response}")
+        
+        # 4. Test dialogue with system prompt
+        logger.info("Testing dialogue with system prompt...")
+        system = "You are a concise assistant, answer should be short."
+        prompt = "Explain what is artificial intelligence."
+        response = bridge.run(prompt, system=system)
+        logger.info(f"Dialogue response with system prompt: {response}")
+        
+        # 5. Test tag generation scenario
+        logger.info("Testing tag generation scenario...")
+        system = """You are a tag generation expert. Please analyze the given text and generate the most relevant tags.
+Requirements:
+1. Each tag should be concise and accurate
+2. Tags should reflect the main theme and concepts of the text
+3. Return the format as a comma-separated list of tags
+4. Do not explain, just return the list of tags"""
+        
+        test_text = """
+        Python is a popular programming language, known for its concise syntax and rich ecosystem.
+        It is widely used in fields such as web development, data analysis, and artificial intelligence.
+        """
+        response = bridge.run(test_text, system=system)
+        logger.info(f"Tag generation response: {response}")
+        
+        logger.info("All tests completed")
+        
+    except Exception as e:
+        logger.error(f"Error during testing: {e}")
+        logger.error(traceback.format_exc())
+        return False
+        
+    return True
+
+if __name__ == "__main__":
+    success = _test()
+    sys.exit(0 if success else 1) 
\ No newline at end of file
diff --git a/build/lib/simtag/tag_generator.py b/build/lib/simtag/tag_generator.py
new file mode 100644
index 0000000..58ef0cd
--- /dev/null
+++ b/build/lib/simtag/tag_generator.py
@@ -0,0 +1,286 @@
+"""
+Tag Generator Module
+Responsible for generating tag suggestions from text content
+"""
+
+import re
+import logging
+from typing import List, Dict, Any, Optional
+import traceback
+
+# Global singleton instance
+_generator_instance = None
+
+def suggest_tags(text: str, limit: int = 5) -> List[str]:
+    """Extract tags from text.
+    
+    Args:
+        text: The text to analyze
+        limit: The maximum number of tags to return (default 5)
+        
+    Returns:
+        A list of tags
+    """
+    global _generator_instance
+    
+    if _generator_instance is None:
+        _generator_instance = TagGenerator(None)
+    
+    return _generator_instance.suggest_tags(text)  # Don't pass limit parameter
+
+class TagGenerator:
+    """Tag generator class"""
+
+    def __init__(self, ollama_bridge):
+        """Initialize tag generator
+        
+        Args:
+            ollama_bridge: LLM interface object
+        """
+        self.logger = logging.getLogger("simtag.tag_generator")
+        self.ollama = ollama_bridge
+        
+    def suggest_tags(self, text: str, limit: int = 5) -> List[str]:
+        """Generate tag suggestions"""
+        try:
+            self.logger.debug(f"Starting tag generation, text length: {len(text)}")
+            
+            if not text or len(text.strip()) == 0:
+                self.logger.warning("The input text is empty or only contains whitespace")
+                return []
+            
+            try:
+                if isinstance(text, str):
+                    text_bytes = text.encode('utf-8')
+                    text = text_bytes.decode('utf-8')
+            except UnicodeError as e:
+                self.logger.warning(f"Text encoding issue: {e}")
+                try:
+                    if isinstance(text, str):
+                        text = text.encode('utf-8', errors='replace').decode('utf-8')
+                except Exception as e:
+                    self.logger.error(f"I can't fix the text encoding: {e}")
+                    return []
+            
+            preview = text[:100] + "..." if len(text) > 100 else text
+            self.logger.debug(f"Preview: {preview}")
+            
+            if not self.ollama:
+                self.logger.error("Ollama client not initialized")
+                return []
+
+            cleaned_text = text.strip()
+            if len(cleaned_text) < 10:
+                self.logger.warning(f"The text is too short: '{cleaned_text}'")
+                if cleaned_text:
+                    return [cleaned_text]
+                return []
+
+            prompt = f"""Extract 5 significant tags from the following text:
+
+TEXT START
+{cleaned_text}
+TEXT END
+
+Return ONLY a comma-separated list of tags, with no explanations or other text.
+Example format: tag1, tag2, tag3, tag4, tag5"""
+
+            # Add system variable definition
+            system = """You are a tag generation expert. Your task is to generate relevant tags for the given text.
+Guidelines:
+1. Each tag should be concise and accurate
+2. Tags should reflect the main topics and concepts in the text
+3. Return ONLY a comma-separated list of tags
+4. Do not include any explanations or other text in your response"""
+
+            # Try to call Ollama directly
+            self.logger.debug("Preparing to call Ollama API...")
+            
+            # Check Ollama status and add additional tests
+            try:
+                # Test using ollama command line directly
+                import subprocess
+                result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
+                self.logger.debug(f"Ollama command line test: {result.stdout.strip()}")
+                
+                # Check API availability
+                import requests
+                test_response = requests.get("http://127.0.0.1:11434/api/tags")
+                self.logger.debug(f"Ollama API test: {test_response.status_code}")
+                
+                # Check object status
+                if hasattr(self.ollama, 'status'):
+                    status = self.ollama.status()
+                    self.logger.debug(f"Ollama status: {status}")
+            except Exception as e:
+                self.logger.error(f"Failed to check Ollama status: {e}")
+            
+            # Call Ollama
+            try:
+                self.logger.debug("Calling Ollama to generate tags")
+                
+                # Log request details
+                self.logger.debug(f"System prompt: {system}")
+                # Avoid logging long prompts, only log first 200 and last 100 characters
+                if len(prompt) > 300:
+                    self.logger.debug(f"User prompt (truncated): {prompt[:200]}...{prompt[-100:]}")
+                else:
+                    self.logger.debug(f"User prompt: {prompt}")
+                self.logger.info(f"User prompt length: {len(prompt)} characters")
+                
+                # Verify if text is actually included in prompt
+                text_in_prompt = "TEXT START" in prompt and "TEXT END" in prompt
+                self.logger.debug(f"Text correctly included in prompt: {text_in_prompt}")
+                
+                # Call Ollama and log details
+                self.logger.debug(f"Starting Ollama.run(), prompt length: {len(prompt)}")
+                response = self.ollama.run(prompt, system=system)
+                self.logger.debug(f"Ollama.run() completed, response length: {len(response) if response else 0}")
+                
+                # Log raw response
+                raw_response = response if response else "No response"
+                if len(raw_response) > 200:
+                    self.logger.debug(f"Ollama raw response (truncated): {raw_response[:200]}...")
+                else:
+                    self.logger.debug(f"Ollama raw response: '{raw_response}'")
+                self.logger.info(f"Received Ollama response, length: {len(raw_response)}")
+                
+                # Identify and handle special response cases
+                lower_response = raw_response.lower() if raw_response else ""
+                special_phrases = [
+                    "please provide", "i need", "please give", 
+                    "the text is empty", "no text provided", "i don't see any text",
+                    "cannot generate", "unable to generate"
+                ]
+                
+                is_error_response = any(phrase in lower_response for phrase in special_phrases)
+                if is_error_response:
+                    self.logger.warning(f"Ollama returned error response: '{raw_response}'")
+                    
+                    # Try again with clearer instructions
+                    self.logger.info("Attempting with backup prompt...")
+                    
+                    # Backup prompt is simpler and more direct
+                    backup_prompt = f"Generate 5 tags for this text: {cleaned_text[:1000]}"
+                    self.logger.debug(f"Backup prompt: {backup_prompt[:200]}...")
+                    
+                    try:
+                        backup_response = self.ollama.run(backup_prompt, system=system)
+                        if backup_response and not any(phrase in backup_response.lower() for phrase in special_phrases):
+                            self.logger.info(f"Backup request successful: '{backup_response}'")
+                            response = backup_response
+                        else:
+                            self.logger.warning("Backup request also failed")
+                            return []
+                    except Exception as e:
+                        self.logger.error(f"Backup request failed: {e}")
+                        return []
+                    
+                # If still no valid response, return empty list
+                if not response:
+                    self.logger.warning("Ollama returned empty response")
+                    return []
+                
+                # Enhanced tag extraction logic
+                # First try direct comma separation
+                raw_tags = [tag.strip() for tag in response.split(',')]
+                
+                # If only one element, response format might be incorrect
+                if len(raw_tags) <= 1:
+                    self.logger.warning("Response format may be incorrect, trying alternative splitting methods")
+                    
+                    # First try splitting by newlines
+                    line_tags = []
+                    for line in response.split('\n'):
+                        line = line.strip()
+                        # Skip empty lines and obvious non-tag lines
+                        if not line or len(line) > 100:
+                            continue
+                        # Check if it's a list item (starts with number or hyphen)
+                        if line.startswith(('-', '*', '1.', '2.', '3.', '4.', '5.')):
+                            # Remove list markers
+                            line = line.lstrip('-*0123456789. ')
+                        # If line has commas, might be multiple tags
+                        if ',' in line:
+                            line_tags.extend([t.strip() for t in line.split(',')])
+                        else:
+                            line_tags.append(line)
+                    
+                    if line_tags:
+                        self.logger.debug(f"Extracted {len(line_tags)} tags using newline splitting")
+                        raw_tags = line_tags
+                    else:
+                        # If newline splitting also failed, try more aggressive splitting
+                        # Look for possible tag patterns like quoted content or content after colons
+                        potential_tags = re.findall(r'"([^"]+)"|\'([^\']+)\'|:\s*([^,\n]+)', response)
+                        
+                        extracted_tags = []
+                        for tag_tuple in potential_tags:
+                            # findall returns tuples, get non-empty value
+                            tag = next((t for t in tag_tuple if t), None)
+                            if tag:
+                                extracted_tags.append(tag.strip())
+                        
+                        if extracted_tags:
+                            self.logger.debug(f"Extracted {len(extracted_tags)} tags using regex")
+                            raw_tags = extracted_tags
+                        elif raw_tags[0]:  # If there's only one non-empty element
+                            # Use the original single element as the only tag
+                            self.logger.debug(f"Using original response as single tag: {raw_tags[0]}")
+                        else:
+                            self.logger.warning("Unable to extract tags from response")
+                            return []
+                
+                self.logger.debug(f"Raw tag list ({len(raw_tags)}): {raw_tags}")
+                
+                # Clean and filter tags
+                valid_tags = []
+                for tag in raw_tags:
+                    # Skip obviously invalid tags
+                    if not tag or len(tag) > 50:
+                        self.logger.debug(f"Skipping invalid tag: '{tag}'")
+                        continue
+                        
+                    # Normalize tag (lowercase, remove extra spaces and punctuation)
+                    clean_tag = tag.strip().lower()
+                    # Remove quotes and brackets
+                    clean_tag = re.sub(r'[\'"`\(\)\[\]\{\}]', '', clean_tag)
+                    # Remove leading numbers and dots
+                    clean_tag = re.sub(r'^[\d\.\-\s]+', '', clean_tag)
+                    # Remove backslash characters
+                    clean_tag = clean_tag.replace('\\', '')
+                    # Replace internal spaces with underscores
+                    clean_tag = re.sub(r'\s+', '_', clean_tag)
+                    # No longer truncate tag length
+                    
+                    if clean_tag and clean_tag not in valid_tags:
+                        valid_tags.append(clean_tag)
+                    else:
+                        self.logger.debug(f"Ignoring duplicate or empty tag: '{tag}'")
+                
+                # Ensure we have tags
+                if not valid_tags:
+                    self.logger.warning("No valid tags extracted")
+                    # If unable to extract tags, try using first line as tag
+                    first_line = response.split('\n')[0].strip()
+                    if first_line and len(first_line) <= 50:
+                        self.logger.info(f"Using first line of response as tag: '{first_line}'")
+                        valid_tags = [first_line.lower()]
+                    else:
+                        return []
+                
+                if limit and valid_tags:
+                    valid_tags = valid_tags[:limit]
+                    
+                self.logger.info(f"Final valid tags ({len(valid_tags)}): {valid_tags}")
+                return valid_tags
+                
+            except Exception as e:
+                self.logger.error(f"Ollama call failed: {e}")
+                self.logger.error(traceback.format_exc())
+                return []
+                
+        except Exception as e:
+            self.logger.error(f"Tag generation process failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return []  # Return empty list instead of raising exception
\ No newline at end of file
diff --git a/build/lib/simtag/tag_relation_analyzer.py b/build/lib/simtag/tag_relation_analyzer.py
new file mode 100644
index 0000000..f76a89e
--- /dev/null
+++ b/build/lib/simtag/tag_relation_analyzer.py
@@ -0,0 +1,138 @@
+"""
+SimTag tag relation analysis module
+Analyzes semantic relationships between tags
+"""
+
+import logging
+from typing import List, Dict, Any, Optional
+
+# Predefined relation types
+RELATIONS = {
+    "CONTRAST": "Comparison or contrast relationship",
+    "RELATE": "General association relationship",
+    "INFLUENCE": "Influence relationship",
+    "CONTAIN": "Containment relationship (parent)",
+    "BELONG": "Subordinate relationship (child)",
+    "PARALLEL": "Parallel relationship",
+    "DEPENDENCY": "Dependency relationship",
+    "CAUSE": "Causal relationship (cause)",
+    "EFFECT": "Causal relationship (effect)",
+    "COOCCURRENCE": "Co-occurrence relationship"
+}
+
+class TagRelationAnalyzer:
+    """Tag relation analyzer"""
+    
+    def __init__(self, ollama_bridge: Any = None):
+        """Initialize tag relation analyzer
+        
+        Args:
+            ollama_bridge: Ollama bridge object for relation analysis
+        """
+        self.logger = logging.getLogger("simtag.tag_relation")
+        self.ollama = ollama_bridge
+        
+    def analyze_relations(self, tag: str, tags: List[str]) -> List[Dict[str, Any]]:
+        """Analyze relationships between tags
+        
+        Args:
+            tag: Target tag
+            tags: List of tags to analyze
+            
+        Returns:
+            List of tag relationships, each containing:
+                - tag: Related tag
+                - relation: Relation type
+                - reason: Relation explanation
+        """
+        if not self.ollama:
+            self.logger.error("No Ollama instance provided, cannot analyze tag relations")
+            return []
+            
+        system = """You are a tag relationship analyzer. Your task is to determine the relationship between two tags.
+
+Available relationship types:
+CONTRAST   - Tags represent contrasting or comparable concepts
+RELATE     - Tags have a general association
+INFLUENCE  - First tag has significant impact on second tag
+CONTAIN    - First tag is a broader category that includes second tag
+BELONG     - First tag is a subset or member of second tag
+PARALLEL   - Tags represent similar-level concepts
+DEPENDENCY - First tag requires or depends on second tag
+CAUSE      - First tag leads to or causes second tag
+EFFECT     - First tag is a result of second tag
+COOCCURRENCE - Tags commonly appear together
+
+Response format requirements:
+1. Use EXACTLY this format (including newline):
+   RELATION: <TYPE>
+   REASON: <brief explanation>
+2. Choose only ONE relationship type from the list above
+3. Provide a clear, concise reason (1-2 sentences)
+4. Use technical language when appropriate
+5. Be specific about the relationship direction
+
+Example response:
+RELATION: BELONG
+REASON: Python is a specific programming language, making it a subset of programming.
+
+DO NOT include any other text or explanations."""
+
+        results = []
+        for related_tag in tags:
+            prompt = f"""How is "{related_tag}" related to "{tag}"?
+
+Choose ONE relationship type and explain why.
+Use EXACTLY this format (including newline):
+RELATION: <TYPE>
+REASON: <explanation>"""
+
+            try:
+                response = self.ollama.run(prompt, system)
+                response = response.strip()
+                
+                # Validate response format
+                if not ('\nREASON:' in response and response.startswith('RELATION:')):
+                    self.logger.warning(f"Invalid response format for tag '{related_tag}', retrying...")
+                    response = self.ollama.run(prompt, system)
+                    if not ('\nREASON:' in response and response.startswith('RELATION:')):
+                        self.logger.warning(f"Still invalid format after retry, skipping tag '{related_tag}'")
+                        continue
+                
+                # Parse response
+                lines = response.strip().split('\n')
+                relation_line = next(line for line in lines if line.startswith('RELATION:'))
+                reason_line = next(line for line in lines if line.startswith('REASON:'))
+                
+                relation = relation_line.split(':', 1)[1].strip().upper()
+                reason = reason_line.split(':', 1)[1].strip()
+                
+                # Validate relation type
+                if relation not in RELATIONS:
+                    self.logger.warning(f"Invalid relation type '{relation}' for tag '{related_tag}'")
+                    continue
+                
+                results.append({
+                    'tag': related_tag,
+                    'relation': relation.lower(),
+                    'reason': reason
+                })
+                
+            except Exception as e:
+                self.logger.error(f"Error parsing response for tag '{related_tag}': {e}")
+                self.logger.debug(f"Raw response:\n{response}")
+                continue
+                
+        return results
+
+# Global singleton instance
+_analyzer_instance = None
+
+def analyze_tag_relations(tag: str, tags: List[str]) -> List[Dict[str, Any]]:
+    """Global function for analyzing tag relations, called by EPC server"""
+    global _analyzer_instance
+    
+    if _analyzer_instance is None:
+        _analyzer_instance = TagRelationAnalyzer()
+        
+    return _analyzer_instance.analyze_relations(tag, tags)
\ No newline at end of file
diff --git a/build/lib/simtag/tag_vectors.py b/build/lib/simtag/tag_vectors.py
new file mode 100644
index 0000000..c9514d6
--- /dev/null
+++ b/build/lib/simtag/tag_vectors.py
@@ -0,0 +1,570 @@
+"""
+SimTag tag vector processing module
+Provides functions for generating, storing, and querying tag vectors
+"""
+
+import os
+import json
+import logging
+import traceback
+import re
+import time
+from datetime import datetime
+from typing import List, Dict, Any, Tuple, Optional
+import numpy as np
+from sentence_transformers import SentenceTransformer
+import torch
+
+class TagVectorEngine:
+    """Tag vector engine class"""
+    
+    def __init__(self, vector_file: str = None):
+        """Initializes the tag vector engine
+        
+        Args:
+            vector_file: Path to the vector file
+        """
+        self.logger = logging.getLogger("simtag.tag_vectors")
+        self.vector_file = vector_file
+        self.tag_vectors = {}  # Tag vector dictionary
+        self.is_initialized = False
+        self.model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+        self._model = None
+        
+        if vector_file and os.path.exists(vector_file):
+            self.load_vectors(vector_file)
+            
+    @property
+    def model(self):
+        """Lazily loads the model"""
+        if self._model is None:
+            self.logger.info(f"loading model: {self.model_name}")
+            self._model = SentenceTransformer(
+                self.model_name,
+                cache_folder=os.path.join(os.path.dirname(__file__), 'models')
+            )
+            # Set device
+            device = self._get_device()
+            if device.type != 'cpu':
+                self._model = self._model.to(device)
+                self.logger.info(f"enabled {device.type.upper()} acceleration")
+                
+        return self._model
+        
+    def _get_device(self):
+        """Gets the best available device"""
+        if torch.cuda.is_available():
+            return torch.device('cuda')
+        elif torch.backends.mps.is_available():
+            return torch.device('mps')
+        return torch.device('cpu')
+        
+    def status(self) -> Dict[str, Any]:
+        """Gets the engine status
+        
+        Returns:
+            Status information dictionary
+        """
+        return {
+            "initialized": self.is_initialized,
+            "vector_file": self.vector_file,
+            "vector_count": len(self.tag_vectors),
+            "file_exists": os.path.exists(self.vector_file) if self.vector_file else False,
+            "file_size": os.path.getsize(self.vector_file) if self.vector_file and os.path.exists(self.vector_file) else 0
+        }
+            
+    def load_vectors(self, vector_file: str) -> bool:
+        """Loads tag vectors
+        
+        Args:
+            vector_file: Path to the vector file
+            
+        Returns:
+            True if loaded successfully, False otherwise
+        """
+        try:
+            if not os.path.exists(vector_file):
+                self.logger.error(f"vector file not found: {vector_file}")
+                return False
+                
+            self.logger.info(f"loading vector file: {vector_file}")
+            with open(vector_file, 'r') as f:
+                data = json.load(f)
+                
+            if not isinstance(data, dict) or 'tags' not in data:
+                self.logger.error(f"invalid vector file format")
+                return False
+                
+            # Update vectors
+            self.tag_vectors = {
+                tag_id: np.array(info['vector']) 
+                for tag_id, info in data['tags'].items()
+            }
+            
+            self.vector_file = vector_file
+            self.is_initialized = True
+            self.logger.info(f"successfully loaded {len(self.tag_vectors)} tag vectors")
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"error loading vector file: {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+            
+    def find_similar(self, tag_name: str, top_k: int = 5) -> List[Tuple[str, float]]:
+        """Finds tags similar to the given tag
+        
+        Args:
+            tag_name: Tag name
+            top_k: Number of results to return
+            
+        Returns:
+            List of similar tags, each element is (tag_name, similarity_score)
+        """
+        self.logger.info(f"finding similar tags: tag={tag_name}, top_k={top_k}")
+        
+        # Check vector file
+        if not self.is_initialized:
+            if not self.vector_file or not os.path.exists(self.vector_file):
+                self.logger.error("vector file not specified or not found")
+                return []
+            if not self.load_vectors(self.vector_file):
+                self.logger.error("unable to load vector file")
+                return []
+        
+        # Check tag vector dictionary
+        if not self.tag_vectors:
+            self.logger.error("no available tag vectors")
+            return []
+            
+        # Get target tag vector
+        if tag_name not in self.tag_vectors:
+            try:
+                self.logger.info(f"tag '{tag_name}' not in vector library, generating vector...")
+                target_vector = self.model.encode(tag_name)
+                self.logger.info(f"vector generated successfully, dimension: {target_vector.shape}")
+            except Exception as e:
+                self.logger.error(f"error generating vector: {e}")
+                return []
+        else:
+            target_vector = self.tag_vectors[tag_name]
+            self.logger.info(f"tag '{tag_name}' vector already exists")
+            
+        # Calculate similarity
+        start_time = time.time()
+        similarities = []
+        self.logger.info(f"calculating similarity with {len(self.tag_vectors)} tags...")
+        
+        for other_tag, other_vector in self.tag_vectors.items():
+            if other_tag != tag_name:
+                try:
+                    # Calculate cosine similarity
+                    sim = self._compute_similarity(target_vector, other_vector)
+                    similarities.append((other_tag, sim))
+                except Exception as e:
+                    self.logger.error(f"error calculating similarity with tag '{other_tag}': {e}")
+                    continue
+        
+        # Sort by similarity
+        similarities.sort(key=lambda x: x[1], reverse=True)
+        
+        # Return top_k results
+        results = similarities[:top_k]
+        
+        elapsed = time.time() - start_time
+        self.logger.info(f"similarity calculation completed, time: {elapsed:.2f} seconds, found {len(results)} similar tags")
+        
+        return results
+        
+    def _compute_similarity(self, vec1, vec2) -> float:
+        """Computes the similarity between two vectors
+        
+        Args:
+            vec1: The first vector
+            vec2: The second vector
+            
+        Returns:
+            Similarity score
+        """
+        try:
+            # Ensure vectors are numpy arrays
+            if not isinstance(vec1, np.ndarray):
+                vec1 = np.array(vec1)
+            if not isinstance(vec2, np.ndarray):
+                vec2 = np.array(vec2)
+            
+            # Ensure vectors are 2D
+            if len(vec1.shape) == 1:
+                vec1 = vec1.reshape(1, -1)
+            if len(vec2.shape) == 1:
+                vec2 = vec2.reshape(1, -1)
+            
+            # Calculate cosine similarity
+            sim = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
+            return float(sim[0][0])  # Ensure to return Python native float
+        except Exception as e:
+            self.logger.error(f"error calculating similarity: {e}")
+            self.logger.error(traceback.format_exc())
+            raise
+            
+    def test_engine(self, test_text: str) -> np.ndarray:
+        """Tests if the engine is working correctly
+        
+        Args:
+            test_text: Test text
+            
+        Returns:
+            Generated vector
+            
+        Raises:
+            Exception: If the engine is not working correctly
+        """
+        self.logger.info("testing engine functionality...")
+        
+        # Ensure the model is loaded
+        if not self.model:
+            raise RuntimeError("text similarity model not loaded")
+            
+        try:
+            # Generate vector
+            vector = self.model.encode(test_text)
+            
+            # Verify vector
+            if not isinstance(vector, np.ndarray):
+                raise TypeError(f"vector type error: {type(vector)}")
+                
+            if vector.shape[0] != 384:  # MiniLM-L6 dimension
+                raise ValueError(f"vector dimension error: {vector.shape}")
+                
+            self.logger.info("engine test successful")
+            return vector.tolist()  # Convert to list for serialization
+            
+        except Exception as e:
+            self.logger.error(f"engine test failed: {e}")
+            self.logger.error(traceback.format_exc())
+            raise
+
+    def initialize(self, db_file: str, vector_file: str, tag_data: List[Dict] = None) -> Dict[str, Any]:
+        """Initializes the tag library
+        
+        Args:
+            db_file: Path to the database file
+            vector_file: Output path for the vector file
+            tag_data: List of tag data
+            
+        Returns:
+            Initialization result information
+        """
+        try:
+            # Ensure parameter types are correct
+            if not isinstance(db_file, str):
+                self.logger.error(f"db_file 参数类型错误: {type(db_file)}")
+                return {
+                    "status": "error",
+                    "message": f"数据库文件路径必须是字符串，而不是 {type(db_file)}",
+                    "result": None
+                }
+                
+            if not isinstance(vector_file, str):
+                self.logger.error(f"vector_file 参数类型错误: {type(vector_file)}")
+                return {
+                    "status": "error",
+                    "message": f"vector file path must be a string, not {type(vector_file)}",
+                    "result": None
+                }
+            
+            # Log initialization parameters
+            self.logger.info(f"initializing tag library:")
+            self.logger.info(f"- database file: {db_file}")
+            self.logger.info(f"- vector file: {vector_file}")
+            self.logger.info(f"- tag data: {len(tag_data) if tag_data else 'none'}")
+            
+            self.vector_file = vector_file
+            
+            # Test the engine first
+            try:
+                test_vector = self.test_engine("test sentence for initialization")
+                self.logger.info("engine test successful")
+            except Exception as e:
+                return {
+                    "status": "error",
+                    "message": f"engine test failed: {e}",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+
+            # First ensure the model is loaded successfully
+            if not self.model:
+                return {
+                    "status": "error",
+                    "message": "unable to load text similarity model",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+            
+            # Test if the model can work normally
+            try:
+                test_text = "test sentence for model verification"
+                test_vector = self.model.encode(test_text)
+                if not isinstance(test_vector, np.ndarray) or test_vector.shape[0] != 384:  # MiniLM-L6 dimension
+                    raise ValueError(f"model output vector dimension error: {test_vector.shape}")
+            except Exception as e:
+                self.logger.error(f"model functionality test failed: {e}")
+                return {
+                    "status": "error",
+                    "message": f"model functionality test failed: {e}",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+
+            # Ensure the output directory exists
+            os.makedirs(os.path.dirname(vector_file), exist_ok=True)
+            
+            # Parse tag data
+            tag_info = {}
+            if tag_data:
+                for tag_dict in tag_data:
+                    # Handle tag data in different formats
+                    if isinstance(tag_dict, dict):
+                        tag_data_dict = tag_dict
+                    elif isinstance(tag_dict, list):
+                        # Convert list to dictionary
+                        tag_data_dict = {}
+                        for item in tag_dict:
+                            if isinstance(item, tuple) and len(item) == 2:
+                                key, value = item
+                                if isinstance(value, list) and len(value) == 1:
+                                    value = value[0]
+                                tag_data_dict[key] = value
+                    else:
+                        self.logger.warning(f"skipping invalid tag data: {tag_dict}")
+                        continue
+                        
+                    # Extract tag ID
+                    tag_id = tag_data_dict.get('id') or tag_data_dict.get('name')
+                    if not tag_id:
+                        continue
+                        
+                    # If tag_id is a list, take the first element
+                    if isinstance(tag_id, list):
+                        tag_id = tag_id[0]
+                        
+                    # Process fields
+                    fields = self._process_fields(tag_data_dict.get('fields', []))
+                    
+                    # Process behaviors
+                    behaviors = self._process_behaviors(tag_data_dict.get('behaviors', []))
+                        
+                    # Process relations
+                    relations = self._process_relations(tag_data_dict.get('relations', []))
+                        
+                    # Store tag information
+                    tag_info[tag_id] = {
+                        'name': tag_id,
+                        'type': 'tag',
+                        'fields': fields,
+                        'behaviors': behaviors,
+                        'relations': relations,
+                    }
+            else:
+                # Parse from database file
+                tag_info = self.parse_supertag_db(db_file)
+            
+            self.logger.info(f"processed {len(tag_info)} tags")
+            
+            if not tag_info:
+                self.logger.error("no valid tag data found")
+                return {"status": "error", "message": "no valid tag data found"}
+            
+            # Generate tag vectors
+            tag_vectors = {}
+            for tag_id, info in tag_info.items():
+                try:
+                    # Generate vector
+                    tag_vector = self.model.encode(tag_id)
+                    tag_vectors[tag_id] = tag_vector
+                except Exception as e:
+                    self.logger.error(f"error generating tag '{tag_id}' vector: {e}")
+                    continue
+            
+            # Build cache data
+            cache_data = {
+                'tags': {
+                    tag_id: {
+                        'name': info['name'],
+                        'vector': tag_vectors[tag_id].tolist() if tag_id in tag_vectors else [],
+                        'info': info
+                    }
+                    for tag_id, info in tag_info.items() if tag_id in tag_vectors
+                },
+                'metadata': {
+                    'total_tags': len(tag_vectors),
+                    'vector_dim': 384,  # MiniLM-L6 dimension
+                    'created_at': datetime.now().isoformat(),
+                    'model_name': 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+                }
+            }
+            
+            # Save to file
+            with open(vector_file, 'w') as f:
+                json.dump(cache_data, f, indent=2)
+                
+            self.logger.info(f"tag library initialized, saved to {vector_file}")
+            self.logger.info(f"file size: {os.path.getsize(vector_file)} bytes")
+            
+            # Update status
+            self.tag_vectors = {
+                tag_id: np.array(vector) for tag_id, vector in tag_vectors.items()
+            }
+            self.is_initialized = True
+            
+            return {
+                "status": "success",
+                "result": {
+                    "vector_file": vector_file,
+                    "db_file": db_file,
+                    "model": 'sentence-transformers/paraphrase-MiniLM-L6-v2',
+                    "tag_count": len(tag_vectors),
+                    "file_size": os.path.getsize(vector_file)
+                }
+            }
+            
+        except Exception as e:
+            self.logger.error(f"error initializing tag library: {e}")
+            self.logger.error(traceback.format_exc())
+            return {
+                "status": "error", 
+                "message": str(e),
+                "result": {
+                    "vector_file": vector_file,
+                    "db_file": db_file,
+                    "model": 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+                }
+            }
+    
+    def _process_fields(self, raw_fields):
+        """Processes field data"""
+        fields = []
+        if raw_fields:
+            for field in raw_fields:
+                if isinstance(field, dict):
+                    fields.append(field)
+                elif isinstance(field, (list, tuple)):
+                    field_dict = {}
+                    for i in range(0, len(field), 2):
+                        if i + 1 < len(field):
+                            key = field[i]
+                            value = field[i + 1]
+                            if key == 'name':
+                                field_dict['name'] = value
+                            elif key == 'type':
+                                field_dict['type'] = value
+                            elif key == 'description':
+                                field_dict['description'] = value
+                            elif key == 'options':
+                                if isinstance(value, list):
+                                    field_dict['options'] = value
+                    if field_dict:
+                        fields.append(field_dict)
+        return fields
+    
+    def _process_behaviors(self, raw_behaviors):
+        """Processes behavior data"""
+        behaviors = []
+        if isinstance(raw_behaviors, dict):
+            behaviors = list(raw_behaviors.keys())
+        elif isinstance(raw_behaviors, list):
+            behaviors = [b for b in raw_behaviors if b]
+        return behaviors
+    
+    def _process_relations(self, raw_relations):
+        """Processes relation data"""
+        relations = []
+        if isinstance(raw_relations, list):
+            relations = [r for r in raw_relations if r]
+        return relations
+            
+    def parse_supertag_db(self, db_file_path: str) -> Dict[str, Dict]:
+        """Parses the supertag-db.el file to extract tag information
+        
+        Args:
+            db_file_path: Path to the database file
+            
+        Returns:
+            Dictionary of tag information
+        """
+        self.logger.info(f"parsing database file: {db_file_path}")
+        tag_info = {}
+        
+        try:
+            with open(db_file_path) as f:
+                content = f.read()
+                
+            # Extract tag definitions
+            tag_pattern = r'\(ht-set!\s+org-supertag-db--object\s+"([^"]+)"\s+\'(\(:type\s+:tag.*?\))\)'
+            for match in re.finditer(tag_pattern, content, re.DOTALL):
+                tag_id = match.group(1)
+                tag_props = match.group(2)
+                
+                # Skip metadata
+                if tag_id == "metadata":
+                    continue
+                    
+                # Extract field definitions
+                fields = []
+                fields_match = re.search(r':fields\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if fields_match and fields_match.group(1) != 'nil':
+                    field_str = fields_match.group(1)
+                    # Parse field list
+                    field_pattern = r'\(([^)]+)\)'
+                    for field_match in re.finditer(field_pattern, field_str):
+                        field_def = field_match.group(1)
+                        field_parts = field_def.strip().split()
+                        if len(field_parts) >= 2:
+                            field = {
+                                'name': field_parts[0].strip(':'),
+                                'type': field_parts[1].strip(':')
+                            }
+                            if len(field_parts) > 2:
+                                field['description'] = ' '.join(field_parts[2:]).strip('"')
+                            fields.append(field)
+                
+                # Extract behavior definitions
+                behaviors = []
+                behaviors_match = re.search(r':behaviors\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if behaviors_match and behaviors_match.group(1) != 'nil':
+                    behavior_str = behaviors_match.group(1).strip('()')
+                    behaviors = [b.strip('"') for b in behavior_str.split()]
+                
+                # Extract relation definitions
+                relations = []
+                relations_match = re.search(r':relations\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if relations_match and relations_match.group(1) != 'nil':
+                    relation_str = relations_match.group(1).strip('()')
+                    relations = [r.strip('"') for r in relation_str.split()]
+                
+                # Store tag information
+                tag_info[tag_id] = {
+                    'name': tag_id,
+                    'type': 'tag',
+                    'fields': fields,
+                    'behaviors': behaviors,
+                    'relations': relations
+                }
+            
+            self.logger.info(f"found {len(tag_info)} tag definitions")
+            return tag_info
+            
+        except Exception as e:
+            self.logger.error(f"error parsing database file: {e}")
+            self.logger.error(traceback.format_exc())
+            return {}
\ No newline at end of file
diff --git a/build/lib/simtag/utils/__init__.py b/build/lib/simtag/utils/__init__.py
new file mode 100644
index 0000000..9c8283e
--- /dev/null
+++ b/build/lib/simtag/utils/__init__.py
@@ -0,0 +1,25 @@
+"""
+SimTag Toolkit
+Provides logging and serialization utilities
+"""
+
+from .logging import setup_logging, get_logger
+from .serialization import (
+    to_serializable,
+    serialize_to_json,
+    save_to_json_file,
+    load_from_json_file,
+    normalize_response
+)
+
+__all__ = [
+    'setup_logging',
+    'get_logger',
+    'to_serializable',
+    'serialize_to_json',
+    'save_to_json_file',
+    'load_from_json_file',
+    'normalize_response'
+]
+
+"""SimTag Toolkit Module"""
\ No newline at end of file
diff --git a/build/lib/simtag/utils/logging.py b/build/lib/simtag/utils/logging.py
new file mode 100644
index 0000000..fa5854d
--- /dev/null
+++ b/build/lib/simtag/utils/logging.py
@@ -0,0 +1,52 @@
+"""
+SimTag Logging Utility Module
+Provides unified logging functionality
+"""
+
+import os
+import sys
+import logging
+from typing import Dict, Any, Optional
+
+def setup_logging(log_file: Optional[str], log_level: int = logging.INFO):
+    """Set up logging configuration
+    
+    Args:
+        log_file: Log file path, if None, only output to console
+        log_level: Log level
+    """
+    # Basic configuration
+    logging.basicConfig(
+        level=log_level,
+        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+        datefmt='%Y-%m-%d %H:%M:%S'
+    )
+    
+    # If a log file is specified, add a file handler
+    if log_file:
+        # Ensure the log directory exists
+        log_dir = os.path.dirname(log_file)
+        if log_dir and not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+            
+        # Create a file handler
+        file_handler = logging.FileHandler(log_file)
+        file_handler.setLevel(log_level)
+        file_handler.setFormatter(logging.Formatter(
+            '%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+            '%Y-%m-%d %H:%M:%S'
+        ))
+        
+        # Add to the root logger
+        logging.getLogger().addHandler(file_handler)
+
+def get_logger(name: str) -> logging.Logger:
+    """Get a logger with the specified name
+    
+    Args:
+        name: Logger name
+        
+    Returns:
+        Named logger
+    """
+    return logging.getLogger(name) 
\ No newline at end of file
diff --git a/build/lib/simtag/utils/serialization.py b/build/lib/simtag/utils/serialization.py
new file mode 100644
index 0000000..66936d4
--- /dev/null
+++ b/build/lib/simtag/utils/serialization.py
@@ -0,0 +1,88 @@
+"""
+SimTag Serialization Utility Module
+Provides data serialization and deserialization functionality
+"""
+
+import json
+import numpy as np
+from typing import Any, Dict, List, Union
+from datetime import datetime
+
+def to_serializable(obj: Any) -> Any:
+    """Converts an object to a serializable format
+    
+    Args:
+        obj: The object to be serialized
+        
+    Returns:
+        The serialized object
+    """
+    if isinstance(obj, dict):
+        return {k: to_serializable(v) for k, v in obj.items()}
+    elif isinstance(obj, list):
+        return [to_serializable(v) for v in obj]
+    elif isinstance(obj, tuple):
+        return tuple(to_serializable(v) for v in obj)
+    elif isinstance(obj, np.ndarray):
+        return obj.tolist()
+    elif isinstance(obj, (np.int32, np.int64)):
+        return int(obj)
+    elif isinstance(obj, (np.float32, np.float64)):
+        return float(obj)
+    elif isinstance(obj, datetime):
+        return obj.isoformat()
+    return obj
+
+def serialize_to_json(obj: Any) -> str:
+    """Serializes an object to a JSON string
+    
+    Args:
+        obj: The object to be serialized
+        
+    Returns:
+        The JSON string
+    """
+    return json.dumps(to_serializable(obj), ensure_ascii=False, indent=2)
+
+def save_to_json_file(obj: Any, file_path: str) -> None:
+    """Saves an object to a JSON file
+    
+    Args:
+        obj: The object to be serialized
+        file_path: The path of the JSON file
+    """
+    with open(file_path, 'w', encoding='utf-8') as f:
+        json.dump(to_serializable(obj), f, ensure_ascii=False, indent=2)
+
+def load_from_json_file(file_path: str) -> Any:
+    """Loads an object from a JSON file
+    
+    Args:
+        file_path: The path of the JSON file
+        
+    Returns:
+        The loaded object
+    """
+    with open(file_path, 'r', encoding='utf-8') as f:
+        return json.load(f)
+
+def normalize_response(data: Any, status: str = "success", message: str = None) -> Dict[str, Any]:
+    """Generates a standardized response format
+    
+    Args:
+        data: The response data
+        status: The response status, "success" or "error"
+        message: The status message, usually used for error explanation
+        
+    Returns:
+        The standardized response dictionary
+    """
+    response = {
+        "status": status,
+        "result": to_serializable(data)
+    }
+    
+    if message:
+        response["message"] = message
+        
+    return response 
\ No newline at end of file
diff --git a/cache/cooccur_matrix.pkl b/cache/cooccur_matrix.pkl
new file mode 100644
index 0000000..eef3a50
Binary files /dev/null and b/cache/cooccur_matrix.pkl differ
diff --git a/deadline-list-error-fix.md b/deadline-list-error-fix.md
new file mode 100755
index 0000000..ad552de
--- /dev/null
+++ b/deadline-list-error-fix.md
@@ -0,0 +1,60 @@
+# SQLite List Type Error 修复总结
+
+## 问题描述
+在同步节点数据时出现错误：
+```
+sqlite3.ProgrammingError: Error binding parameter 10: type 'list' is not supported
+```
+
+错误发生在尝试插入节点数据到 SQLite 数据库时，`deadline` 字段（参数 10）接收了一个 list 类型的值。
+
+## 问题原因
+Emacs/Elisp 使用内部时间格式，通常是一个包含 3-4 个整数的列表：
+- `[high low microsecs]` 或
+- `[high low microsecs picosecs]`
+
+当这些时间数据从 Elisp 传递到 Python 时，它们保持为列表格式，而 SQLite 的 TEXT 字段不能直接存储列表。
+
+## 解决方案
+
+### 1. 数据规范化阶段（推荐）
+在 `simtag/utils/unified_tag_processor.py` 的 `_parse_elisp_data` 函数中添加了对 Emacs 时间列表的特殊处理：
+
+```python
+# Special case: Emacs time list format
+if (len(data) in [3, 4] and 
+    all(isinstance(x, (int, float)) for x in data) and
+    len([x for x in data if isinstance(x, int) and x >= 0]) == len(data)):
+    return f"emacs-time:{data}"
+```
+
+这会将时间列表转换为字符串格式，例如：`"emacs-time:[26709, 58437, 123456]"`
+
+### 2. 数据库插入阶段（备用）
+在 `simtag/core/graph_service.py` 的 `_upsert_nodes_internal` 函数中添加了类型检查和转换：
+
+```python
+for col in columns:
+    val = node.get(col)
+    # Convert list types to string for datetime fields
+    if col in ['scheduled', 'deadline'] and isinstance(val, list):
+        val = str(val) if val else None
+    # Ensure olp is serialized as string if it's a list
+    elif col == 'olp' and isinstance(val, list):
+        val = json.dumps(val)
+    # Ensure properties is serialized as string
+    elif col == 'properties' and isinstance(val, dict):
+        val = json.dumps(val)
+    values.append(val)
+```
+
+这确保了即使数据规范化阶段遗漏了某些列表，在插入数据库前也会被转换为字符串。
+
+## 影响的字段
+- `deadline`: Org-mode 的 DEADLINE 时间戳
+- `scheduled`: Org-mode 的 SCHEDULED 时间戳
+- `olp`: Outline path (大纲路径)，通常是一个字符串列表
+- `properties`: 属性字典，需要序列化为 JSON 字符串
+
+## 未来改进
+可以考虑将 Emacs 时间列表转换为标准的 ISO 8601 时间字符串格式，这样更易于在 Python 端处理和查询。 
\ No newline at end of file
diff --git a/debug-object-count.el b/debug-object-count.el
new file mode 100755
index 0000000..19ed3c1
--- /dev/null
+++ b/debug-object-count.el
@@ -0,0 +1,142 @@
+;;; debug-object-count.el --- Debug object count differences
+
+(message "=== OBJECT COUNT ANALYSIS ===")
+
+;; 1. 统计当前数据库状态
+(message "\n1. Current database state:")
+(let ((total-objects (hash-table-count org-supertag-db--object))
+      (total-links (hash-table-count org-supertag-db--link))
+      (nodes-count 0)
+      (tags-count 0)
+      (other-count 0)
+      (active-nodes 0))
+  
+  ;; 统计各类型对象
+  (maphash (lambda (_id props)
+             (let ((type (plist-get props :type)))
+               (cond
+                ((eq type :node) 
+                 (cl-incf nodes-count)
+                 ;; 检查节点是否"活跃"（有内容或标题）
+                 (when (or (plist-get props :content)
+                          (plist-get props :title)
+                          (plist-get props :raw-value))
+                   (cl-incf active-nodes)))
+                ((eq type :tag) (cl-incf tags-count))
+                (t (cl-incf other-count)))))
+           org-supertag-db--object)
+  
+  (message "   Total objects in hash table: %d" total-objects)
+  (message "   - Nodes: %d (active: %d)" nodes-count active-nodes)
+  (message "   - Tags: %d" tags-count)
+  (message "   - Others: %d" other-count)
+  (message "   Total links: %d" total-links)
+  (message "   Grand total: %d" (+ total-objects total-links)))
+
+;; 2. 模拟 get-changed-objects 的计算逻辑
+(message "\n2. Simulating get-changed-objects logic:")
+(let ((counted-objects 0)
+      (counted-links 0)
+      (nodes-to-upsert 0)
+      (tags-to-upsert 0)
+      (links-to-upsert 0)
+      (ids-to-delete 0))
+  
+  ;; 模拟对象表的遍历
+  (maphash (lambda (id props)
+             (cl-incf counted-objects)
+             (let ((type (plist-get props :type)))
+               (cond
+                ((eq type :node) (cl-incf nodes-to-upsert))
+                ((eq type :tag) (cl-incf tags-to-upsert)))))
+           org-supertag-db--object)
+  
+  ;; 模拟链接表的遍历
+  (maphash (lambda (_id _props)
+             (cl-incf counted-links)
+             (cl-incf links-to-upsert))
+           org-supertag-db--link)
+  
+  ;; 模拟删除检查（哈希表为空，所以没有删除）
+  (setq ids-to-delete (hash-table-count org-supertag-background-sync--last-sync-hashes))
+  
+  (message "   Objects processed: %d" counted-objects)
+  (message "   Links processed: %d" counted-links)
+  (message "   Total checked: %d" (+ counted-objects counted-links))
+  (message "   Changes breakdown:")
+  (message "     - Nodes to upsert: %d" nodes-to-upsert)
+  (message "     - Tags to upsert: %d" tags-to-upsert)
+  (message "     - Links to upsert: %d" links-to-upsert)
+  (message "     - IDs to delete: %d" ids-to-delete)
+  (message "   Total changes: %d" (+ nodes-to-upsert tags-to-upsert links-to-upsert ids-to-delete)))
+
+;; 3. 检查具体的ID提取逻辑
+(message "\n3. ID extraction analysis:")
+(let ((successful-extractions 0)
+      (failed-extractions 0)
+      (sample-failures '()))
+  
+  (maphash (lambda (id props)
+             (let* ((extracted-id (or (plist-get props :id)
+                                     (plist-get props :node-id)
+                                     (plist-get props :tag-id)
+                                     (plist-get props :link-id)
+                                     (when (eq (plist-get props :type) :link)
+                                       (format "%s->%s" 
+                                               (or (plist-get props :source) "")
+                                               (or (plist-get props :target) ""))))))
+               (if extracted-id
+                   (cl-incf successful-extractions)
+                 (cl-incf failed-extractions)
+                 (when (< (length sample-failures) 5)
+                   (push (list id (plist-get props :type)) sample-failures)))))
+           org-supertag-db--object)
+  
+  (maphash (lambda (id props)
+             (let* ((extracted-id (or (plist-get props :id)
+                                     (plist-get props :node-id)
+                                     (plist-get props :tag-id)
+                                     (plist-get props :link-id)
+                                     (when (eq (plist-get props :type) :link)
+                                       (format "%s->%s" 
+                                               (or (plist-get props :source) "")
+                                               (or (plist-get props :target) ""))))))
+               (if extracted-id
+                   (cl-incf successful-extractions)
+                 (cl-incf failed-extractions)
+                 (when (< (length sample-failures) 5)
+                   (push (list id "link") sample-failures)))))
+           org-supertag-db--link)
+  
+  (message "   Successful ID extractions: %d" successful-extractions)
+  (message "   Failed ID extractions: %d" failed-extractions)
+  (when sample-failures
+    (message "   Sample failures:")
+    (dolist (failure sample-failures)
+      (message "     - Key: %s, Type: %s" (car failure) (cadr failure)))))
+
+;; 4. 检查文件范围
+(message "\n4. File scope analysis:")
+(let ((files-with-nodes (make-hash-table :test 'equal))
+      (unique-files 0))
+  
+  (maphash (lambda (_id props)
+             (when (eq (plist-get props :type) :node)
+               (let ((file-path (plist-get props :file-path)))
+                 (when file-path
+                   (puthash file-path t files-with-nodes)))))
+           org-supertag-db--object)
+  
+  (setq unique-files (hash-table-count files-with-nodes))
+  (message "   Unique files with nodes: %d" unique-files)
+  
+  (when (> unique-files 0)
+    (message "   Sample files:")
+    (let ((count 0))
+      (maphash (lambda (file _)
+                 (when (< count 5)
+                   (message "     - %s" file)
+                   (cl-incf count)))
+               files-with-nodes))))
+
+(message "\n=== ANALYSIS COMPLETED ===") 
\ No newline at end of file
diff --git a/debug-remaining-changes.el b/debug-remaining-changes.el
new file mode 100755
index 0000000..8c3e2ed
--- /dev/null
+++ b/debug-remaining-changes.el
@@ -0,0 +1,115 @@
+;;; Debug remaining changes issue
+
+(require 'org-supertag-background-sync)
+(require 'org-supertag-db)
+
+(defun safe-substring (str start end)
+  "Safely extract substring, handling cases where string is shorter than expected."
+  (when str
+    (let ((str-len (length str)))
+      (substring str start (min end str-len)))))
+
+(defun debug-remaining-changes ()
+  "Analyze why we still have too many changes."
+  (interactive)
+  (message "\n=== DEBUGGING REMAINING CHANGES ===")
+  
+  ;; Helper function (same as in the fixed code)
+  (cl-flet ((extract-id (props)
+             (or (plist-get props :id)
+                 (plist-get props :node-id)
+                 (plist-get props :tag-id)
+                 (plist-get props :link-id)
+                 ;; For links, try source/target as fallback
+                 (when (eq (plist-get props :type) :link)
+                   (format "%s->%s" 
+                           (or (plist-get props :source) "")
+                           (or (plist-get props :target) ""))))))
+    
+    ;; 1. Check basic counts
+    (message "\n1. Basic Counts:")
+    (message "   Objects: %d" (hash-table-count org-supertag-db--object))
+    (message "   Links: %d" (hash-table-count org-supertag-db--link))
+    (message "   Hash records: %d" (hash-table-count org-supertag-background-sync--last-sync-hashes))
+    
+    ;; 2. Check ID extraction success rate
+    (let ((successful-extractions 0)
+          (failed-extractions 0)
+          (sample-failed-ids '()))
+      
+      (maphash (lambda (db-id props)
+                 (let ((extracted-id (extract-id props)))
+                   (if extracted-id
+                       (cl-incf successful-extractions)
+                     (cl-incf failed-extractions)
+                     (when (< (length sample-failed-ids) 3)
+                       (push (safe-substring db-id 0 20) sample-failed-ids)))))
+               org-supertag-db--object)
+      
+      (message "\n2. ID Extraction Analysis:")
+      (message "   Successful extractions: %d" successful-extractions)
+      (message "   Failed extractions: %d" failed-extractions)
+      (when sample-failed-ids
+        (message "   Sample failed IDs: %s" sample-failed-ids)))
+    
+    ;; 3. Hash lookup investigation
+    (message "\n3. Hash Lookup Investigation:")
+    (let ((test-id "29DB1144-A40B-4598-A6E9-1CE802751A72"))
+      (message "   Testing specific ID: %s" test-id)
+      (message "   Found in hash table: %s" 
+               (if (gethash test-id org-supertag-background-sync--last-sync-hashes) "YES" "NO")))
+    
+    ;; Check a few sample hash keys
+    (message "   Sample hash table keys (first 5):")
+    (let ((count 0))
+      (catch 'done
+        (maphash (lambda (key value)
+                   (when (>= count 5) (throw 'done nil))
+                   (message "     Key: %s, Hash: %s" 
+                           (safe-substring key 0 30)
+                           (safe-substring value 0 8))
+                   (cl-incf count))
+                 org-supertag-background-sync--last-sync-hashes)))
+    
+    ;; 4. Detailed hash comparison for first few objects
+    (message "\n4. Detailed Hash Comparison (first 3 objects):")
+    (let ((count 0))
+      (catch 'done
+        (maphash (lambda (db-id props)
+                   (when (>= count 3) (throw 'done nil))
+                   (let* ((extracted-id (extract-id props))
+                          (current-hash (org-supertag-background-sync--calculate-object-hash props))
+                          (last-hash (gethash extracted-id org-supertag-background-sync--last-sync-hashes))
+                          (db-id-short (safe-substring db-id 0 20))
+                          (extracted-id-short (if extracted-id (safe-substring extracted-id 0 20) "nil")))
+                     (message "   Object %d:" (1+ count))
+                     (message "     DB ID: %s" db-id-short)
+                     (message "     Extracted ID: %s" extracted-id-short)
+                     (message "     Current hash: %s" (if current-hash (safe-substring current-hash 0 8) "nil"))
+                     (message "     Last hash: %s" (if last-hash (safe-substring last-hash 0 8) "nil"))
+                     (message "     Match: %s" (if (and current-hash last-hash extracted-id)
+                                                   (string= current-hash last-hash)
+                                                 "N/A"))
+                     (message "     ID match: %s" (if extracted-id (string= db-id extracted-id) "N/A"))
+                     ;; Additional debugging: manually check if this ID exists in hash table
+                     (message "     Manual hash lookup: %s"
+                             (if (gethash extracted-id org-supertag-background-sync--last-sync-hashes) "FOUND" "NOT FOUND"))
+                     (cl-incf count)))
+                 org-supertag-db--object)))
+    
+    ;; 5. Check hash table integrity
+    (message "\n5. Hash Table Integrity Check:")
+    (message "   Hash table test (manual lookup):")
+    (let ((first-object-id nil))
+      (catch 'found
+        (maphash (lambda (db-id props)
+                   (setq first-object-id (extract-id props))
+                   (throw 'found t))
+                 org-supertag-db--object))
+      (when first-object-id
+        (message "     First object ID: %s" first-object-id)
+        (message "     Direct gethash result: %s" 
+                (or (gethash first-object-id org-supertag-background-sync--last-sync-hashes) "nil"))))))
+
+;; Run the analysis
+(debug-remaining-changes) 
diff --git a/debug-sync-cleanup-issue.el b/debug-sync-cleanup-issue.el
new file mode 100755
index 0000000..4e3fb62
--- /dev/null
+++ b/debug-sync-cleanup-issue.el
@@ -0,0 +1,205 @@
+;;; debug-sync-cleanup-issue.el --- 诊断同步清理问题
+
+(require 'org-supertag-sync)
+(require 'org-supertag-db)
+
+(defun debug-sync-cleanup-issue ()
+  "诊断同步清理问题，检查为什么所有节点都被删除了。"
+  (interactive)
+  (message "\n=== 同步清理问题诊断 ===")
+  
+  ;; 1. 检查同步目录配置
+  (message "\n1. 同步目录配置检查：")
+  (message "   同步目录: %S" org-supertag-sync-directories)
+  (message "   排除目录: %S" org-supertag-sync-exclude-directories)
+  (message "   数据目录: %s" org-supertag-data-directory)
+  
+  ;; 2. 检查同步状态
+  (message "\n2. 同步状态检查：")
+  (message "   同步状态文件: %s" org-supertag-sync-state-file)
+  (message "   状态文件存在: %s" (file-exists-p org-supertag-sync-state-file))
+  (message "   同步状态中的文件数: %d" (hash-table-count org-supertag-sync--state))
+  
+  ;; 显示同步状态中的文件（前10个）
+  (let ((count 0))
+    (message "   同步状态中的文件（前10个）：")
+    (maphash (lambda (file time)
+               (when (< count 10)
+                 (message "     %s -> %s" file time)
+                 (cl-incf count)))
+             org-supertag-sync--state))
+  
+  ;; 3. 检查数据库状态
+  (message "\n3. 数据库状态检查：")
+  (let ((node-count 0)
+        (files-in-db (make-hash-table :test 'equal)))
+    (maphash (lambda (id node)
+               (when (eq (plist-get node :type) :node)
+                 (cl-incf node-count)
+                 (let ((file (plist-get node :file-path)))
+                   (when file
+                     (puthash file t files-in-db)))))
+             org-supertag-db--object)
+    (message "   数据库中的节点数: %d" node-count)
+    (message "   数据库中涉及的文件数: %d" (hash-table-count files-in-db))
+    
+    ;; 显示数据库中的文件（前10个）
+    (let ((count 0))
+      (message "   数据库中的文件（前10个）：")
+      (maphash (lambda (file _)
+                 (when (< count 10)
+                   (message "     %s (存在: %s)" file (file-exists-p file))
+                   (cl-incf count)))
+               files-in-db)))
+  
+  ;; 4. 检查文件范围匹配
+  (message "\n4. 文件范围匹配检查：")
+  (let ((in-scope-count 0)
+        (out-scope-count 0))
+    (maphash (lambda (id node)
+               (when (eq (plist-get node :type) :node)
+                 (let ((file (plist-get node :file-path)))
+                   (if (org-supertag-sync--in-sync-scope-p file)
+                       (cl-incf in-scope-count)
+                     (cl-incf out-scope-count)))))
+             org-supertag-db--object)
+    (message "   在同步范围内的节点: %d" in-scope-count)
+    (message "   超出同步范围的节点: %d" out-scope-count))
+  
+  ;; 5. 检查节点检测逻辑
+  (message "\n5. 节点检测逻辑测试：")
+  (let ((test-count 0)
+        (found-count 0)
+        (not-found-count 0))
+    (maphash (lambda (id node)
+               (when (and (eq (plist-get node :type) :node)
+                         (< test-count 5)) ; 只测试前5个节点
+                 (cl-incf test-count)
+                 (let ((file (plist-get node :file-path)))
+                   (if (org-supertag-sync--check-node-exists-in-file id file)
+                       (progn
+                         (cl-incf found-count)
+                         (message "   ✓ 节点 %s 在文件 %s 中找到" id file))
+                     (progn
+                       (cl-incf not-found-count)
+                       (message "   ✗ 节点 %s 在文件 %s 中未找到" id file))))))
+             org-supertag-db--object)
+    (message "   测试节点: %d, 找到: %d, 未找到: %d" test-count found-count not-found-count))
+  
+  ;; 6. 检查最近的删除操作
+  (message "\n6. 可能的问题原因：")
+  (cond
+   ((= (hash-table-count org-supertag-sync--state) 0)
+    (message "   ❌ 同步状态为空！这会导致所有节点被认为'不在同步范围'"))
+   ((not org-supertag-sync-directories)
+    (message "   ❌ 同步目录未配置！"))
+   (t
+    (message "   ✓ 同步配置看起来正常")))
+  
+  ;; 7. 提供修复建议
+  (message "\n7. 修复建议：")
+  (message "   - 如果同步状态为空，运行: (org-supertag-sync-recover)")
+  (message "   - 如果同步目录未配置，运行: (org-supertag-sync-add-directory \"~/org\")")
+  (message "   - 如果节点检测有问题，检查org文件中的ID格式")
+  (message "   - 如果需要停止自动清理，运行: (org-supertag-sync-stop-auto-sync)"))
+
+(defun debug-check-node-detection ()
+  "详细检查节点检测逻辑是否正确。"
+  (interactive)
+  (message "\n=== 节点检测逻辑详细检查 ===")
+  
+  ;; 检查当前buffer中的节点
+  (when (and (derived-mode-p 'org-mode) (buffer-file-name))
+    (let ((current-file (buffer-file-name))
+          (found-ids '())
+          (db-nodes '()))
+      
+      ;; 1. 扫描当前文件中实际存在的ID
+      (save-excursion
+        (save-restriction
+          (widen)
+          (goto-char (point-min))
+          ;; 搜索所有ID
+          (while (re-search-forward "^[ \t]*:ID:[ \t]+\\([^[:space:]]+\\)" nil t)
+            (push (match-string 1) found-ids))))
+      
+      ;; 2. 检查数据库中属于这个文件的节点
+      (maphash (lambda (id node)
+                 (when (and (eq (plist-get node :type) :node)
+                           (string= (plist-get node :file-path) current-file))
+                   (push id db-nodes)))
+               org-supertag-db--object)
+      
+      (message "文件: %s" current-file)
+      (message "文件中实际的ID数量: %d" (length found-ids))
+      (message "数据库中的节点数量: %d" (length db-nodes))
+      
+      ;; 3. 比较差异
+      (let ((missing-in-file '())
+            (missing-in-db '()))
+        (dolist (id db-nodes)
+          (unless (member id found-ids)
+            (push id missing-in-file)))
+        (dolist (id found-ids)
+          (unless (member id db-nodes)
+            (push id missing-in-db)))
+        
+        (when missing-in-file
+          (message "❌ 数据库中有但文件中找不到的节点: %S" missing-in-file))
+        (when missing-in-db
+          (message "⚠️  文件中有但数据库中没有的节点: %S" missing-in-db))
+        
+        (when (and (null missing-in-file) (null missing-in-db))
+          (message "✓ 数据库与文件状态一致"))))))
+
+(defun debug-test-cleanup-logic ()
+  "测试清理逻辑而不实际删除节点。"
+  (interactive)
+  (message "\n=== 清理逻辑测试（干运行）===")
+  
+  ;; 模拟 org-supertag-sync--cleanup-database 的逻辑但不删除
+  (let ((nodes-to-remove nil)
+        (files-in-db (make-hash-table :test 'equal))
+        (sync-files (make-hash-table :test 'equal)))
+    
+    ;; 收集同步状态中的文件
+    (maphash (lambda (file _)
+               (let ((normalized (expand-file-name file)))
+                 (puthash normalized t sync-files)))
+             org-supertag-sync--state)
+    
+    ;; 收集数据库中的文件
+    (maphash (lambda (id node)
+               (when (eq (plist-get node :type) :node)
+                 (let ((file (plist-get node :file-path)))
+                   (when file
+                     (let ((normalized (expand-file-name file)))
+                       (puthash normalized t files-in-db))))))
+             org-supertag-db--object)
+    
+    ;; 检查哪些文件的节点会被删除
+    (maphash (lambda (file _)
+               (unless (gethash file sync-files)
+                 (message "❌ 文件 %s 不在同步状态，其节点会被删除" file)
+                 (maphash (lambda (id node)
+                           (when (and (eq (plist-get node :type) :node)
+                                    (string= (expand-file-name (plist-get node :file-path)) file))
+                             (push id nodes-to-remove)))
+                         org-supertag-db--object)))
+             files-in-db)
+    
+    (message "总结:")
+    (message "  同步状态中的文件: %d" (hash-table-count sync-files))
+    (message "  数据库中的文件: %d" (hash-table-count files-in-db))
+    (message "  将被删除的节点: %d" (length nodes-to-remove))
+    
+    (when nodes-to-remove
+      (message "前10个将被删除的节点:")
+      (let ((count 0))
+        (dolist (id nodes-to-remove)
+          (when (< count 10)
+            (let ((node (org-supertag-db-get id)))
+              (message "    %s (文件: %s)" id (plist-get node :file-path)))
+            (cl-incf count)))))))
+
+(provide 'debug-sync-cleanup-issue) 
\ No newline at end of file
diff --git a/debug-type-distribution.el b/debug-type-distribution.el
new file mode 100755
index 0000000..3d1102c
--- /dev/null
+++ b/debug-type-distribution.el
@@ -0,0 +1,134 @@
+;;; debug-type-distribution.el --- Analyze object type distribution
+
+(message "=== DETAILED TYPE DISTRIBUTION ANALYSIS ===")
+
+;; 1. 统计所有对象类型
+(message "\n1. Object type distribution:")
+(let ((type-counts (make-hash-table :test 'equal))
+      (total-objects 0))
+  
+  ;; 统计对象表中的类型
+  (maphash (lambda (_id props)
+             (cl-incf total-objects)
+             (let ((type (plist-get props :type)))
+               (if type
+                   (let ((type-str (if (symbolp type) (symbol-name type) (format "%S" type))))
+                     (puthash type-str (1+ (gethash type-str type-counts 0)) type-counts))
+                 (puthash "NO-TYPE" (1+ (gethash "NO-TYPE" type-counts 0)) type-counts))))
+           org-supertag-db--object)
+  
+  (message "   Total objects: %d" total-objects)
+  (message "   Type breakdown:")
+  (maphash (lambda (type count)
+             (message "     - %s: %d" type count))
+           type-counts))
+
+;; 2. 统计链接类型
+(message "\n2. Link type distribution:")
+(let ((link-type-counts (make-hash-table :test 'equal))
+      (total-links 0))
+  
+  (maphash (lambda (_id props)
+             (cl-incf total-links)
+             (let ((type (plist-get props :type)))
+               (if type
+                   (let ((type-str (if (symbolp type) (symbol-name type) (format "%S" type))))
+                     (puthash type-str (1+ (gethash type-str link-type-counts 0)) link-type-counts))
+                 (puthash "NO-TYPE" (1+ (gethash "NO-TYPE" link-type-counts 0)) link-type-counts))))
+           org-supertag-db--link)
+  
+  (message "   Total links: %d" total-links)
+  (message "   Link type breakdown:")
+  (maphash (lambda (type count)
+             (message "     - %s: %d" type count))
+           link-type-counts))
+
+;; 3. 检查节点对象的详细信息
+(message "\n3. Detailed node analysis:")
+(let ((node-count 0)
+      (sample-nodes '()))
+  
+  (maphash (lambda (id props)
+             (when (eq (plist-get props :type) :node)
+               (cl-incf node-count)
+               (when (< (length sample-nodes) 3)
+                 (push (list :id id
+                           :title (plist-get props :title)
+                           :raw-value (plist-get props :raw-value)
+                           :file-path (plist-get props :file-path)
+                           :content (let ((content (plist-get props :content)))
+                                     (if (and content (> (length content) 50))
+                                         (concat (substring content 0 50) "...")
+                                       content)))
+                       sample-nodes))))
+           org-supertag-db--object)
+  
+  (message "   Found %d :node type objects" node-count)
+  (when sample-nodes
+    (message "   Sample nodes:")
+    (dolist (node sample-nodes)
+      (message "     ID: %s" (plist-get node :id))
+      (message "       Title: %s" (plist-get node :title))
+      (message "       Raw-value: %s" (plist-get node :raw-value))
+      (message "       File: %s" (plist-get node :file-path))
+      (message "       Content: %s" (plist-get node :content))
+      (message ""))))
+
+;; 4. 检查是否有嵌入向量相关的对象
+(message "\n4. Embedding-related objects:")
+(let ((embedding-count 0)
+      (vector-count 0)
+      (embedding-samples '()))
+  
+  (maphash (lambda (id props)
+             (let ((id-str (format "%S" id))
+                   (type (plist-get props :type)))
+               (when (or (string-match-p "embed" id-str)
+                        (string-match-p "vector" id-str)
+                        (string-match-p "embed" (format "%S" type))
+                        (string-match-p "vector" (format "%S" type)))
+                 (cl-incf embedding-count)
+                 (when (< (length embedding-samples) 3)
+                   (push (list :id id :type type) embedding-samples)))))
+           org-supertag-db--object)
+  
+  (maphash (lambda (id props)
+             (let ((id-str (format "%S" id))
+                   (type (plist-get props :type)))
+               (when (or (string-match-p "embed" id-str)
+                        (string-match-p "vector" id-str)
+                        (string-match-p "embed" (format "%S" type))
+                        (string-match-p "vector" (format "%S" type)))
+                 (cl-incf vector-count)
+                 (when (< (length embedding-samples) 3)
+                   (push (list :id id :type type) embedding-samples)))))
+           org-supertag-db--link)
+  
+  (message "   Embedding-related objects: %d" embedding-count)
+  (message "   Embedding-related links: %d" vector-count)
+  (when embedding-samples
+    (message "   Samples:")
+    (dolist (sample embedding-samples)
+      (message "     ID: %s, Type: %s" (plist-get sample :id) (plist-get sample :type)))))
+
+;; 5. 查找可能的原始节点数据
+(message "\n5. Looking for original node data patterns:")
+(let ((uuid-pattern "[0-9A-F]\\{8\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{12\\}")
+      (uuid-objects 0)
+      (uuid-samples '()))
+  
+  (maphash (lambda (id props)
+             (let ((id-str (format "%S" id)))
+               (when (string-match-p uuid-pattern id-str)
+                 (cl-incf uuid-objects)
+                 (when (< (length uuid-samples) 5)
+                   (push (list :id id :type (plist-get props :type)) uuid-samples)))))
+           org-supertag-db--object)
+  
+  (message "   Objects with UUID-like IDs: %d" uuid-objects)
+  (when uuid-samples
+    (message "   UUID samples:")
+    (dolist (sample uuid-samples)
+      (message "     ID: %s, Type: %s" (plist-get sample :id) (plist-get sample :type)))))
+
+(message "\n=== ANALYSIS COMPLETED ===") 
\ No newline at end of file
diff --git a/debug_deletion_issue.py b/debug_deletion_issue.py
new file mode 100755
index 0000000..1008e2f
--- /dev/null
+++ b/debug_deletion_issue.py
@@ -0,0 +1,142 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+"""
+Debug script to analyze why delete_nodes_by_ids is not working
+"""
+
+import sqlite3
+import json
+import sys
+import os
+
+# Add the simtag directory to path
+sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'simtag'))
+
+def debug_deletion_issue():
+    """Debug the deletion issue by analyzing database content and delete IDs"""
+    
+    # Use the correct database path from simtag config
+    db_path = os.path.expanduser("~/.emacs.d/org-supertag/supertag_vector.db")
+    
+    if not os.path.exists(db_path):
+        print(f"❌ Database file not found: {db_path}")
+        return
+    
+    print(f"🔍 Analyzing database: {db_path}")
+    
+    try:
+        conn = sqlite3.connect(db_path)
+        cursor = conn.cursor()
+        
+        # 1. Check database schema
+        print("\n=== DATABASE SCHEMA ===")
+        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
+        tables = cursor.fetchall()
+        print(f"Available tables: {[table[0] for table in tables]}")
+        
+        # 2. Check nodes table structure and content
+        print("\n=== NODES TABLE ANALYSIS ===")
+        try:
+            cursor.execute("PRAGMA table_info(nodes)")
+            columns = cursor.fetchall()
+            print("Nodes table columns:", [col[1] for col in columns])
+            
+            cursor.execute("SELECT COUNT(*) FROM nodes")
+            node_count = cursor.fetchone()[0]
+            print(f"Total nodes in database: {node_count}")
+            
+            if node_count > 0:
+                cursor.execute("SELECT node_id, type FROM nodes LIMIT 10")
+                sample_nodes = cursor.fetchall()
+                print("Sample nodes:")
+                for node_id, node_type in sample_nodes:
+                    print(f"  - {node_id} (type: {node_type})")
+        except sqlite3.OperationalError as e:
+            print(f"❌ Error accessing nodes table: {e}")
+        
+        # 3. Load hash file to check what IDs are being marked for deletion
+        hash_file_path = os.path.expanduser("~/.emacs.d/org-supertag/sync_hashes.json")
+        if os.path.exists(hash_file_path):
+            print(f"\n=== HASH FILE ANALYSIS ===")
+            with open(hash_file_path, 'r') as f:
+                hash_data = json.load(f)
+            
+            hash_ids = list(hash_data.keys())
+            print(f"Hash file contains {len(hash_ids)} IDs")
+            
+            # Sample hash IDs
+            print("Sample hash IDs:")
+            for i, hash_id in enumerate(hash_ids[:10]):
+                print(f"  - {hash_id}")
+            
+            # 4. Check which hash IDs exist in the database
+            print(f"\n=== ID MATCHING ANALYSIS ===")
+            existing_ids = []
+            missing_ids = []
+            
+            for hash_id in hash_ids[:20]:  # Check first 20 to avoid too much output
+                cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id = ?", (hash_id,))
+                exists = cursor.fetchone()[0] > 0
+                if exists:
+                    existing_ids.append(hash_id)
+                else:
+                    missing_ids.append(hash_id)
+            
+            print(f"From first 20 hash IDs:")
+            print(f"  - Existing in DB: {len(existing_ids)}")
+            print(f"  - Missing from DB: {len(missing_ids)}")
+            
+            if missing_ids:
+                print("Sample missing IDs:")
+                for missing_id in missing_ids[:5]:
+                    print(f"  - {missing_id}")
+        
+        # 5. Simulate the deletion process
+        print(f"\n=== DELETION SIMULATION ===")
+        
+        # Get some actual node IDs from the database
+        cursor.execute("SELECT node_id FROM nodes LIMIT 5")
+        actual_node_ids = [row[0] for row in cursor.fetchall()]
+        
+        if actual_node_ids:
+            print(f"Testing deletion with {len(actual_node_ids)} actual node IDs...")
+            
+            # Test the rowid query first
+            id_placeholders = ','.join('?' for _ in actual_node_ids)
+            rowid_query = f"SELECT rowid, node_id FROM nodes WHERE node_id IN ({id_placeholders})"
+            cursor.execute(rowid_query, actual_node_ids)
+            rowid_results = cursor.fetchall()
+            
+            print(f"Rowid query returned {len(rowid_results)} results:")
+            for rowid, node_id in rowid_results:
+                print(f"  - rowid: {rowid}, node_id: {node_id}")
+            
+            # Simulate delete query (but don't actually delete)
+            count_query = f"SELECT COUNT(*) FROM nodes WHERE node_id IN ({id_placeholders})"
+            cursor.execute(count_query, actual_node_ids)
+            would_delete_count = cursor.fetchone()[0]
+            print(f"Would delete {would_delete_count} nodes")
+        
+        # 6. Check for potential ID format issues
+        print(f"\n=== ID FORMAT ANALYSIS ===")
+        cursor.execute("SELECT DISTINCT type FROM nodes")
+        node_types = [row[0] for row in cursor.fetchall()]
+        print(f"Node types in database: {node_types}")
+        
+        for node_type in node_types:
+            cursor.execute("SELECT COUNT(*) FROM nodes WHERE type = ?", (node_type,))
+            type_count = cursor.fetchone()[0]
+            cursor.execute("SELECT node_id FROM nodes WHERE type = ? LIMIT 3", (node_type,))
+            sample_ids = [row[0] for row in cursor.fetchall()]
+            print(f"  - {node_type}: {type_count} nodes, sample IDs: {sample_ids}")
+        
+        conn.close()
+        
+    except Exception as e:
+        print(f"❌ Error during analysis: {e}")
+        import traceback
+        traceback.print_exc()
+
+if __name__ == "__main__":
+    debug_deletion_issue() 
\ No newline at end of file
diff --git a/dist/simtag-0.1.0-py3.12.egg b/dist/simtag-0.1.0-py3.12.egg
new file mode 100644
index 0000000..f3de4a0
Binary files /dev/null and b/dist/simtag-0.1.0-py3.12.egg differ
diff --git a/fix-node-file-paths.el b/fix-node-file-paths.el
new file mode 100755
index 0000000..7ec7535
--- /dev/null
+++ b/fix-node-file-paths.el
@@ -0,0 +1,149 @@
+;;; fix-node-file-paths.el --- Fix file paths for recovered nodes
+
+(message "=== ANALYZING NODE FILE PATHS ===")
+
+(defun org-supertag-analyze-node-file-paths ()
+  "Analyze file path issues in recovered nodes."
+  (interactive)
+  (let ((nodes-with-files 0)
+        (nodes-without-files 0)
+        (orphaned-nodes '())
+        (sample-nodes '()))
+    
+    (message "\n1. Analyzing node file paths...")
+    
+    ;; Check all nodes
+    (maphash (lambda (id props)
+               (when (eq (plist-get props :type) :node)
+                 (let ((file-path (plist-get props :file-path))
+                       (title (plist-get props :title)))
+                   (if (and file-path (not (string-empty-p file-path)))
+                       (cl-incf nodes-with-files)
+                     (cl-incf nodes-without-files)
+                     (push (cons id props) orphaned-nodes))
+                   
+                   ;; Collect sample for analysis
+                   (when (< (length sample-nodes) 10)
+                     (push (list :id id 
+                                :title title
+                                :file-path file-path
+                                :has-file (if (and file-path (not (string-empty-p file-path))) t nil))
+                           sample-nodes)))))
+             org-supertag-db--object)
+    
+    (message "   Nodes with files: %d" nodes-with-files)
+    (message "   Nodes without files: %d" nodes-without-files)
+    (message "   Sample node analysis:")
+    (dolist (node sample-nodes)
+      (message "     %s: %s [File: %s]" 
+               (plist-get node :id)
+               (or (plist-get node :title) "No Title")
+               (if (plist-get node :has-file) "✓" "✗")))
+    
+    ;; Check if orphaned nodes might be recoverable
+    (when orphaned-nodes
+      (message "\n2. Analyzing orphaned nodes...")
+      (message "   Found %d orphaned nodes (no file path)" (length orphaned-nodes))
+      
+      ;; Check if these might be actual file-based nodes
+      (let ((possible-file-nodes 0)
+            (probable-tag-nodes 0))
+        (dolist (node-pair orphaned-nodes)
+          (let* ((props (cdr node-pair))
+                 (title (plist-get props :title))
+                 (raw-value (plist-get props :raw-value)))
+            ;; Heuristic: if title looks like a heading, might be file-based
+            (if (and title 
+                     (or (string-match-p "\\[.*\\]" title)  ; Date/time format
+                         (> (length title) 20)              ; Long descriptive title
+                         (string-match-p "[A-Z]" title)))   ; Mixed case
+                (cl-incf possible-file-nodes)
+              (cl-incf probable-tag-nodes))))
+        
+        (message "     Possibly from files: %d" possible-file-nodes)
+        (message "     Probably tag-like: %d" probable-tag-nodes)))
+    
+    ;; Return analysis data
+    (list :nodes-with-files nodes-with-files
+          :nodes-without-files nodes-without-files
+          :orphaned-nodes orphaned-nodes
+          :sample-nodes sample-nodes)))
+
+(defun org-supertag-fix-orphaned-nodes-strategy ()
+  "Provide strategies to handle orphaned nodes."
+  (interactive)
+  (message "\n=== ORPHANED NODE STRATEGIES ===")
+  
+  (let ((analysis (org-supertag-analyze-node-file-paths)))
+    (let ((orphaned-count (plist-get analysis :nodes-without-files)))
+      
+      (if (= orphaned-count 0)
+          (message "✅ No orphaned nodes found!")
+        
+        (message "\n🔧 Available strategies:")
+        (message "")
+        (message "1. 🗑️  DELETE ORPHANED NODES")
+        (message "   - Remove nodes without file paths")
+        (message "   - Assume they're data artifacts")
+        (message "   - Risk: Lose potentially valid data")
+        (message "")
+        (message "2. 🏷️  CONVERT TO METADATA OBJECTS")
+        (message "   - Change type from :node to :tag or :metadata")
+        (message "   - Keep data but exclude from sync")
+        (message "   - Risk: Data type confusion")
+        (message "")
+        (message "3. 📁 ASSIGN PLACEHOLDER FILES")
+        (message "   - Set file-path to a placeholder")
+        (message "   - Include in sync but as special nodes")
+        (message "   - Risk: Confuse file-based logic")
+        (message "")
+        (message "4. 🔍 MANUAL INVESTIGATION")
+        (message "   - Check if these correspond to real org content")
+        (message "   - Try to locate original source")
+        (message "   - Risk: Time consuming")
+        
+        (message "\n💡 RECOMMENDATION:")
+        (if (> orphaned-count 10)
+            (message "   Many orphaned nodes (%d) - likely data corruption" orphaned-count)
+          (message "   Few orphaned nodes (%d) - worth investigating" orphaned-count))
+        
+        (message "   Suggested approach: DELETE orphaned nodes")
+        (message "   Reason: They lack file context and may be corrupted data")))))
+
+;; Execute analysis
+(org-supertag-analyze-node-file-paths)
+(org-supertag-fix-orphaned-nodes-strategy)
+
+(message "\nTo proceed with deletion, run:")
+(message "(org-supertag-delete-orphaned-nodes)")
+
+(defun org-supertag-delete-orphaned-nodes ()
+  "Delete nodes without file paths to clean up corrupted data."
+  (interactive)
+  (when (yes-or-no-p "Delete all nodes without file paths? This cannot be undone! ")
+    (let ((deleted-count 0)
+          (nodes-to-delete '()))
+      
+      ;; Collect orphaned nodes
+      (maphash (lambda (id props)
+                 (when (and (eq (plist-get props :type) :node)
+                           (or (null (plist-get props :file-path))
+                               (string-empty-p (plist-get props :file-path))))
+                   (push id nodes-to-delete)))
+               org-supertag-db--object)
+      
+      ;; Delete them
+      (dolist (id nodes-to-delete)
+        (org-supertag-db-remove-object id)
+        (cl-incf deleted-count))
+      
+      ;; Save database
+      (when (> deleted-count 0)
+        (org-supertag-db--mark-dirty)
+        (org-supertag-db-save))
+      
+      (message "Deleted %d orphaned nodes" deleted-count)
+      (message "Remaining nodes should have proper file paths")
+      deleted-count)))
+
+(message "\n=== ANALYSIS COMPLETED ===") 
\ No newline at end of file
diff --git a/fix-node-type-corruption.el b/fix-node-type-corruption.el
new file mode 100755
index 0000000..bc3f3a4
--- /dev/null
+++ b/fix-node-type-corruption.el
@@ -0,0 +1,143 @@
+;;; fix-node-type-corruption.el --- Fix corrupted node type data
+
+(message "=== FIXING NODE TYPE CORRUPTION ===")
+
+(defun org-supertag-fix-corrupted-node-types ()
+  "Fix objects that should be nodes but are marked as tags.
+Identifies objects with UUID-like IDs that are incorrectly typed as :tag
+and converts them back to :node type."
+  (interactive)
+  (let ((fixed-count 0)
+        (tag-count 0)
+        (error-count 0)
+        (preserved-tags '())
+        (converted-nodes '())
+        (failed-conversions '()))
+    
+    (message "\n1. Analyzing object types...")
+    
+    ;; First pass: identify and fix corrupted nodes
+    (maphash (lambda (id props)
+               (let ((type (plist-get props :type)))
+                 (cond
+                                     ;; Object marked as :tag but has UUID-like ID (should be node)
+                   ((and (eq type :tag)
+                         (string-match-p "^[0-9A-F]\\{8\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{4\\}-[0-9A-F]\\{12\\}$" id))
+                    (message "Converting corrupted tag to node: %s" id)
+                    
+                    ;; Create clean node properties (only keep valid node attributes)
+                    (let* ((title (or (plist-get props :name)
+                                     (plist-get props :title)
+                                     (plist-get props :raw-value)
+                                     "Recovered Node"))
+                           (raw-value (or (plist-get props :raw-value)
+                                         (plist-get props :title)
+                                         (plist-get props :name)
+                                         title))
+                           (created-at (or (plist-get props :created-at)
+                                          (current-time)))
+                           ;; Build clean node properties from scratch (ensure all required properties)
+                           (fixed-props (list :type :node
+                                            :id id
+                                            :title title
+                                            :raw-value raw-value
+                                            ;; Required properties for :node type
+                                            :file-path nil
+                                            :pos 1
+                                            :olp nil  ; Required: outline path
+                                            :level 1
+                                            ;; Optional but common properties
+                                            :tags nil
+                                            :todo-type nil
+                                            :priority nil
+                                            :properties nil
+                                            :begin 1
+                                            :contents-begin nil
+                                            :contents-end nil
+                                            :created-at created-at
+                                            :modified-at (current-time))))
+                      
+                      ;; Debug: Verify properties before adding
+                      (let ((required-props '(:id :title :file-path :pos :olp :level)))
+                        (dolist (prop required-props)
+                          (unless (plist-member fixed-props prop)
+                            (message "Warning: Missing required property %s for node %s" prop id))))
+                      
+                      ;; Update in database
+                      (condition-case err
+                          (progn
+                            (org-supertag-db-add id fixed-props)
+                            (push (cons id fixed-props) converted-nodes)
+                            (cl-incf fixed-count)
+                            (message "Successfully converted: %s -> %s" id title))
+                                                 (error
+                          (message "Failed to convert %s: %s" id (error-message-string err))
+                          (push (cons id (error-message-string err)) failed-conversions)
+                          (cl-incf error-count)))))
+                  
+                  ;; Real tags (keep as-is)
+                  ((eq type :tag)
+                   (cl-incf tag-count)
+                   (push (cons id props) preserved-tags))
+                  
+                  ;; Other types (keep as-is)
+                  (t
+                   nil))))
+             org-supertag-db--object)
+    
+    (message "   Fixed %d corrupted nodes" fixed-count)
+    (message "   Preserved %d real tags" tag-count)
+    (message "   Failed conversions: %d" error-count)
+    
+    (when (> error-count 0)
+      (message "   Failed conversion details:")
+      (dolist (failure (seq-take failed-conversions 5))
+        (message "     %s: %s" (car failure) (cdr failure))))
+    
+    (when converted-nodes
+      (message "   Sample converted nodes:")
+      (dolist (node (seq-take converted-nodes 5))
+        (let ((id (car node))
+              (props (cdr node)))
+          (message "     %s: %s" id (plist-get props :title)))))
+    
+    ;; Force database save
+    (when (> fixed-count 0)
+      (message "\n2. Saving corrected database...")
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db-save))
+    
+    (message "\n=== CORRUPTION FIX COMPLETED ===")
+    (message "Total objects fixed: %d" fixed-count)
+    (message "Failed conversions: %d" error-count)
+    (if (= error-count 0)
+        (message "✅ All corrupted nodes successfully converted!")
+      (message "⚠️  Some conversions failed - check messages above"))
+    (message "Database should now have correct node types!")
+    
+    fixed-count))
+
+;; Execute the fix
+(org-supertag-fix-corrupted-node-types)
+
+(message "\n=== POST-FIX VERIFICATION ===")
+
+;; Verify fix results
+(let ((node-count 0)
+      (tag-count 0)
+      (other-count 0))
+  (maphash (lambda (id props)
+             (let ((type (plist-get props :type)))
+               (cond
+                ((eq type :node) (cl-incf node-count))
+                ((eq type :tag) (cl-incf tag-count))
+                (t (cl-incf other-count)))))
+           org-supertag-db--object)
+  
+  (message "Final object count:")
+  (message "  Nodes: %d" node-count)
+  (message "  Tags: %d" tag-count)
+  (message "  Other: %d" other-count)
+  (message "  Total: %d" (+ node-count tag-count other-count)))
+
+(message "\nRun org-supertag-background-sync again to test incremental sync!") 
\ No newline at end of file
diff --git a/fix-sync-cleanup-issue.el b/fix-sync-cleanup-issue.el
new file mode 100755
index 0000000..55caadb
--- /dev/null
+++ b/fix-sync-cleanup-issue.el
@@ -0,0 +1,286 @@
+;;; fix-sync-cleanup-issue.el --- Complete solution to fix sync cleanup issues
+
+(require 'org-supertag-sync)
+(require 'org-supertag-db)
+(require 'org-supertag-background-sync)
+
+(defun fix-sync-cleanup-issue ()
+  "Complete solution to fix sync cleanup issues.
+This function will:
+1. Stop auto-sync to prevent further damage
+2. Clean up problematic nodes in the database (nodes with nil file-path)
+3. Rescan sync directories and rebuild the database
+4. Rebuild the sync hash baseline
+5. Restart the sync system"
+  (interactive)
+  (when (yes-or-no-p "This will completely rebuild the database and sync state. Are you sure you want to continue?")
+    (message "\n=== Starting sync cleanup issue fix ===")
+
+    ;; 1. Stop auto-sync
+    (message "\n1. Stopping auto-sync...")
+    (when (boundp 'org-supertag-sync--timer)
+      (when org-supertag-sync--timer
+        (cancel-timer org-supertag-sync--timer)
+        (setq org-supertag-sync--timer nil)))
+    (message "   ✓ Auto-sync stopped")
+
+    ;; 2. Clean up corrupted nodes
+    (message "\n2. Cleaning up corrupted nodes (file-path is nil)...")
+    (let ((cleaned-count 0)
+          (nodes-to-clean '()))
+      (maphash (lambda (id node)
+                 (when (and (eq (plist-get node :type) :node)
+                           (null (plist-get node :file-path)))
+                   (push id nodes-to-clean)))
+               org-supertag-db--object)
+      (dolist (id nodes-to-clean)
+        (org-supertag-db-remove-object id)
+        (cl-incf cleaned-count))
+      (message "   ✓ Cleaned up %d corrupted nodes" cleaned-count))
+
+    ;; 3. Clear and rebuild sync state
+    (message "\n3. Rebuilding sync state...")
+    (clrhash org-supertag-sync--state)
+    (message "   ✓ Sync state cleared")
+
+    ;; 4. Rescan sync directories
+    (message "\n4. Rescanning sync directories...")
+    (let ((total-files 0)
+          (processed-files 0)
+          (errors '()))
+
+      ;; Scan all configured sync directories
+      (dolist (dir org-supertag-sync-directories)
+        (when (file-exists-p dir)
+          (message "   Scanning directory: %s" dir)
+          (let ((files (directory-files-recursively dir "\\.org$")))
+            (cl-incf total-files (length files))
+            (dolist (file files)
+              (condition-case err
+                  (when (org-supertag-sync--in-sync-scope-p file)
+                    ;; Update sync state
+                    (org-supertag-sync-update-state file)
+                    ;; Parse nodes in the file
+                    (with-current-buffer (find-file-noselect file)
+                      (let ((node-count 0))
+                        (org-map-entries
+                         (lambda ()
+                           (when (org-id-get)
+                             (condition-case parse-err
+                                 (when-let* ((props (org-supertag-db--parse-node-at-point)))
+                                   (org-supertag-db-add (plist-get props :id) props)
+                                   (cl-incf node-count))
+                               (error
+                                (message "Warning: Error parsing node in %s: %s"
+                                         file (error-message-string parse-err))))))
+                         t nil)
+                        (message "     Processed %d nodes" node-count)))
+                    (cl-incf processed-files))
+                (error
+                 (push (cons file (error-message-string err)) errors)
+                 (message "Error: Error processing file %s: %s" file (error-message-string err))))))))
+
+      (message "   ✓ Scan complete: %d files, %d files processed" total-files processed-files)
+      (when errors
+        (message "   ⚠️  %d files failed to process" (length errors))))
+
+    ;; 5. Save sync state
+    (message "\n5. Saving sync state...")
+    (org-supertag-sync-save-state)
+    (message "   ✓ Sync state saved: %d files" (hash-table-count org-supertag-sync--state))
+
+    ;; 6. Rebuild hash baseline
+    (message "\n6. Rebuilding sync hash baseline...")
+    (when (fboundp 'org-supertag-background-sync-rebuild-baseline)
+      (condition-case err
+          (progn
+            (org-supertag-background-sync-rebuild-baseline)
+            (message "   ✓ Hash baseline rebuilt"))
+        (error
+         (message "   ⚠️  Hash baseline rebuild failed: %s" (error-message-string err)))))
+
+    ;; 7. Verify fix results
+    (message "\n7. Verifying fix results...")
+    (let ((node-count 0)
+          (valid-nodes 0)
+          (invalid-nodes 0))
+      (maphash (lambda (id node)
+                 (when (eq (plist-get node :type) :node)
+                   (cl-incf node-count)
+                   (if (plist-get node :file-path)
+                       (cl-incf valid-nodes)
+                     (cl-incf invalid-nodes))))
+               org-supertag-db--object)
+      (message "   Database status:")
+      (message "     Total nodes: %d" node-count)
+      (message "     Valid nodes: %d" valid-nodes)
+      (message "     Invalid nodes: %d" invalid-nodes)
+      (message "     Sync state files: %d" (hash-table-count org-supertag-sync--state)))
+
+    ;; 8. Restart sync
+    (message "\n8. Restarting sync system...")
+    (when (fboundp 'org-supertag-sync-start-auto-sync)
+      (org-supertag-sync-start-auto-sync))
+    (message "   ✓ Sync system restarted")
+
+    (message "\n=== Fix complete ===")
+    (message "Suggestions:")
+    (message "- Run (debug-sync-cleanup-issue) to verify status again")
+    (message "- Run (org-supertag-background-sync-status) to check sync status")
+    (message "- If issues persist, manual inspection of specific org files may be needed")))
+
+(defun fix-cleanup-nil-file-paths ()
+  "Specifically cleans up nodes with nil file-path.
+This is a safe cleanup operation, only removing clearly corrupted data."
+  (interactive)
+  (let ((cleaned-count 0)
+        (nodes-to-clean '()))
+    (message "\n=== Cleaning up nodes with nil file-path ===")
+
+    ;; 1. Find all nodes with nil file-path
+    (maphash (lambda (id node)
+               (when (and (eq (plist-get node :type) :node)
+                         (null (plist-get node :file-path)))
+                 (push (cons id node) nodes-to-clean)))
+             org-supertag-db--object)
+
+    (if (null nodes-to-clean)
+        (message "✓ No nodes with nil file-path found")
+      (progn
+        (message "Found %d nodes with nil file-path:" (length nodes-to-clean))
+        (dolist (entry nodes-to-clean)
+          (let ((id (car entry))
+                (node (cdr entry)))
+            (message "  - %s (Title: %s)" id (plist-get node :title))))
+
+        (when (yes-or-no-p (format "Delete these %d corrupted nodes?" (length nodes-to-clean)))
+          (dolist (entry nodes-to-clean)
+            (let ((id (car entry)))
+              (org-supertag-db-remove-object id)
+              (cl-incf cleaned-count)))
+          (message "✓ Cleaned up %d corrupted nodes" cleaned-count))))
+
+    cleaned-count))
+
+(defun fix-prevent-auto-cleanup ()
+  "Temporarily disable auto-cleanup to prevent further node loss."
+  (interactive)
+  (message "\n=== Disabling auto-cleanup function ===")
+
+  ;; Stop auto-sync
+  (when (boundp 'org-supertag-sync--timer)
+    (when org-supertag-sync--timer
+      (cancel-timer org-supertag-sync--timer)
+      (setq org-supertag-sync--timer nil)
+      (message "✓ Auto-sync stopped")))
+
+  ;; Temporarily redefine cleanup functions to no-ops
+  (fset 'org-supertag-sync--cleanup-database
+        (lambda ()
+          (message "Cleanup function has been temporarily disabled to prevent data loss")))
+
+  (fset 'org-supertag-sync-validate-and-cleanup-zombie-nodes
+        (lambda ()
+          (message "Zombie node cleanup function has been temporarily disabled")
+          0))
+
+  (message "✓ Cleanup function has been temporarily disabled")
+  (message "Note: To re-enable, you need to reload org-supertag-sync.el"))
+
+(defun fix-restore-from-files ()
+  "Rebuild the entire database from the file system.
+This is the safest recovery method, directly rereading all data from org files."
+  (interactive)
+  (when (yes-or-no-p "This will clear the database and rebuild from org files. Are you sure you want to continue?")
+    (message "\n=== Rebuilding database from files ===")
+
+    ;; 1. Backup current database
+    (message "\n1. Backing up current database...")
+    (when (fboundp 'org-supertag-db-backup)
+      (org-supertag-db-backup))
+
+    ;; 2. Clear database (preserve non-node data)
+    (message "\n2. Clearing node data...")
+    (let ((preserved-count 0)
+          (removed-count 0))
+      (maphash (lambda (id entity)
+                 (let ((type (plist-get entity :type)))
+                   (if (eq type :node)
+                       (progn
+                         (ht-remove! org-supertag-db--object id)
+                         (cl-incf removed-count))
+                     (cl-incf preserved-count))))
+               (ht-copy org-supertag-db--object))
+      (message "   ✓ Removed %d nodes, preserved %d other entities" removed-count preserved-count))
+
+    ;; 3. Rescan and import all nodes
+    (message "\n3. Rescanning org files...")
+    (let ((total-nodes 0)
+          (error-files '()))
+      (dolist (dir org-supertag-sync-directories)
+        (when (file-exists-p dir)
+          (message "   Scanning directory: %s" dir)
+          (let ((files (directory-files-recursively dir "\\.org$")))
+            (dolist (file files)
+              (condition-case err
+                  (when (org-supertag-sync--in-sync-scope-p file)
+                    (with-current-buffer (find-file-noselect file)
+                      (let ((file-nodes 0))
+                        (org-map-entries
+                          (lambda ()
+                            (when (org-id-get)
+                              (condition-case err
+                                  (when-let* ((props (org-supertag-db--parse-node-at-point)))
+                                    (org-supertag-db-add (plist-get props :id) props)
+                                    (cl-incf file-nodes)
+                                    (cl-incf total-nodes))
+                                (error
+                                 (let ((id (org-id-get)))
+                                   (message "Error processing node %s in %s: %s"
+                                           id file (error-message-string err)))))))
+                          t nil)
+                        (when (> file-nodes 0)
+                          (message "     %s: %d nodes" file file-nodes)))))
+                (error
+                 (push file error-files)
+                 (message "     Error: %s - %s" file (error-message-string err))))))))
+
+      (message "\n✓ Rebuild complete:")
+      (message "   Total nodes: %d" total-nodes)
+      (when error-files
+        (message "   Error files: %d" (length error-files))))
+
+    ;; 4. Rebuild sync state and hash baseline
+    (message "\n4. Rebuilding sync infrastructure...")
+    (org-supertag-sync-save-state)
+    (when (fboundp 'org-supertag-background-sync-rebuild-baseline)
+      (org-supertag-background-sync-rebuild-baseline))
+
+    (message "\n=== Rebuild complete ===")
+    (message "Database completely rebuilt from org files")))
+
+(defun test-node-creation ()
+  "Test if node creation meets requirements."
+  (interactive)
+  (when (and (derived-mode-p 'org-mode) (org-at-heading-p))
+    (condition-case err
+        (if-let* ((props (org-supertag-db--parse-node-at-point)))
+            (progn
+              (message "✓ Node parsed successfully!")
+              (message "  ID: %s" (plist-get props :id))
+              (message "  Title: %s" (plist-get props :title))
+              (message "  File: %s" (plist-get props :file-path))
+              (message "  Level: %s" (plist-get props :level))
+              (message "  Position: %s" (plist-get props :pos))
+              (message "  Outline path: %S" (plist-get props :olp))
+
+              ;; Test adding to database
+              (org-supertag-db-add (plist-get props :id) props)
+              (message "✓ Node added to database successfully!")
+              props)
+          (message "❌ Could not parse current node (ID might be missing)"))
+      (error
+       (message "❌ Node processing failed: %s" (error-message-string err))
+       nil))))
+
+(provide 'fix-sync-cleanup-issue)
\ No newline at end of file
diff --git a/fix-sync-hash-issue.el b/fix-sync-hash-issue.el
new file mode 100755
index 0000000..19588f7
--- /dev/null
+++ b/fix-sync-hash-issue.el
@@ -0,0 +1,126 @@
+;;; Fix sync hash issue - ensure incremental sync works properly
+
+(require 'org-supertag-background-sync)
+
+(defun fix-sync-hash-issue ()
+  "Fix the sync_hashes.json file not being created issue."
+  (interactive)
+  (message "\n=== Fixing Sync Hash Issue ===")
+  
+  ;; 1. Ensure data directory exists
+  (unless (file-exists-p org-supertag-data-directory)
+    (make-directory org-supertag-data-directory t)
+    (message "Created data directory: %s" org-supertag-data-directory))
+  
+  ;; 2. Force a manual sync to establish baseline hashes
+  (message "\nForcing initial sync to establish baseline...")
+  
+  ;; 3. Calculate hashes for all current objects
+  (let ((node-count 0)
+        (tag-count 0)
+        (link-count 0))
+    
+    ;; Clear existing hashes
+    (clrhash org-supertag-background-sync--last-sync-hashes)
+    
+    ;; Process all objects in the database
+    (maphash
+     (lambda (id props)
+       (let ((hash (org-supertag-background-sync--calculate-object-hash props))
+             (type (plist-get props :type)))
+         (when hash
+           (puthash id hash org-supertag-background-sync--last-sync-hashes)
+           (cond
+            ((eq type :node) (cl-incf node-count))
+            ((eq type :tag) (cl-incf tag-count))))))
+     org-supertag-db--object)
+    
+    ;; Process all links
+    (maphash
+     (lambda (id props)
+       (let ((hash (org-supertag-background-sync--calculate-object-hash props)))
+         (when hash
+           (puthash id hash org-supertag-background-sync--last-sync-hashes)
+           (cl-incf link-count))))
+     org-supertag-db--link)
+    
+    (message "Calculated hashes for: %d nodes, %d tags, %d links" 
+             node-count tag-count link-count))
+  
+  ;; 4. Save the hashes
+  (condition-case err
+      (progn
+        (org-supertag-background-sync--save-hashes)
+        (message "\n✅ Hash file created successfully!")
+        
+        ;; Verify the file
+        (if (file-exists-p org-supertag-background-sync--hash-file)
+            (let ((size (file-attribute-size 
+                        (file-attributes org-supertag-background-sync--hash-file))))
+              (message "   File: %s" org-supertag-background-sync--hash-file)
+              (message "   Size: %d bytes" size)
+              (message "   Hash count: %d" 
+                       (hash-table-count org-supertag-background-sync--last-sync-hashes)))
+          (message "❌ Hash file still doesn't exist!")))
+    (error
+     (message "❌ Error saving hashes: %s" (error-message-string err))))
+  
+  ;; 5. Update sync statistics to reflect current state
+  (setq org-supertag-background-sync--last-sync-time (current-time))
+  (message "\nSync state updated. Next sync will be incremental."))
+
+(defun verify-incremental-sync ()
+  "Verify that incremental sync is working."
+  (interactive)
+  (message "\n=== Verifying Incremental Sync ===")
+  
+  ;; Check if hash file exists
+  (if (file-exists-p org-supertag-background-sync--hash-file)
+      (progn
+        (message "✅ Hash file exists: %s" org-supertag-background-sync--hash-file)
+        ;; Load hashes
+        (org-supertag-background-sync--load-hashes)
+        (message "   Loaded %d hashes" 
+                 (hash-table-count org-supertag-background-sync--last-sync-hashes))
+        
+        ;; Test change detection
+        (let ((changes (org-supertag-background-sync--get-changed-objects)))
+          (message "\nChange detection results:")
+          (message "   Nodes to upsert: %d" (length (nth 0 changes)))
+          (message "   Tags to upsert: %d" (length (nth 1 changes)))
+          (message "   Links to upsert: %d" (length (nth 2 changes)))
+          (message "   IDs to delete: %d" (length (nth 3 changes)))
+          
+          (if (= 0 (+ (length (nth 0 changes))
+                     (length (nth 1 changes))
+                     (length (nth 2 changes))
+                     (length (nth 3 changes))))
+              (message "\n✅ Incremental sync is working! No changes detected.")
+            (message "\n⚠️  Changes detected. This is normal if you've modified data."))))
+    (message "❌ Hash file doesn't exist. Run fix-sync-hash-issue first!")))
+
+(defun force-reset-sync-hashes ()
+  "Force reset all sync hashes to establish a new baseline.
+This will clear the hash file and recalculate all hashes."
+  (interactive)
+  (when (yes-or-no-p "This will reset all sync hashes. The next sync will process all objects. Continue? ")
+    (message "\n=== Force Resetting Sync Hashes ===")
+    
+    ;; 1. Delete existing hash file
+    (when (file-exists-p org-supertag-background-sync--hash-file)
+      (delete-file org-supertag-background-sync--hash-file)
+      (message "Deleted existing hash file"))
+    
+    ;; 2. Clear in-memory hashes
+    (clrhash org-supertag-background-sync--last-sync-hashes)
+    
+    ;; 3. Recalculate and save new hashes
+    (fix-sync-hash-issue)
+    
+    (message "\nSync hashes have been reset. Next sync will be incremental from this baseline.")))
+
+;; Provide interactive commands
+(message "\nAvailable commands:")
+(message "  M-x fix-sync-hash-issue       - Fix the missing hash file issue")
+(message "  M-x verify-incremental-sync   - Verify incremental sync is working")
+(message "  M-x force-reset-sync-hashes   - Force reset all hashes (use with caution)") 
\ No newline at end of file
diff --git a/fix_data_sync.el b/fix_data_sync.el
new file mode 100755
index 0000000..2de429a
--- /dev/null
+++ b/fix_data_sync.el
@@ -0,0 +1,124 @@
+;;; fix-data-sync.el --- Force complete data synchronization
+
+(message "=== FORCE COMPLETE SYNC ===")
+
+(defun org-supertag-force-complete-sync ()
+  "Force a complete synchronization of all data to Python backend."
+  (interactive)
+  (message "\n1. Checking current database state...")
+  
+  ;; 统计当前数据
+  (let ((total-objects (hash-table-count org-supertag-db--object))
+        (total-links (hash-table-count org-supertag-db--link))
+        (nodes-count 0)
+        (tags-count 0)
+        (other-count 0))
+    
+    ;; 统计各类型对象
+    (maphash (lambda (_id props)
+               (let ((type (plist-get props :type)))
+                 (cond
+                  ((eq type :node) (cl-incf nodes-count))
+                  ((eq type :tag) (cl-incf tags-count))
+                  (t (cl-incf other-count)))))
+             org-supertag-db--object)
+    
+    (message "   Total objects: %d" total-objects)
+    (message "   - Nodes: %d" nodes-count)
+    (message "   - Tags: %d" tags-count)
+    (message "   - Others: %d" other-count)
+    (message "   - Links: %d" total-links)
+    
+    ;; 检查Python后端是否就绪
+    (message "\n2. Checking Python backend status...")
+    (unless (and (featurep 'org-supertag-bridge)
+                 (boundp 'org-supertag-bridge--ready-p)
+                 org-supertag-bridge--ready-p)
+      (message "   Python backend not ready. Starting...")
+      (org-supertag-init-vectorization)
+      (sleep-for 3))
+    
+    (if (and (boundp 'org-supertag-bridge--ready-p)
+             org-supertag-bridge--ready-p)
+        (message "   ✓ Python backend is ready")
+      (error "❌ Python backend failed to start"))
+    
+    ;; 准备完整快照数据
+    (message "\n3. Preparing complete snapshot...")
+    (let ((all-nodes '())
+          (all-tags '())
+          (all-links '()))
+      
+      ;; 收集所有节点和标签
+      (maphash (lambda (id props)
+                 (let ((type (plist-get props :type)))
+                   (cond
+                    ((eq type :node)
+                     (push props all-nodes))
+                    ((eq type :tag)
+                     (push props all-tags)))))
+               org-supertag-db--object)
+      
+      ;; 收集所有链接
+      (maphash (lambda (_id link-props)
+                 (push link-props all-links))
+               org-supertag-db--link)
+      
+      (message "   Collected: %d nodes, %d tags, %d links"
+               (length all-nodes) (length all-tags) (length all-links))
+      
+      ;; 构建完整快照
+      (let ((snapshot-data `(("nodes" . ,all-nodes)
+                            ("tags" . ,all-tags)
+                            ("links" . ,all-links)
+                            ("ids_to_delete" . [])  ; 空数组，不删除任何东西
+                            ("sync_timestamp" . ,(format-time-string "%Y-%m-%dT%H:%M:%SZ" (current-time) t)))))
+        
+        (message "\n4. Sending complete snapshot to Python backend...")
+        (message "   Snapshot size: %d total items" 
+                 (+ (length all-nodes) (length all-tags) (length all-links)))
+        
+        ;; 发送数据
+        (condition-case err
+            (let ((result (org-supertag-api-bulk-process-snapshot
+                          snapshot-data
+                          (lambda (sync-result)
+                            (if (and sync-result (equal (plist-get sync-result :status) "success"))
+                                (progn
+                                  (message "   ✓ Complete sync successful!")
+                                  (message "   Result: %S" (plist-get sync-result :result))
+                                  
+                                  ;; 重置哈希表以触发重新计算
+                                  (message "\n5. Updating sync hashes...")
+                                  (clrhash org-supertag-background-sync--last-sync-hashes)
+                                  
+                                  ;; 为所有对象计算并保存新哈希
+                                  (maphash (lambda (id props)
+                                             (let ((hash (org-supertag-background-sync--calculate-object-hash props)))
+                                               (when hash
+                                                 (puthash id hash org-supertag-background-sync--last-sync-hashes))))
+                                           org-supertag-db--object)
+                                  
+                                  (maphash (lambda (id props)
+                                             (let ((hash (org-supertag-background-sync--calculate-object-hash props)))
+                                               (when hash
+                                                 (puthash id hash org-supertag-background-sync--last-sync-hashes))))
+                                           org-supertag-db--link)
+                                  
+                                  (org-supertag-background-sync--save-hashes)
+                                  (message "   ✓ Sync hashes updated: %d records"
+                                           (hash-table-count org-supertag-background-sync--last-sync-hashes))
+                                  
+                                  (message "\n=== COMPLETE SYNC FINISHED SUCCESSFULLY ==="))
+                              (error "Complete sync failed: %s" 
+                                     (or (plist-get sync-result :message) "Unknown error")))))))
+              
+              (setq org-supertag-background-sync--last-sync-time (current-time))
+              result)
+          (error
+           (message "❌ Error during complete sync: %s" (error-message-string err))
+           (error "Complete sync failed: %s" (error-message-string err))))))))
+
+;; 运行完整同步
+(message "Starting force complete sync...")
+(org-supertag-force-complete-sync) 
\ No newline at end of file
diff --git a/org-supertag AI Workflow.org b/org-supertag AI Workflow.org
new file mode 100644
index 0000000..61aa568
--- /dev/null
+++ b/org-supertag AI Workflow.org	
@@ -0,0 +1,198 @@
+#+TITLE: org-supertag-ai Usage Documentation
+#+AUTHOR: yibie
+#+DATE: 2025-05-02
+
+* Introduction
+`org-supertag-ai.el` provides an AI workflow engine integrated with Org mode. It allows users to define and execute complex workflows directly within Org files by leveraging Org headline properties. The engine can execute nodes sequentially or based on defined logic, interact with Large Language Models (LLMs) via EPC (External Process Communication), call Elisp functions, execute behaviors, handle user input/output, and manage conditional logic.
+
+* Dependencies
+** Emacs Packages
+- `cl-lib`: Common Lisp extensions.
+- `org`: Org mode core.
+- `org-element`: Org element parsing library.
+- `org-id`: Org ID management.
+- `org-supertag-behavior`: Required if using `behavior` node types. Assumes the function `org-supertag-behavior-execute` is available.
+- `org-supertag-sim-epc`: Required if using `llm` node types for interaction with Ollama via the SimTag EPC server.
+
+** External
+- A running Python environment with the `simtag` package installed (for `org-supertag-sim-epc`).
+- A running Ollama instance (or compatible service) if using the default LLM integration.
+
+* Installation
+1. Ensure all dependency packages are installed.
+2. Place `org-supertag-ai.el` in your Emacs `load-path`.
+3. Add the following to your Emacs configuration file (e.g., `init.el`):
+   #+BEGIN_SRC emacs-lisp
+   (require 'org-supertag-ai)
+   #+END_SRC
+4. (Optional) Customize the data directory:
+   #+BEGIN_SRC emacs-lisp
+   (setq org-supertag-ai-data-dir "/path/to/your/ai-data")
+   #+END_SRC
+
+* Core Concepts
+
+** Workflow Nodes
+- Workflows are sequences of operations defined as Org headlines.
+- Each headline represents a node in the workflow.
+- The behavior and connections of each node are determined by its Org properties.
+
+** Node Properties
+- Standard Org properties (key-value pairs under the `:PROPERTIES:` drawer) define everything about a node.
+- Key properties include `:NODE_TYPE:`, `:ID:`, and `:AI_SUCCESSORS:`.
+
+** Node Types (`:NODE_TYPE:`)
+The engine supports several types of nodes, specified by the `:NODE_TYPE:` property:
+  - `llm`: Interacts with an LLM (via EPC) using a prompt template.
+  - `function`: Executes a specified Emacs Lisp function.
+  - `behavior`: Executes a predefined behavior using `org-supertag-behavior-execute`. Requires an `:ID:` property on the node.
+  - `input`: Prompts the user for input in the minibuffer.
+  - `output`: Displays information, typically rendered from a template.
+  - `conditional`: Evaluates an Elisp expression to determine the next step (action).
+
+** Shared Context
+- A hash table (`shared-context-hash`) is passed between nodes, allowing data sharing.
+- Nodes can read data from previous nodes specified in `:AI_CONTEXT_SOURCES:`.
+- Nodes can write their primary result to the context, accessible via the key `"last_result"`.
+- Specific node types (like `input`) can store results under custom keys defined by `:AI_CONTEXT_KEY:`.
+- The context always contains:
+    - `"last_result"`: The primary result of the previously executed node.
+    - `"last_action"`: The action string returned by the previous node (used for successor logic).
+    - `"current_node_title"`: The title of the node currently being executed.
+    - `"current_node_id"`: The `:ID:` property of the node currently being executed (if present).
+
+** Output and Thought Blocks
+- `:AI_OUTPUT_BLOCK:`: Specifies the name of an Org dynamic block where the main result of the node (e.g., LLM response) will be written. The block type typically matches the block name (e.g., `#+BEGIN_LLMResponse LLMResponse`).
+- `:AI_THOUGHTS_BLOCK:`: Specifies the name of an Org dynamic block where internal processing details, logs, or intermediate "thoughts" of the node execution are stored. Useful for debugging.
+
+* Defining Workflows
+
+A workflow node is an Org headline with a `:PROPERTIES:` drawer:
+
+#+BEGIN_SRC org
+,* My Workflow Node Title
+:PROPERTIES:
+:NODE_TYPE:        <node_type_string>
+:ID:               <unique_node_id_string>
+:AI_SUCCESSORS:    <successor_logic_string>
+; ... other properties specific to NODE_TYPE ...
+:END:
+#+END_SRC
+
+** Standard Property Keys
+
+These are the core properties used across different node types:
+
+- `:NODE_TYPE:` (String): Mandatory. Defines the node's type (e.g., `"llm"`, `"function"`, `"input"`).
+- `:ID:` (String): Highly recommended. A unique identifier for the node. Used for finding nodes (especially by successors or context sources) and required by `behavior` nodes. Org IDs created with `M-x org-id-get-create` are ideal.
+- `:AI_SUCCESSORS:` (String - Elisp Alist): Optional. Defines the next node(s) to execute based on the `action` returned by the current node. The format is a string representing an association list: `"((\"action1\" . \"next_node_id_1\") (\"action2\" . \"next_node_id_2\") (\"default\" . \"default_node_id\") (\"*\" . \"catch_all_node_id\"))"`.
+    - The engine looks for a match in this order: Specific action -> `"default"` -> `"*"` (catch-all).
+    - If no successor is found via this property, the engine attempts to execute the next sibling headline in the Org file. If no suitable successor is found, the workflow ends.
+- `:AI_CONTEXT_SOURCES:` (String - Elisp Alist): Optional. Specifies where to fetch context data *from*. The format is a string representing an association list: `"((\"source_node_id_1\" . \"block_name_1\") (\"source_node_id_2\" . \"block_name_2\"))"`.
+    - For each pair, the engine finds the node `source_node_id` and reads the content of the dynamic block named `block_name`.
+    - The content is added to the current node's context hash table with a key formatted as `"context_<BlockName>"` (e.g., `"context_LLMResponse"`).
+- `:AI_OUTPUT_BLOCK:` (String): Optional. The name for the dynamic block where the node's primary result (e.g., LLM output, function return value) will be written within the current node's content area. E.g., `"LLMResult"`.
+- `:AI_THOUGHTS_BLOCK:` (String): Optional. The name for the dynamic block where execution details and logs for the current node will be written. E.g., `"NodeThoughts"`.
+
+** Node-Specific Properties
+
+*** LLM Node (`:NODE_TYPE: "llm"`)
+  - `:AI_PROMPT:` (String): Mandatory. A template string for the main prompt sent to the LLM. Can contain placeholders like `{{variable_name}}` which will be replaced by values from the context hash.
+  - `:AI_SYSTEM_PROMPT:` (String): Optional. A template string for the system prompt. Also supports `{{variable_name}}` templating.
+  - `:AI_MODEL:` (String): Optional. Specifies the LLM model name (e.g., `"gemma:2b"`). If omitted, the default model configured in the EPC server is used. *Note: Currently, the Elisp side only verifies this model exists; it doesn't dynamically set it per call in the backend `run_ollama` function.*
+
+*** Function Node (`:NODE_TYPE: "function"`)
+  - `:AI_FUNCTION:` (String): Mandatory. The name of the Elisp function to execute (e.g., `"my-workflow-function"`).
+  - *Function Signature*: The specified Elisp function MUST accept two arguments: `(node-element context-hash)` where `node-element` is the Org element data structure for the current headline, and `context-hash` is the current shared context hash table.
+  - *Function Return Value*: The function MUST return a cons cell `(action . result)`, where `action` is a string indicating the outcome (used for successor logic, e.g., `"success"`, `"failure"`, `"custom_action"`, or `"default"`) and `result` is the primary output of the function (this value will be stored in `"last_result"` in the context).
+
+*** Behavior Node (`:NODE_TYPE: "behavior"`)
+  - `:AI_BEHAVIOR_NAME:` (String): Mandatory. The name of the behavior to execute.
+  - `:AI_BEHAVIOR_PARAMS:` (String): Optional. Parameters to pass to the behavior function.
+  - `:ID:` (String): *Mandatory for Behavior nodes*. The node's ID is passed to `org-supertag-behavior-execute`.
+  - *Dependency*: Relies on `org-supertag-behavior-execute` function being defined and loaded.
+  - *Return*: Assumes success and returns `("default" . nil)`. Error messages are logged and stored in the thoughts block.
+
+*** Input Node (`:NODE_TYPE: "input"`)
+  - `:AI_PROMPT:` (String): Mandatory. The prompt string displayed to the user in the minibuffer (e.g., `"Enter your name:"`).
+  - `:AI_CONTEXT_KEY:` (String - Elisp String): Optional. A string (read from the property value, e.g., `"\"user_name\""`) specifying the key under which the user's input will be stored in the shared context hash table *in addition* to being stored in `"last_result"`.
+
+*** Output Node (`:NODE_TYPE: "output"`)
+  - `:AI_TEMPLATE:` (String): Mandatory. A template string to be rendered. Supports `{{variable_name}}` placeholders using values from the context.
+  - `:AI_OUTPUT_TARGET:` (String): Optional. Specifies where to display the rendered output. Currently, only `"message"` is supported (displays in the echo area/`*Messages*` buffer). Defaults to `"message"`.
+  - `:AI_CONTEXT_SOURCES:` (String - Elisp Alist): Typically used here to fetch data needed for the template (e.g., fetching an LLM response from a previous node).
+
+*** Conditional Node (`:NODE_TYPE: "conditional"`)
+  - `:AI_CONDITIONS:` (String - Elisp Code String): Mandatory. A string containing an Elisp expression *that evaluates to an action string*. This expression is evaluated within a lambda that receives the `context` hash table as its single argument. Example: `"\"(if (> (string-to-number (gethash \\\"user_age\\\" context)) 18) \\\"adult\\\" \\\"minor\\\")\""`. Note the escaping of quotes.
+  - *Return*: The node itself doesn't produce a primary result (`"last_result"` will likely be from the previous node). Its purpose is to generate an `action` string based on the evaluated condition, which is then used by `:AI_SUCCESSORS:` logic to determine the next node.
+
+* Running Workflows
+
+** Interactive Command
+- `M-x org-supertag-ai-workflow-run`
+- Prompts for the starting node's ID or exact title.
+- Executes the workflow starting from that node in the current buffer.
+
+** Programmatic Function
+- `(org-supertag-ai-workflow-run START-NODE-IDENTIFIER &optional INITIAL-CONTEXT BUFFER)`
+- `START-NODE-IDENTIFIER` (String): The ID or title of the starting node.
+- `INITIAL-CONTEXT` (Hash Table): Optional. A hash table to pre-populate the shared context.
+- `BUFFER` (Buffer): Optional. The buffer containing the workflow definition (defaults to the current buffer).
+- Returns the final shared context hash table after the workflow completes or stops.
+
+** EPC Server Handling
+- When a workflow involves an `llm` node, the engine attempts to ensure the SimTag EPC server is running and initialized automatically using functions from `org-supertag-sim-epc`. Errors related to the EPC server will be reported.
+
+* Helper Commands
+
+** `M-x org-supertag-ai-select-model`
+- Fetches available Ollama models via the EPC server.
+- Presents a list for selection using `completing-read`.
+- Inserts the selected model name at the current point in the buffer (useful when writing `:AI_MODEL:` properties).
+
+* Example Workflow: Simple Greeting
+
+#+BEGIN_SRC org
+* Start: Get User Name
+:PROPERTIES:
+:NODE_TYPE:       input
+:ID:              GET_NAME
+:AI_PROMPT:       "Please enter your name"
+:AI_CONTEXT_KEY:  "\"user_name\""
+:AI_SUCCESSORS:   "((\"default\" . \"GREET_USER\"))"
+:AI_THOUGHTS_BLOCK: GetNameThoughts
+:END:
+
+** Process: Generate Greeting
+:PROPERTIES:
+:NODE_TYPE:       llm
+:ID:              GREET_USER
+:AI_CONTEXT_SOURCES: "((\"GET_NAME\" . \"LLMResponse\"))" ; Note: INPUT nodes don't usually write to OUTPUT_BLOCK, use context key. Need to fix this too!
+:AI_PROMPT:       "Create a short, friendly greeting for a user named {{user_name}}. Just the greeting."
+:AI_OUTPUT_BLOCK: "GreetingText"
+:AI_THOUGHTS_BLOCK: GreetUserThoughts
+:AI_SUCCESSORS:   "((\"default\" . \"SHOW_GREETING\"))" ; <-- Correct format
+:END:
+
+
+** End: Display Greeting
+:PROPERTIES:
+:NODE_TYPE:       output
+:ID:              SHOW_GREETING
+:AI_CONTEXT_SOURCES: "((\"GREET_USER\" . \"GreetingText\"))"
+:AI_TEMPLATE:     "{{context_GreetingText}}"
+:AI_OUTPUT_TARGET: "message"
+:AI_THOUGHTS_BLOCK: ShowGreetingThoughts
+:END:
+#+END_SRC
+
+* Troubleshooting
+- **Check `*Messages*` Buffer:** The engine logs information, warnings, and errors here during execution. EPC server messages also often appear here.
+- **Examine Thoughts Blocks:** If you defined `:AI_THOUGHTS_BLOCK:` for nodes, check their contents for detailed execution steps and potential error messages.
+- **Verify Property Syntax:** Ensure property values are correctly formatted, especially strings containing Elisp code or alists (pay attention to quoting and escaping). Use `M-x eval-expression` to test parts of Elisp strings if needed.
+- **Check Node IDs:** Ensure `:ID:` properties are unique and that `:AI_SUCCESSORS:` and `:AI_CONTEXT_SOURCES:` refer to the correct IDs.
+- **EPC Issues:**
+    - Make sure the Python `simtag` EPC server is running.
+    - Check for Python errors in the terminal where the server is running.
+    - Ensure the correct Ollama models are available to the server.
+- **Function Node Errors:** If using `function` nodes, ensure the target Elisp function exists, accepts the correct arguments `(element context)`, and returns the correct `(action . result)` cons cell format.
diff --git a/org-supertag-ai.el b/org-supertag-ai.el
old mode 100755
new mode 100644
index 1bfd5ee..3ca4ef1
--- a/org-supertag-ai.el
+++ b/org-supertag-ai.el
@@ -20,7 +20,8 @@
 (require 'org)
 (require 'org-element) ; Needed for property parsing
 (require 'org-id)
-
+(require 'org-supertag-behavior) ; Added require for behavior execution
+(require 'org-supertag-sim-epc) ; Added require for LLM/EPC interaction
 
 (defgroup org-supertag-ai nil
   "Customization options for org-supertag AI capabilities."
@@ -393,11 +394,82 @@ CONTEXT-HASH: A hash table where keys are variable names (strings)
 
 ;; --- LLM Interaction Wrapper ---
 
-;; This entire section has been moved to `org-supertag-bridge.el`
-;; and refactored to use the new bridge architecture.
-;; The old functions `org-supertag-ai--invoke-llm-sync`,
-;; `org-supertag-ai--fetch-available-models`, `org-supertag-ai-select-model`
-;; are now obsolete.
+(defun org-supertag-ai--invoke-llm-sync (prompt &optional system-prompt model)
+  "Synchronously invoke the LLM via the EPC bridge.
+Assumes the EPC server is running AND initialized by `org-supertag--enable`.
+Calls the 'run_ollama' method on the Python side using `epc:call-sync`,
+passing the model hint.
+
+PROMPT: The main user prompt string.
+SYSTEM-PROMPT: Optional system prompt string.
+MODEL: Optional model name string. If non-nil, this will be passed
+       to the Python backend to override the default model for this call.
+
+Returns the AI response string on success.
+Signals an error on failure (EPC error, AI error, server not running)."
+  (org-supertag-sim-epc-log "Invoking LLM synchronously (Prompt: %s..., SystemPrompt: %s, Model: %s)..."
+                           (truncate-string-to-width prompt 50)
+                           (if system-prompt "yes" "no")
+                           (or model "Default"))
+
+  ;; 1. Ensure system is properly initialized
+  (deferred:sync!
+    (org-supertag-sim--ensure-initialized))
+
+  ;; 2. Clean prompt and system prompt values (remove quotes)
+  (let* ((clean-prompt (replace-regexp-in-string "\"" "" prompt))
+         (clean-system-prompt (when system-prompt (replace-regexp-in-string "\"" "" system-prompt))))
+
+    ;; 3. If model is provided, set it as default model before calling Ollama
+    (when model
+      (let ((clean-model (replace-regexp-in-string "\"" "" model)))
+        (message "Setting model to: %s" clean-model)
+        ;; Call set_default_model if available, otherwise log a message
+        (if (org-supertag-sim-epc-verify-ollama-model clean-model)
+            (message "Using model: %s" clean-model)
+          (message "Warning: Model %s not verified, using default model" clean-model))))
+    
+    ;; 4. Prepare arguments for EPC call (only prompt and system)
+    (let ((args (list clean-prompt clean-system-prompt)))
+      (org-supertag-sim-epc-log "Calling EPC method 'run_ollama' with args: %S" args)
+
+      (condition-case err
+          (let ((response (epc:call-sync org-supertag-sim-epc-manager 'run_ollama args)))
+            (org-supertag-sim-epc-log "Raw EPC response: %S" response)
+
+            ;; 5. Process Response
+            (if (and response (listp response) (plistp response))
+                (let ((status (plist-get response :status))
+                      (result (plist-get response :result))
+                      (message (plist-get response :message)))
+                  (if (string= status "success")
+                      (progn
+                        (org-supertag-sim-epc-log "LLM call successful.")
+                        result)
+                    ;; Check for specific error types and handle accordingly
+                    (cond
+                     ;; Model not found error
+                     ((string-match-p "model.*not found" (or message ""))
+                      (let ((fallback-models '("gemma:2b" "llama2" "mistral")))
+                        (catch 'found-model
+                          (dolist (fallback-model fallback-models)
+                            (when (org-supertag-sim-epc-verify-ollama-model fallback-model)
+                              (message "Original model not found, using fallback model: %s" fallback-model)
+                              (throw 'found-model
+                                     (org-supertag-ai--invoke-llm-sync prompt system-prompt fallback-model)))))))
+                     ;; Service not initialized error
+                     ((string-match-p "not initialized" (or message ""))
+                      (message "Service not initialized, attempting to reinitialize...")
+                      (org-supertag-sim-init)
+                      (org-supertag-ai--invoke-llm-sync prompt system-prompt model))
+                     ;; Other errors
+                     (t
+                      (error "LLM Error: %s" (or message "Unknown error"))))))
+              (error "Invalid response format from EPC: %S" response)))
+        ;; Handle EPC communication errors
+        (error
+         (error "EPC communication error: %s" (error-message-string err)))))))
+
 
 ;; --- Internal Node Type Executors (Implementations/Refined Stubs) ---
 
@@ -418,9 +490,7 @@ Returns: (list ACTION RESULT-STRING THOUGHTS-STRING)."
 
     (if rendered-prompt
         (condition-case err ;; This catches errors from invoke-llm-sync
-            ;; TODO: This node is now broken as `org-supertag-ai--invoke-llm-sync` was removed.
-            ;; It should be updated to call `org-supertag-bridge-llm-invoke`.
-            (let* ((llm-result (error "LLM invocation is disabled. `org-supertag-ai--invoke-llm-sync` was removed."))
+            (let* ((llm-result (org-supertag-ai--invoke-llm-sync rendered-prompt rendered-system-prompt model))
                    (final-thoughts (concat thoughts (format "\nLLM Result:\n%s\n---" llm-result))))
               (list "default" llm-result final-thoughts))
           (error ;; Handle invoke-llm-sync error
@@ -753,8 +823,22 @@ Returns the final shared-context hash-table after the workflow completes or stop
   (interactive "sStart Node ID or Title: ")
   
   ;; Ensure EPC service is properly initialized
-  ;; TODO: This logic needs to be updated to use the new bridge.
-  (message "Warning: AI workflow is likely non-functional due to refactoring.")
+  (when (featurep 'org-supertag-sim-epc)
+    (unless (org-supertag-sim-epc-server-running-p)
+      (org-supertag-sim-epc-start-server)
+      (sit-for 1))
+    (when (org-supertag-sim-epc-server-running-p)
+      (condition-case err
+          (progn
+            (org-supertag-sim-epc-init)
+<<<<<<< HEAD
+            (unless org-supertag-sim-epc-initialized
+              (error "EPC service initialization failed"))
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+            (message "EPC service initialized successfully"))
+        (error
+         (message "Warning: Failed to initialize EPC service: %s" (error-message-string err))))))
 
   (let* ((buf (or buffer (current-buffer)))
          (shared-context (if initial-context
@@ -857,11 +941,192 @@ Returns the final shared-context hash-table after the workflow completes or stop
 
 ;; --- Workflow Design Helpers ---
 
-;; This functionality has been moved to `org-supertag-bridge.el`.
+(defun org-supertag-ai--fetch-available-models ()
+  "Fetch the list of available Ollama models via EPC.
+Returns a list of model name strings on success, or signals an error
+if the EPC server is unreachable or the backend fails to get models."
+  (org-supertag-sim-epc-log "Fetching available Ollama models via EPC...")
+  ;; 1. Ensure EPC Server is ready
+  (condition-case err
+      (progn
+<<<<<<< HEAD
+        (org-supertag-sim-epc-ensure-server)
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+        (unless (org-supertag-sim-epc-server-running-p)
+          (error "SimTag EPC server is not running or failed to start.")))
+    (error (error "Failed to ensure EPC server environment: %s" (error-message-string err))))
+
+  ;; 2. Call the new EPC method 'get_available_models' synchronously
+  (condition-case err
+      (let ((response (epc:call-sync org-supertag-sim-epc-manager 'get_available_models nil))) ; No arguments needed
+        (org-supertag-sim-epc-log "Raw EPC response for models: %S" response)
+
+        ;; 3. Process Response
+        (if (and response (listp response) (plistp response))
+            (let ((status (plist-get response :status))
+                  (result (plist-get response :result)) ; Should be the list of model names
+                  (message (plist-get response :message)))
+              (if (string= status "success")
+                  (if (listp result) ; Verify the result is a list
+                      (progn
+                        (org-supertag-sim-epc-log "Successfully fetched %d models." (length result))
+                        result) ; Return the list of model names
+                    (error "Invalid result format received for models: Expected list, got %S" result))
+                  (error "Error fetching models from backend: %s" (or message "Unknown error."))))
+          (error "Invalid response format received from EPC for models: %S" response)))
+    ;; Handle EPC communication errors
+    (error (error "EPC communication error while fetching models: %s" (error-message-string err)))))
+
+(defun org-supertag-ai-select-model ()
+  "Interactively select an available Ollama model.
+Fetches the list of models from the running Ollama service via the
+SimTag EPC server and presents them for selection in the minibuffer.
+Inserts the selected model name into the current buffer at point."
+  (interactive)
+  (condition-case err
+      ;; Fetch the list of models first
+      (let* ((available-models (org-supertag-ai--fetch-available-models))
+             (selected-model nil))
+        (if available-models
+            ;; Use completing-read for selection
+            (setq selected-model (completing-read "Select Ollama Model: "
+                                                  available-models ; Candidates
+                                                  nil ; Predicate
+                                                  t ; Require match
+                                                  nil ; Initial input
+                                                  nil ; History var
+                                                  (car available-models))) ; Default value
+          (message "No available models fetched from Ollama service."))
+
+        ;; Insert the selected model if one was chosen
+        (if (and selected-model (stringp selected-model) (not (string-empty-p selected-model)))
+            (progn
+              (insert selected-model)
+              (message "Inserted model: %s" selected-model))
+          (message "No model selected.")))
+    ;; Catch errors during fetching or selection
+    (error
+     (message "Error selecting model: %s" (error-message-string err)))))
 
 ;; --- Convert Markdown to Org ---
+;; Inspired by the Ollama-Buddy: https://github.com/captainflasmr/ollama-buddy/blob/main/ollama-buddy-core.el
+(defun org-supertag-ai--convert-md-to-org (md-string)
+  "Convert a Markdown string MD-STRING to Org format string.
+Handles common Markdown elements like headers, lists, bold, italics,
+links, inline code, code blocks, and horizontal rules.
+
+Inspired by the logic in `ollama-buddy--md-to-org-convert-region`."
+  (if (not (and md-string (stringp md-string) (not (string-empty-p md-string))))
+      ;; Return empty string if input is invalid
+      ""
+    (with-temp-buffer
+      (insert md-string)
+      ;; Perform conversions within the temporary buffer
+      (goto-char (point-min))
+
+      ;; --- Conversion logic (adapted from ollama-buddy) ---
+
+      ;; 1. Protect code blocks
+      (let ((code-blocks nil)
+            (counter 0)
+            block-start block-end lang content placeholder)
+        (save-match-data
+          (goto-char (point-min)) ; Ensure search starts from beginning
+          ;; Regex slightly adjusted for potentially missing language and newline variations
+          (while (re-search-forward "```\\([^\n`]*?\\)\\(?:\n\\|\\s-*$\\)\\(\\(?:.\\|\n\\)*?\\)```" nil t)
+            (setq lang (or (match-string 1) "") ; Handle potentially nil language
+                  content (match-string 3) ; Content is group 3
+                  block-start (match-beginning 0)
+                  block-end (match-end 0)
+                  placeholder (format "CODE_BLOCK_PLACEHOLDER_%d_AI" counter)) ; Unique placeholder
+
+            (push (list placeholder lang content) code-blocks)
+            (delete-region block-start block-end)
+            (goto-char block-start)
+            (insert placeholder)
+            (setq counter (1+ counter)))))
+
+      ;; 2. Apply Markdown to Org transformations
+      ;; Lists: Translate `-`, `*`, or `+` lists
+      (save-match-data
+        (goto-char (point-min))
+        (while (re-search-forward "^\\([ \t]*\\)[*-+] \\(.*\\)$" nil t)
+          (replace-match (concat (match-string 1) "- \\2"))))
+
+      ;; Bold: `**bold**` -> `*bold*`
+      (save-match-data
+        (goto-char (point-min))
+        ;; Match non-space character at boundaries
+        (while (re-search-forward "\\*\\*\\([^[:space:]]\\(?:.\\|\n\\)*?[^[:space:]]\\)\\*\\*" nil t)
+          (replace-match "*\\1*")))
+
+      ;; Italics: `_italic_` or `*italic*` -> `/italic/`
+      ;; Note: Simple `*italic*` conversion might conflict with bold if not careful.
+      ;; Let's prioritize `_italic_` first.
+      (save-match-data
+        (goto-char (point-min))
+        ;; Match non-space boundary for _italic_
+        (while (re-search-forward "\\([[:space:]]\\|^\\)_\\([^[:space:]]\\(?:.\\|\n\\)*?[^[:space:]]\\)_\\([[:space:]]\\|$\\)" nil t)
+          (replace-match "\\1/\\2/\\3")))
+      ;; Consider adding conversion for *italic* if needed, carefully avoiding bold conflict
+
+      ;; Links: `[text](url)` -> `[[url][text]]`
+      (save-match-data
+        (goto-char (point-min))
+        (while (re-search-forward "\\[\\(.*?\\)\\](\\(.*?\\))" nil t)
+          (replace-match "[[\\2][\\1]]")))
+
+      ;; Inline code: `code` -> `=code=`
+      (save-match-data
+        (goto-char (point-min))
+        ;; Match non-backtick content
+        (while (re-search-forward "`\\([^`\n]+\\)`" nil t)
+          (replace-match "=\\1=")))
 
-;; This function has been removed as it is not core to this module's purpose.
+      ;; Horizontal rules: `---` or `***` -> `-----`
+      (save-match-data
+        (goto-char (point-min))
+        (while (re-search-forward "^[ \t]*\\(?:-{3,}\\|\\*{3,}\\)[ \t]*$" nil t)
+          (replace-match "-----")))
+
+      ;; Images: `![alt text](url)` -> `[[url]]` (Org usually displays images directly from links)
+      (save-match-data
+        (goto-char (point-min))
+        (while (re-search-forward "!\\[.*?\\](\\([^)]+\\))" nil t)
+          (replace-match "[[\\1]]")))
+
+      ;; Headers: `# header` -> `* header` (Only converts level 1-6)
+      (save-match-data
+        (goto-char (point-min))
+        (while (re-search-forward "^\\(#{1,6}\\) \\(.*\\)$" nil t)
+          (replace-match (concat (make-string (length (match-string 1)) ?*) " " (match-string 2)))))
+
+      ;; Remove potential Markdown blockquote leaders "> "
+      (save-match-data
+         (goto-char (point-min))
+         (while (re-search-forward "^> \\(.*\\)$" nil t)
+           (replace-match "\\1")))
+
+
+      ;; 3. Restore code blocks with proper Org syntax
+      (save-match-data
+        (dolist (block (nreverse code-blocks))
+          (let ((placeholder (nth 0 block))
+                (lang (nth 1 block)) ; Language might be empty string
+                (content (nth 2 block)))
+            (goto-char (point-min))
+            (when (search-forward placeholder nil t)
+              ;; Ensure newline before/after block if needed
+              (unless (or (bobp) (eq (char-before) ?\n)) (insert "\n"))
+              (replace-match (format "#+begin_src%s\n%s\n#+end_src"
+                                     (if (string-empty-p lang) "" (concat " " lang)) ; Add space only if lang exists
+                                     content)
+                             t t)
+              (unless (or (eobp) (eq (char-after) ?\n)) (insert "\n"))))))
+
+      ;; Return the converted buffer content
+      (buffer-string))))
 
 (provide 'org-supertag-ai)
 ;;; org-supertag-ai.el ends here
diff --git a/org-supertag-api.el b/org-supertag-api.el
index 1b7fecb..b68b493 100755
--- a/org-supertag-api.el
+++ b/org-supertag-api.el
@@ -12,7 +12,6 @@
 (require 'epc)
 (require 'json)
 (require 'org-supertag-bridge)
-(require 'cl-lib)
 
 (defvar org-supertag-api--session-id nil
   "The current session ID for the conversation.")
@@ -33,78 +32,300 @@
 ;; (defun org-supertag-api--handle-async-response (response callback-fn)
 ;;   \"Generic handler for async responses from the bridge.
 ;; RESPONSE is the data from `org-supertag-bridge-call-async`.
-;; CALLBACK-FN is the user-provided function to call with the actual result.\"\n;; (if (and (listp response) (eq (car response) :error))\n;; (progn\n;; (message \"Org SuperTag API Error: %s\" (plist-get response :message))\n;; ;; Potentially call callback-fn with nil or an error indicator\n;; (when callback-fn (funcall callback-fn nil)))\n;; (when callback-fn (funcall callback-fn response))))rtag-api-get-tag-relationship-suggestions (target-tag &optional desired-types context callback-fn)
-;;   ...)
+;; CALLBACK-FN is the user-provided function to call with the actual result.\"\n;; (if (and (listp response) (eq (car response) :error))\n;; (progn\n;; (message \"Org SuperTag API Error: %s\" (plist-get response :message))\n;; ;; Potentially call callback-fn with nil or an error indicator\n;; (when callback-fn (funcall callback-fn nil)))\n;; (when callback-fn (funcall callback-fn response))))
 
-;; --- Query Functions ---
+;; --- RAG Functions ---
 
-(defun org-supertag-api-query-similar-nodes (query-text &optional top-k callback-fn)
-  "Fetch similar nodes for QUERY-TEXT from the Python backend.
-QUERY-TEXT can be a node ID or any string.
-TOP-K is the number of results to return.
+(defun org-supertag-api-rag-query (query-text session-id &optional callback-fn)
+  "Send a query to the RAG system and retrieve results.
+QUERY-TEXT is the user's query string.
+SESSION-ID is the unique identifier for the current dialogue session.
+If CALLBACK-FN is provided, the call is asynchronous, and CALLBACK-FN
+will be called with the result. Otherwise, the call is synchronous.
+The Python method called is \"ask_question\"."
+  (interactive "sQuery: ")
+  (org-supertag-bridge-ensure-ready) ; Ensure connection is up
+  (let* ((method-name "ask_question")
+         (question-data (let ((ht (make-hash-table :test 'equal)))
+                          (puthash "query_text" query-text ht)
+                          (puthash "session_id" session-id ht)
+                          ht)))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name question-data callback-fn)
+      (org-supertag-bridge-call-sync method-name question-data 30))))
+
+(defun org-supertag-api-set-dialogue-mode (session-id mode-name &optional callback-fn)
+  "Set the dialogue mode for a specific session.
+SESSION-ID is the unique identifier for the dialogue session.
+MODE-NAME is the name of the mode to set (e.g., \"socratic\").
+If CALLBACK-FN is provided, the call is asynchronous.
+The Python method called is \"set_dialogue_mode\"."
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "set_dialogue_mode"))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name (list session-id mode-name) callback-fn)
+      (org-supertag-bridge-call-sync method-name (list session-id mode-name) 10)))) ; 10s timeout
+
+;; --- Memory Functions ---
+
+(defun org-supertag-api-memory-retrieve (aspect &optional callback-fn)
+  "Retrieve memory from the Python backend (currently fetches the whole dashboard).
+ASPECT is the conceptual aspect to retrieve (e.g., 'user_preferences').
+If CALLBACK-FN is provided, the call is asynchronous.
+The Python method called is \"get_memory_dashboard\"."
+  ;; ASPECT is currently ignored in the call to Python, 
+  ;; but kept for potential client-side filtering or future backend changes.
+  (interactive "sRetrieve memory aspect: ")
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "get_memory_dashboard"))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name nil callback-fn)
+      (org-supertag-bridge-call-sync method-name nil 10)))) ; 10s timeout
+
+(defun org-supertag-api-memory-store (aspect data &optional callback-fn)
+  "Store data for a specific aspect in the memory system.
+ASPECT is the category of memory (e.g., 'user_preference').
+DATA is the content to store.
+If CALLBACK-FN is provided, the call is asynchronous.
+The Python method called is \"update_memory_epc\"."
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "update_memory_epc")
+        (update-data `(:action "store" :aspect ,aspect :payload ,data)))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name update-data callback-fn)
+      (org-supertag-bridge-call-sync method-name update-data 10)))) ; 10s timeout
+
+(defun org-supertag-api-memory-clear (aspect &optional item-id callback-fn)
+  "Clear a specific aspect or an item within an aspect from memory.
+ASPECT is the category (e.g., 'dialogue_history').
+ITEM-ID (optional) is the specific item to clear.
 If CALLBACK-FN is provided, the call is asynchronous.
-Python method called: \"query/get_similar_nodes\"."
+The Python method called is \"update_memory_epc\"."
+  (interactive "sClear memory aspect: \\nP(Optional) Item ID: ")
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "update_memory_epc")
+        (update-data `(:action "clear" :aspect ,aspect :item-id ,item-id)))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name update-data callback-fn)
+      (org-supertag-bridge-call-sync method-name update-data 10)))) ; 10s timeout
+
+;; --- Dialogue History Functions ---
+
+(defun org-supertag-api-get-dialogue-history (session-id &optional count callback-fn)
+  "Retrieve the recent dialogue history for a specific session.
+SESSION-ID is the unique identifier for the dialogue session.
+COUNT (optional) specifies the number of turns to retrieve.
+If CALLBACK-FN is provided, the call is asynchronous.
+The Python method called is \"epc_get_dialogue_history\"."
+  (interactive "P")
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "epc_get_dialogue_history")
+        (max-turns (if (integerp count) count nil)))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name (list session-id max-turns) callback-fn)
+      (org-supertag-bridge-call-sync method-name (list session-id max-turns) 10)))) ; 10s timeout
+
+(defun org-supertag-api-log-dialogue (session-id user-query assistant-response &optional callback-fn)
+  "Log a user query and assistant response to a specific dialogue history.
+SESSION-ID is the unique identifier for the dialogue session.
+If CALLBACK-FN is provided, the call is asynchronous (fire and forget for user query,
+callback attached to assistant response logging).
+The Python method called is \"epc_add_dialogue_turn\"."
   (org-supertag-bridge-ensure-ready)
-  (let ((method-name "query/get_similar_nodes")
-        (k (or top-k 10)))
+  (let ((method-name "epc_add_dialogue_turn")
+        (user-speaker "USER")
+        (ai-speaker "AI"))
+
+    ;; Log user query (fire-and-forget or with its own error reporting if needed)
+    (org-supertag-bridge-call-async method-name 
+                                    (list session-id user-speaker user-query nil)
+                                    (lambda (response) ; Minimal error reporting for user query log
+                                      (when (and (listp response) (eq (car response) :error))
+                                        (message "Error logging user query: %s" (plist-get response :message)))))
+
+    ;; Log assistant response (attach the main callback-fn here if provided)
     (if callback-fn
-        (org-supertag-bridge-call-async method-name (list query-text k) callback-fn)
-      (org-supertag-bridge-call-sync method-name (list query-text k) 15))))
+        (org-supertag-bridge-call-async method-name (list session-id ai-speaker assistant-response nil) callback-fn)
+      (org-supertag-bridge-call-async method-name 
+                                      (list session-id ai-speaker assistant-response nil)
+                                      (lambda (response) ; Minimal error reporting for AI response log
+                                        (when (and (listp response) (eq (car response) :error))
+                                          (message "Error logging AI response: %s" (plist-get response :message))))))))
+
+;; --- Tag Similarity Functions ---
+
+(defun org-supertag-api-get-similar-tags (&optional target-tag callback-fn)
+  "Fetch similar tags for TARGET-TAG from the Python backend.
+TARGET-TAG can be a string. If nil, backend might use current context.
+If CALLBACK-FN is provided, the call is asynchronous.
+Python method called: \"find_similar\"."
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "find_similar"))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name (list (list target-tag 10)) callback-fn) ; Assuming limit 10
+      (org-supertag-bridge-call-sync method-name (list (list target-tag 10)) 15)))) ; 15s timeout, limit 10
 
 ;; --- Unified Entity Extraction for Auto-Tagging ---
 
+(defun org-supertag-api-extract-entities-for-tagging (content &optional node-id existing-tags callback-fn)
+  "Unified API to fetch entity-based tag suggestions.
+This function calls the 'extract_entities_for_tagging' endpoint and
+is the single point of entry for this functionality.
+If CALLBACK-FN is provided, the call is asynchronous."
+  (org-supertag-bridge-ensure-ready)
+  (let* ((method-name "extract_entities_for_tagging")
+         ;; The payload is a single alist, which is the standard format
+         ;; expected by the Python backend handler.
+         (payload `(("node_id" . ,(or node-id ""))
+                    ("content" . ,content)
+                    ("existing_tags" . ,(or existing-tags '())))))
+    (if callback-fn
+        ;; We wrap the payload in a list to conform to the bridge contract.
+        (org-supertag-bridge-call-async method-name (list payload) callback-fn)
+      (org-supertag-bridge-call-sync method-name (list payload) 20))))
+
 (defun org-supertag-api-batch-generate-tags (payload callback)
   "Sends a batch of nodes to the backend for tag suggestion generation.
 PAYLOAD is an alist containing nodes and model configuration.
 CALLBACK is the function to call with the results."
   (org-supertag-bridge-ensure-ready)
-  (let ((method-name "autotag/batch_generate_tags"))
-    (org-supertag-bridge--log "API: Calling autotag/batch_generate_tags with payload.")
-    ;; The payload from the caller is already a hash-table.
+  (let ((method-name "batch_generate_tags"))
+    (org-supertag-bridge--log "API: Calling batch_generate_tags with payload.")
+    ;; The payload from the caller is already an alist.
     ;; We wrap it in a list to conform to the data contract.
     (org-supertag-bridge-call-async method-name (list payload) callback)))
 
-;; --- Background Sync Functions ---
+;; --- Tag Relationship Suggestion Functions ---
 
-(defun org-supertag-api-sync-bulk-process (payload callback)
-  "Sends a bulk snapshot of database changes to the Python backend for processing.
-This is used by the background sync system.
-PAYLOAD is an alist containing nodes, tags, links to upsert, and IDs to delete.
-CALLBACK is the function to call with the results.
-Python method called: \"sync/bulk_process\"."
+(defun org-supertag-api-get-tag-relationship-suggestions (target-tag &optional desired-types context callback-fn)
+  "Fetch inferred tag relationship suggestions for TARGET-TAG.
+DESIRED-TYPES is an optional list of relationship types (strings).
+CONTEXT is optional additional context for the backend.
+If CALLBACK-FN is provided, the call is asynchronous.
+Anticipated Python method: \"get_inferred_tag_relationships\"."
   (org-supertag-bridge-ensure-ready)
-  (let ((method-name "sync/bulk_process"))
-    (org-supertag-bridge--log "API: Calling sync/bulk_process with payload.")
-    (org-supertag-bridge-call-async method-name (list payload) callback)))
+  (let ((method-name "get_inferred_tag_relationships")
+        (payload (let ((ht (make-hash-table :test 'equal)))
+                   (puthash "target_tag" target-tag ht)
+                   (puthash "desired_types" desired-types ht)
+                   (puthash "context" context ht)
+                   ht)))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name payload callback-fn)
+      (org-supertag-bridge-call-sync method-name payload 20))))
+
+;; --- Associated Nodes Functions ---
+
+(defun org-supertag-api-get-associated-nodes (&optional context-id callback-fn)
+  "Fetch associated/related nodes for CONTEXT-ID from the Python backend.
+CONTEXT-ID could be a node ID, tag, or nil for general context.
+If CALLBACK-FN is provided, the call is asynchronous.
+Python method called: \"get_similar_nodes\"."
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "get_similar_nodes"))
+    (if callback-fn
+        (org-supertag-bridge-call-async method-name (list (or context-id "") 10) callback-fn) ; Pass context, default top_k 10
+      (org-supertag-bridge-call-sync method-name (list (or context-id "") 10) 15)))) ; 15s timeout, default top_k 10
+
+;; --- Knowledge Archaeology Functions ---
 
-;; --- Generic Text Generation ---
+(defun org-supertag-api-knowledge-archaeology-dig (query-text &optional top-k)
+  "Perform a knowledge archaeology dig for a given query.
+QUERY-TEXT is the search term.
+TOP-K is the number of results to return."
+  (let ((top-k (or top-k 50)))
+    (org-supertag-bridge-call-sync "knowledge_archaeology_dig" (list query-text top-k) 30)))
 
 (defun org-supertag-api-generate-text (prompt &optional callback-fn)
   "Call the generic text generation endpoint.
-This is intended for simple, one-off generation tasks and likely maps
-to a reasoning or query handler in the backend.
 PROMPT is the string to send to the LLM.
+If CALLBACK-FN is provided, the call is asynchronous."
+  (if callback-fn
+      (org-supertag-bridge-call-async "generate_text" prompt callback-fn)
+    (org-supertag-bridge-call-sync "generate_text" prompt 120)))
+
+;;; Memory Synthesis
+
+(defun org-supertag-api-trigger-memory-synthesis (session-id &optional callback-fn)
+  "Trigger the memory synthesis process for a given dialogue session.
+SESSION-ID is the ID of the conversation to analyze."
+  (if callback-fn
+      (org-supertag-bridge-call-async "trigger_memory_synthesis_for_session" session-id callback-fn)
+    (org-supertag-bridge-call-sync "trigger_memory_synthesis_for_session" session-id 10)))
+
+(defun org-supertag-api-get-candidate-memories ()
+  "Fetch all pending candidate memories awaiting user review."
+  (org-supertag-bridge-call-sync "get_candidate_memories" nil 10))
+
+(defun org-supertag-api-process-candidate-memory (candidate-id action)
+  "Process a user's decision on a candidate memory.
+CANDIDATE-ID is the ID of the memory candidate.
+ACTION is a string, either \"accept\" or \"reject\"."
+  (org-supertag-bridge-call-sync "process_candidate_memory" (list candidate-id action) 10))
+
+;; --- Proactive Engine Functions ---
+
+(defun org-supertag-api-analyze-node-context (context-data &optional callback-fn)
+  "Send the current node's context to the backend for full analysis.
+This includes entity extraction, graph building, vectorization, and checking for conceptual resonance.
+CONTEXT-DATA is a plist containing information like node ID, content, etc.
 If CALLBACK-FN is provided, the call is asynchronous.
-Assumed Python method: \"reasoning/run_cycle\" or a similar generic generator."
+The Python method called is \"analyze_node_context\"."
   (org-supertag-bridge-ensure-ready)
-  ;; NOTE: The backend method for generic text generation is not clearly
-  ;; defined post-refactor. After refactor, use 'knowledge/run_cycle'.
-  (let ((method-name "knowledge/run_cycle"))
+  (let* ((method-name "analyze_node_context")
+         (payload (apply #'ht-create context-data)))
     (if callback-fn
-        (org-supertag-bridge-call-async method-name (list prompt) callback-fn)
-      (org-supertag-bridge-call-sync method-name (list prompt) 120))))
+        (org-supertag-bridge-call-async method-name payload callback-fn)
+      (org-supertag-bridge-call-sync method-name payload 60))))
+
+;; --- Bulk Sync Functions ---
+
+(defun org-supertag-api-bulk-process-snapshot (snapshot-data &optional callback-fn)
+  "Send a snapshot of incrementally changed nodes and links to the backend.
+SNAPSHOT-DATA must be an alist containing 'nodes' and 'links' keys.
+This function wraps the data in a list to conform to the data contract.
+If CALLBACK-FN is provided, the call is asynchronous.
+The Python method called is \"bulk_process_snapshot\"."
+  (org-supertag-bridge-ensure-ready)
+  (let ((method-name "bulk_process_snapshot"))
+    ;; The validation for hash-table is removed, as we now use alists for reliability.
+    (if callback-fn
+        ;; Wrap the payload in a list to conform to the data contract.
+        (org-supertag-bridge-call-async method-name (list snapshot-data) callback-fn)
+      (org-supertag-bridge-call-sync method-name (list snapshot-data) 300)))) ; 300s (5min) timeout for potentially large snapshots
+
+(defun org-supertag-api-generate-single-node-tags (node-data callback)
+  "Asynchronously generate tags for a single node.
+NODE-DATA should be a hash-table with node properties.
+CALLBACK is a function that will be called with the result."
+  (if callback
+      (org-supertag-bridge-call-async "generate_single_node_tags" node-data callback)
+    (org-supertag-bridge-call-sync "generate_single_node_tags" node-data 60)))
+
+(defun org-supertag-api-proactive-get-resonance (node-id content callback)
+  "Asynchronously get conceptual resonance for a given content.
+NODE-ID is the ID of the current node.
+CONTENT is the text content to analyze.
+CALLBACK is a function that will be called with the resonance results."
+  (let ((payload `(:node_id ,node-id :content ,content)))
+    (if callback
+        (org-supertag-bridge-call-async "proactive_get_resonance" payload callback)
+      (org-supertag-bridge-call-sync "proactive_get_resonance" payload 30))))
 
-;; --- REMAINING OBSOLETE FUNCTIONS ---
-;; (defun org-supertag-api-trigger-memory-synthesis ...)
-;; (defun org-supertag-api-get-similar-tags ...)
-;; (defun org-supertag-api-extract-entities-for-tagging ...)
-;; (defun org-supertag-api-get-associated-nodes ...)
-;; (defun org-supertag-api-knowledge-archaeology-dig ...)
+;;;###autoload
+(defun org-supertag-api-diagnostics-get-status (callback)
+  "Get the system status from the Python backend."
+  (if callback
+      (org-supertag-bridge-call-async "get_status" nil callback)
+    (org-supertag-bridge-call-sync "get_status" nil 10)))
 
-(defun org-supertag-api-get-all-tags ()
-  "Get all tags from the database."
-  (org-supertag-db-get-all-tags))
+(defun org-supertag-api--get-model-config-for-tagging ()
+  "Constructs a plist of the current model configuration for tagging."
+  (list :provider org-supertag-api-tagging-provider
+        :model org-supertag-api-tagging-model
+        :temperature org-supertag-api-tagging-temperature
+        :max_tokens org-supertag-api-tagging-max-tokens))
 
 (provide 'org-supertag-api)
 
-;;; org-supertag-api.el ends here
+;;; org-supertag-api.el ends here 
diff --git a/org-supertag-auto-tag.el b/org-supertag-auto-tag.el
index c8a05b3..b6f3e34 100755
--- a/org-supertag-auto-tag.el
+++ b/org-supertag-auto-tag.el
@@ -23,7 +23,6 @@
 (require 'org-supertag-inline)
 (require 'org-supertag-node)
 (require 'org-supertag-db)
-(require 'org-supertag-scheduler)
 (require 'cl-lib)
 
 ;;; === Core Configuration ===
@@ -37,9 +36,9 @@
   :type 'boolean
   :group 'org-supertag-auto-tag)
 
-(defcustom org-supertag-auto-tag-scan-daily-time "02:30"
-  "The time of day (HH:MM format) to run the silent background scan for untagged nodes."
-  :type 'string
+(defcustom org-supertag-auto-tag-silent-scan-interval 7200
+  "Time interval (seconds) for background silent scanning of untagged nodes, default 10 minutes."
+  :type 'integer
   :group 'org-supertag-auto-tag)
 
 (defcustom org-supertag-auto-tag-batch-min-content-length 10
@@ -67,35 +66,6 @@
   :type 'string
   :group 'org-supertag-auto-tag)
 
-(defcustom org-supertag-auto-tag-provider 'ollama
-  "The AI provider to use for auto-tagging.
-Supported values match the Python backend's LLMClient configuration."
-  :type '(choice (const :tag "Ollama" ollama)
-                 (const :tag "OpenAI" openai)
-                 (const :tag "Google" google))
-  :group 'org-supertag-auto-tag)
-
-(defcustom org-supertag-auto-tag-model "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"
-  "The model name to use for auto-tagging."
-  :type 'string
-  :group 'org-supertag-auto-tag)
-
-(defcustom org-supertag-auto-tag-temperature 0.7
-  "The sampling temperature for the auto-tagging model (0.0 - 2.0)."
-  :type 'float
-  :group 'org-supertag-auto-tag)
-
-(defcustom org-supertag-auto-tag-max-tokens 2048
-  "The maximum number of tokens for the auto-tagging model's response."
-  :type 'integer
-  :group 'org-supertag-auto-tag)
-
-(defcustom org-supertag-auto-tag-endpoint "http://localhost:11434"
-  "The API endpoint for the auto-tagging provider.
-This is particularly important for local providers like Ollama."
-  :type 'string
-  :group 'org-supertag-auto-tag)
-
 ;;; === Global Variables ===
 
 (defvar org-supertag-auto-tag--suggestion-queue '()
@@ -104,156 +74,137 @@ This is particularly important for local providers like Ollama."
 (defvar org-supertag-auto-tag--node-content-cache (make-hash-table :test 'equal)
   "Cache node content to avoid duplicate IO.")
 
+(defvar org-supertag-auto-tag--silent-scan-timer nil
+  "Timer for background silent scanning.")
+
+(defvar org-supertag-auto-tag--reminder-timer nil
+  "Timer for daily reminders.")
+
 (defvar org-supertag-auto-tag--last-prompt-date nil
   "Date (YYYY-MM-DD) of last reminder prompt, to avoid duplicate reminders.")
 
 ;;; === Auto Mode and Background Scanning ===
 
-(defun org-supertag-auto-tag--prompt-for-review ()
-  "Prompt user to review tags if conditions are met."
+(defun org-supertag-auto-tag-start-silent-scan ()
+  "Start background silent scan timer.
+This function should be called after Python bridge is ready."
+  (interactive)
+  (when (and org-supertag-auto-tag-enable-silent-scan
+             (not (timerp org-supertag-auto-tag--silent-scan-timer)))
+    (message "Auto-tag: Background silent scan service started, running every %d seconds."
+             org-supertag-auto-tag-silent-scan-interval)
+    (setq org-supertag-auto-tag--silent-scan-timer
+          (run-with-timer 5 org-supertag-auto-tag-silent-scan-interval
+                          'org-supertag-auto-tag-silent-scan-and-generate))))
+
+(defun org-supertag-auto-tag-stop-silent-scan ()
+  "Stop background silent scan timer."
+  (interactive)
+  (when (timerp org-supertag-auto-tag--silent-scan-timer)
+    (cancel-timer org-supertag-auto-tag--silent-scan-timer)
+    (setq org-supertag-auto-tag--silent-scan-timer nil)
+    (message "Auto-tag: Background silent scan service stopped.")))
+
+(defun org-supertag-auto-tag-start-reminder-timer ()
+  "Start daily reminder timer."
+  (interactive)
   (when (and org-supertag-auto-tag-enable-daily-reminder
-             (not (seq-empty-p org-supertag-auto-tag--suggestion-queue)))
-    (when (yes-or-no-p (format "Auto-tag: %d suggestions pending, review now?"
-                               (length org-supertag-auto-tag--suggestion-queue)))
-      ;; This function must be called from a timer, so it's safe
-      ;; to assume we can pop up a window.
-      (with-current-buffer (get-buffer-create "*Org Supertag Batch Add*")
+             (not (timerp org-supertag-auto-tag--reminder-timer)))
+    (message "Auto-tag: Daily reminder enabled, will remind at %s." org-supertag-auto-tag-daily-reminder-time)
+    (setq org-supertag-auto-tag--reminder-timer
+          (run-with-timer 60 60 'org-supertag-auto-tag--check-and-prompt-for-review))))
+
+(defun org-supertag-auto-tag-stop-reminder-timer ()
+  "Stop daily reminder timer."
+  (interactive)
+  (when (timerp org-supertag-auto-tag--reminder-timer)
+    (cancel-timer org-supertag-auto-tag--reminder-timer)
+    (setq org-supertag-auto-tag--reminder-timer nil)
+    (message "Auto-tag: Daily reminder disabled.")))
+
+(defun org-supertag-auto-tag--check-and-prompt-for-review ()
+  "Check current time and prompt user to review tags if conditions are met."
+  (let ((today (format-time-string "%Y-%m-%d")))
+    (when (and org-supertag-auto-tag-enable-daily-reminder
+               (not (seq-empty-p org-supertag-auto-tag--suggestion-queue))
+               (string= (format-time-string "%H:%M") org-supertag-auto-tag-daily-reminder-time)
+               (not (string= org-supertag-auto-tag--last-prompt-date today)))
+      (setq org-supertag-auto-tag--last-prompt-date today)
+      (when (yes-or-no-p (format "Auto-tag: %d suggestions pending, review now?"
+                                 (length org-supertag-auto-tag--suggestion-queue)))
         (org-supertag-auto-tag-batch-add)))))
 
 ;;;###autoload
 (define-minor-mode org-supertag-auto-tag-mode
   "Enable auto-tagging for org-supertag.
 This mode itself doesn't start any process, it's just a switch.
-This mode registers and deregisters the background tasks with the central scheduler."
+Actual background scanning is started by `org-supertag-auto-tag-start-silent-scan`."
   :init-value nil
   :lighter " ST-Auto"
   :group 'org-supertag
   (if org-supertag-auto-tag-mode
-      ;; When mode is enabled, register tasks with the scheduler.
-      (progn
-        (when org-supertag-auto-tag-enable-silent-scan
-          (org-supertag-scheduler-register-task
-           'auto-tag-silent-scan
-           :daily
-           #'org-supertag-auto-tag-silent-scan-and-generate
-           :time org-supertag-auto-tag-scan-daily-time))
-        (when org-supertag-auto-tag-enable-daily-reminder
-          (org-supertag-scheduler-register-task
-           'auto-tag-daily-reminder
-           :daily
-           #'org-supertag-auto-tag--prompt-for-review
-           :time org-supertag-auto-tag-daily-reminder-time))
-        (message "Org SuperTag Auto-Tag Mode enabled."))
-    ;; When mode is disabled, deregister tasks.
-    (org-supertag-scheduler-deregister-task 'auto-tag-silent-scan)
-    (org-supertag-scheduler-deregister-task 'auto-tag-daily-reminder)
-    (message "Org SuperTag Auto-Tag Mode disabled.")))
+      ;; Logic when mode is enabled (if needed), but we choose to put startup logic
+      ;; in an externally callable function to ensure correct timing.
+      (message "Org SuperTag Auto-Tag Mode enabled.")
+    ;; When mode is disabled, stop timers
+    (org-supertag-auto-tag-stop-silent-scan)
+    (org-supertag-auto-tag-stop-reminder-timer)))
 
 (defun org-supertag-auto-tag-silent-scan-and-generate ()
-  "Scan database for untagged nodes and generate tag suggestions in batch.
-Nodes for which suggestions already exist in the queue are skipped to avoid reprocessing.
+  "Scan database for all untagged nodes and generate tag suggestions in batch.
 If `org-supertag-auto-tag-batch-enable-limit` is t,
 process at most `org-supertag-auto-tag-batch-max-nodes-per-run` nodes per run."
   (interactive)
-  (message "[Auto-tag] org-supertag-auto-tag-silent-scan-and-generate called at %s" (format-time-string "%H:%M:%S.%3N"))
   (let* ((all-nodes (org-supertag-node-get-all))
          (untagged-nodes-all (seq-filter
                               (lambda (node-id)
                                 (seq-empty-p (org-supertag-node-get-tags node-id)))
                               all-nodes))
-         ;; Get IDs of nodes that are already in the suggestion queue
-         (processed-node-ids (delete-dups (mapcar (lambda (s) (plist-get s :node-id)) org-supertag-auto-tag--suggestion-queue)))
-         ;; Filter out already processed nodes
-         (nodes-to-process (seq-remove (lambda (node-id) (member node-id processed-node-ids))
-                                       untagged-nodes-all))
          (untagged-nodes (if (and org-supertag-auto-tag-batch-enable-limit
-                                  (> (length nodes-to-process) org-supertag-auto-tag-batch-max-nodes-per-run))
-                             (seq-take nodes-to-process org-supertag-auto-tag-batch-max-nodes-per-run)
-                           nodes-to-process)))
+                                  (> (length untagged-nodes-all) org-supertag-auto-tag-batch-max-nodes-per-run))
+                             (seq-take untagged-nodes-all org-supertag-auto-tag-batch-max-nodes-per-run)
+                           untagged-nodes-all)))
     (if (seq-empty-p untagged-nodes)
-        (message "Auto-tag: No new untagged nodes found to process.")
+        (message "Auto-tag: No untagged nodes found.")
       (progn
-        (message "Auto-tag: Found %d new untagged nodes to process (total untagged: %d, with suggestions: %d). Generating..."
+        (message "Auto-tag: Found %d untagged nodes (total %d), batch generating suggestions..."
                  (length untagged-nodes)
-                 (length untagged-nodes-all)
-                 (length processed-node-ids))
+                 (length untagged-nodes-all))
         (org-supertag-auto-tag--batch-extract-and-send-content untagged-nodes)))))
 
 (defun org-supertag-auto-tag--batch-extract-and-send-content (node-ids)
   "Extract content from given node IDs and send asynchronously to the backend.
 The data is structured according to the unified data contract."
-  (message "[Auto-tag] org-supertag-auto-tag--batch-extract-and-send-content called at %s" (format-time-string "%H:%M:%S.%3N"))
   (let ((nodes-to-process '()))
-    ;; 1. Create a simplified data structure that EPC can properly serialize
+    ;; 1. Create a list of alists, where each alist represents a node.
     (dolist (node-id node-ids)
       (when-let* ((node-data (org-supertag-db-get node-id))
                   (content (org-supertag-auto-tag--get-node-content node-data)))
-        (let* ((title (plist-get node-data :title))
-               (combined (concat (or title "") " " (or content ""))))
-          (when (>= (length (string-trim combined)) org-supertag-auto-tag-batch-min-content-length)
-            (let ((node-dict `(("id" . ,node-id)
-                               ("content" . ,combined))))
-              (push node-dict nodes-to-process))))))
+        (when (>= (length content) org-supertag-auto-tag-batch-min-content-length)
+          ;; Use an alist `'(("key" . value) ...)` for each node. This is robust.
+          (let ((node-alist `(("id" . ,node-id)
+                              ("content" . ,content))))
+            (push node-alist nodes-to-process)))))
 
     ;; 2. Only send data to backend when there are eligible nodes.
     (if nodes-to-process
         (progn
           (message "Auto-tag: Preparing to send %d eligible nodes to backend for processing..." (length nodes-to-process))
-          ;; Construct the final payload using list format for proper EPC serialization
+          ;; Construct the final payload as a top-level alist, conforming to the unified data contract.
           (let* ((reversed-nodes (reverse nodes-to-process))
-                 (model-config (org-supertag-auto-tag--get-model-config))
-                 ;; Use list format instead of alist - EPC serializes this as Python dict
-                 (payload `(("nodes" ,reversed-nodes)
-                           ("model_config" ,model-config))))
-
-            ;; (message "--- ELISP DEBUG: PAYLOAD SENT ---")
-            ;; (message "Payload type: %s" (type-of payload))
-            ;; (message "Payload content: %S" payload)
-            ;; (message "--- END ELISP DEBUG ---")
-
-            ;; print the first 5 nodes
-            ;; (let ((node-count 0))
-            ;;   (dolist (node reversed-nodes)
-            ;;     (when (< node-count 5)
-            ;;       (message "--- Node %d ---" (1+ node-count))
-            ;;       (message "Node structure type: %s" (type-of node))
-            ;;       (message "Node is list with length: %d" (length node))
-            ;;       (dolist (pair node)
-            ;;         (when (listp pair)
-            ;;           (message "  %S => %S (type: %s)" (car pair) (cadr pair) (type-of (cadr pair)))))
-            ;;       (cl-incf node-count))))
-
-            ;; (message "--- Final Payload Structure ---")
-            ;; (message "Payload type: %s" (type-of payload))
-            ;; (message "Payload is list with length: %d" (length payload))
-            ;; (dolist (top-pair payload)
-            ;;   (when (listp top-pair)
-            ;;     (message "  Top-level %S => type: %s" (car top-pair) (type-of (cadr top-pair)))
-            ;;     (when (string= (car top-pair) "nodes")
-            ;;       (message "    Nodes count: %d" (length (cadr top-pair))))
-            ;;     (when (string= (car top-pair) "model_config")
-            ;;       (message "    Model config: %S" (cadr top-pair)))))
-            ;; (message "=== END ELISP DEBUG ===")
-
+                 (model-config (org-supertag-api--get-model-config-for-tagging))
+                 ;; The final payload is a single alist.
+                 (payload `(("nodes" . ,reversed-nodes)
+                            ("model_config" . ,model-config))))
             ;; (message "Auto-tag DEBUG: Sending payload with %d nodes to API layer." (length reversed-nodes))
             ;; The API layer will wrap this payload in a list before sending.
             (org-supertag-api-batch-generate-tags
-             (list payload) ;; Wrap the payload in a list to conform to the data contract
+             payload
              #'org-supertag-auto-tag--batch-handle-completion)))
-
+      
       ;; If no eligible nodes, just print a message, do nothing.
       (message "Auto-tag: All untagged nodes' content is too short or cannot be extracted, skipped."))))
 
-(defun org-supertag-auto-tag--get-model-config ()
-  "Get model configuration for auto-tagging from this module's custom variables.
-This ensures auto-tagging is self-contained and not dependent on other modules like `org-supertag-ai`."
-  (list
-   (cons 'provider org-supertag-auto-tag-provider)
-   (cons 'model org-supertag-auto-tag-model)
-   (cons 'temperature org-supertag-auto-tag-temperature)
-   (cons 'max_tokens org-supertag-auto-tag-max-tokens)
-   (cons 'endpoint org-supertag-auto-tag-endpoint)))
-
 (defun org-supertag-auto-tag--get-node-content (node-data)
   "Extract content for analysis from node data, including title and body."
   (let* ((file-path (plist-get node-data :file-path))
@@ -280,34 +231,35 @@ This ensures auto-tagging is self-contained and not dependent on other modules l
      title
      "")))
 
-(defun org-supertag-auto-tag--batch-handle-completion (results)
-  "Handle batch processing completion callback from Python backend.
-RESULTS is a vector of plists, each representing a suggestion for a node."
-  (message "Auto-tag: Received %d suggestions from backend." (length results))
-  (let ((new-suggestions-count 0))
-    (dolist (suggestion results)
-      (let ((node-id (plist-get suggestion :node_id))
-            (tags (plist-get suggestion :tags)))
-        (when (and node-id tags)
-          ;; Check if a suggestion for this node already exists to avoid duplicates.
-          (unless (cl-some (lambda (q-item) (string= (plist-get q-item :node-id) node-id))
-                           org-supertag-auto-tag--suggestion-queue)
-            (let* ((node-data (org-supertag-db-get node-id))
-                   (content (or (gethash node-id org-supertag-auto-tag--node-content-cache)
-                                (org-supertag-auto-tag--get-node-content node-data))))
-              (when content
-                ;; Add to suggestion queue with a consistent structure
-                (push (list :node-id node-id
-                            :content content
-                            :tags tags)
-                      org-supertag-auto-tag--suggestion-queue)
-                (puthash node-id content org-supertag-auto-tag--node-content-cache)
-                (cl-incf new-suggestions-count)))))))
-    (if (> new-suggestions-count 0)
-        (message "Auto-tag: Added %d new suggestions to the queue. Total pending: %d."
-                 new-suggestions-count
-                 (length org-supertag-auto-tag--suggestion-queue))
-      (message "Auto-tag: No new suggestions were added (already in queue or invalid format)."))))
+(defun org-supertag-auto-tag--batch-handle-completion (result)
+  "Handle batch processing completion callback, unify suggestions into main suggestion queue."
+  (let* ((all-suggestions (plist-get result :suggestions))
+         (failed-count (or (plist-get result :failed_count) 0))
+         (new-suggestions-count 0))
+
+    (when all-suggestions
+      (dolist (node-suggestion-group all-suggestions)
+        (let* ((node-id (plist-get node-suggestion-group :node_id))
+               (suggestions-for-node (plist-get node-suggestion-group :suggestions))
+               (existing-tags (org-supertag-node-get-tags node-id)))
+
+          (dolist (suggestion-data suggestions-for-node)
+            (let ((tag-name (plist-get suggestion-data :tag_name)))
+              ;; Check if tag already exists on node
+              (unless (member tag-name existing-tags)
+                (let ((new-suggestion
+                       (list :node-id node-id
+                             :tag-name (plist-get suggestion-data :tag_name)
+                             :confidence (plist-get suggestion-data :confidence)
+                             :reasoning (plist-get suggestion-data :reasoning))))
+                  (cl-pushnew new-suggestion org-supertag-auto-tag--suggestion-queue :test 'equal)
+                  (cl-incf new-suggestions-count))))))))
+
+    (message "Auto-tag: Background processing completed: %d new suggestions, %d failed nodes."
+             new-suggestions-count failed-count)
+
+    (when (> new-suggestions-count 0)
+      (message "Auto-tag: New suggestions added to queue, use M-x org-supertag-auto-tag-batch-add to review."))))
 
 ;;; === Batch Tag Addition Interface ===
 
@@ -374,12 +326,9 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
            (nodes-to-process '())
            (node-ids-processed (make-hash-table :test 'equal)))
 
-      ;; Correctly build the suggestions hash table using the :tags key
-      (dolist (item org-supertag-auto-tag--suggestion-queue)
-        (let ((node-id (plist-get item :node-id))
-              (tags (plist-get item :tags)))
-          ;; Only store the actual list of tag suggestion plists
-          (puthash node-id tags suggestions-by-node)))
+      (dolist (suggestion org-supertag-auto-tag--suggestion-queue)
+        (let ((node-id (plist-get suggestion :node-id)))
+          (push suggestion (gethash node-id suggestions-by-node '()))))
 
       (maphash (lambda (node-id _suggestions)
                  (unless (gethash node-id node-ids-processed)
@@ -407,7 +356,7 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
     (org-supertag-auto-tag--batch-insert-compact-display)
     (when current-tag-info
       (goto-char (point-min))
-      (when (re-search-forward (format "^\\s-*\\(?:[✓□]\\) %s (" (regexp-quote (car current-tag-info))) nil t)
+      (when (re-search-forward (format "^\\s-*[✓□] %s (" (regexp-quote (car current-tag-info))) nil t)
         (beginning-of-line)))))
 
 (defun org-supertag-auto-tag--batch-insert-compact-display ()
@@ -425,8 +374,7 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
              (file-path (plist-get node-data :file-path))
              (suggestions (gethash node-id org-supertag-auto-tag--batch-all-suggestions))
              (selected-tags (gethash node-id org-supertag-auto-tag--batch-all-selected))
-             (progress (format "[%d/%d]" (1+ node-index) (length org-supertag-auto-tag--batch-nodes)))
-             (start (point)))
+             (progress (format "[%d/%d]" (1+ node-index) (length org-supertag-auto-tag--batch-nodes))))
 
         ;; Diagnostic messages for missing data
         (unless title
@@ -435,35 +383,40 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
           (message "Auto-tag Diagnostics: Node ID '%s' is missing a file-path." node-id))
 
         (when (> node-index 0)
-          (insert "\n" (propertize "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━" 'face 'org-meta-line) "\n\n"))
+          (insert "\n" (propertize (make-string 80 ?═) 'face 'org-meta-line) "\n\n"))
+        
+        (insert (propertize (format "%s 📄 %s" progress (or title "Untitled Node")) 'face 'bold) "\n")
+        (insert (propertize (format "    %s" (if file-path (file-name-nondirectory file-path) "<No File>")) 'face 'org-meta-line) "\n\n")
         
-        (let ((node-start (point)))
-          (insert (propertize (format "%s 📄 %s" progress (or title "Untitled Node")) 'face 'bold) "\n")
-          (insert (propertize (format "    %s" (if file-path (file-name-nondirectory file-path) "<No File>")) 'face 'org-meta-line) "\n\n")
-          (let ((content (org-supertag-auto-tag--get-node-content node-data)))
-            (when content
-              (let ((content-lines (split-string content "\n" t)))
-                (dotimes (i (min 2 (length content-lines)))
-                  (let ((line (nth i content-lines)))
-                    (when (and line (> (length (string-trim line)) 0))
-                      (insert "    " (substring line 0 (min (length line) 76))
-                              (if (> (length line) 76) "..." "") "\n")))))
-              (insert "\n")))
-          (insert (propertize "    🏷️ Suggested Tags:\n" 'face 'org-level-2))
-          (if (or (not suggestions) (seq-empty-p suggestions))
-              (insert "        No suggestions or generating...\n")
-            (dolist (tag-name suggestions)
-              (when (stringp tag-name) ; Add safeguard
-                (let* ((selected-p (member tag-name selected-tags))
-                       (line-start (point)))
-                  (insert (format "        %s %s\n"
-                                  (if selected-p "✓" "□")
-                                  (propertize tag-name 'face 'bold)))
-                  (put-text-property line-start (point) 'tag-name tag-name)
-                  (put-text-property line-start (point) 'node-id node-id)))))
-          (insert "\n")
-          (add-text-properties node-start (point) `(node-id ,node-id)))
+        (let ((content (org-supertag-auto-tag--get-node-content node-data)))
+          (when content
+            (let ((content-lines (split-string content "\n" t)))
+              (dotimes (i (min 2 (length content-lines)))
+                (let ((line (nth i content-lines)))
+                  (when (and line (> (length (string-trim line)) 0))
+                    (insert "    " (substring line 0 (min (length line) 76))
+                            (if (> (length line) 76) "..." "") "\n")))))
+            (insert "\n")))
+
+        (insert (propertize "    🏷️ Suggested Tags:\n" 'face 'org-level-2))
+        
+        (if (null suggestions)
+            (insert "        No suggestions or generating...\n")
+          (dolist (suggestion suggestions)
+            (let* ((tag-name (plist-get suggestion :tag-name))
+                   (confidence (plist-get suggestion :confidence))
+                   (selected-p (member tag-name selected-tags))
+                   (confidence-color (cond ((>= confidence 0.8) 'success)
+                                           ((>= confidence 0.6) 'warning)
+                                           (t 'error)))
+                   (line-start (point)))
+              (insert (format "        %s %s " (if selected-p "✓" "□") (propertize tag-name 'face 'bold)))
+              (insert (propertize (format "(%.2f)" confidence) 'face confidence-color) "\n")
+              (put-text-property line-start (point) 'tag-name tag-name)
+              (put-text-property line-start (point) 'node-id node-id))))
+        (insert "\n")
         (cl-incf node-index))))
+  
   (insert (propertize "j/k: node switch n/p: up/down move SPC: toggle select RET: apply current A: apply all M: manual add q: quit" 
                      'face 'org-meta-line)))
 
@@ -475,8 +428,8 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
   "Get current cursor tag name and node ID"
   (save-excursion
     (beginning-of-line)
-    (when (re-search-forward "^\\s-*\\(?:[✓□]\\) \\(.+?\\)\\s-*$" (line-end-position) t)
-      (let ((tag-name (string-trim (match-string-no-properties 1)))
+    (when (re-search-forward "^\\s-*[✓□] \\(.+?\\) (" (line-end-position) t)
+      (let ((tag-name (match-string-no-properties 1))
             (node-id (get-text-property (point) 'node-id)))
         (when node-id (cons tag-name node-id))))))
 
@@ -490,12 +443,8 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
       (if (member tag-name selected-tags)
           (puthash node-id (remove tag-name selected-tags) org-supertag-auto-tag--batch-all-selected)
         (puthash node-id (cons tag-name selected-tags) org-supertag-auto-tag--batch-all-selected))
-      (let ((pos (point))
-            (win-start (window-start)))
-        (org-supertag-auto-tag--batch-refresh-current-node)
-        ;; 恢复光标和窗口位置
-        (goto-char pos)
-        (set-window-start (selected-window) win-start)))))
+      (let ((inhibit-read-only t))
+        (org-supertag-auto-tag--batch-refresh-display)))))
 
 (defun org-supertag-auto-tag--batch-apply-current-tag ()
   "Apply current cursor tag"
@@ -538,30 +487,19 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
         (org-supertag-auto-tag--batch-refresh-display)))))
 
 (defun org-supertag-auto-tag--apply-tag-to-node (node-id tag-name)
-  "Apply tag to node, ensuring it's fully registered in the database and file."
-  (message "Applying tag '%s' to node '%s'..." tag-name node-id)
-  ;; 1. Sanitize the tag name to get a valid ID.
-  (let ((tag-id (org-supertag-sanitize-tag-name tag-name)))
-    ;; 2. Check if the tag object exists in the database. If not, create it.
-    (unless (org-supertag-tag-get tag-id)
-      (message "Tag '%s' not found in DB, creating it." tag-id)
-      (org-supertag-tag--create tag-id))
-    
-    ;; 3. Link the node to the tag in the database.
-    (message "Linking node '%s' to tag '%s'." node-id tag-id)
+  "Apply tag to node (database and file)."
+  (let* ((tag-result (org-supertag-inline--ensure-tag tag-name))
+         (tag-id (plist-get tag-result :tag-id)))
     (org-supertag-node-db-add-tag node-id tag-id)
+    (org-supertag-inline-insert-tag-for-node node-id tag-name)
     
-    ;; 4. Insert the tag visually into the Org file.
-    (message "Inserting tag '%s' into file for node '%s'." tag-name node-id)
-    (org-supertag-inline-insert-tag-for-autotag node-id tag-name)
-    
-    ;; 5. Remove the corresponding suggestion from the queue after applying.
+    ;; Remove processed suggestions from queue
     (setq org-supertag-auto-tag--suggestion-queue
-          (cl-remove-if
-           (lambda (q-item)
-             (string= (plist-get q-item :node-id) node-id))
-           org-supertag-auto-tag--suggestion-queue))
-    (message "Tag '%s' applied and all suggestions for this node removed from queue." tag-name)))
+          (seq-remove (lambda (s)
+                        (and (equal (plist-get s :node-id) node-id)
+                             (equal (plist-get s :tag-name) tag-name)))
+                      org-supertag-auto-tag--suggestion-queue)))
+  (message "Applied tag '%s' to node" tag-name))
 
 (defun org-supertag-auto-tag-get-tags-from-llm (nodes callback)
   "Send text chunks in NODES to LLM and get tags asynchronously."
@@ -595,79 +533,4 @@ RESULTS is a vector of plists, each representing a suggestion for a node."
        (unless result
          (message "Auto-tagging returned no result."))))))
 
-(defun org-supertag-auto-tag--apply-suggestion-for-node (node-id tag-suggestions)
-  "Apply selected tag suggestions to a specific node.
-This function now correctly registers the tag in the database."
-  (dolist (suggestion tag-suggestions)
-    (let ((tag-name (plist-get suggestion :tag-name)))
-      (when (and (stringp tag-name) (not (string-empty-p tag-name)))
-        (message "Auto-tag: Applying suggestion '%s' to node '%s'" tag-name node-id)
-        
-        ;; --- Database Registration Logic ---
-        ;; 1. Sanitize and ensure the tag object exists in the database.
-        (let ((sanitized-tag-name (org-supertag-sanitize-tag-name tag-name)))
-          (unless (org-supertag-tag-get sanitized-tag-name)
-            (org-supertag-tag--create sanitized-tag-name))
-          
-          ;; 2. Link the node to the tag in the database.
-          (org-supertag-node-db-add-tag node-id sanitized-tag-name))
-
-        ;; --- UI/File Update Logic ---
-        ;; 3. Visually add the tag to the Org file as an inline tag.
-        (org-supertag-inline-tag-add (list tag-name) node-id)))))
-
-(defun org-supertag-auto-tag--get-all-suggestions-for-node (node-id)
-  "Get all suggestions for a given NODE-ID from the suggestion-queue."
-  (seq-filter (lambda (s)
-                (and (equal (plist-get s :node-id) node-id)
-                     (stringp (plist-get s :tag-name))))
-              org-supertag-auto-tag--suggestion-queue))
-
-(defun org-supertag-auto-tag--batch-refresh-current-node ()
-  "Only refresh the display of the current node, not the entire buffer."
-  (let* ((node-id (org-supertag-auto-tag--batch-get-current-node-id)))
-    (when node-id
-      (save-excursion
-        (let ((inhibit-read-only t))
-          (let* ((start (previous-single-property-change (point) 'node-id nil (point-min)))
-                 (end (next-single-property-change (point) 'node-id nil (point-max))))
-            (when (and start end)
-              (goto-char start)
-              (delete-region start end)
-              (let* ((node-entry (assoc node-id org-supertag-auto-tag--batch-nodes))
-                     (node-data (cdr node-entry))
-                     (title (plist-get node-data :title))
-                     (file-path (plist-get node-data :file-path))
-                     (suggestions (gethash node-id org-supertag-auto-tag--batch-all-suggestions))
-                     (selected-tags (gethash node-id org-supertag-auto-tag--batch-all-selected))
-                     (progress (format "[%d/%d]"
-                                       (1+ (cl-position node-entry org-supertag-auto-tag--batch-nodes :test #'equal))
-                                       (length org-supertag-auto-tag--batch-nodes)))
-                     (node-start (point)))
-                (insert (propertize (format "%s 📄 %s" progress (or title "Untitled Node")) 'face 'bold) "\n")
-                (insert (propertize (format "    %s" (if file-path (file-name-nondirectory file-path) "<No File>")) 'face 'org-meta-line) "\n\n")
-                (let ((content (org-supertag-auto-tag--get-node-content node-data)))
-                  (when content
-                    (let ((content-lines (split-string content "\n" t)))
-                      (dotimes (i (min 2 (length content-lines)))
-                        (let ((line (nth i content-lines)))
-                          (when (and line (> (length (string-trim line)) 0))
-                            (insert "    " (substring line 0 (min (length line) 76))
-                                    (if (> (length line) 76) "..." "") "\n")))))
-                    (insert "\n")))
-                (insert (propertize "    🏷️ Suggested Tags:\n" 'face 'org-level-2))
-                (if (or (not suggestions) (seq-empty-p suggestions))
-                    (insert "        No suggestions or generating...\n")
-                  (dolist (tag-name suggestions)
-                    (when (stringp tag-name) ; Add safeguard
-                      (let* ((selected-p (member tag-name selected-tags))
-                             (line-start (point)))
-                        (insert (format "        %s %s\n"
-                                        (if selected-p "✓" "□")
-                                        (propertize tag-name 'face 'bold)))
-                        (put-text-property line-start (point) 'tag-name tag-name)
-                        (put-text-property line-start (point) 'node-id node-id)))))
-                (insert "\n")
-                (add-text-properties node-start (point) `(node-id ,node-id))))))))))
-
 (provide 'org-supertag-auto-tag)
diff --git a/org-supertag-background-sync.el b/org-supertag-background-sync.el
index 3533449..809a4e0 100755
--- a/org-supertag-background-sync.el
+++ b/org-supertag-background-sync.el
@@ -21,7 +21,6 @@
 (require 'org-supertag-bridge)
 (require 'org-supertag-api)
 (require 'ht) ;; Ensure ht is required
-(require 'org-supertag-scheduler)
 
 ;; === Configuration variables ===
 
@@ -52,37 +51,19 @@ Example: '(\"09:00\" \"18:00\") for sync at 9 AM and 6 PM daily."
   :type '(repeat string)
   :group 'org-supertag)
 
-(defcustom org-supertag-tag-refresh-time "03:00"
-  "Daily time (HH:MM) to refresh STALE tag embeddings via embedding/refresh_stale_tags."
-  :type 'string
-  :group 'org-supertag)
-
-;; === Knowledge cycle configuration ===
-
-(defcustom org-supertag-knowledge-cycle-interval 300
-  "Interval (in seconds) for triggering the backend knowledge extraction cycle.
-This controls how often Emacs 调用 `knowledge/run_cycle` 以及队列检查逻辑。
-建议值：300 (5 分钟) 到 1800 (30 分钟) 之间。"
-  :type 'integer
-  :group 'org-supertag)
-
 ;; === Runtime variables ===
 
-(defvar org-supertag-background-sync--registered-task-ids nil
-  "A list of task IDs registered with the central scheduler.")
-
-(defvar org-supertag-background-sync--phase :idle
-  "Background sync phase. Possible values:
-- :idle            - Idle
-- :waiting-backend - Waiting for Python backend to be ready
-- :embedding       - Phase 1: Syncing embeddings and metadata
-- :reasoning       - Phase 2: Inferring relations for nodes")
-
 (defvar org-supertag-background-sync--timer nil
-  "Timer for interval-based background sync.")
+  "Background sync timer.")
 
 (defvar org-supertag-background-sync--schedule-timer nil
-  "Timer for scheduled background sync.")
+  "Scheduled sync timer (for daily checks).")
+
+(defvar org-supertag-background-sync--state :idle
+  "Background sync state. Possible values:
+- :idle            - Idle state
+- :waiting-backend - Waiting for Python backend to be ready
+- :syncing         - Syncing")
 
 (defvar org-supertag-background-sync--last-sync-time nil
   "Last successful sync time.")
@@ -106,24 +87,19 @@ Value: hash string")
   (expand-file-name "sync_hashes.json" org-supertag-data-directory)
   "Hash record persistence file path.")
 
-;;------------------------------------------------------------------------------ 
-;; Hash Persistence 
-;;------------------------------------------------------------------------------
+;; === Hash Persistence ===
 
 (defun org-supertag-background-sync--save-hashes ()
-  "Save the current sync hashes to the hash file in JSON format.
-Only writes the file if the hash table is not empty to avoid creating
-empty or invalid JSON files."
+  "Save the current sync hashes to the hash file in JSON format."
   (interactive)
-  (unless (hash-table-empty-p org-supertag-background-sync--last-sync-hashes)
-    (require 'json)
-    (org-supertag-background-sync--ensure-data-directory)
-    (let ((temp-file (make-temp-file "sync-hashes-json-")))
-      (with-temp-buffer
-        (insert (json-encode org-supertag-background-sync--last-sync-hashes))
-        (write-file temp-file nil))
-      (rename-file temp-file org-supertag-background-sync--hash-file t)
-      (message "[background-sync] Hashes saved to %s" org-supertag-background-sync--hash-file))))
+  (require 'json)
+  (org-supertag-background-sync--ensure-data-directory)
+  (let ((temp-file (make-temp-file "sync-hashes-json-")))
+    (with-temp-buffer
+      (insert (json-encode org-supertag-background-sync--last-sync-hashes))
+      (write-file temp-file nil))
+    (rename-file temp-file org-supertag-background-sync--hash-file t)
+    (message "[background-sync] Hashes saved to %s" org-supertag-background-sync--hash-file)))
 
 (defun org-supertag-background-sync--load-hashes ()
   "Load sync hashes from the JSON hash file if it exists.
@@ -131,136 +107,41 @@ If the file doesn't exist, automatically create a baseline hash file
 from current database state to enable incremental sync."
   (interactive)
   (if (file-exists-p org-supertag-background-sync--hash-file)
-      (let ((json-string (with-temp-buffer
-                           (insert-file-contents org-supertag-background-sync--hash-file)
-                           (buffer-string))))
-        (if (or (null json-string) (string-empty-p (string-trim json-string)))
-            (progn
-              (message "[background-sync] Hash file is empty. Creating baseline...")
-              (org-supertag-background-sync--create-baseline-hashes))
-          (require 'json)
-          (let* ((data-hash (condition-case nil
-                              (json-parse-string json-string)
-                            (error
-                             (json-read-from-string json-string)))))
-            (clrhash org-supertag-background-sync--last-sync-hashes)
-            (cond
-             ((hash-table-p data-hash)
-              (maphash (lambda (key value)
-                         (puthash key value org-supertag-background-sync--last-sync-hashes))
-                       data-hash))
-             ((listp data-hash)
-              (dolist (pair data-hash)
-                (let ((key (if (symbolp (car pair))
-                              (symbol-name (car pair))
-                            (car pair))))
-                  (puthash key (cdr pair) org-supertag-background-sync--last-sync-hashes)))))
-            (message "[background-sync] Hashes loaded from JSON file %s. Total: %d"
-                     org-supertag-background-sync--hash-file
-                     (hash-table-count org-supertag-background-sync--last-sync-hashes)))))
-    ;; Hash file doesn't exist - check if database has content before creating baseline
-    (progn
-      (message "[background-sync] JSON hash file not found.")
-      (if (and (boundp 'org-supertag-db--object)
-               (hash-table-p org-supertag-db--object)
-               (> (hash-table-count org-supertag-db--object) 0))
-          (progn
-            (message "[background-sync] Database has content. Creating baseline from current database state...")
-            (org-supertag-background-sync--create-baseline-hashes)
-            (message "[background-sync] Baseline created with %d hash records. Incremental sync now enabled."
-                     (hash-table-count org-supertag-background-sync--last-sync-hashes)))
-        (progn
-          (message "[background-sync] Database is empty or not loaded. Hash baseline will be created when database has content.")
-          (clrhash org-supertag-background-sync--last-sync-hashes))))))
-
-;;;###autoload
-(defun org-supertag-background-sync-regenerate-baseline ()
-  "Manually regenerate the baseline hash file from current database state.
-This is useful when the hash file is missing or corrupted, or when you want to
-reset the incremental sync baseline."
-  (interactive)
-  (if (and (boundp 'org-supertag-db--object)
-           (hash-table-p org-supertag-db--object)
-           (> (hash-table-count org-supertag-db--object) 0))
       (progn
-        (message "[background-sync] Regenerating baseline from current database state...")
-        (org-supertag-background-sync--create-baseline-hashes)
-        (message "[background-sync] Baseline regenerated successfully with %d hash records."
-                 (hash-table-count org-supertag-background-sync--last-sync-hashes)))
-    (message "[background-sync] Cannot regenerate baseline: database is empty or not loaded.")))
-
-;;;###autoload
-(defun org-supertag-background-sync-check-hash-status ()
-  "Check the status of the hash file and provide recommendations."
-  (interactive)
-  (let ((hash-file-exists (file-exists-p org-supertag-background-sync--hash-file))
-        (hash-count (hash-table-count org-supertag-background-sync--last-sync-hashes))
-        (db-objects (if (and (boundp 'org-supertag-db--object)
-                            (hash-table-p org-supertag-db--object))
-                       (hash-table-count org-supertag-db--object)
-                     0))
-        (db-links (if (and (boundp 'org-supertag-db--link)
-                          (hash-table-p org-supertag-db--link))
-                     (hash-table-count org-supertag-db--link)
-                   0)))
-    
-    (with-current-buffer (get-buffer-create "*org-supertag-hash-status*")
-      (erase-buffer)
-      (insert "=== org-supertag Background Sync Hash Status ===\n\n")
-      
-      (insert (format "Hash file path: %s\n" org-supertag-background-sync--hash-file))
-      (insert (format "Hash file exists: %s\n" (if hash-file-exists "Yes" "No")))
-      (insert (format "In-memory hash count: %d\n" hash-count))
-      (insert (format "Database objects: %d\n" db-objects))
-      (insert (format "Database links: %d\n\n" db-links))
-      
-      (cond
-       ;; Case 1: Everything looks good
-       ((and hash-file-exists (> hash-count 0) (> db-objects 0))
-        (insert "✅ Status: GOOD\n")
-        (insert "Hash file exists and contains data. Incremental sync is working properly.\n"))
-       
-       ;; Case 2: Hash file missing but database has content
-       ((and (not hash-file-exists) (> db-objects 0))
-        (insert "⚠️  Status: HASH FILE MISSING\n")
-        (insert "Database has content but hash file is missing.\n")
-        (insert "Recommendation: Run M-x org-supertag-background-sync-regenerate-baseline\n"))
-       
-       ;; Case 3: Hash file exists but empty
-       ((and hash-file-exists (= hash-count 0) (> db-objects 0))
-        (insert "⚠️  Status: HASH FILE EMPTY\n")
-        (insert "Hash file exists but contains no data.\n")
-        (insert "Recommendation: Run M-x org-supertag-background-sync-regenerate-baseline\n"))
-       
-       ;; Case 4: Database is empty
-       ((= db-objects 0)
-        (insert "ℹ️  Status: DATABASE EMPTY\n")
-        (insert "Database has no content. Hash baseline will be created automatically\n")
-        (insert "when database is populated with nodes.\n"))
-       
-       ;; Case 5: Mismatch between hash count and database content
-       ((and (> hash-count 0) (> db-objects 0) 
-             (> (abs (- hash-count (+ db-objects db-links))) 10))
-        (insert "⚠️  Status: HASH/DATABASE MISMATCH\n")
-        (insert "Significant difference between hash count and database content.\n")
-        (insert "This may indicate the hash file is outdated.\n")
-        (insert "Recommendation: Run M-x org-supertag-background-sync-regenerate-baseline\n"))
-       
-       ;; Default case
-       (t
-        (insert "❓ Status: UNKNOWN\n")
-        (insert "Unable to determine status. Please check manually.\n")))
-      
-      (insert "\n=== Actions ===\n")
-      (insert "• M-x org-supertag-background-sync-regenerate-baseline - Regenerate hash baseline\n")
-      (insert "• M-x org-supertag-background-sync-reset-and-force-resync - Reset and force full resync\n")
-      (insert "• M-x org-supertag-background-sync-status - Check background sync status\n")
-      
-      (display-buffer (current-buffer)))
-    
-    (message "Hash status displayed in *org-supertag-hash-status* buffer")))
+        (require 'json)
+        (let* ((json-string (with-temp-buffer
+                              (insert-file-contents org-supertag-background-sync--hash-file)
+                              (buffer-string)))
+               ;; Use json-parse-string with string keys to avoid symbol conversion
+               (data-hash (condition-case nil
+                            (json-parse-string json-string)
+                          (error
+                           ;; Fallback to old method if json-parse-string not available
+                           (json-read-from-string json-string)))))
+          (clrhash org-supertag-background-sync--last-sync-hashes)
+          (cond
+           ;; If we got a hash table from json-parse-string, iterate directly
+           ((hash-table-p data-hash)
+            (maphash (lambda (key value)
+                       (puthash key value org-supertag-background-sync--last-sync-hashes))
+                     data-hash))
+           ;; If we got an alist from json-read-from-string, convert symbol keys to strings
+           ((listp data-hash)
+            (dolist (pair data-hash)
+              (let ((key (if (symbolp (car pair)) 
+                            (symbol-name (car pair))
+                          (car pair))))
+                (puthash key (cdr pair) org-supertag-background-sync--last-sync-hashes)))))
+          (message "[background-sync] Hashes loaded from JSON file %s. Total: %d"
+                   org-supertag-background-sync--hash-file
+                   (hash-table-count org-supertag-background-sync--last-sync-hashes))))
+    ;; Hash file doesn't exist - create baseline from current database state
+    (progn
+      (message "[background-sync] JSON hash file not found. Creating baseline from current database state...")
+      (org-supertag-background-sync--create-baseline-hashes)
+      (message "[background-sync] Baseline created with %d hash records. Incremental sync now enabled."
+               (hash-table-count org-supertag-background-sync--last-sync-hashes)))))
 
-;; Hash calculation functions 
 (defun org-supertag-background-sync--create-baseline-hashes ()
   "Create baseline hashes from current database state.
 This scans all objects and links in the current database and calculates
@@ -274,17 +155,11 @@ incremental sync when no hash file exists."
                  (plist-get props :node-id)
                  (plist-get props :tag-id)
                  (plist-get props :link-id)
-                 ;; 修复链接 ID 提取逻辑
-                 (when (or (eq (plist-get props :type) :link)
-                          (eq (plist-get props :type) :tag-relation)
-                          (string-match-p "relation" (format "%s" (plist-get props :type))))
-                   (let ((source (plist-get props :source))
-                         (target (plist-get props :target)))
-                     (when (and source target)
-                       (format "%s->%s" source target))))
-                 ;; 如果以上都失败，尝试使用哈希表的 key 作为 ID
-                 ;; 这在 maphash 中会作为第一个参数传入
-                 nil))) ; 在 maphash 中，我们需要使用传入的 id 参数
+                 ;; For links, try source/target as fallback
+                 (when (eq (plist-get props :type) :link)
+                   (format "%s->%s" 
+                           (or (plist-get props :source) "")
+                           (or (plist-get props :target) ""))))))
     
     (let ((processed-objects 0)
           (processed-links 0)
@@ -293,57 +168,35 @@ incremental sync when no hash file exists."
       ;; 1. Process all objects in the main object table
       (maphash
        (lambda (id props)
-         ;; Only process objects that are not :metadata
-         (unless (eq (plist-get props :type) :metadata)
-           (let ((actual-id (or (extract-id props) id)))
-             (when actual-id
-               (puthash actual-id t current-ids)
-               (let* ((type-key (when (string-match "^:\\([^:]+\\):" id)
-                                  (intern (concat ":" (match-string 1 id)))))
-                      (props-with-type (if type-key
-                                           (plist-put (copy-sequence props) :type type-key)
-                                         props))
-                      (current-hash (org-supertag-background-sync--calculate-object-hash props-with-type))
-                      (last-hash (gethash actual-id org-supertag-background-sync--last-sync-hashes)))
-                 (unless (and current-hash last-hash (string= current-hash last-hash))
-                   (cl-incf changed-count)
-                   (push props-with-type links-to-upsert)
-                   ;; 同步其关联实体
-                   (when-let ((source-id (plist-get props-with-type :from)))
-                     (unless (gethash source-id entities-to-upsert)
-                       (when-let ((entity-data (org-supertag-db-get source-id)))
-                         (puthash source-id entity-data entities-to-upsert))))
-                   (when-let ((target-id (plist-get props-with-type :to)))
-                     (unless (gethash target-id entities-to-upsert)
-                       (when-let ((entity-data (org-supertag-db-get target-id)))
-                         (puthash target-id entity-data entities-to-upsert)))))))))
+         (let ((actual-id (extract-id props)))
+           (if actual-id
+               (condition-case err
+                   (let ((hash (org-supertag-background-sync--calculate-object-hash props)))
+                     (when hash
+                       (puthash actual-id hash org-supertag-background-sync--last-sync-hashes)
+                       (cl-incf processed-objects)))
+                 (error
+                  (cl-incf failed-objects)
+                  (message "[background-sync] Failed to hash object %s: %s" 
+                           actual-id (error-message-string err))))
+             (cl-incf failed-objects))))
        org-supertag-db--object)
       
       ;; 2. Process all links in the link table
       (maphash
        (lambda (id props)
-         (unless (eq (plist-get props :type) :metadata)
-           (let ((actual-id (or (extract-id props) id)))
-             (when actual-id
-               (puthash actual-id t current-ids)
-               (let* ((type-key (when (string-match "^:\\([^:]+\\):" id)
-                                  (intern (concat ":" (match-string 1 id)))))
-                      (props-with-type (if type-key
-                                           (plist-put (copy-sequence props) :type type-key)
-                                         props))
-                      (current-hash (org-supertag-background-sync--calculate-object-hash props-with-type))
-                      (last-hash (gethash actual-id org-supertag-background-sync--last-sync-hashes)))
-                 (unless (and current-hash last-hash (string= current-hash last-hash))
-                   (cl-incf changed-count)
-                   (push props-with-type links-to-upsert)
-                   (when-let ((source-id (plist-get props-with-type :from)))
-                     (unless (gethash source-id entities-to-upsert)
-                       (when-let ((entity-data (org-supertag-db-get source-id)))
-                         (puthash source-id entity-data entities-to-upsert))))
-                   (when-let ((target-id (plist-get props-with-type :to)))
-                     (unless (gethash target-id entities-to-upsert)
-                       (when-let ((entity-data (org-supertag-db-get target-id)))
-                         (puthash target-id entity-data entities-to-upsert)))))))))
+         (let ((actual-id (extract-id props)))
+           (if actual-id
+               (condition-case err
+                   (let ((hash (org-supertag-background-sync--calculate-object-hash props)))
+                     (when hash
+                       (puthash actual-id hash org-supertag-background-sync--last-sync-hashes)
+                       (cl-incf processed-links)))
+                 (error
+                  (cl-incf failed-objects)
+                  (message "[background-sync] Failed to hash link %s: %s" 
+                           actual-id (error-message-string err))))
+             (cl-incf failed-objects))))
        org-supertag-db--link)
       
       ;; 3. Save the baseline to file
@@ -361,611 +214,261 @@ incremental sync when no hash file exists."
       (list :processed-objects processed-objects
             :processed-links processed-links
             :failed-objects failed-objects
-            :total-hashes (hash-table-count org-supertag-background-sync--last-sync-hashes)))))))
+            :total-hashes (hash-table-count org-supertag-background-sync--last-sync-hashes)))))
 
-
-(defun org-supertag-background-sync-create-baseline ()
-  "Create baseline hashes from current database state."
-  (interactive)
-  (org-supertag-background-sync--create-baseline-hashes))  
+;; === Hash calculation functions ===
 
 (defun org-supertag-background-sync--calculate-object-hash (props)
-  "Calculate object hash based on its type and core content fields.
-For `:node`, hash is based on `:title`, `:content`, and `:tags`.
-For `:tag`, hash is based on `:id`.
-For links, hash is based on their core properties like `:type`, `:from`, and `:to`.
-This ensures the hash is stable and only changes when meaningful data is modified."
+  "Calculate object hash from its property list.
+Excludes time-sensitive and position-sensitive fields to ensure stable hashes."
   (when (plistp props)
-    (let* ((type (plist-get props :type))
-           (stable-props
-            (pcase type
-              (:node
-               (let (plist)
-                 (when-let ((title (plist-get props :title)))
-                   (setq plist (plist-put plist :title title)))
-                 (when-let ((content (plist-get props :content)))
-                   (setq plist (plist-put plist :content content)))
-                 (when-let ((tags (plist-get props :tags)))
-                   ;; Sort tags to ensure hash is stable regardless of order.
-                   (setq plist (plist-put plist :tags (sort (copy-sequence tags) #'string<))))
-                 plist))
-              (:tag
-               (let (plist)
-                 (when-let ((id (plist-get props :id)))
-                   (setq plist (plist-put plist :id id)))
-                 plist))
-              ;; Handle all known link types.
-              ((or :node-tag :tag-ref :tag-tag :relation-group :relation-member)
-               (let (plist)
-                 (setq plist (plist-put plist :type type))
-                 (when-let ((from (plist-get props :from)))
-                   (setq plist (plist-put plist :from from)))
-                 (when-let ((to (plist-get props :to)))
-                   (setq plist (plist-put plist :to to)))
-                 plist))
-              (:node-field
-               (let (plist)
-                 (setq plist (plist-put plist :type type))
-                 (when-let ((from (plist-get props :from)))
-                   (setq plist (plist-put plist :from from)))
-                 (when-let ((to (plist-get props :to)))
-                   (setq plist (plist-put plist :to to)))
-                 (when-let ((value (plist-get props :value)))
-                   (setq plist (plist-put plist :value value)))
-                 (when-let ((tag-id (plist-get props :tag-id)))
-                   (setq plist (plist-put plist :tag-id tag-id)))
-                 plist))
-              ;; Default/fallback for any other types.
-              (_
-               (let (plist)
-                 (when-let ((id (plist-get props :id)))
-                   (setq plist (plist-put plist :id id)))
-                 plist)))))
-      ;; Only calculate hash if we have stable properties to work with.
-      (when stable-props
-        (let ((hash-content (format "%S" stable-props)))
-          (secure-hash 'sha1 hash-content))))))
-
-(defun org-supertag-background-sync--is-alist-p (data)
-  "Check if DATA is likely an alist (a list of cons pairs).
-This is a dependency-free replacement for `(every #'consp data)`."
-  (and (listp data)
-       (let ((res t))
-         (dolist (x data)
-           (unless (consp x) (setq res nil)))
-         res)))
+    (let ((stable-props '())
+          ;; Fields to exclude from hash calculation.
+          (excluded-fields '(:modified-at :created-at :last-modified 
+                            :timestamp :update-time :sync-time
+                            ;; Exclude positional fields
+                            :pos :begin :contents-begin :contents-end :olp
+                            ;; File path is essential, but other transient fields should be excluded.
+                            )))
+      ;; Build a new plist with only stable fields
+      (let ((remaining props))
+        (while remaining
+          (let ((key (car remaining))
+                (value (cadr remaining)))
+            (unless (memq key excluded-fields)
+              (setq stable-props (append stable-props (list key value))))
+            (setq remaining (cddr remaining)))))
+      ;; Calculate hash from stable properties only
+      (let ((hash-content (format "%S" stable-props)))
+        (secure-hash 'sha1 hash-content)))))
 
-(defun org-supertag-background-sync--deep-prepare-for-python (data)
-  "Recursively normalize any Lisp data into a clean structure for JSON serialization.
-The primary goal is to convert any object-like structure (hash-table, plist)
-into an association list (alist) with string keys, which `json-encode`
-correctly serializes into a JSON object."
-  (cond
-   ;; --- Base Cases for Atoms ---
-   ((null data) nil)
-   ((stringp data) data)
-   ((numberp data) data)
-   ((eq data t) t)
-
-   ;; --- Recursive Cases ---
-   ;; Hash-table: convert to alist and recurse.
-   ((hash-table-p data)
-    (org-supertag-background-sync--deep-prepare-for-python (ht->alist data)))
-
-   ;; Symbol: convert to string (keywords are stripped of ':').
-   ((symbolp data)
-    (if (keywordp data)
-        (substring (symbol-name data) 1)
-      (symbol-name data)))
+(defun org-supertag-background-sync--get-changed-objects ()
+  "Get all changed objects (nodes, tags, links) since the last sync.
+This function iterates over both `org-supertag-db--object` and
+`org-supertag-db--link` to build a complete picture of the changes."
+  ;; Helper function to extract ID from properties (same as in update-hashes)
+  (cl-flet ((extract-id (props)
+             (or (plist-get props :id)
+                 (plist-get props :node-id)
+                 (plist-get props :tag-id)
+                 (plist-get props :link-id)
+                 ;; For links, try source/target as fallback
+                 (when (eq (plist-get props :type) :link)
+                   (format "%s->%s" 
+                           (or (plist-get props :source) "")
+                           (or (plist-get props :target) ""))))))
+    
+    (let ((nodes-to-upsert '())
+          (tags-to-upsert '())
+          (links-to-upsert '())
+          (ids-to-delete '())
+          (current-ids (make-hash-table :test 'equal))
+          (changed-count 0))
+
+      ;; 1. Find created/updated objects from the main object table
+      (maphash
+       (lambda (id props)
+         (let ((actual-id (extract-id props)))
+           (when actual-id
+             (puthash actual-id t current-ids) ; Track current IDs using extracted ID
+             (let ((current-hash (org-supertag-background-sync--calculate-object-hash props))
+                   (last-hash (gethash actual-id org-supertag-background-sync--last-sync-hashes)))
+               (unless (and current-hash last-hash (string= current-hash last-hash))
+                 (cl-incf changed-count)
+                 (let ((type (plist-get props :type)))
+                   (cond
+                    ((eq type :node) (push props nodes-to-upsert))
+                    ((eq type :tag) (push props tags-to-upsert))
+                    (t nil))))))))
+       org-supertag-db--object)
 
-   ;; List-like structures (the most complex case).
-   ((consp data)
-    (cond
-     ;; Heuristic for Plist: a list starting with a keyword.
-     ;; This must be checked before other list types.
-     ((and (keywordp (car data)) (plistp data))
-      (let (alist)
-        (while (and data (cdr data)) ; Safe for odd-length plists
-          (push (cons (pop data) (pop data)) alist))
-        ;; After converting plist to alist, recurse on the alist.
-        (org-supertag-background-sync--deep-prepare-for-python (nreverse alist))))
-
-     ;; Heuristic for Alist: a list where each element is a cons pair.
-     ;; This is the target format for JSON objects.
-     ((org-supertag-background-sync--is-alist-p data)
-      (mapcar (lambda (pair)
-                (cons (org-supertag-background-sync--deep-prepare-for-python (car pair))
-                      (org-supertag-background-sync--deep-prepare-for-python (cdr pair))))
-              data))
-
-     ;; Proper list of other items.
-     ((listp data)
-      (mapcar #'org-supertag-background-sync--deep-prepare-for-python data))
-
-     ;; Fallback for improper lists or dotted pairs.
-     (t
-      (cons (org-supertag-background-sync--deep-prepare-for-python (car data))
-            (org-supertag-background-sync--deep-prepare-for-python (cdr data))))))
-
-   ;; --- Fallback for any other data type ---
-   (t data)))
+      ;; 2. Find created/updated links from the link table
+      (maphash
+       (lambda (id props)
+         (let ((actual-id (extract-id props)))
+           (when actual-id
+             (puthash actual-id t current-ids) ; Also track link IDs using extracted ID
+             (let ((current-hash (org-supertag-background-sync--calculate-object-hash props))
+                   (last-hash (gethash actual-id org-supertag-background-sync--last-sync-hashes)))
+               (unless (and current-hash last-hash (string= current-hash last-hash))
+                 (cl-incf changed-count)
+                 (push props links-to-upsert))))))
+       org-supertag-db--link)
 
-;; ---------------------------------------------------------------------------
-;; Incremental change detection
-;; ---------------------------------------------------------------------------
-(defun org-supertag-background-sync--get-changed-objects ()
-  "Scan DB, compare with baseline hashes, and return (ENTITIES LINKS IDS-TO-DELETE).
-
-ENTITIES     - list of node/tag property plists that need to be upserted.
-LINKS        - list of link property plists that need to be upserted.
-IDS-TO-DELETE - list of IDs that existed in previous baseline but are now missing.
-
-The function skips any object whose `:type` is `:metadata`.
-It also ensures each link plist contains both `:link-id` and `:id` so that
-subsequent hash-update logic can extract identifiers reliably."
-  (let* ((entities-to-upsert (make-hash-table :test 'equal)) ; id -> props
-         (links-to-upsert   '())
-         (current-ids       (make-hash-table :test 'equal)))
-
-    ;; Helper to extract stable ID from props or fallback to key supplied by maphash
-    (cl-labels
-        ((extract-id
-          (id-from-hash props)
-          (or (plist-get props :id)
-              (plist-get props :node-id)
-              (plist-get props :tag-id)
-              (plist-get props :link-id)
-              id-from-hash))
-         (ensure-entity-upsert
-          (entity-id)
-          (when (and entity-id (not (gethash entity-id entities-to-upsert)))
-            (when-let ((entity-data (org-supertag-db-get entity-id)))
-              (puthash entity-id entity-data entities-to-upsert)))))
-
-      ;; ----------------------------- Objects (nodes / tags) ---------------
-      (when (hash-table-p org-supertag-db--object)
-        (maphash
-         (lambda (id props)
-           (unless (eq (plist-get props :type) :metadata)
-             (let* ((actual-id (extract-id id props))
-                    (current-hash (org-supertag-background-sync--calculate-object-hash props))
-                    (baseline-hash (and actual-id (gethash actual-id org-supertag-background-sync--last-sync-hashes))))
-               (when actual-id (puthash actual-id t current-ids))
-               ;; New object or hash changed ⇒ upsert
-               (unless (and current-hash baseline-hash (string= current-hash baseline-hash))
-                 (puthash actual-id props entities-to-upsert)))))
-         org-supertag-db--object))
-
-      ;; ----------------------------- Links ---------------------------------
-      (when (hash-table-p org-supertag-db--link)
-        (maphash
-         (lambda (id props)
-           (unless (eq (plist-get props :type) :metadata)
-             ;; Prepare a copy with explicit IDs for later steps
-             (let* ((props-copy (copy-sequence props))
-                    (_ (setq props-copy (plist-put props-copy :link-id id)))
-                    (_ (setq props-copy (plist-put props-copy :id id)))
-                    (actual-id id)
-                    (current-hash (org-supertag-background-sync--calculate-object-hash props-copy))
-                    (baseline-hash (gethash actual-id org-supertag-background-sync--last-sync-hashes)))
-               (puthash actual-id t current-ids)
-               ;; Changed or new link
-               (unless (and current-hash baseline-hash (string= current-hash baseline-hash))
-                 (push props-copy links-to-upsert)
-                 ;; Also enqueue the related entities for upsert to keep hashes fresh
-                 (ensure-entity-upsert (plist-get props-copy :from))
-                 (ensure-entity-upsert (plist-get props-copy :to))))))
-         org-supertag-db--link))
-
-      ;; ----------------------------- Deletions -----------------------------
-      (let (ids-to-delete)
-        (maphash (lambda (baseline-id _hash)
-                   (unless (gethash baseline-id current-ids)
-                     (push baseline-id ids-to-delete)))
-                 org-supertag-background-sync--last-sync-hashes)
-
-        ;; Return three lists: entities, links, deletions
-        (list (ht-values entities-to-upsert)
-              (nreverse links-to-upsert)
-              ids-to-delete)))))
-
-
-;; ---------------------------------------------------------------------------
-;; Main sync function
-;; ---------------------------------------------------------------------------
-
-(defun org-supertag-background-sync--finish-sync (&optional status)
-  "Finalizes the sync process, resetting state and logging completion.
-This function is now more robust to prevent type errors in logging."
-  (message "[background-sync] Finishing sync process with status: %s."
-           (cond
-            ((null status) "completed")
-            ((symbolp status) (symbol-name status))
-            ((stringp status) status)
-            ;; Use %S for any other type to prevent errors.
-            (t (format "%S" status))))
-  (org-supertag-background-sync--finish-progress)
-  (setq org-supertag-background-sync--phase :idle)
-  t) ;; Explicitly return t to avoid returning :idle to the scheduler.
-
-(defun org-supertag-background-sync--handle-reasoning-result (result)
-  "Callback to handle the result of a reasoning cycle.
-If more nodes were processed, it triggers the next cycle.
-Otherwise, it finalizes the entire sync process."
-  (if (and result (equal (plist-get result :status) "success"))
-      (let* ((stage-a-raw (plist-get result :stage_a_processed))
-             (stage-b-raw (plist-get result :stage_b_processed))
-             (total-raw (or (plist-get result :total_processed) ; Use new key
-                            (plist-get result :processed_count))) ; Fallback
-             (stage-a (if (numberp stage-a-raw) stage-a-raw (string-to-number (format "%s" (or stage-a-raw 0)))))
-             (stage-b (if (numberp stage-b-raw) stage-b-raw (string-to-number (format "%s" (or stage-b-raw 0)))))
-             (total (if (numberp total-raw) total-raw (string-to-number (format "%s" (or total-raw 0))))))
-        (message "[background-sync::reasoning] Cycle complete. Processed %d nodes (Stage A: %d, Stage B: %d)." total stage-a stage-b)
-        (if (> total 0)
-            ;; More nodes to process, continue the cycle.
-            (progn
-              (message "[background-sync::reasoning] More nodes to process, triggering next cycle...")
-              (org-supertag-background-sync--trigger-reasoning-cycle))
-          ;; No more nodes to process, finish the entire sync.
-          (progn
-            (message "[background-sync::reasoning] All nodes processed. Finalizing sync.")
-            (setq org-supertag-background-sync--last-sync-time (current-time))
-            (org-supertag-background-sync--finish-sync :success))))
-    ;; Reasoning phase failed.
-    (message "[background-sync::reasoning] Reasoning cycle failed: %s" (or (plist-get result :message) "Unknown error"))
-    (org-supertag-background-sync--finish-sync :reasoning-failed)))
-
-(defun org-supertag-background-sync--trigger-reasoning-cycle ()
-  "Initiates one cycle of the relation inference process."
-  (message "[background-sync::reasoning] Triggering relation inference cycle...")
-  (setq org-supertag-background-sync--phase :reasoning)
-  (org-supertag-bridge-call-async "knowledge/run_cycle"
-                                 nil
-                                 #'org-supertag-background-sync--handle-reasoning-result))
+      ;; 3. Find deleted objects by comparing old hashes with current IDs
+      (maphash
+       (lambda (id _last-hash)
+         (unless (gethash id current-ids)
+           (cl-incf changed-count)
+           (push id ids-to-delete)))
+       org-supertag-background-sync--last-sync-hashes)
+
+      (message "[background-sync] checked %d objects, found %d changes: %d nodes, %d tags, %d links, %d deletions"
+               (+ (hash-table-count org-supertag-db--object)
+                  (hash-table-count org-supertag-db--link))
+               changed-count
+               (length nodes-to-upsert)
+               (length tags-to-upsert)
+               (length links-to-upsert)
+               (length ids-to-delete))
+
+      (list nodes-to-upsert tags-to-upsert links-to-upsert ids-to-delete))))
+
+;; === Main sync function ===
 
 (defun org-supertag-background-sync--do-sync ()
-  "Perform a full, two-phase background sync operation: embedding then reasoning."
-  (setq org-supertag-background-sync--phase :embedding)
-  (let* ((start-time (current-time))
-         (changes (org-supertag-background-sync--get-changed-objects))
-         (entities-to-upsert (nth 0 changes))
-         (links-to-upsert (nth 1 changes))
-         (ids-to-delete (nth 2 changes)))
-
-    (let ((total-changed (+ (length entities-to-upsert)
-                            (length links-to-upsert)
-                            (length ids-to-delete))))
-      (if (= total-changed 0)
+  "Perform a full background sync operation."
+  (setq org-supertag-background-sync--state :syncing)
+  (condition-case-unless-debug err
+      (let* ((start-time (current-time))
+             (changes (org-supertag-background-sync--get-changed-objects))
+             (nodes-to-upsert (nth 0 changes))
+             (tags-to-upsert (nth 1 changes))
+             (links-to-upsert (nth 2 changes))
+             (ids-to-delete (nth 3 changes))
+             (total-changed (+ (length nodes-to-upsert)
+                               (length tags-to-upsert)
+                               (length links-to-upsert)
+                               (length ids-to-delete))))
+        (if (= total-changed 0)
+            (progn
+              (message "[background-sync] No changes detected. Sync complete.")
+              (setq org-supertag-background-sync--state :idle))
           (progn
-            (message "[background-sync] No changes detected. Sync complete.")
-            (org-supertag-background-sync--finish-sync :no-changes)
-            nil) ;; Explicitly return nil for success, as the scheduler expects.
-        (progn
-          (message "[background-sync::embedding] Starting embedding sync for %d changes." total-changed)
-          (org-supertag-background-sync--start-progress total-changed)
-          (let* ((snapshot-data (org-supertag-background-sync--prepare-snapshot-for-contract 
-                                entities-to-upsert links-to-upsert ids-to-delete)))
-            (unless (org-supertag-background-sync--validate-snapshot-data snapshot-data)
-              (error "[background-sync] Snapshot data validation failed"))
-            
-            (message "[background-sync] Data validation passed, sending to backend...")
-             (org-supertag-api-sync-bulk-process
-              snapshot-data
-             (lambda (result)
-               (message "[background-sync::embedding] Callback received, result status: %s"
-                        (if result (plist-get result :status) "nil"))
-               (if (and result (equal (plist-get result :status) "success"))
-                   (progn
-                     (message "[background-sync::embedding] Updating hashes for %d entities, %d links, deleting %d"
-                              (length entities-to-upsert) (length links-to-upsert) (length ids-to-delete))
-                     (org-supertag-background-sync--update-hashes entities-to-upsert links-to-upsert ids-to-delete)
-                     (setq org-supertag-background-sync--stats
-                           (list :synced-entities (length entities-to-upsert)
-                                 :synced-links (length links-to-upsert)
-                                 :deleted-count (length ids-to-delete)
-                                 :total-objects total-changed))
-                     (message "[background-sync::embedding] Phase 1 (Embedding) successful. Proceeding to Phase 2 (Reasoning).")
-                     (org-supertag-background-sync--trigger-reasoning-cycle))
-                 (message "[background-sync::embedding] sync failed: %s" (or (plist-get result :message) "Unknown error"))
-                 (org-supertag-background-sync--finish-sync :embedding-failed))))))))))
-
-;; ---------------------------------------------------------------------------
-;; Data contract preparation functions
-;; ---------------------------------------------------------------------------
-(defun org-supertag-background-sync--prepare-entity-for-contract (entity-props)
-  "Prepare a single entity for the data contract.
-Ensure the entity contains the required fields: id, type, and other standard fields."
-  (let* ((entity-type (or (plist-get entity-props :type)
-                         (plist-get entity-props 'type)))
-         (entity-id (or (plist-get entity-props :id)
-                       (plist-get entity-props :node-id)
-                       (plist-get entity-props :tag-id)
-                       (plist-get entity-props 'id)
-                       (plist-get entity-props 'node-id)
-                       (plist-get entity-props 'tag-id)))
-         (title (or (plist-get entity-props :title)
-                   (plist-get entity-props :name)
-                   (plist-get entity-props 'title)
-                   (plist-get entity-props 'name)))
-         (content (or (plist-get entity-props :content)
-                     (plist-get entity-props 'content)))
-         (file-path (or (plist-get entity-props :file-path)
-                       (plist-get entity-props 'file-path)))
-         (pos (or (plist-get entity-props :pos)
-                 (plist-get entity-props 'pos)))
-         (properties (or (plist-get entity-props :properties)
-                        (plist-get entity-props 'properties)))
-         (modified-at (or (plist-get entity-props :modified-at)
-                         (plist-get entity-props 'modified-at)
-                         (format-time-string "%Y-%m-%dT%H:%M:%S"))))
-
-    ;; Normalize fields
-    (setq entity-id (org-supertag-background-sync--normalize-link-field entity-id "id"))
-    (setq title (org-supertag-background-sync--normalize-link-field title "title"))
-    
-    ;; Validate required fields
-    (unless entity-id
-      (error "Entity missing required 'id' field: %S" entity-props))
-    (unless entity-type
-      (error "Entity missing required 'type' field: %S" entity-props))
-    
-    ;; Normalize type field
-    (setq entity-type 
-          (cond
-           ((or (eq entity-type :node) (string= entity-type "node")) "node")
-           ((or (eq entity-type :tag) (string= entity-type "tag")) "tag")
-           (t (error "Invalid entity type %S, must be 'node' or 'tag'" entity-type))))
+            (org-supertag-background-sync--start-progress total-changed)
+            (let* ((nodes-data (org-supertag-background-sync--prepare-nodes-for-python nodes-to-upsert))
+                   (tags-data (org-supertag-background-sync--prepare-tags-for-python tags-to-upsert))
+                   (links-data (org-supertag-background-sync--prepare-links-for-python links-to-upsert))
+                   ;; Build an alist instead of a hash-table for reliable serialization.
+                   (snapshot-data `(("nodes" . ,nodes-data)
+                                    ("links" . ,links-data)
+                                    ("ids_to_delete" . ,ids-to-delete)
+                                    ("sync_timestamp" . ,(format-time-string "%Y-%m-%dT%H:%M:%SZ" (current-time) t)))))
+              ;; We must use the designated API function which handles correct payload wrapping.
+              (org-supertag-api-bulk-process-snapshot
+               snapshot-data
+               (lambda (result)
+                 (let ((end-time (current-time)))
+                   (message "[background-sync] Callback received, result status: %s" 
+                            (if result (plist-get result :status) "nil"))
+                   (if (and result (equal (plist-get result :status) "success"))
+                       (progn
+                         (message "[background-sync] Updating hashes for %d nodes, %d tags, %d links, deleting %d" 
+                                  (length nodes-to-upsert) (length tags-to-upsert) 
+                                  (length links-to-upsert) (length ids-to-delete))
+                         (org-supertag-background-sync--update-hashes nodes-to-upsert tags-to-upsert links-to-upsert ids-to-delete)
+                         (message "[background-sync] Hash update completed, total hashes: %d" 
+                                  (hash-table-count org-supertag-background-sync--last-sync-hashes))
+                         (setq org-supertag-background-sync--stats
+                               (list :synced-nodes (length nodes-to-upsert)
+                                     :synced-tags (length tags-to-upsert)
+                                     :synced-links (length links-to-upsert)
+                                     :deleted-count (length ids-to-delete)
+                                     :total-objects total-changed))
+                         (setq org-supertag-background-sync--last-sync-time end-time)
+                         (message "[background-sync] Sync successful."))
+                     (message "[background-sync] sync failed: %s" (or (plist-get result :message) "Unknown error")))
+                   (org-supertag-background-sync--finish-progress)
+                   (setq org-supertag-background-sync--state :idle))))))))
+    (error
+     (setq org-supertag-background-sync--state :idle)
+     (org-supertag-background-sync--finish-progress)
+     (message "[background-sync] error during sync: %s" (error-message-string err)))))
 
-    ;; Build entity data according to the data contract
-    (let ((entity-data `((id . ,entity-id)
-                        (type . ,entity-type))))
-      
-      ;; Add optional fields (only add if non-empty)
-      (when title
-        (setq entity-data (append entity-data `((title . ,title)))))
-      (when content
-        (setq entity-data (append entity-data `((content . ,content)))))
-      (when file-path
-        (setq entity-data (append entity-data `((file_path . ,file-path)))))
-      (when pos
-        (setq entity-data (append entity-data `((pos . ,pos)))))
-      (when modified-at
-        (setq entity-data (append entity-data `((modified_at . ,modified-at)))))
-      
-      ;; Process properties field
-      (when properties
-        (let ((normalized-props
-               (cond
-                ;; If already an alist, use it directly
-                ((and (listp properties) 
-                      (org-supertag-background-sync--is-alist-p properties))
-                 properties)
-                ;; If a plist, convert to an alist
-                ((plistp properties)
-                 (let (alist)
-                   (while (and properties (cdr properties))
-                     (let ((key (pop properties))
-                           (value (pop properties)))
-                       (push (cons (cond
-                                   ((keywordp key) (substring (symbol-name key) 1))
-                                   ((symbolp key) (symbol-name key))
-                                   ((stringp key) key)
-                                   (t (format "%s" key)))
-                                 value) alist)))
-                   (nreverse alist)))
-                ;; For other cases, try to use directly
-                (t properties))))
-          (setq entity-data (append entity-data `((properties . ,normalized-props))))))
-      
-      entity-data)))
-
-(defun org-supertag-background-sync--normalize-link-field (field field-name)
-  "Recursively clean and normalize a field to ensure it's a valid string ID.
-This function is the single gatekeeper for sanitizing data before it's
-sent to Python. It handles various data types, including symbols, numbers,
-and malformed, nested list structures from completion frameworks."
+(defun org-supertag-background-sync--deep-prepare-for-python (data)
+  "Recursively convert Elisp data (plist, alist) to a pure hash table, for safe sending to Python.
+This handles nested structures and special data types like time objects."
   (cond
-   ;; Base case: Already a clean string.
-   ((stringp field) field)
-   ;; Base case: Nil, normalize to empty string.
-   ((null field) "")
-   ;; Base case: A symbol, convert to its name.
-   ((symbolp field) (symbol-name field))
-   ;; Base case: A number, convert to string.
-   ((numberp field) (number-to-string field))
-
-   ;; Recursive case: A list, which is often a malformed structure.
-   ((listp field)
-    (if (null field)
-        ""
-      ;; The core assumption is that the actual text is the first element
-      ;; of some list, which may itself be nested. We recursively process
-      ;; the first element of the list until we hit a base case (a non-list atom).
-      (let ((car (car field)))
-        (message "[background-sync] Normalizing list in '%s', processing first element: %S" field-name car)
-        (org-supertag-background-sync--normalize-link-field car field-name))))
-
-   ;; Final fallback for any other unexpected data type.
-   (t
-    (message "[background-sync] Warning: Invalid type '%s' for field '%s', converting to string representation."
-             (type-of field) field-name)
-    (format "%S" field))))
-
-(defun org-supertag-background-sync--prepare-link-for-contract (link-props)
-  "Prepare a single link for the data contract.
-Ensure the link contains the required fields: source, target, and optional type field."
-  (let* ((source (or (plist-get link-props :source)
-                    (plist-get link-props 'source)
-                    (plist-get link-props :from)
-                    (plist-get link-props 'from)))
-         (target (or (plist-get link-props :target)
-                    (plist-get link-props 'target)
-                    (plist-get link-props :to)
-                    (plist-get link-props 'to)))
-         (link-type (or (plist-get link-props :type)
-                       (plist-get link-props 'type)
-                       "REF_TO")))
-
-    ;; Clean and normalize source and target fields
-    (setq source (org-supertag-background-sync--normalize-link-field source "source"))
-    (setq target (org-supertag-background-sync--normalize-link-field target "target"))
-
-    ;; Validate required fields
-    (unless source
-      (error "Link missing required 'source' field: %S" link-props))
-    (unless target
-      (error "Link missing required 'target' field: %S" link-props))
-    (unless (stringp source)
-      (error "Link 'source' must be a string, got %s: %S" (type-of source) link-props))
-    (unless (stringp target)
-      (error "Link 'target' must be a string, got %s: %S" (type-of target) link-props))
-
-    ;; Normalize type field
-    (setq link-type
-          (cond
-           ((symbolp link-type) (symbol-name link-type))
-           ((stringp link-type) link-type)
-           (t "REF_TO")))
-
-    ;; Build link data according to the data contract
-    `((source . ,source)
-      (target . ,target)
-      (type . ,link-type))))
-
-(defun org-supertag-background-sync--prepare-snapshot-for-contract (entities links ids-to-delete)
-  "Prepare a complete snapshot of data according to the data contract.
-Ensure the data structure matches the expected format for the Python backend."
-  (let ((prepared-entities '())
-        (prepared-links '())
-        (valid-ids-to-delete '()))
-
-    ;; Prepare entity data
-    (dolist (entity entities)
-      ;; Skip metadata-type entities
-      (let ((entity-type (or (plist-get entity :type)
-                           (plist-get entity 'type))))
-        (unless (eq entity-type :metadata)
-          (condition-case err
-              (let ((prepared-entity (org-supertag-background-sync--prepare-entity-for-contract entity)))
-                (push prepared-entity prepared-entities))
-            (error
-             (message "[background-sync] Failed to prepare entity %S: %s" 
-                      entity (error-message-string err)))))))
-
-    ;; Prepare link data
-    (dolist (link links)
-      ;; Skip links with nil source or target
-      (let ((source (or (plist-get link :source)
-                       (plist-get link 'source)
-                       (plist-get link :from)
-                       (plist-get link 'from)))
-            (target (or (plist-get link :target)
-                       (plist-get link 'target)
-                       (plist-get link :to)
-                       (plist-get link 'to))))
-        (when (and source target)  ; Only process valid links
-          (condition-case err
-              (let ((prepared-link (org-supertag-background-sync--prepare-link-for-contract link)))
-                (push prepared-link prepared-links))
-            (error
-             (message "[background-sync] Failed to prepare link %S: %s" 
-                      link (error-message-string err)))))))
-
-    ;; Validate deletion ID list
-    (dolist (id ids-to-delete)
-      (if (and id (stringp id) (not (string-empty-p (string-trim id))))
-          (push (string-trim id) valid-ids-to-delete)
-        (message "[background-sync] Invalid deletion ID: %S" id)))
-
-    ;; Build final snapshot data
-    (let ((snapshot `((entities . ,(nreverse prepared-entities))
-                     (links . ,(nreverse prepared-links))
-                     (ids_to_delete . ,(nreverse valid-ids-to-delete)))))
-      
-      (message "[background-sync] Prepared snapshot: %d entities, %d links, %d deletions"
-               (length prepared-entities)
-               (length prepared-links)
-               (length valid-ids-to-delete))
-      
-      snapshot)))
-
-(defun org-supertag-background-sync--validate-entity-data (entity)
-  "Validate entity data against the data contract in Elisp."
-  (let ((id (alist-get 'id entity))
-        (type (alist-get 'type entity)))
-    (and id
-         (stringp id)
-         (not (string-empty-p (string-trim id)))
-         type
-         (member type '("node" "tag")))))
-
-(defun org-supertag-background-sync--validate-link-data (link)
-  "Validate link data against the data contract in Elisp."
-  (let ((source (alist-get 'source link))
-        (target (alist-get 'target link)))
-    (and source
-         (stringp source)
-         (not (string-empty-p (string-trim source)))
-         target
-         (stringp target)
-         (not (string-empty-p (string-trim target))))))
-
-(defun org-supertag-background-sync--validate-snapshot-data (snapshot)
-  "Validate snapshot data against the data contract in Elisp."
-  (let ((entities (alist-get 'entities snapshot))
-        (links (alist-get 'links snapshot))
-        (ids-to-delete (alist-get 'ids_to_delete snapshot)))
-    (and (listp entities)
-         (listp links)
-         (listp ids-to-delete)
-         (cl-every #'org-supertag-background-sync--validate-entity-data entities)
-         (cl-every #'org-supertag-background-sync--validate-link-data links)
-         (cl-every (lambda (id) (and (stringp id) (not (string-empty-p (string-trim id))))) ids-to-delete))))
-
-;; ---------------------------------------------------------------------------
-;; Update hashes
-;; ---------------------------------------------------------------------------
-
-(defun org-supertag-background-sync--update-hashes (entities-upserted links-upserted ids-deleted)
+   ;; Emacs internal time list (e.g., (26701 18355 90563 0)) -> ISO 8601 string
+   ((and (listp data) (numberp (car data)) (>= (length data) 3))
+    (format-time-string "%Y-%m-%dT%H:%M:%SZ" data t))
+   ;; alist (e.g., '((a . 1) (b . 2))) -> hash-table
+   ((and (listp data) (consp data) (consp (car data)))
+    (let ((table (make-hash-table :test 'equal)))
+      (dolist (pair data)
+        (when (consp pair)
+          (puthash (car pair) (org-supertag-background-sync--deep-prepare-for-python (cdr pair)) table)))
+      table))
+   ;; plist (e.g., '(:a 1 :b 2)) -> hash-table
+   ((and (listp data) (not (null data)) (symbolp (car data)) (evenp (length data)))
+    (let ((table (make-hash-table :test 'equal)))
+      (while data
+        (let ((key (pop data))
+              (value (pop data)))
+          (puthash (substring (symbol-name key) 1) (org-supertag-background-sync--deep-prepare-for-python value) table)))
+      table))
+   ;; list -> list (with values processed)
+   ((listp data)
+    (mapcar #'org-supertag-background-sync--deep-prepare-for-python data))
+   ;; Handle symbols explicitly to prevent sending them raw over EPC.
+   ((symbolp data)
+    ;; `t` and `nil` are handled correctly as booleans by the bridge.
+    ;; Other symbols must be converted to their string representation.
+    (if (memq data '(t nil))
+        data
+      (symbol-name data)))
+   ;; other types -> return as is
+   (t data)))
+
+(defun org-supertag-background-sync--prepare-nodes-for-python (nodes)
+  "Prepare node data for sending to Python, using deep conversion."
+  (mapcar
+   (lambda (node-props)
+     (org-supertag-background-sync--deep-prepare-for-python node-props))
+   nodes))
+
+(defun org-supertag-background-sync--prepare-tags-for-python (tags)
+  "Prepare tag data for sending to Python, using deep conversion."
+  (mapcar
+   (lambda (tag-props)
+     (org-supertag-background-sync--deep-prepare-for-python tag-props))
+   tags))
+
+(defun org-supertag-background-sync--prepare-links-for-python (links)
+  "Prepare link data for sending to Python, using deep conversion."
+  (mapcar
+   (lambda (link-props)
+     (org-supertag-background-sync--deep-prepare-for-python link-props))
+   links))
+
+(defun org-supertag-background-sync--update-hashes (nodes-upserted tags-upserted links-upserted ids-deleted)
   "Update hash records for synchronized objects and save them."
-  ;; Helper function to extract ID from properties, now consistent with get-changed-objects
+  ;; Helper function to extract ID from properties
   (cl-flet ((extract-id (props)
              (or (plist-get props :id)
                  (plist-get props :node-id)
                  (plist-get props :tag-id)
                  (plist-get props :link-id)
-                 ;; This logic reconstructs a link ID from its properties.
-                 ;; It's crucial that the `props` has a :type field for this to work.
-                 (when-let ((type (plist-get props :type)))
-                   (when (memq type '(:node-tag :node-field :tag-ref :tag-tag :relation-group :relation-member))
-                     (when-let ((from (plist-get props :from))
-                                (to (plist-get props :to)))
-                       (format "%s:%s->%s" type from to)))))))
+                 ;; For links, try source/target as fallback
+                 (when (eq (plist-get props :type) :link)
+                   (format "%s->%s" 
+                           (or (plist-get props :source) "")
+                           (or (plist-get props :target) ""))))))
     
-    ;; Update hashes for upserted entities (nodes and tags)
-    (dolist (props entities-upserted)
+    ;; Update hashes for upserted nodes
+    (dolist (props nodes-upserted)
       (let* ((id (extract-id props))
              (hash (org-supertag-background-sync--calculate-object-hash props)))
-        (if (and id hash)
-            (progn
-              (puthash id hash org-supertag-background-sync--last-sync-hashes)
-              (message "[background-sync] Updated hash for entity: %s" id))
-          (message "[background-sync] Failed to update hash for entity, ID or hash missing: %S" props))))
+        (when (and id hash)
+          (puthash id hash org-supertag-background-sync--last-sync-hashes))))
+    
+    ;; Update hashes for upserted tags
+    (dolist (props tags-upserted)
+      (let* ((id (extract-id props))
+             (hash (org-supertag-background-sync--calculate-object-hash props)))
+        (when (and id hash)
+          (puthash id hash org-supertag-background-sync--last-sync-hashes))))
     
     ;; Update hashes for upserted links
     (dolist (props links-upserted)
       (let* ((id (extract-id props))
              (hash (org-supertag-background-sync--calculate-object-hash props)))
-        (if (and id hash)
-            (progn
-              (puthash id hash org-supertag-background-sync--last-sync-hashes)
-              (message "[background-sync] Updated hash for link: %s" id))
-          (message "[background-sync] Failed to update hash for link, ID or hash missing: %S" props))))
+        (when (and id hash)
+          (puthash id hash org-supertag-background-sync--last-sync-hashes))))
     
     ;; Remove hashes for deleted objects
     (dolist (id ids-deleted)
-      (when (remhash id org-supertag-background-sync--last-sync-hashes)
-        (message "[background-sync] Removed hash for deleted object: %s" id)))
+      (remhash id org-supertag-background-sync--last-sync-hashes))
     
     ;; Persist the updated hashes to the file
     (org-supertag-background-sync--save-hashes)))
@@ -975,9 +478,7 @@ Ensure the data structure matches the expected format for the Python backend."
   (unless (file-exists-p org-supertag-data-directory)
     (make-directory org-supertag-data-directory t)))
 
-;; ---------------------------------------------------------------------------
-;; Timer management
-;; ---------------------------------------------------------------------------
+;; === Timer management ===
 
 (defun org-supertag-background-sync--python-ready-p ()
   "Check if Python backend is ready."
@@ -1000,7 +501,7 @@ Ensure the data structure matches the expected format for the Python backend."
 
 (defun org-supertag-background-sync--wait-for-backend ()
   "Wait for Python backend to be ready."
-  (setq org-supertag-background-sync--phase :waiting-backend)
+  (setq org-supertag-background-sync--state :waiting-backend)
   
   ;; Try to start Python backend (if not already started)
   (org-supertag-background-sync--try-start-backend)
@@ -1024,7 +525,7 @@ Ensure the data structure matches the expected format for the Python backend."
 
 (defun org-supertag-background-sync--start-sync-timer ()
   "Start actual sync timer."
-  (setq org-supertag-background-sync--phase :idle)
+  (setq org-supertag-background-sync--state :idle)
   (setq org-supertag-background-sync--timer
         (run-with-timer 0 ; Start immediately for the first sync
                         org-supertag-background-sync-interval
@@ -1033,114 +534,76 @@ Ensure the data structure matches the expected format for the Python backend."
 
 (defun org-supertag-background-sync--timer-function ()
   "Timer callback function."
-  (when (eq org-supertag-background-sync--phase :idle)
+  (when (eq org-supertag-background-sync--state :idle)
     ;; Here Python backend should be ready, but just to be safe, check again
     (if (org-supertag-background-sync--python-ready-p)
-        (org-supertag-background-sync--check-queue-and-sync)
+        (org-supertag-background-sync--do-sync)
       ;; If backend is not available, re-enter waiting state
       ;; (message "[background-sync] Python backend connection lost, re-waiting...")
       (org-supertag-background-sync--wait-for-backend))))
 
-(defun org-supertag-background-sync--check-queue-and-sync ()
-  "Check the queue status and decide whether to sync."
-  (org-supertag-bridge-call-async "knowledge/get_queue_status"
-                                 nil
-                                 #'org-supertag-background-sync--handle-queue-status-result))
-
-(defun org-supertag-background-sync--handle-queue-status-result (result)
-  "Handle the result of the queue status check and decide the next action."
-  (if (and result (equal (plist-get result :status) "success"))
-      (let* ((pending-count (or (plist-get result :pending)   ; New backend field
-                                (plist-get result :pending_count) ; Old field
-                                0))
-             (next-delay (* 5 60))) ; Default to 5 minutes if not otherwise set
-        (message "[background-sync] Pending nodes in queue: %d" pending-count)
-        (cond
-         ;; If there's a large backlog, sync immediately and schedule the next check soon.
-         ((> pending-count 500)
-          (message "[background-sync] Large backlog detected. Syncing now and will re-check in 1 minute.")
-          (setq next-delay 60) ; 1 minute
-          (org-supertag-background-sync--do-sync))
-         
-         ;; If there's a medium backlog, sync and schedule a check sooner than the default.
-         ((> pending-count 50)
-          (message "[background-sync] Medium backlog detected. Syncing now and will re-check in 15 minutes.")
-          (setq next-delay (* 15 60)) ; 15 minutes
-          (org-supertag-background-sync--do-sync))
-         
-         ;; If there's a small backlog, sync and use the default interval.
-         ((> pending-count 0)
-          (message "[background-sync] Small backlog detected. Syncing now. (Queue: %d)" pending-count)
-          (setq next-delay org-supertag-knowledge-cycle-interval)
-          (org-supertag-background-sync--do-sync))
-         
-         ;; If the queue is empty, just schedule the next check at the default interval.
-         (t
-          (message "[background-sync] Queue is empty. Scheduling next check.")
-          (setq next-delay org-supertag-knowledge-cycle-interval)))
-        
-        ;; Cancel any existing timer and set a new one with the calculated delay.
-        (when (timerp org-supertag-background-sync--timer)
-          (cancel-timer org-supertag-background-sync--timer))
-        (setq org-supertag-background-sync--timer
-              (run-with-timer next-delay nil #'org-supertag-background-sync--check-queue-and-sync))
-        (message "[background-sync] Next queue check scheduled in %d seconds." next-delay))
-    
-    (message "[background-sync] Failed to get queue status: %s" (or (plist-get result :message) "Unknown error"))))
-
-
 (defun org-supertag-background-sync-start (&optional force)
-  "Start background sync service by registering tasks with the central scheduler.
+  "Start background sync service.
+This function now assumes Python bridge is already ready when called.
 If FORCE is non-nil or called interactively, ignore auto-start setting."
   (interactive "P")
-  ;; First, stop any existing tasks to ensure a clean slate.
-  (org-supertag-background-sync-stop)
   (let ((should-start (or force (called-interactively-p 'any) org-supertag-background-sync-auto-start)))
     (if (not should-start)
-        (message "[background-sync] Auto-start is disabled.")
-      (progn
-        (org-supertag-background-sync--load-hashes)
-        (pcase org-supertag-background-sync-mode
-          ('interval
-           (message "[background-sync] Registering interval task (interval: %ds)..." org-supertag-background-sync-interval)
-           (let ((task-id 'background-sync-interval))
-             (org-supertag-scheduler-register-task
-              task-id
-              :interval
-              #'org-supertag-background-sync-run-now
-              :interval org-supertag-background-sync-interval)
-             (setq org-supertag-background-sync--registered-task-ids (list task-id))))
-          ('scheduled
-           (message "[background-sync] Registering scheduled tasks (times: %s)..." (mapconcat 'identity org-supertag-background-sync-schedule ", "))
-           (setq org-supertag-background-sync--registered-task-ids nil)
-           (dolist (time-str org-supertag-background-sync-schedule)
-             (let ((task-id (intern (format "background-sync-scheduled-%s" time-str))))
-               (org-supertag-scheduler-register-task
-                task-id
-                :daily
-                #'org-supertag-background-sync-run-now
-                :time time-str)
-               (push task-id org-supertag-background-sync--registered-task-ids))))
-          ('manual
-           (message "[background-sync] Manual mode. No tasks registered.")))
-        ;; Register daily tag embedding refresh task (runs after other tasks)
-        (let ((task-id 'tag-embedding-refresh-daily))
-          (org-supertag-scheduler-register-task
-           task-id
-           :daily
-           #'org-supertag-background-sync-refresh-stale-tags
-           :time org-supertag-tag-refresh-time)
-          (push task-id org-supertag-background-sync--registered-task-ids))))))
+        (message "[background-sync] Auto-start is disabled (org-supertag-background-sync-auto-start is nil)")
+      (cond
+       ;; Interval mode (traditional behavior)
+       ((eq org-supertag-background-sync-mode 'interval)
+        (if (timerp org-supertag-background-sync--timer)
+            (message "[background-sync] Interval service is already running.")
+          (progn
+            (message "[background-sync] Starting interval sync service (interval: %ds)..." org-supertag-background-sync-interval)
+            (setq org-supertag-background-sync--state :idle)
+            (org-supertag-background-sync--load-hashes)
+            (setq org-supertag-background-sync--timer
+                  (run-with-timer 0 org-supertag-background-sync-interval
+                                  #'org-supertag-background-sync--trigger-sync)))))
+       
+       ;; Scheduled mode
+       ((eq org-supertag-background-sync-mode 'scheduled)
+        (if (timerp org-supertag-background-sync--schedule-timer)
+            (message "[background-sync] Scheduled service is already running.")
+          (progn
+            (message "[background-sync] Starting scheduled sync service (times: %s)..." 
+                     (mapconcat 'identity org-supertag-background-sync-schedule ", "))
+            (setq org-supertag-background-sync--state :idle)
+            (org-supertag-background-sync--load-hashes)
+            (org-supertag-background-sync--start-schedule-timer))))
+       
+       ;; Manual mode
+       ((eq org-supertag-background-sync-mode 'manual)
+        (message "[background-sync] Manual mode - no automatic sync will be started. Use M-x org-supertag-background-sync-run-now to sync."))
+       
+       (t
+        (message "[background-sync] Unknown sync mode: %s" org-supertag-background-sync-mode))))))
 
 (defun org-supertag-background-sync-stop ()
-  "Stop background sync service by deregistering its tasks from the scheduler.
-This function will only print a message if tasks were actually deregistered."
+  "Stop background sync service."
   (interactive)
-  (when org-supertag-background-sync--registered-task-ids
-    (dolist (task-id org-supertag-background-sync--registered-task-ids)
-      (org-supertag-scheduler-deregister-task task-id))
-    (setq org-supertag-background-sync--registered-task-ids nil)
-    (message "[background-sync] Tasks deregistered.")))
+  (let ((stopped-services '()))
+    (when (timerp org-supertag-background-sync--timer)
+      (cancel-timer org-supertag-background-sync--timer)
+      (setq org-supertag-background-sync--timer nil)
+      (push "continuous" stopped-services))
+    (when (timerp org-supertag-background-sync--schedule-timer)
+      (cancel-timer org-supertag-background-sync--schedule-timer)
+      (setq org-supertag-background-sync--schedule-timer nil)
+      (push "scheduled" stopped-services))
+    (setq org-supertag-background-sync--state :idle)
+    (if stopped-services
+        (message "[background-sync] Stopped %s service(s)." (mapconcat 'identity stopped-services " and "))
+      (message "[background-sync] No services were running."))))
+
+(defun org-supertag-background-sync--trigger-sync ()
+  "Trigger sync entry point, check state."
+  (if (eq org-supertag-background-sync--state :idle)
+      (org-supertag-background-sync--do-sync)
+    (message "[background-sync] Skipping sync run because service is not idle (state: %s)."
+             org-supertag-background-sync--state)))
 
 (defun org-supertag-background-sync-restart ()
   "Restart background sync."
@@ -1151,7 +614,7 @@ This function will only print a message if tasks were actually deregistered."
 (defun org-supertag-background-sync-toggle-auto-start ()
   "Toggle automatic background sync startup."
   (interactive)
-  (setq org-supertag-background-sync-auto-start
+  (setq org-supertag-background-sync-auto-start 
         (not org-supertag-background-sync-auto-start))
   (message "[background-sync] Auto-start %s. %s"
            (if org-supertag-background-sync-auto-start "enabled" "disabled")
@@ -1159,77 +622,7 @@ This function will only print a message if tasks were actually deregistered."
                "Background sync will start automatically when bridge is ready."
              "Use M-x org-supertag-background-sync-start to start manually.")))
 
-;; ---------------------------------------------------------------------------
-;; Manual sync and status query
-;; ---------------------------------------------------------------------------
-
-(defun org-supertag-background-sync-run-now ()
-  "Run background sync immediately."
-  (interactive)
-  (cond 
-   ((not (eq org-supertag-background-sync--phase :idle))
-    (message "[background-sync] Already running, please wait (current phase: %s)..." (symbol-name org-supertag-background-sync--phase)))
-   
-   ((eq org-supertag-background-sync--phase :waiting-backend)
-    (message "[background-sync] Waiting for Python backend to be ready, please wait..."))
-   
-   ((not (org-supertag-background-sync--python-ready-p))
-    (message "[background-sync] Python backend not ready. Please start Python backend: M-x org-supertag-bridge-start-process"))
-   
-   (t 
-    (org-supertag-background-sync--do-sync))))
-
-(defun org-supertag-background-sync-status ()
-  "Display background sync status."
-  (interactive)
-  (let* ((python-ready (org-supertag-background-sync--python-ready-p))
-         (timer-status (cond 
-                       ((and org-supertag-background-sync--timer 
-                             org-supertag-background-sync--schedule-timer)
-                        "Both continuous and scheduled timers running")
-                       (org-supertag-background-sync--timer "Continuous timer running")
-                       (org-supertag-background-sync--schedule-timer "Scheduled timer running")
-                       (org-supertag-background-sync--backend-check-timer "Waiting for backend timer running")
-                       (t "Stopped")))
-         (mode-info (cond
-                    ((eq org-supertag-background-sync-mode 'interval)
-                     (format "Interval (every %d hours)" 
-                             (/ org-supertag-background-sync-interval 3600)))
-                    ((eq org-supertag-background-sync-mode 'scheduled)
-                     (format "Scheduled (times: %s, next: %s)"
-                             (mapconcat 'identity org-supertag-background-sync-schedule ", ")
-                             (or (org-supertag-background-sync--next-scheduled-time) "None")))
-                    ((eq org-supertag-background-sync-mode 'manual)
-                     "Manual (no automatic sync)")
-                    (t (format "Unknown mode: %s" org-supertag-background-sync-mode))))
-         (status-msg
-          (format "Background sync status:
-- State: %s
-- Mode: %s
-- Timer: %s
-- Auto-start: %s
-- Python backend: %s
-- Last sync: %s
-- Last sync stats: entities %d, links %d, total %d
-- Hash records: %d"
-                  (symbol-name org-supertag-background-sync--phase)
-                  mode-info
-                  timer-status
-                  (if org-supertag-background-sync-auto-start "Enabled" "Disabled")
-                  (if python-ready "Ready" "Not ready")
-                  (if org-supertag-background-sync--last-sync-time
-                      (format-time-string "%Y-%m-%d %H:%M:%S" org-supertag-background-sync--last-sync-time)
-                    "Never synced")
-                  (plist-get org-supertag-background-sync--stats :synced-entities)
-                  (plist-get org-supertag-background-sync--stats :synced-links)
-                  (plist-get org-supertag-background-sync--stats :total-objects)
-                  (hash-table-count org-supertag-background-sync--last-sync-hashes))))
-    (message "%s" status-msg)
-    status-msg))
-
-;; ---------------------------------------------------------------------------
-;; Scheduled Sync Functions
-;; ---------------------------------------------------------------------------
+;;; === Scheduled Sync Functions ===
 
 (defun org-supertag-background-sync--start-schedule-timer ()
   "Start the scheduled sync timer that checks every minute."
@@ -1303,6 +696,112 @@ This function will only print a message if tasks were actually deregistered."
     (message "[background-sync] Schedule set to: %s. Restart service to apply changes." 
              (mapconcat 'identity time-list ", "))))
 
+;;; Manual sync and status query
+
+(defun org-supertag-background-sync-run-now ()
+  "Run background sync immediately."
+  (interactive)
+  (cond 
+   ((eq org-supertag-background-sync--state :syncing)
+    (message "[background-sync] Already running, please wait..."))
+   
+   ((eq org-supertag-background-sync--state :waiting-backend)
+    (message "[background-sync] Waiting for Python backend to be ready, please wait..."))
+   
+   ((not (org-supertag-background-sync--python-ready-p))
+    (message "[background-sync] Python backend not ready. Please start Python backend: M-x org-supertag-bridge-start-process"))
+   
+   (t 
+    (org-supertag-background-sync--do-sync))))
+
+(defun org-supertag-background-sync-status ()
+  "Display background sync status."
+  (interactive)
+  (let* ((python-ready (org-supertag-background-sync--python-ready-p))
+         (timer-status (cond 
+                       ((and org-supertag-background-sync--timer 
+                             org-supertag-background-sync--schedule-timer)
+                        "Both continuous and scheduled timers running")
+                       (org-supertag-background-sync--timer "Continuous timer running")
+                       (org-supertag-background-sync--schedule-timer "Scheduled timer running")
+                       (org-supertag-background-sync--backend-check-timer "Waiting for backend timer running")
+                       (t "Stopped")))
+         (mode-info (cond
+                    ((eq org-supertag-background-sync-mode 'interval)
+                     (format "Interval (every %d hours)" 
+                             (/ org-supertag-background-sync-interval 3600)))
+                    ((eq org-supertag-background-sync-mode 'scheduled)
+                     (format "Scheduled (times: %s, next: %s)"
+                             (mapconcat 'identity org-supertag-background-sync-schedule ", ")
+                             (or (org-supertag-background-sync--next-scheduled-time) "None")))
+                    ((eq org-supertag-background-sync-mode 'manual)
+                     "Manual (no automatic sync)")
+                    (t (format "Unknown mode: %s" org-supertag-background-sync-mode))))
+         (status-msg
+          (format "Background sync status:
+- State: %s
+- Mode: %s
+- Timer: %s
+- Auto-start: %s
+- Python backend: %s
+- Last sync: %s
+- Last sync stats: nodes %d, tags %d, links %d, total %d
+- Hash records: %d"
+                  org-supertag-background-sync--state
+                  mode-info
+                  timer-status
+                  (if org-supertag-background-sync-auto-start "Enabled" "Disabled")
+                  (if python-ready "Ready" "Not ready")
+                  (if org-supertag-background-sync--last-sync-time
+                      (format-time-string "%Y-%m-%d %H:%M:%S" org-supertag-background-sync--last-sync-time)
+                    "Never synced")
+                  (plist-get org-supertag-background-sync--stats :synced-nodes)
+                  (plist-get org-supertag-background-sync--stats :synced-tags)
+                  (plist-get org-supertag-background-sync--stats :synced-links)
+                  (plist-get org-supertag-background-sync--stats :total-objects)
+                  (hash-table-count org-supertag-background-sync--last-sync-hashes))))
+    (message "%s" status-msg)
+    status-msg))
+
+;;; 初始化
+
+(defun org-supertag-background-sync-reset-hashes ()
+  "Reset all hash records, force full sync next time."
+  (interactive)
+  (clrhash org-supertag-background-sync--last-sync-hashes)
+  (setq org-supertag-background-sync--last-sync-time nil)
+  ;; (message "[background-sync] Hash records reset, full sync will be triggered next time")
+  )
+
+(defun org-supertag-background-sync-rebuild-baseline ()
+  "Rebuild baseline hashes from current database state.
+This is useful after database recovery or when you want to re-establish
+the sync baseline. All objects will be considered 'synced' in their current state."
+  (interactive)
+  (when (yes-or-no-p "Rebuild sync baseline from current database state? This will mark all current objects as 'synced'. ")
+    (message "[background-sync] Rebuilding baseline from current database state...")
+    (let ((result (org-supertag-background-sync--create-baseline-hashes)))
+      (message "[background-sync] Baseline rebuild completed:")
+      (message "  Total objects processed: %d" 
+               (+ (plist-get result :processed-objects)
+                  (plist-get result :processed-links)))
+      (message "  Total hashes created: %d" (plist-get result :total-hashes))
+      (when (> (plist-get result :failed-objects) 0)
+        (message "  Failed to process: %d objects" (plist-get result :failed-objects)))
+      (message "  Incremental sync is now enabled with new baseline")
+      result)))
+
+(defun org-supertag-background-sync-initialize ()
+  "Initialize background sync system."
+  ;; Load history hash records
+  (org-supertag-background-sync--load-hashes)
+  
+  (when org-supertag-background-sync-auto-start
+    (org-supertag-background-sync-start)))
+
+;; This module is now initialized directly by org-supertag.el
+;; No automatic hooks to avoid duplicate initialization
+
 ;;; Debug functions
 
 (defun org-supertag-background-sync--log-debug (msg &rest args)
@@ -1325,85 +824,45 @@ LIMIT: Maximum number of records to display, default 10."
      org-supertag-background-sync--last-sync-hashes)
     (message "Total hash records: %d" (hash-table-count org-supertag-background-sync--last-sync-hashes))))
 
-;; ---------------------------------------------------------------------------
-;; Progress tracking functions
-;; ---------------------------------------------------------------------------
+;;; Progress handling
 
-(defvar org-supertag-background-sync--progress-total 0
-  "Total number of items to process in current sync.")
+(defvar org-supertag-background-sync--current-progress nil
+  "Current sync progress information, format (current total start-time).")
 
-(defvar org-supertag-background-sync--progress-current 0
-  "Current number of items processed in current sync.")
+(defun org-supertag-background-sync--update-progress (current total)
+  "Update sync progress. Called by Python backend."
+  (when org-supertag-background-sync--current-progress
+    (let* ((start-time (nth 2 org-supertag-background-sync--current-progress))
+           (elapsed (float-time (time-subtract (current-time) start-time)))
+           (percentage (if (> total 0) (/ (* current 100.0) total) 0))
+           (eta (if (and (> current 0) (> total current))
+                    (/ (* elapsed (- total current)) current)
+                  0)))
+      
+      (setq org-supertag-background-sync--current-progress (list current total start-time))
+      
+      ;; (message "[background-sync] Progress: %d/%d (%.1f%%) - Elapsed time %.1fs%s"
+      ;;          current total percentage elapsed
+      ;;          (if (> eta 0) (format " - 预计剩余 %.1fs" eta) ""))
+      )))
 
 (defun org-supertag-background-sync--start-progress (total)
-  "Start progress tracking for sync operation with TOTAL items."
-  (setq org-supertag-background-sync--progress-total total)
-  (setq org-supertag-background-sync--progress-current 0)
-  (message "[background-sync] Starting sync progress tracking for %d items" total))
+  "Start progress tracking."
+  (setq org-supertag-background-sync--current-progress 
+        (list 0 total (current-time)))
+  ;; (message "[background-sync] Starting sync of %d objects..." total)
+  )
 
 (defun org-supertag-background-sync--finish-progress ()
-  "Finish progress tracking for sync operation."
-  (setq org-supertag-background-sync--progress-total 0)
-  (setq org-supertag-background-sync--progress-current 0)
-  (message "[background-sync] Progress tracking completed"))
-
-(defun org-supertag-background-sync--update-progress (processed)
-  "Update progress tracking with PROCESSED items."
-  (setq org-supertag-background-sync--progress-current processed)
-  (when (> org-supertag-background-sync--progress-total 0)
-    (let ((percentage (/ (* 100.0 processed) org-supertag-background-sync--progress-total)))
-      (message "[background-sync] Progress: %d/%d (%.1f%%)" 
-               processed org-supertag-background-sync--progress-total percentage))))
-
-;; ---------------------------------------------------------------------------  
-;; Reset and force resync functions
-;; ---------------------------------------------------------------------------
-
-(defun org-supertag-background-sync-reset-and-force-resync ()
-  "Reset all hash records and force a full resync.
-This will delete all existing hash records, ensuring that all objects
-are considered changed during the next sync."
-  (interactive)
-  (when (y-or-n-p "This will reset all sync status and force a full resync. Continue?")
-    (message "[background-sync] Starting reset and force resync...")
-    
-    ;; 1. Clear hash table in memory
-    (clrhash org-supertag-background-sync--last-sync-hashes)
-    (message "[background-sync] Cleared hash table in memory")
-    
-    ;; 2. Delete hash file
-    (when (file-exists-p org-supertag-background-sync--hash-file)
-      (delete-file org-supertag-background-sync--hash-file)
-      (message "[background-sync] Deleted hash file: %s" org-supertag-background-sync--hash-file))
-    
-    ;; 3. Reset sync time
-    (setq org-supertag-background-sync--last-sync-time nil)
-    (message "[background-sync] Reset sync time")
-    
-    ;; 4. Reset sync phase
-    (setq org-supertag-background-sync--phase :idle)
-    (message "[background-sync] Reset sync phase")
-    
-    ;; 5. Clear statistics
-    (setq org-supertag-background-sync--stats nil)
-    (message "[background-sync] Cleared statistics")
-    
-    ;; 6. Immediately start a full sync
-    (message "[background-sync] Starting full sync...")
-    (org-supertag-background-sync-run-now)
-    
-    (message "[background-sync] Reset and force resync started")))
+  "Finish progress tracking."
+  (setq org-supertag-background-sync--current-progress nil))
 
-(defun org-supertag-background-sync-refresh-stale-tags ()
-  "Call backend RPC to refresh embeddings for STALE tags."
-  (interactive)
-  (message "[background-sync] Triggering backend refresh of STALE tag embeddings...")
-  (org-supertag-bridge-call-async
-   "embedding/refresh_stale_tags"
-   nil
-   (lambda (result)
-     (message "[background-sync] refresh_stale_tags result: %S" result))))
+;; Hook registration moved to org-supertag.el to avoid duplication
+;; (when org-supertag-background-sync-auto-start
+;;   (add-hook 'org-supertag-bridge-ready-hook #'org-supertag-background-sync-start))
 
+;; Load hashes on startup, after all functions are defined.
+(org-supertag-background-sync--load-hashes)
 
 (provide 'org-supertag-background-sync)
 ;;; org-supertag-background-sync.el ends here 
diff --git a/org-supertag-backlink.el b/org-supertag-backlink.el
new file mode 100644
index 0000000..b34ca76
--- /dev/null
+++ b/org-supertag-backlink.el
@@ -0,0 +1,150 @@
+;;; org-supertag-backlink.el --- Backlink view for org-supertag -*- lexical-binding: t; -*-
+
+(require 'org-supertag-db)
+(require 'org-supertag-node)
+(require 'org-supertag-view-utils)
+
+;;----------------------------------------------------------------------
+;; Variables
+;;----------------------------------------------------------------------
+
+(defvar org-supertag-backlink--current-node-id nil
+  "Current node ID being viewed in backlink buffer.")
+
+;;----------------------------------------------------------------------
+;; Mode Definition
+;;----------------------------------------------------------------------
+
+(defvar org-supertag-backlink-mode-map
+  (let ((map (make-sparse-keymap)))
+    (define-key map (kbd "n") 'next-line)
+    (define-key map (kbd "p") 'previous-line)
+    (define-key map (kbd "RET") 'org-supertag-backlink--view-node)
+    (define-key map (kbd "g") 'org-supertag-backlink--refresh)
+    (define-key map (kbd "q") 'quit-window)
+    map)
+  "Keymap for `org-supertag-backlink-mode'.")
+
+(define-derived-mode org-supertag-backlink-mode special-mode "Org-Supertag-Backlink"
+  "Major mode for viewing node backlinks."
+  :group 'org-supertag)
+
+;;----------------------------------------------------------------------
+;; Core Functions
+;;----------------------------------------------------------------------
+
+(defun org-supertag-backlink--get-references (node-id)
+  "Get all nodes referenced by NODE-ID.
+Returns a list of node IDs."
+  (when-let* ((node (org-supertag-db-get node-id)))
+    (plist-get node :ref-to)))
+
+(defun org-supertag-backlink--get-referenced-by (node-id)
+  "Get all nodes that reference NODE-ID.
+Returns a list of node IDs."
+  (when-let* ((node (org-supertag-db-get node-id)))
+    (plist-get node :ref-from)))
+
+(defun org-supertag-backlink--format-node-content (node-id)
+  "Format node content for display.
+NODE-ID is the node identifier.
+Returns formatted string with node content."
+  (when-let* ((node (org-supertag-db-get node-id))
+              (title (plist-get node :title))
+              (file-path (plist-get node :file-path))
+              (content (or (plist-get node :content) "")))
+    (let* ((file-name (file-name-nondirectory file-path))
+           (styled-title (propertize title 'face '(:weight bold))))
+      (format "%s (%s)\n%s\n\n" styled-title file-name content))))
+
+(defun org-supertag-backlink--show-buffer ()
+  "Show backlink buffer with current node's references."
+  (let ((buffer (get-buffer-create "*Org SuperTag Backlink*")))
+    (with-current-buffer buffer
+      (let ((inhibit-read-only t))
+        (erase-buffer)
+        (org-supertag-backlink-mode)
+        
+        ;; Header
+        (when-let* ((node (org-supertag-db-get org-supertag-backlink--current-node-id))
+                   (title (plist-get node :title)))
+          (insert (propertize "Current Node: " 'face '(:weight bold)))
+          (insert title "\n")
+          
+          ;; Reference counts
+          (let ((refs-count (length (org-supertag-backlink--get-references org-supertag-backlink--current-node-id)))
+                (refd-by-count (length (org-supertag-backlink--get-referenced-by org-supertag-backlink--current-node-id))))
+            (insert (format "References: %d    Referenced By: %d\n\n" refs-count refd-by-count)))
+          
+          ;; References section
+          (insert (propertize "References\n" 'face '(:weight bold)))
+          (insert "────────────────────────────────\n")
+          (let ((refs (org-supertag-backlink--get-references org-supertag-backlink--current-node-id))
+                (index 1))
+            (if refs
+                (dolist (ref-id refs)
+                  (let ((content (org-supertag-backlink--format-node-content ref-id)))
+                    (insert (format "(%d) %s" index content))
+                    (setq index (1+ index))))
+              (insert "  No references found\n")))
+          
+          (insert "\n")
+          
+          ;; Referenced by section
+          (insert (propertize "Referenced By\n" 'face '(:weight bold)))
+          (insert "────────────────────────────────\n")
+          (let ((refd-by (org-supertag-backlink--get-referenced-by org-supertag-backlink--current-node-id))
+                (index 1))
+            (if refd-by
+                (dolist (ref-id refd-by)
+                  (let ((content (org-supertag-backlink--format-node-content ref-id)))
+                    (insert (format "(%d) %s" index content))
+                    (setq index (1+ index))))
+              (insert "  Not referenced by any nodes\n")))
+          
+          ;; Operations
+          (insert "\nOperations:\n")
+          (insert " [n/p] Navigate    [RET] View    [g] Refresh    [q] Quit\n\n")
+          (insert "Note: Press [RET] to view the selected node"))))
+    
+    ;; Display buffer
+    (org-supertag-view--display-buffer-right buffer)
+    (select-window (get-buffer-window buffer))
+    (goto-char (point-min))))
+
+(defun org-supertag-backlink--view-node ()
+  "View the node at current position."
+  (interactive)
+  (when-let* ((node-id (org-supertag-backlink--get-node-at-point)))
+    (org-supertag-view--goto-node node-id)
+    (select-window (get-buffer-window "*Org SuperTag Backlink*"))))
+
+(defun org-supertag-backlink--get-node-at-point ()
+  "Get node ID at current position in backlink buffer."
+  (save-excursion
+    (beginning-of-line)
+    (when (looking-at "^\\([0-9]+\\) \\(.*\\) (.*)")
+      (let ((title (match-string 2)))
+        (org-supertag-node--find-by-title title)))))
+
+(defun org-supertag-backlink--refresh ()
+  "Refresh the backlink buffer."
+  (interactive)
+  (when (eq major-mode 'org-supertag-backlink-mode)
+    (org-supertag-backlink--show-buffer)))
+
+;;;###autoload
+(defun org-supertag-backlink-show ()
+  "Show backlink window for current node."
+  (interactive)
+  (unless (org-at-heading-p)
+    (user-error "Must be on a heading"))
+  
+  (let ((node-id (or (org-id-get)
+                     (progn 
+                       (org-supertag-node-create)
+                       (org-id-get)))))
+    (setq org-supertag-backlink--current-node-id node-id)
+    (org-supertag-backlink--show-buffer)))
+
+(provide 'org-supertag-backlink) 
\ No newline at end of file
diff --git a/org-supertag-behavior-library.el b/org-supertag-behavior-library.el
index a91b630..d1ea626 100755
--- a/org-supertag-behavior-library.el
+++ b/org-supertag-behavior-library.el
@@ -58,8 +58,13 @@
 
 (require 'org)
 (require 'org-supertag-db)
+<<<<<<< HEAD
+(require 'org-supertag-sim-epc)
+(require 'epc)
+=======
 (require 'org-supertag-bridge) ;; Modern Bridge
 (require 'org-supertag-api)   ;; Modern API
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 
 ;;------------------------------------------------------------------------------
@@ -68,6 +73,48 @@
 
 (defun org-supertag-behavior--call-ai (prompt &optional system-prompt)
   "Call the configured AI model with PROMPT and optional SYSTEM-PROMPT.
+<<<<<<< HEAD
+Uses the EPC connection managed by `org-supertag-sim-epc`.
+
+PROMPT: The main user prompt string.
+SYSTEM-PROMPT: An optional system prompt string to guide the AI's behavior.
+
+Returns the AI's text response as a string on success.
+Returns nil and logs an error on failure (e.g., EPC connection error,
+AI error, timeout)."
+  (org-supertag-sim-epc-log "Calling AI with prompt (length %d)..." (length prompt))
+  (condition-case err
+      (progn
+        ;; Ensure EPC server is running
+        (org-supertag-sim-epc-ensure-server)
+        (unless (org-supertag-sim-epc-server-running-p)
+          (error "SimTag EPC server is not running."))
+
+        ;; Prepare arguments for the EPC call
+        (let* ((args (if system-prompt
+                         (list prompt system-prompt)
+                       (list prompt)))
+               ;; Assuming 'run_ollama' is the correct Python function name
+               (response (epc:call-sync org-supertag-sim-epc-manager 'run_ollama args)))
+
+          (org-supertag-sim-epc-log "AI raw response: %S" response)
+
+          ;; Check response status
+          (if (and response
+                   (plistp response)
+                   (string= (plist-get response :status) "success"))
+              (let ((result (plist-get response :result)))
+                (org-supertag-sim-epc-log "AI call successful, response length: %d" (if result (length result) 0))
+                result)
+            ;; Handle AI-side error
+            (let ((error-msg (or (plist-get response :message) "Unknown AI error")))
+              (org-supertag-sim-epc-log "AI call failed: %s" error-msg)
+              (error "AI Error: %s" error-msg)
+              nil))))
+    ;; Handle Elisp-side errors (EPC connection, timeout, etc.)
+    (error
+     (org-supertag-sim-epc-log "Error calling AI: %s" (error-message-string err))
+=======
 Uses the modern `org-supertag-api` via `org-supertag-bridge`.
 
 PROMPT: The main user prompt string.
@@ -99,6 +146,7 @@ Returns nil and logs an error on failure."
     ;; Handle Elisp-side errors (EPC connection, timeout, etc.)
     (error
      (org-supertag-bridge--log "Error calling AI: %s" (error-message-string err))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
      (message "Error communicating with AI: %s" (error-message-string err))
      nil)))
 
@@ -432,14 +480,20 @@ Example:
 
 (defun org-supertag-behavior--get-property (node-id params)
   "Get property value for NODE-ID based on PARAMS.
-PARAMS is a plist with :name key and optional :inherit flag."
+PARAMS is a plist with :name key and optional :inherit flag.
+
+Example:
+  ;; Get local property
+  (org-supertag-behavior--get-property node-id '(:name \"CATEGORY\"))
+  ;; Get with inheritance
+  (org-supertag-behavior--get-property node-id '(:name \"CATEGORY\" :inherit t))"
   (message "Debug get-property - node=%s params=%S" node-id params)
   (when-let* ((name (plist-get params :name))
               (inherit (plist-get params :inherit))
               (pos (org-supertag-db-get-pos node-id)))
     (message "Debug get-property - Getting %s (inherit=%s)" name inherit)
     (save-excursion
-      (org-with-point-at posp
+      (org-with-point-at pos
         (if inherit
             (org-entry-get nil name t)  ; t means inherit
           (org-entry-get nil name))))))
diff --git a/org-supertag-behavior.el b/org-supertag-behavior.el
index 2a32c76..bf6d6b2 100755
--- a/org-supertag-behavior.el
+++ b/org-supertag-behavior.el
@@ -10,10 +10,10 @@
 ;; 3. Supports automated execution
 ;; 4. Supports scheduled tasks
 
+(require 'org-supertag-tag)
 (require 'org-supertag-behavior-library)
 (require 'org-supertag-behavior-template)
-(require 'org-supertag-scheduler)
-
+(require 'org-supertag-db)
 
 ;;------------------------------------------------------------------------------
 ;; Behavior Registry
@@ -38,39 +38,10 @@ Key is the behavior name (string), value is a plist containing:
 (defvar org-supertag-behavior--initialized nil
   "Flag to track if behavior system has been initialized.")
 
-(with-eval-after-load 'org-supertag-tag
-  (defun org-supertag-behavior--get-behavior (tag-name)
-    "Get behavior definition for tag with TAG-NAME.
-First try to get behavior directly from registry.
-If not found, check if tag has associated behaviors."
-    (or
-     ;; Look up directly from registry
-     (gethash tag-name org-supertag-behavior-registry)
-     ;; Look up from tag's associated behaviors
-     (when-let* ((tag (org-supertag-tag-get tag-name))
-                 (behaviors (plist-get tag :behaviors)))
-       ;; If multiple behaviors exist, return the first one
-       (when (car behaviors)
-         (gethash (car behaviors) org-supertag-behavior-registry)))))
-
-  (defun org-supertag-behavior--cleanup-duplicates ()
-    "Clean up duplicate behaviors in all tags."
-    (interactive)
-    (maphash
-     (lambda (key value)
-       (when (plist-get value :behaviors)
-         (let ((unique-behaviors (delete-dups (plist-get value :behaviors))))
-           (org-supertag-tag--create
-            key
-            :type :tag
-            :behaviors unique-behaviors))))
-     org-supertag-db--object)))
-
 (defun org-supertag-behavior-register (behavior-name &rest props)
   "Register behavior with BEHAVIOR-NAME.
 PROPS is a plist with:
 :trigger  - When to execute
-:schedule - Cron format string (\"minute hour day month weekday\") for :schedule trigger
 :action   - Function or behavior list
 :style    - Visual properties
 :hooks    - Optional hooks
@@ -94,60 +65,515 @@ PROPS is a plist with:
     
     behavior))
 
+
+(defun org-supertag-behavior--get-behavior (tag-name)
+  "Get behavior definition for tag with TAG-NAME.
+First try to get behavior directly from registry.
+If not found, check if tag has associated behaviors."
+  (or
+   ;; Look up directly from registry
+   (gethash tag-name org-supertag-behavior-registry)
+   ;; Look up from tag's associated behaviors
+   (when-let* ((tag (org-supertag-tag-get tag-name))
+               (behaviors (plist-get tag :behaviors)))
+     ;; If multiple behaviors exist, return the first one
+     (when (car behaviors)
+       (gethash (car behaviors) org-supertag-behavior-registry)))))
+
+(defun org-supertag-behavior--cleanup-duplicates ()
+  "Clean up duplicate behaviors in all tags."
+  (interactive)
+  (maphash
+   (lambda (key value)
+     (when (plist-get value :behaviors)
+       (let ((unique-behaviors (delete-dups (plist-get value :behaviors))))
+         (org-supertag-tag-create
+          key
+          :type :tag
+          :behaviors unique-behaviors))))
+   org-supertag-db--object))
+
 ;;------------------------------------------------------------------------------
-;; Secheduler System Integration
+;; Secheduler System
 ;;------------------------------------------------------------------------------
 
-(defun org-supertag-behavior--execute-scheduled-task (node-id behavior-name)
-  "A wrapper function to be called by the scheduler.
-This function executes a specific BEHAVIOR-NAME on a specific NODE-ID."
-  (message "Scheduler executing behavior '%s' on node '%s'" behavior-name node-id)
-  (org-supertag-behavior-execute node-id behavior-name))
+(defvar org-supertag-scheduled-tasks (make-hash-table :test 'equal)
+  "Store scheduled tasks. Key is task ID, value is task property list.")
+
+(defvar org-supertag-scheduler-timer nil
+  "Main timer for the scheduler.")
+
+;; Time format parsing and validation
+(defun org-supertag-parse-time-spec (time-spec)
+  "Parse time specification string.
+TIME-SPEC can be:
+1. Cron format: \"minute hour day month weekday\"
+2. Absolute time: \"YYYY-MM-DD HH:MM\"
+3. Relative time: \"now+2h\", \"now-1d\", etc.
+4. Property-based: \"${prop:DEADLINE}-2h\"
+5. Org timestamp: \"<2024-03-20 Wed>\" or \"[2024-03-20 Wed]\"
+6. Special keywords: \"scheduled\", \"deadline\""
+  (cond
+   ;; Special keywords for org timestamps
+   ((string= time-spec "scheduled")
+    (list :type :org-scheduled))
+   
+   ((string= time-spec "deadline")
+    (list :type :org-deadline))
+   
+   ;; Org timestamp format
+   ((string-match org-ts-regexp time-spec)
+    (list :type :org-timestamp
+          :value (org-time-string-to-time time-spec)))
+   
+   ;; Cron format
+   ((string-match-p "^[0-9*/]+ [0-9*/]+ [0-9*/]+ [0-9*/]+ [0-9*/]+$" time-spec)
+    (if (org-supertag-cron-valid-p time-spec)
+        (list :type :cron
+              :value (org-supertag-parse-cron time-spec))
+      (signal 'org-supertag-behavior-error
+              (list :invalid-cron-format time-spec))))
+   
+   ;; Absolute time
+   ((string-match "^\\([0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\}\\) \\([0-9]\\{2\\}:[0-9]\\{2\\}\\)$" time-spec)
+    (list :type :absolute
+          :value (encode-time 
+                 0                                    ; seconds
+                 (string-to-number (substring (match-string 2 time-spec) 3 5)) ; minutes
+                 (string-to-number (substring (match-string 2 time-spec) 0 2)) ; hours
+                 (string-to-number (substring (match-string 1 time-spec) 8 10)) ; day
+                 (string-to-number (substring (match-string 1 time-spec) 5 7)) ; month
+                 (string-to-number (substring (match-string 1 time-spec) 0 4))))) ; year
+   
+   ;; Relative time from now
+   ((string-match "^now\\([+-]\\)\\([0-9]+\\)\\([hdwmy]\\)$" time-spec)
+    (let* ((op (match-string 1 time-spec))
+           (num (string-to-number (match-string 2 time-spec)))
+           (unit (match-string 3 time-spec))
+           (seconds (pcase unit
+                     ("h" (* num 3600))     ; hours
+                     ("d" (* num 86400))    ; days
+                     ("w" (* num 604800))   ; weeks
+                     ("m" (* num 2592000))  ; months (approx)
+                     ("y" (* num 31557600)) ; years (approx)
+                     (_ 0))))
+      (list :type :relative
+            :value (time-add (current-time)
+                           (if (string= op "+")
+                               seconds
+                             (- seconds))))))
+   
+   ;; Property-based time
+   ((string-match "\\${prop:\\([^}]+\\)}\\([+-][0-9]+[hdwmy]\\)?" time-spec)
+    (list :type :property
+          :prop (match-string 1 time-spec)
+          :offset (match-string 2 time-spec)))
+   
+   (t nil)))
+
+(defun org-supertag-time-matches-p (time-spec current-time node-id)
+  "Check if CURRENT-TIME matches TIME-SPEC for NODE-ID."
+  (when-let* ((spec (org-supertag-parse-time-spec time-spec)))
+    (pcase (plist-get spec :type)
+      (:org-scheduled
+       (when-let* ((pos (org-supertag-db-get-pos node-id))
+                  (scheduled-time (org-with-point-at pos
+                                  (org-get-scheduled-time nil))))
+         (time-equal-p (time-convert scheduled-time 'integer)
+                      (time-convert current-time 'integer))))
+      
+      (:org-deadline
+       (when-let* ((pos (org-supertag-db-get-pos node-id))
+                  (deadline-time (org-with-point-at pos
+                                 (org-get-deadline-time nil))))
+         (time-equal-p (time-convert deadline-time 'integer)
+                      (time-convert current-time 'integer))))
+      
+      (:org-timestamp
+       (let ((target-time (plist-get spec :value)))
+         (time-equal-p (time-convert target-time 'integer)
+                      (time-convert current-time 'integer))))
+      
+      (:cron
+       (org-supertag-cron-matches-p time-spec current-time))
+      
+      (:absolute
+       (let ((target-time (plist-get spec :value)))
+         (time-equal-p (time-convert target-time 'integer)
+                      (time-convert current-time 'integer))))
+      
+      (:relative
+       (let ((target-time (plist-get spec :value)))
+         (time-less-p current-time target-time)))
+      
+      (:property
+       (when-let* ((prop (plist-get spec :prop))
+                  (offset (plist-get spec :offset))
+                  (prop-time (org-supertag-behavior--get-property-time node-id prop)))
+         (when offset
+           (setq prop-time 
+                 (org-supertag-behavior--apply-time-offset prop-time offset)))
+         (time-equal-p (time-convert prop-time 'integer)
+                      (time-convert current-time 'integer)))))))
+
+(defun org-supertag-behavior--get-property-time (node-id prop)
+  "Get time value from property PROP of NODE-ID."
+  (when-let* ((pos (org-supertag-db-get-pos node-id))
+              (value (org-with-point-at pos
+                      (org-entry-get nil prop))))
+    (org-time-string-to-time value)))
+
+(defun org-supertag-behavior--apply-time-offset (time offset)
+  "Apply time OFFSET to TIME.
+OFFSET format: +/-Nh/d/w/m/y"
+  (when (string-match "\\([+-]\\)\\([0-9]+\\)\\([hdwmy]\\)" offset)
+    (let* ((op (match-string 1 offset))
+           (num (string-to-number (match-string 2 offset)))
+           (unit (match-string 3 offset))
+           (seconds (pcase unit
+                     ("h" (* num 3600))     ; hours
+                     ("d" (* num 86400))    ; days
+                     ("w" (* num 604800))   ; weeks
+                     ("m" (* num 2592000))  ; months (approx)
+                     ("y" (* num 31557600)) ; years (approx)
+                     (_ 0))))
+      (time-add time
+                (if (string= op "+")
+                    seconds
+                  (- seconds))))))
+
+;; Cron format parsing and validation
+(defun org-supertag-parse-cron (cron-string)
+  "Parse cron expression.
+CRON-STRING format: \"minute hour day month weekday\"
+Supports:
+- Numbers (e.g. \"5\")
+- Wildcards (\"*\")
+- Step values (\"*/5\" for every 5 units)"
+  (let ((fields (split-string cron-string " ")))
+    (when (= (length fields) 5)
+      (cl-destructuring-bind (minute hour day month weekday) fields
+        ;; Validate each field
+        (unless (and (org-supertag-validate-cron-field 
+                     minute "^\\([0-9]\\|[1-5][0-9]\\)$")
+                    (org-supertag-validate-cron-field 
+                     hour "^\\([0-9]\\|1[0-9]\\|2[0-3]\\)$")
+                    (org-supertag-validate-cron-field 
+                     day "^\\([1-9]\\|[12][0-9]\\|3[01]\\)$")
+                    (org-supertag-validate-cron-field 
+                     month "^\\([1-9]\\|1[0-2]\\)$")
+                    (org-supertag-validate-cron-field 
+                     weekday "^[0-6]$"))
+          (error "Invalid cron expression: %s" cron-string))
+        (list :minute minute
+              :hour hour
+              :day day
+              :month month
+              :weekday weekday)))))
+
+(defun org-supertag-validate-cron-field (value pattern)
+  "Validate cron field.
+VALUE is the field value
+PATTERN is the validation pattern"
+  (or (string= value "*")
+      (string-match "^\\*/[0-9]+$" value)
+      (string-match-p pattern value)))
+
+(defun org-supertag-cron-valid-p (cron-string)
+  "Check if cron expression is valid.
+CRON-STRING is the complete cron expression"
+  (when-let* ((fields (org-supertag-parse-cron cron-string)))
+    (and
+     ;; Minutes (0-59)
+     (org-supertag-validate-cron-field 
+      (plist-get fields :minute)
+      "^\\(?:\\*\\|[0-5]?[0-9]\\)$")
+     ;; Hours (0-23)
+     (org-supertag-validate-cron-field 
+      (plist-get fields :hour)
+      "^\\(?:\\*\\|\\(?:[0-1]?[0-9]\\|2[0-3]\\)\\)$")
+     ;; Days (1-31)
+     (org-supertag-validate-cron-field 
+      (plist-get fields :day)
+      "^\\(?:\\*\\|\\(?:[1-2]?[0-9]\\|3[0-1]\\)\\)$")
+     ;; Months (1-12)
+     (org-supertag-validate-cron-field 
+      (plist-get fields :month)
+      "^\\(?:\\*\\|\\(?:[1-9]\\|1[0-2]\\)\\)$")
+     ;; Weekdays (0-6)
+     (org-supertag-validate-cron-field 
+      (plist-get fields :weekday)
+      "^\\(?:\\*\\|[0-6]\\)$"))))
+
+(defun org-supertag-cron-match-field (field-value current-value)
+  "Check if cron field matches.
+FIELD-VALUE is the cron field value
+CURRENT-VALUE is the current time value"
+  (cond
+   ;; Wildcard matches everything
+   ((string= field-value "*")
+    t)
+   ;; Handle */n format (every n units)
+   ((string-match "^\\*/\\([0-9]+\\)$" field-value)
+    (= (mod current-value (string-to-number (match-string 1 field-value))) 0))
+   ;; Direct number match
+   ((string= field-value (number-to-string current-value))
+    t)
+   ;; No match
+   (t nil)))
+
+(defun org-supertag-cron-matches-p (cron-expr time)
+  "Check if given time matches cron expression.
+CRON-EXPR is the cron expression
+TIME is the time to check"
+  (let* ((fields (org-supertag-parse-cron cron-expr))
+         (time-fields (decode-time time))
+         ;; Extract time fields
+         (current-minute (nth 1 time-fields))
+         (current-hour (nth 2 time-fields))
+         (current-day (nth 3 time-fields))
+         (current-month (nth 4 time-fields))
+         (current-weekday (nth 6 time-fields)))
+    ;; All fields must match
+    (and (org-supertag-cron-match-field (plist-get fields :minute) current-minute)
+         (org-supertag-cron-match-field (plist-get fields :hour) current-hour)
+         (org-supertag-cron-match-field (plist-get fields :day) current-day)
+         (org-supertag-cron-match-field (plist-get fields :month) current-month)
+         (org-supertag-cron-match-field (plist-get fields :weekday) current-weekday))))
+
+;; Task management
+(defun org-supertag-schedule-add-task (behavior)
+  "Add scheduled task.
+BEHAVIOR is the behavior definition with:
+:id        - Task ID (behavior name)
+:schedule  - Time specification (cron, absolute, relative, etc.)
+:action    - Function to execute
+:list      - Optional list of behaviors to execute
+:params    - Optional parameters for the action
+:tag-id    - Associated tag ID
+:node-id   - Associated node ID"
+  (let* ((id (plist-get behavior :id))
+         (schedule (plist-get behavior :schedule))
+         (action (plist-get behavior :action))
+         (behavior-list (plist-get behavior :list))
+         (task-params (plist-get behavior :params))
+         (tag-id (plist-get behavior :tag-id))
+         (node-id (plist-get behavior :node-id)))
+    
+    (message "Debug add-task - Adding task with ID: %s" id)
+    (message "Debug add-task - Schedule: %s" schedule)
+    (message "Debug add-task - Action: %S" action)
+    (message "Debug add-task - Tag: %s" tag-id)
+    (message "Debug add-task - Node: %s" node-id)
+    
+    ;; Validate required fields
+    (unless (and id schedule tag-id node-id
+                 (or action behavior-list))  ; Must have either action or list
+      (error "Missing required fields in behavior: %S" behavior))
+    
+    ;; Validate schedule format
+    (unless (org-supertag-parse-time-spec schedule)
+      (error "Invalid time specification: %s" schedule))
+    
+    ;; Add task
+    (puthash id
+             (list :schedule schedule
+                   :action action
+                   :list behavior-list
+                   :params task-params
+                   :tag-id tag-id
+                   :node-id node-id
+                   :last-run nil)
+             org-supertag-scheduled-tasks)
+    
+    ;; Verify task was added
+    (let ((added-task (gethash id org-supertag-scheduled-tasks)))
+      (message "Debug add-task - Verification - Task added: %S" added-task)
+      (message "Debug add-task - Current tasks in scheduler: %d"
+               (hash-table-count org-supertag-scheduled-tasks)))
+    
+    ;; Return the task ID
+    id))
+
+(defun org-supertag-schedule-remove-task (id)
+  "Remove scheduled task with ID."
+  (message "Removing scheduled task: %s" id)
+  (remhash id org-supertag-scheduled-tasks))
+
+
+(defun org-supertag-scheduler--execute-single-task (node-id action)
+  "Execute single ACTION on NODE-ID."
+  (if (functionp action)
+      (funcall action node-id)
+    (message "Warning: Invalid action: %S" action)))
+
+(defun org-supertag-scheduler--execute-behavior-list (node-id behavior-list)
+  "Execute BEHAVIOR-LIST on NODE-ID."
+  (dolist (sub-behavior behavior-list)
+    (let* ((parts (split-string sub-behavior "="))
+           (name (car parts))
+           (args (cadr parts)))
+      (message "Executing sub-behavior: %s with args: %s" name args)
+      (org-supertag-behavior-execute node-id name args))))
+
+(defun org-supertag-scheduler--execute-task-at-point (node-id task)
+  "Execute task at current point for NODE-ID and TASK."
+  (let ((action (plist-get task :action))
+        (behavior-list (plist-get task :list))
+        (task-params (plist-get task :params)))
+    (cond
+     (behavior-list
+      (org-supertag-scheduler--execute-behavior-list node-id behavior-list))
+     ((functionp action)
+      (if task-params
+          (funcall action node-id task-params)
+        (funcall action node-id nil)))
+     (t
+      (message "Warning: Invalid action in task: %S" action)))))
+
+(defun org-supertag-scheduler--update-task (id task now)
+  "Update task execution time and status."
+  (puthash id
+           (list :schedule (plist-get task :schedule)
+                 :action (plist-get task :action)
+                 :list (plist-get task :list)
+                 :params (plist-get task :params)
+                 :tag-id (plist-get task :tag-id)
+                 :node-id (plist-get task :node-id)
+                 :last-run (format-time-string "%Y-%m-%d %H:%M" now))
+           org-supertag-scheduled-tasks))
+
+(defun org-supertag-scheduler--should-execute-p (task now node-id)
+  "Check if TASK should be executed at NOW for NODE-ID."
+  (let ((schedule (plist-get task :schedule))
+        (last-run (plist-get task :last-run))
+        (current-time-str (format-time-string "%Y-%m-%d %H:%M" now)))
+    (message "Debug - Schedule: %s, Last run: %s, Current time: %s"
+             schedule last-run current-time-str)
+    (and (org-supertag-time-matches-p schedule now node-id)
+         (not (equal last-run current-time-str)))))
+
+(defun org-supertag-scheduler--process-task (id task now)
+  "Process single task with ID and TASK at time NOW."
+  (let ((tag-id (plist-get task :tag-id))
+        (node-id (plist-get task :node-id)))
+    
+    ;; 验证节点和标签
+    (when-let* ((node-tags (org-supertag-node-get-tags node-id)))
+      (message "Task %s - Node %s has tags: %S. Required tag: %s"
+               id node-id node-tags tag-id)
+      
+      ;; 检查标签是否存在
+      (if (not (member tag-id node-tags))
+          (message "Task %s - SKIPPING, node does not have tag %s" 
+                   id tag-id)
+        
+        ;; 检查是否应该执行
+        (if (not (org-supertag-scheduler--should-execute-p task now node-id))
+            (message "Task %s - SKIPPING, time does not match or already ran" 
+                     id)
+          
+          ;; 执行任务
+          (message "Task %s - EXECUTING" id)
+          (condition-case err
+              (save-excursion
+                (when-let* ((pos (org-supertag-db-get-pos node-id)))
+                  (org-with-point-at pos
+                    (org-supertag-scheduler--execute-task-at-point node-id task)
+                    (org-supertag-scheduler--update-task id task now))))
+            (error
+             (message "Error in task %s: %s" 
+                      id (error-message-string err)))))))))
+
+(defun org-supertag-scheduler-check-tasks ()
+  "Check and execute tasks matching current time."
+  (let ((now (current-time)))
+    ;; 基本信息日志
+    (message "Scheduler: Checking tasks at %s. Total tasks: %d"
+             (format-time-string "%Y-%m-%d %H:%M:%S" now)
+             (hash-table-count org-supertag-scheduled-tasks))
+    
+    ;; 处理所有任务
+    (maphash
+     (lambda (id task)
+       (message "Checking task: %s" id)
+       (org-supertag-scheduler--process-task id task now))
+     org-supertag-scheduled-tasks)))
+
+
+(defun org-supertag-behavior--setup-scheduled-behaviors ()
+  "Scan all tags and their attached behaviors to set up scheduled tasks."
+  ;; 1. Clear and initialize the scheduled tasks hash table.
+  (setq org-supertag-scheduled-tasks (make-hash-table :test 'equal))
 
-(defun org-supertag-behavior--setup-all-scheduled-behaviors ()
-  "Scan all tags and nodes to register all scheduled behaviors with the central scheduler.
-This is typically run once on initialization."
-  ;; 1. Iterate through all registered behaviors
+  ;; 2. Iterate through all objects in the main database.
   (maphash
-   (lambda (behavior-name behavior-plist)
-     (when (and (eq (plist-get behavior-plist :trigger) :on-schedule)
-                (plist-get behavior-plist :schedule))
-       ;; 2. Find all tags that have this behavior attached
-       (let ((tags-with-behavior (org-supertag-tag-get-by-property :behaviors behavior-name)))
-         (dolist (tag-id tags-with-behavior)
-           ;; 3. Find all nodes with that tag
-           (let ((nodes (org-supertag-db-find-nodes-by-tag tag-id)))
-             (dolist (node-id nodes)
-               ;; 4. Register the scheduled task for this specific node and behavior
-               (org-supertag-behavior--register-scheduled-task node-id tag-id behavior-name)))))))
-   org-supertag-behavior-registry)
-  (message "Behavior Scheduler: Setup complete. All scheduled tasks registered."))
-
-(defun org-supertag-behavior--register-scheduled-task (node-id tag-id behavior-name)
-  "Registers a single scheduled task for a given node and behavior."
-  (when-let* ((behavior (gethash behavior-name org-supertag-behavior-registry))
-              (schedule-time (plist-get behavior :schedule)))
-    (unless (and (stringp schedule-time) (string-match-p "^[0-2][0-9]:[0-5][0-9]$" schedule-time))
-      (warn "Invalid schedule format for behavior '%s' on tag '%s'. Expected HH:MM, got '%s'."
-            behavior-name tag-id schedule-time)
-      (cl-return))
-
-    (let ((task-id (intern (format "behavior-%s-%s" node-id behavior-name)))
-          (action (plist-get behavior :action)))
-      (message "Behavior Scheduler: Registering task '%s' for node '%s' at %s."
-               task-id node-id schedule-time)
-      (org-supertag-scheduler-register-task
-       task-id
-       :daily
-       ;; Use a lambda to wrap the execution with the correct parameters
-       (lambda () (org-supertag-behavior--execute-scheduled-task node-id behavior-name))
-       :time schedule-time))))
-
-(defun org-supertag-behavior--deregister-scheduled-task (node-id behavior-name)
-  "Deregisters a scheduled task for a given node and behavior."
-  (let ((task-id (intern (format "behavior-%s-%s" node-id behavior-name))))
-    (message "Behavior Scheduler: Deregistering task '%s'." task-id)
-    (org-supertag-scheduler-deregister-task task-id)))
+   (lambda (tag-name tag-plist)
+     (when (eq (plist-get tag-plist :type) :tag)
+       (let ((behaviors (plist-get tag-plist :behaviors)))
+         (when behaviors
+           (dolist (behavior-name behaviors)
+             (let ((behavior (gethash behavior-name org-supertag-behavior-registry)))
+               (when (and behavior
+                          (eq (plist-get behavior :trigger) :on-schedule)
+                          (plist-get behavior :schedule))
+                 (let ((nodes (org-supertag-db-find-nodes-by-tag tag-name)))
+                   (dolist (node-id nodes)
+                     (let ((task-id (format "%s-%s" node-id behavior-name)))
+                       (ignore-errors
+                         (org-supertag-schedule-add-task
+                          (list :id task-id
+                                :schedule (plist-get behavior :schedule)
+                                :action (plist-get behavior :action)
+                                :list (plist-get behavior :list)
+                                :params (plist-get behavior :params)
+                                :tag-id tag-name
+                                :node-id node-id)))))))))))))
+   org-supertag-db--object)
+
+  ;; 3. Start the scheduler timer.
+  (org-supertag-scheduler-start)
+
+  ;; 4. Log the total number of tasks scheduled.
+  (message "Scheduled behaviors setup completed. Total tasks in queue: %d"
+           (hash-table-count org-supertag-scheduled-tasks)))
+
+(defun org-supertag-scheduler-start ()
+  "Start the scheduler."
+  (message "Starting scheduler...")
+  
+  ;; 检查现有定时器
+  (when org-supertag-scheduler-timer
+    (message "Found existing timer: %S" org-supertag-scheduler-timer)
+    (cancel-timer org-supertag-scheduler-timer)
+    (setq org-supertag-scheduler-timer nil)
+    (message "Cancelled existing timer"))
+  
+  ;; 检查任务数量
+  (let ((task-count (hash-table-count org-supertag-scheduled-tasks)))
+    (message "Current scheduled tasks: %d" task-count)
+    
+    ;; 只有在有任务时才启动定时器
+    (if (> task-count 0)
+        (progn
+          (setq org-supertag-scheduler-timer
+                (run-at-time t 300 #'org-supertag-scheduler-check-tasks))
+          (message "Scheduler started with timer: %S" org-supertag-scheduler-timer)
+          
+          ;; 立即执行一次检查
+          (org-supertag-scheduler-check-tasks))
+      (message "No tasks to schedule, scheduler not started")))
+  
+  ;; 返回定时器状态
+  org-supertag-scheduler-timer)
+
+(defun org-supertag-scheduler-stop ()
+  "Stop the scheduler."
+  (when org-supertag-scheduler-timer
+    (message "Stopping scheduler timer: %S" org-supertag-scheduler-timer)
+    (cancel-timer org-supertag-scheduler-timer)
+    (setq org-supertag-scheduler-timer nil)
+    (message "Scheduler stopped")))
 
 ;;------------------------------------------------------------------------------
 ;; Behavior Execution
@@ -163,33 +589,54 @@ Returns t if valid, nil otherwise."
        ((numberp pos) (goto-char pos)))
       (org-at-heading-p))))
 
-(defun org-supertag-behavior--on-tag-change (node-id tag-id change-type)
-  "Main handler for tag changes.
-Triggers behaviors based on tag and change type.
-CHANGE-TYPE can be :add, :remove, or :update."
-  (message "DEBUG: on-tag-change, node=%s, tag=%s, type=%s" node-id tag-id change-type)
-  ;; A tag can have multiple behaviors.
-  (let* ((tag-info (org-supertag-tag-get tag-id))
-         (behavior-names (plist-get tag-info :behaviors)))
-    (dolist (behavior-name behavior-names)
-      (when-let* ((behavior (gethash behavior-name org-supertag-behavior-registry))
-                  (trigger (plist-get behavior :trigger)))
-        (cond
-         ;; Handle scheduled behaviors by registering/deregistering with the central scheduler
-         ((eq trigger :on-schedule)
-          (cond
-           ((eq change-type :add)
-            (org-supertag-behavior--register-scheduled-task node-id tag-id behavior-name))
-           ((eq change-type :remove)
-            (org-supertag-behavior--deregister-scheduled-task node-id behavior-name))
-           (t nil))) ;; :on-change does nothing for scheduled tasks
-
-         ;; Handle regular behaviors
-         ((or (eq trigger :always)
-              (eq trigger :on-change)
-              (and (eq trigger :on-add) (eq change-type :add))
-              (and (eq trigger :on-remove) (eq change-type :remove)))
-          (org-supertag-behavior-execute node-id behavior-name)))))))
+(defun org-supertag-behavior--on-tag-change (node-id tag-id action)
+  "Handle behavior when TAG-ID is applied to NODE-ID with ACTION."
+  ;; (message "Debug on-tag-change - node=%s tag=%s action=%s" 
+  ;;          node-id tag-id action)
+  (when-let* ((behavior (org-supertag-behavior--get-behavior tag-id))
+              (trigger (plist-get behavior :trigger)))
+    ;; (message "Debug on-tag-change - Found behavior=%S trigger=%S" 
+    ;;          behavior trigger)
+    (cond
+     ;; Handle scheduled behaviors
+     ((eq trigger :on-schedule)
+      (if (eq action :add)
+          (progn
+            (message "Attempting to register scheduled behavior for tag %s on node %s" tag-id node-id)
+            (condition-case err
+                (let* ((behavior-props (org-supertag-behavior--get-behavior tag-id)) ; Get full behavior props
+                       (b-action (plist-get behavior-props :action))
+                       (b-list (plist-get behavior-props :list))
+                       (b-schedule (plist-get behavior-props :schedule))
+                       ;; IMPORTANT: Task ID should be unique per node and behavior combination
+                       (task-unique-id (format "%s-%s" node-id tag-id)))
+                  (message "Registering task with ID: %s, Schedule: %s, Action: %S, List: %S, Tag: %s, Node: %s"
+                           task-unique-id b-schedule b-action b-list tag-id node-id)
+                  (org-supertag-schedule-add-task
+                   (list :id task-unique-id      ; Use unique ID
+                         :schedule b-schedule
+                         :action b-action
+                         :list b-list
+                         :params (plist-get behavior-props :params)
+                         :tag-id tag-id
+                         :node-id node-id))
+                  (message "Successfully registered scheduled behavior %s for tag %s on node %s"
+                           tag-id tag-id node-id)) ; Referring to tag-id as behavior name for this message
+              (error
+               (message "Error registering scheduled behavior for tag %s: %s"
+                        tag-id (error-message-string err)))))
+        ;; Remove scheduled task when tag is removed
+        (when (eq action :remove)
+          (let ((task-unique-id (format "%s-%s" node-id tag-id)))
+            (message "Attempting to remove scheduled task with ID: %s" task-unique-id)
+            (org-supertag-schedule-remove-task task-unique-id)))))
+     
+     ;; Handle regular behaviors
+     ((or (eq trigger :always)
+          (eq trigger :on-change)
+          (and (eq trigger :on-add) (eq action :add))
+          (and (eq trigger :on-remove) (eq action :remove)))
+      (org-supertag-behavior-execute node-id behavior)))))
 
 (defun org-supertag-behavior--plist-p (object)
   "Check if OBJECT is a property list."
@@ -334,7 +781,7 @@ Returns plist like (:fg \"red\" :bg \"blue\" :weight \"bold\")"
     
     ;; Update tag's behaviors property
     (let* ((behaviors (or (plist-get tag :behaviors) '())))
-      (org-supertag-tag--create 
+      (org-supertag-tag-create 
        tag-name 
        :type :tag
        :behaviors (cons behavior-name behaviors)))
@@ -383,14 +830,14 @@ Returns plist like (:fg \"red\" :bg \"blue\" :weight \"bold\")"
     ;; Update tag's behaviors property
     (let* ((behaviors (plist-get tag :behaviors))
            (new-behaviors (delete behavior-name behaviors)))
-      (org-supertag-tag--create 
+      (org-supertag-tag-create 
        tag-name 
        :type :tag
        :behaviors new-behaviors))
     
     ;; Remove scheduled task if it's a scheduled behavior
     (when (eq (plist-get behavior :trigger) :schedule)
-      (org-supertag-behavior--deregister-scheduled-task behavior-name))
+      (org-supertag-schedule-remove-task behavior-name))
     
     ;; Ensure we're at a valid org heading and execute behavior removal
     (when (org-at-heading-p)
@@ -578,8 +1025,8 @@ Prompts for a list of behaviors to execute."
     (advice-add 'org-supertag-behavior--handle-node-change 
                 :around #'org-supertag-behavior--protect-id-locations)
 
-    ;; Initialize and register all existing scheduled tasks from the DB
-    (org-supertag-behavior--setup-all-scheduled-behaviors)
+    ;; Initialize scheduler system
+    (org-supertag-behavior--setup-scheduled-behaviors)
 
     (setq org-supertag-behavior--initialized t)
     (message "=== Behavior System Init Success ===")))
@@ -626,16 +1073,14 @@ Prompts for a list of behaviors to execute."
                #'org-supertag-behavior--handle-tag-add)
   (remove-hook 'org-supertag-after-tag-remove-hook
                #'org-supertag-behavior--handle-tag-remove)
-
-  ;; Stop scheduler - This is now handled by the main org-supertag mode,
-  ;; we just need to make sure no behaviors from this module are left registered.
-  ;; We can do this by iterating through the registry and deregistering.
-  ;; However, a simpler approach is to rely on the user disabling the minor mode
-  ;; which should handle cleanup. For now, we leave this as a no-op as the
-  ;; central scheduler shutdown is handled elsewhere.
+  (remove-hook 'org-supertag-after-load-hook
+               #'org-supertag-behavior--setup-scheduled-behaviors)
+  
+  ;; Stop scheduler
+  (org-supertag-scheduler-stop)
 
   ;; Remove advice
-  (advice-remove 'org-supertag-behavior--handle-node-change
+  (advice-remove 'org-supertag-behavior--handle-node-change 
                 #'org-supertag-behavior--protect-id-locations))
 
 (defun org-supertag-behavior--handle-todo-change ()
@@ -789,6 +1234,20 @@ Returns t if valid, signals error if invalid."
     t))
 
 
+;;------------------------------------------------------------------------------
+;; API Functions
+;;------------------------------------------------------------------------------
+
+(defun org-supertag-behavior-get (tag-id)
+  "Get behavior definition for TAG-ID."
+  (when-let* ((tag (org-supertag-tag-get tag-id)))
+    (org-supertag-tag-get-field-value tag "_behavior")))
+
+(defun org-supertag-behavior-refresh-node (node-id)
+  "Refresh behaviors for NODE-ID."
+  (dolist (tag-id (org-supertag-db-get-tags node-id))
+    (org-supertag-behavior--safe-execute node-id tag-id :add)))
+
 ;;------------------------------------------------------------------------------
 ;; Minor Mode
 ;;------------------------------------------------------------------------------
@@ -952,7 +1411,7 @@ Returns t if valid, signals error if invalid."
 (add-hook 'org-supertag-after-load-hook
           #'org-supertag-behavior-setup)
 (add-hook 'org-supertag-db-after-load-hook 
-          #'org-supertag-behavior--setup-all-scheduled-behaviors)
+          #'org-supertag-behavior--setup-scheduled-behaviors)
 
 
  
diff --git a/org-supertag-bridge.el b/org-supertag-bridge.el
index 212463b..82a5bbc 100755
--- a/org-supertag-bridge.el
+++ b/org-supertag-bridge.el
@@ -111,11 +111,47 @@ Functions added to this hook will be run without arguments.")
 ;; Emacs-side EPC Server (for Python to connect to)
 ;; =============================================================================
 (defun org-supertag-bridge--start-emacs-epc-server ()
-  "This function is now DEPRECATED.
-The new connection logic no longer requires a persistent Emacs-side server
-for the initial handshake. Python server is now polled directly."
-  (message "org-supertag-bridge--start-emacs-epc-server is deprecated and should not be called.")
-  nil)
+  "Start the Emacs-side EPC server for `simtag_bridge.py` to connect to.
+This server allows Python to call methods defined in Emacs."
+  (org-supertag-bridge--log "Attempting to start Emacs-side EPC server (current server: %S, live: %S)..."
+                            org-supertag-bridge--emacs-epc-server
+                            (if (processp org-supertag-bridge--emacs-epc-server)
+                                (process-live-p org-supertag-bridge--emacs-epc-server)
+                              "not a process"))
+  (unless (and org-supertag-bridge--emacs-epc-server (process-live-p org-supertag-bridge--emacs-epc-server))
+    (org-supertag-bridge--log "No live Emacs EPC server found, attempting to create one.")
+    (let ((callback (lambda (manager) ; Callback when Python connects
+                      (org-supertag-bridge--log "Python process connected to Emacs EPC server. Manager: %S" manager)
+                      (org-supertag-bridge-epc-define-method manager
+                                                             'simtag-bridge/report-ready ; Python calls this
+                                                             #'org-supertag-bridge--handle-python-server-ready-signal) ; Our handler
+                      (org-supertag-bridge-epc-define-method manager
+                                                             'eval-in-emacs ; Python calls this
+                                                             #'org-supertag-bridge--eval-in-emacs-func) ; Our handler
+                      (org-supertag-bridge--log "Defined EPC methods for Python on Emacs server."))))
+      (org-supertag-bridge--log "Calling org-supertag-bridge-epc-server-start now with callback...")
+      ;; Ensure `org-supertag-bridge-epc.el` is loaded for `org-supertag-bridge-epc-server-start`
+      (require 'org-supertag-bridge-epc)
+      (setq org-supertag-bridge--emacs-epc-server (org-supertag-bridge-epc-server-start callback))
+      (org-supertag-bridge--log "org-supertag-bridge-epc-server-start returned: %S" org-supertag-bridge--emacs-epc-server)))
+
+  (if org-supertag-bridge--emacs-epc-server
+      (if (process-live-p org-supertag-bridge--emacs-epc-server) ; Extra check for liveness
+          (progn
+            (setq org-supertag-bridge--emacs-epc-server-port (process-contact org-supertag-bridge--emacs-epc-server :service))
+            (org-supertag-bridge--log "Emacs-side EPC server started successfully. Process: %S. Listening on port: %d"
+                                      org-supertag-bridge--emacs-epc-server
+                                      org-supertag-bridge--emacs-epc-server-port)
+            t) ; Success
+        (progn
+          (org-supertag-bridge--log "ERROR: Emacs EPC server process %S is not live after creation." org-supertag-bridge--emacs-epc-server)
+          (message "[OrgSuperTagBridge] Error: Emacs-side EPC server process not live after creation.")
+          (setq org-supertag-bridge--emacs-epc-server nil) ; Clear it if not live
+          nil)) ; Failure
+    (progn
+      (org-supertag-bridge--log "ERROR: Failed to start Emacs-side EPC server (server object is nil after attempt).")
+      (message "[OrgSuperTagBridge] Error: Emacs-side EPC server failed to start (nil server object).")
+      nil))) ; Failure
 
 ;; Handler for 'eval-in-emacs' called by Python
 (defun org-supertag-bridge--eval-in-emacs-func (sexp-string)
@@ -131,24 +167,46 @@ for the initial handshake. Python server is now polled directly."
      ;; Decide if Python should be notified of the error. For now, just log.
      nil)))
 
-;; NEW: Handler for 'simtag-bridge/log' called by Python
-(defun org-supertag-bridge-epc-log-message (message-string)
-  "Receives a MESSAGE-STRING from Python and logs it with a [Python] prefix."
-  (org-supertag-bridge--log "[Python] %s" message-string))
-
 ;; Handler for Python's readiness signal
-(defun org-supertag-bridge--handle-python-server-ready-signal (args)
-  "This function is now DEPRECATED.
-The new connection logic polls the Python server directly."
-  (message "org-supertag-bridge--handle-python-server-ready-signal is deprecated.")
-  nil)
+(defun org-supertag-bridge--handle-python-server-ready-signal (python-server-port)
+  "Handles `simtag-bridge/report-ready` call from Python.
+PYTHON-SERVER-PORT is the port the Python EPC server is listening on.
+Establishes the main EPC connection from Emacs TO the Python server and runs the ready hook."
+  (org-supertag-bridge--log "Python server reported ready. Listening on its port: %s" python-server-port)
+  (condition-case-unless-debug err
+      (progn
+        (setq org-supertag-bridge--python-epc-manager
+              (make-org-supertag-bridge-epc-manager
+               :server-process org-supertag-bridge--python-process
+               :commands (cons org-supertag-bridge--python-program-to-run org-supertag-bridge--python-program-args)
+               :title (format "OrgSuperTagBridge-Client-to-Python:%s" python-server-port)
+               :port python-server-port
+               :connection (org-supertag-bridge-epc-connect "127.0.0.1" python-server-port)))
+        
+        (if (and org-supertag-bridge--python-epc-manager (org-supertag-bridge-epc-live-p org-supertag-bridge--python-epc-manager))
+            (progn
+              (org-supertag-bridge-epc-init-epc-layer org-supertag-bridge--python-epc-manager)
+              (setq org-supertag-bridge--ready-p t)
+              (message "[OrgSuperTagBridge] Successfully connected to SimTagBridge Python server on port %s." python-server-port)
+              (org-supertag-bridge--log "✅ Connection FROM Emacs TO SimTagBridge Python server established. Running ready hook...")
+              (run-hooks 'org-supertag-bridge-ready-hook))
+          (progn
+            (setq org-supertag-bridge--ready-p nil)
+            (error (format "[OrgSuperTagBridge] Failed to establish live connection to Python server on port %s" python-server-port)))))
+    (error
+     (setq org-supertag-bridge--ready-p nil)
+     (org-supertag-bridge--log "ERROR establishing connection TO Python server: %S" err)
+     (message "[OrgSuperTagBridge] Error connecting to Python server: %s" (error-message-string err))
+     (org-supertag-bridge-kill-process) ; Clean up if connection failed
+     nil)))
 
 ;; =============================================================================
 ;; Python Process Management
 ;; =============================================================================
 (cl-defun org-supertag-bridge-start-process ()
-  "Start the `simtag_bridge.py` process and poll it until it's ready."
+  "Start the `simtag_bridge.py` process if it isn't already running and connected."
   (interactive)
+  ;; Use a more robust check based on the actual process object.
   (if (and (processp org-supertag-bridge--python-process)
            (process-live-p org-supertag-bridge--python-process))
       (progn
@@ -156,102 +214,76 @@ The new connection logic polls the Python server directly."
         (message "[OrgSuperTagBridge] Process already running."))
     (org-supertag-bridge--log "Attempting to start SimTagBridge process pipeline...")
     
-    (when (and (processp org-supertag-bridge--python-process)
-               (not (process-live-p org-supertag-bridge--python-process)))
-      (org-supertag-bridge--log "Stale process object found. Cleaning up before starting new process.")
-      (org-supertag-bridge-kill-process))
+    ;; 1. Ensure Emacs-side EPC server is running for Python to connect back
+    (unless (org-supertag-bridge--start-emacs-epc-server)
+      (org-supertag-bridge--log "Aborting Python process start: Emacs-side EPC server failed.")
+      (cl-return-from org-supertag-bridge-start-process nil))
+      
+    (unless org-supertag-bridge--emacs-epc-server-port
+      (org-supertag-bridge--log "Aborting Python process start: Emacs-side EPC server port not available.")
+      (error "[OrgSuperTagBridge] Emacs-side EPC server port not set. Cannot start Python process.")
+      (cl-return-from org-supertag-bridge-start-process nil))
 
-    ;; 1. Prepare and launch the simtag_bridge.py script
-    (setq org-supertag-bridge--ready-p nil)
+    ;; 2. Prepare and launch the simtag_bridge.py script
+    (setq org-supertag-bridge--ready-p nil) ; Reset readiness flag
     (let* ((python-cmd org-supertag-bridge-python-command)
-           ;; CRITICAL: Use the centralized project root variable.
-           (project-root org-supertag-project-root)
-           (port-file (expand-file-name "simtag_bridge.port" org-supertag-data-directory))
-           (args (list "-m" "simtag.simtag_bridge" "--port-file" port-file "--data-directory" org-supertag-data-directory)))
-
-      (when (file-exists-p port-file)
-        (delete-file port-file)) ; Clean up old port file
-
+           (emacs-port-str (number-to-string org-supertag-bridge--emacs-epc-server-port))
+           (data-dir org-supertag-data-directory)
+           (profile-arg (when org-supertag-bridge-enable-profile (list "--profile")))
+           (default-directory (expand-file-name ".." (file-name-directory org-supertag-bridge-python-script))))
+
+      (unless (file-exists-p python-cmd)
+        (let ((setup-script (expand-file-name "simtag/setup.sh" default-directory)))
+          (if (and (file-exists-p setup-script)
+                   (y-or-n-p (format "Python executable '%s' not found. Run setup script to create virtual environment?" python-cmd)))
+              (progn
+                (message "[OrgSuperTag] Starting setup script: %s" setup-script)
+                (start-process "simtag-setup"
+                               (get-buffer-create "*SimTag Setup*")
+                               "bash" setup-script)
+                (display-buffer "*SimTag Setup*")
+                (message "[OrgSuperTag] Setup script running in background. Please wait for it to complete, then try again.")
+                ;; Abort the current start process, user needs to retry.
+                (cl-return-from org-supertag-bridge-start-process nil))
+            ;; If user says no, or script doesn't exist, throw the original error.
+            (error "[OrgSuperTagBridge] Python script not found: %s" python-cmd))))
+
+      (unless (file-directory-p data-dir)
+        (make-directory data-dir t)
+        (org-supertag-bridge--log "Created data directory: %s" data-dir))
+      
       (setq org-supertag-bridge--python-program-to-run python-cmd)
-      (setq org-supertag-bridge--python-program-args (append args (when org-supertag-bridge-enable-profile (list "--profile"))))
-
-      (org-supertag-bridge--log "Starting Python process in directory: %s" project-root)
-      (org-supertag-bridge--log "Command: %s %s" python-cmd (string-join org-supertag-bridge--python-program-args " "))
-
-      ;; CRITICAL FIX: Use `let` to dynamically bind `default-directory` for the child process.
-      ;; This ensures the process starts in the correct project root, regardless of the current buffer's directory.
-      (let ((default-directory project-root))
-        (setq org-supertag-bridge--python-process (apply #'start-process "SimTagBridge-Python" (get-buffer-create org-supertag-bridge-process-buffer-name) python-cmd org-supertag-bridge--python-program-args)))
+      (setq org-supertag-bridge--python-program-args
+            (append (list "-m" "simtag.simtag_bridge"
+                          "--emacs-epc-port" emacs-port-str
+                          "--data-directory" data-dir)
+                    profile-arg))
+
+      (org-supertag-bridge--log "DEBUG: Using Python command from org-supertag-bridge-python-command: '%s'" python-cmd)
+      (org-supertag-bridge--log "Starting Python process with command: %s %s"
+                                org-supertag-bridge--python-program-to-run
+                                (string-join org-supertag-bridge--python-program-args " "))
+      (org-supertag-bridge--log "  Working directory for script: %s" default-directory)
       
-      (when (processp org-supertag-bridge--python-process)
-        (set-process-sentinel org-supertag-bridge--python-process #'org-supertag-bridge-process-sentinel)
-        (org-supertag-bridge--log "Python process started. Polling for port file and connection...")
-        (org-supertag-bridge--poll-for-connection port-file 10)))))
-
-(defun org-supertag-bridge--poll-for-connection (port-file max-wait-seconds)
-  "Poll for PORT-FILE to appear and contain the port number."
-  (let ((start-time (current-time))
-        (port nil))
-    (while (and (not port) (< (time-to-seconds (time-subtract (current-time) start-time)) max-wait-seconds))
-      (when (file-exists-p port-file)
-        (with-temp-buffer
-          (insert-file-contents port-file)
-          (setq port (string-to-number (buffer-string)))))
-      (unless port
-        (sleep-for 0.2)))
-
-    (if (not port)
-        (progn
-          (org-supertag-bridge--log "ERROR: Python server did not write port file in time.")
-          (message "[OrgSuperTagBridge] Error: Connection timed out.")
-          (org-supertag-bridge-kill-process))
-      (org-supertag-bridge--log "Got port %d from file. Attempting to connect..." port)
-      (condition-case-unless-debug err
-          (let ((manager (make-org-supertag-bridge-epc-manager
-                          :server-process org-supertag-bridge--python-process
-                          :commands (cons org-supertag-bridge--python-program-to-run org-supertag-bridge--python-program-args)
-                          :title (format "OrgSuperTagBridge-Client-to-Python:%s" port)
-                          :port port
-                          :connection (org-supertag-bridge-epc-connect "127.0.0.1" port))))
-            (setq org-supertag-bridge--python-epc-manager manager)
-            (org-supertag-bridge-epc-init-epc-layer manager)
-            ;; Final check using a simple ping
-            (if (equal "pong" (org-supertag-bridge-call-sync "ping" nil 5))
-                (progn
-                  (setq org-supertag-bridge--ready-p t)
-                  (message "[OrgSuperTagBridge] Successfully connected to SimTagBridge Python server on port %d." port)
-                  (org-supertag-bridge--log "✅ Connection established. Running ready hook...")
-                  (run-hooks 'org-supertag-bridge-ready-hook))
-              (error "Ping to Python server failed.")))
-        (error
-         (org-supertag-bridge--log "ERROR connecting to Python server: %S" err)
-         (message "[OrgSuperTagBridge] Error connecting to Python server: %s" (error-message-string err))
-         (org-supertag-bridge-kill-process))))))
-
-(defun org-supertag-bridge-process-sentinel (process event)
-  "Sentinel function for the Python process.
-Handles process termination and reports errors."
-  (org-supertag-bridge--log "Process sentinel triggered for %s with event: %s" process event)
-  (let ((exit-status (process-status process)))
-    (unless (memq exit-status '(run signal)) ; Ignore normal running or signal-based termination for now
-      (setq org-supertag-bridge--ready-p nil)
-      (setq org-supertag-bridge--python-process nil)
-      (org-supertag-bridge--log "Python process terminated. Status: %S" exit-status)
-      (message "[OrgSuperTagBridge] Python process terminated: %s" event)
-      (let ((output-buffer (process-buffer process)))
-        (when (buffer-live-p output-buffer)
-          (with-current-buffer output-buffer
-            (let ((output (buffer-string)))
-              (when (string-match-p "\\S-" output) ; If there's non-whitespace output
-                (org-supertag-bridge--log "--- Python Process Output ---")
-                (org-supertag-bridge--log output)
-                (org-supertag-bridge--log "---------------------------")
-                (display-buffer output-buffer) ; Show the user what went wrong
-                (message "[OrgSuperTagBridge] Python process exited with an error. See %s for details."
-                         (buffer-name output-buffer))))))))))
+      (let ((current-process-environment process-environment) ; Save current
+            (process-connection-type nil)) ; For stdio pipes
+        (setq process-environment current-process-environment) ; Restore for other Emacs processes
+        (org-supertag-bridge--log "Starting Python process with command: %s %s"
+                                  org-supertag-bridge--python-program-to-run
+                                  (string-join org-supertag-bridge--python-program-args " "))
+        (setq org-supertag-bridge--python-process
+              (apply #'start-process
+                     "simtag-bridge-py"
+                     (get-buffer-create org-supertag-bridge-process-buffer-name)
+                     org-supertag-bridge--python-program-to-run
+                     org-supertag-bridge--python-program-args)))
+
+      (when (process-live-p org-supertag-bridge--python-process)
+        (org-supertag-bridge--log "SimTagBridge Python process started. Waiting for it to connect back and report ready..."))
+      org-supertag-bridge--python-process)))
 
 (defun org-supertag-bridge-kill-process ()
-  "Kill the `simtag_bridge.py` process and associated EPC connections."
+  "Stop the SimTagBridge Python process and clean up related EPC resources."
   (interactive)
   (org-supertag-bridge--log "Attempting to kill SimTagBridge Python process and EPC connections...")
   
@@ -431,85 +463,6 @@ Returns t if ready, nil otherwise. Waits up to TIMEOUT seconds (default 10)."
   (remove-hook 'post-command-hook #'org-supertag-bridge-ensure-ready t) ; Remove local hook
   (message "[OrgSuperTagBridge] Auto-start disabled."))
 
-;; =============================================================================
-;; AI/LLM Convenience Functions
-;; =============================================================================
-
-(defun org-supertag-bridge-fetch-available-models ()
-  "Fetch the list of available AI models via the Python bridge.
-Returns a list of model name strings on success, or signals an error."
-  (interactive)
-  (org-supertag-bridge--log "Fetching available AI models via bridge...")
-  (unless (org-supertag-bridge-ensure-ready)
-    (error "Bridge is not ready. Cannot fetch models."))
-  (let ((response (org-supertag-bridge-call-sync "get_available_models" nil)))
-    (org-supertag-bridge--log "Raw response for models: %S" response)
-    (if (and (listp response) (plistp response) (string= (plist-get response :status) "success"))
-        (let ((result (plist-get response :result)))
-          (if (listp result)
-              (progn
-                (org-supertag-bridge--log "Successfully fetched %d models." (length result))
-                result)
-            (error "Invalid result format for models: Expected list, got %S" result)))
-      (error "Error fetching models from backend: %S" response))))
-
-(defun org-supertag-bridge-select-model ()
-  "Interactively select an available AI model.
-Fetches the list of models from the backend and presents them for
-selection in the minibuffer. Inserts the selected model name into
-the current buffer at point."
-  (interactive)
-  (condition-case err
-      (let* ((available-models (org-supertag-bridge-fetch-available-models))
-             (selected-model nil))
-        (if available-models
-            (setq selected-model
-                  (completing-read "Select Model: "
-                                   available-models
-                                   nil ; predicate
-                                   t   ; require-match
-                                   nil ; initial-input
-                                   nil ; history var
-                                   (car available-models))) ; default
-          (message "No available models found."))
-        (if (and selected-model (not (string-empty-p selected-model)))
-            (progn
-              (insert selected-model)
-              (message "Inserted model: %s" selected-model))
-          (message "No model selected.")))
-    (error
-     (message "Error selecting model: %s" (error-message-string err)))))
-
-(defun org-supertag-bridge-llm-invoke (prompt &key system-prompt model temperature max-tokens)
-  "Synchronously invoke an LLM via the Python bridge.
-This is a high-level wrapper that sends a request to the `reasoning_invoke`
-method in the Python backend.
-
-PROMPT: The main user prompt string.
-:system-prompt: Optional system prompt string.
-:model: Optional model name string to override the default.
-:temperature: Optional temperature override.
-:max-tokens: Optional max-tokens override.
-
-Returns the AI response string on success, or signals an error."
-  (org-supertag-bridge--log "Invoking LLM with prompt: %s..." (truncate-string-to-width prompt 80))
-  (unless (org-supertag-bridge-ensure-ready)
-    (error "Bridge is not ready. Cannot invoke LLM."))
-
-  (let* ((payload-alist `((prompt . ,prompt)
-                         ,@(when system-prompt `((system_prompt . ,system-prompt)))
-                         ,@(when model `((model . ,model)))
-                         ,@(when temperature `((temperature . ,temperature)))
-                         ,@(when max-tokens `((max_tokens . ,max-tokens)))))
-         (response (org-supertag-bridge-call-sync "reasoning_invoke" (list payload-alist))))
-
-    (org-supertag-bridge--log "Raw LLM response: %S" response)
-    (if (and (listp response) (plistp response) (string= (plist-get response :status) "success"))
-        (let ((result (plist-get response :result)))
-          (org-supertag-bridge--log "LLM call successful.")
-          result)
-      (error "LLM call failed: %S" response))))
-
 (provide 'org-supertag-bridge)
 
 ;;; org-supertag-bridge.el ends here
diff --git a/org-supertag-completion.el b/org-supertag-completion.el
deleted file mode 100644
index df150dd..0000000
--- a/org-supertag-completion.el
+++ /dev/null
@@ -1,127 +0,0 @@
-;;; org-supertag-completion.el --- Universal and robust completion for org-supertag -*- lexical-binding: t; -*-
-
-;; This file provides a completion-at-point function (CAPF) for org-supertag.
-;; It uses the classic, most compatible CAPF design pattern to ensure it works
-;; correctly across all completion UIs, including company-mode and corfu.
-;;
-;; The core principle is to return a list of PURE, PROPERTIZED STRINGS,
-;; and use a SINGLE :exit-function that inspects the properties of the
-;; selected string to decide on the action. This is the "lowest common
-;; denominator" approach that all completion frameworks understand.
-
-(require 'org-supertag-tag)
-(require 'org-supertag-inline)
-
-;;;----------------------------------------------------------------------
-;;; Shared Logic
-;;;----------------------------------------------------------------------
-
-(defun org-supertag--get-prefix-bounds ()
-  "Find the bounds of a tag prefix at point, if any.
-Completion should only trigger when the point is immediately after
-a hash and a sequence of valid tag characters."
-  (let ((end (point)))
-    (save-excursion
-      (skip-chars-backward org-supertag-inline--valid-tag-chars)
-      (when (and (> (point) (point-min))
-               (eq (char-before) ?#))
-        ;; Return the bounds of the text part of the tag (after the '#')
-        (cons (point) end)))))
-
-(defun org-supertag--get-completion-table (prefix)
-  "Return a list of candidates, with '[Create New Tag]' propertized."
-  (let* ((safe-prefix (or prefix ""))
-         (node-id (org-id-get))
-         (current-tags (when node-id (org-supertag-node-get-tags node-id)))
-         (all-tags (org-supertag-get-all-tags))
-         (available-tags (if current-tags
-                             (seq-remove (lambda (tag) (member tag current-tags)) all-tags)
-                           all-tags))
-         (matching-tags (all-completions safe-prefix available-tags))
-         (new-tag-candidate "[Create New Tag]"))
-    (if (and (not (string-empty-p safe-prefix))
-             (not (member safe-prefix matching-tags))
-             (not (member safe-prefix current-tags)))
-        ;; If creating a new tag is possible, add the special candidate
-        ;; to the list and attach the `is-new-tag` property to it.
-        (cons (propertize new-tag-candidate 'is-new-tag t) matching-tags)
-      matching-tags)))
-
-(defun org-supertag--post-completion-action (selected-string original-prefix)
-  "The single, unified post-completion action.
-It handles both existing and new tags correctly by inspecting the
-completion candidate and correcting the buffer if necessary."
-  (let* ((is-new (get-text-property 0 'is-new-tag selected-string))
-         ;; For new tags, the REAL tag name is the prefix the user typed.
-         ;; For existing tags, it's the candidate they selected.
-         ;; --- FIX: Use substring-no-properties to get a clean string for data ---
-         (clean-tag-string (if is-new original-prefix (substring-no-properties selected-string)))
-         (tag-name (org-supertag-sanitize-tag-name clean-tag-string)))
-
-    (when (and tag-name (not (string-empty-p tag-name)))
-
-      ;; --- CRITICAL FIX ---
-      ;; If this is a new tag, the completion UI has inserted the placeholder
-      ;; text "[Create New Tag]". We MUST delete that and insert the actual
-      ;; tag name that the user typed (`original-prefix`).
-      (when is-new
-        (delete-region (- (point) (length selected-string)) (point))
-        (insert original-prefix))
-
-      ;; Now the buffer is in the correct state (e.g., "#mynewtag").
-      ;; We can proceed with the backend logic, but we must tell `tag-apply`
-      ;; NOT to insert text again, as it's already correct in the buffer.
-      (let ((org-supertag-skip-text-insertion t))
-        (when is-new
-          (unless (org-supertag-tag-exists-p tag-name)
-            (org-supertag-tag--create tag-name)))
-        ;; This handles DB relations and behaviors.
-        (org-supertag-tag-apply tag-name))
-
-      ;; Finally, add the trailing space to delimit the tag.
-      (insert " ")
-      (message "Tag '%s' applied." tag-name))))
-
-
-;;;----------------------------------------------------------------------
-;;; Main CAPF Entry Point
-;;;----------------------------------------------------------------------
-
-(defun org-supertag-completion-at-point ()
-  "Main `completion-at-point` function using the classic, compatible API."
-  (when-let* ((bounds (org-supertag--get-prefix-bounds))
-              (start (car bounds))
-              (end (cdr bounds))
-              (prefix (buffer-substring-no-properties start end)))
-
-    (list start end
-          ;; 1. The completion table. It returns a simple list of
-          ;;    propertized strings. This is universally understood.
-          (lambda (str pred action)
-            (if (eq action 'metadata)
-                '((category . org-supertag))
-              (org-supertag--get-completion-table prefix)))
-
-          ;; 2. A SINGLE, UNIFIED :exit-function. This is also
-          ;;    universally understood by all completion frameworks.
-          :exit-function
-          (lambda (selected-string status)
-            ;; The condition now accepts 'finished, 'exact', and 'sole' to be
-            ;; compatible with various completion UIs like Corfu.
-            (when (memq status '(finished exact sole))
-              (org-supertag--post-completion-action selected-string prefix))))))
-
-
-;;;----------------------------------------------------------------------
-;;; Setup
-;;;----------------------------------------------------------------------
-
-;;;###autoload
-(defun org-supertag-setup-completion ()
-  "Setup completion for org-supertag."
-  (add-to-list 'completion-at-point-functions
-               #'org-supertag-completion-at-point))
-
-(add-hook 'org-mode-hook #'org-supertag-setup-completion)
-
-(provide 'org-supertag-completion)
\ No newline at end of file
diff --git a/org-supertag-db.el b/org-supertag-db.el
old mode 100644
new mode 100755
index 990a368..fb70bd3
--- a/org-supertag-db.el
+++ b/org-supertag-db.el
@@ -8,11 +8,47 @@
 ;; - Entities are nodes in relationships, relationships are connections between nodes
 ;; - Connect entities through relationships - using type, from, to to express relationships
 ;;
+<<<<<<< HEAD
+=======
+;; Data Source Unification:
+;; To avoid data duplication and object reference issues, this module follows
+;; a unified data source strategy:
+;;
+;; 1. **Single Title Source**: Only :title field, from org-element :raw-value
+;; 2. **Single File Path**: Only :file-path, from buffer-file-name
+;; 3. **Filtered Properties**: Exclude redundant fields (FILE, ITEM, ALLTAGS, TAGS)
+;; 4. **Consistent Object Creation**: Each string field is a unique object
+;;
+;; Node Data Structure (Unified):
+;; (:type :node
+;;  :id "unique-id"           ; From org-element :ID
+;;  :title "title"            ; From org-element :raw-value (unified source)
+;;  :file-path "/path/file"   ; From buffer-file-name (unified source)
+;;  :pos 123                  ; From org-element :begin
+;;  :level 1                  ; From org-element :level
+;;  :olp ("path")             ; From org-get-outline-path
+;;  :todo "TODO"              ; From org-entry-get
+;;  :priority "A"             ; From org-entry-get
+;;  :scheduled time           ; From org-get-scheduled-time
+;;  :deadline time            ; From org-get-deadline-time
+;;  :properties (filtered)    ; Excludes FILE, ITEM, ALLTAGS, TAGS
+;;  :tags ("tag1" "tag2")     ; From org-get-tags
+;;  :content "content"        ; From buffer content
+;;  :ref-to ("id1" "id2")     ; Parsed from org-links
+;;  :ref-from ("id3")         ; Back-references
+;;  :ref-count 3              ; Total reference count
+;;  :created-at time)         ; Creation timestamp
+;;
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;; Main Features:
 ;; - Entity Management: Create, update, delete, query entities
 ;; - Relationship Management: Create, delete, query relationships  
 ;; - Data Persistence: Save, load, backup data
 ;; - Cache System: Improve query performance
+<<<<<<< HEAD
+=======
+;; - Data Source Unification: Eliminate redundant data collection
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;
 ;; Usage:
 ;; 1. Entity Operations
@@ -32,20 +68,41 @@
 ;;    - Save: (org-supertag-db-save)
 ;;    - Load: (org-supertag-db-load)
 ;;
+<<<<<<< HEAD
 ;; 5. Event System
 ;;    - Add Listener: (org-supertag-db-add-listener event-type function)
 ;;    - Trigger Event: (org-supertag-db-trigger-event event-type)
 ;;
 ;; 6. Cache System
+=======
+;; 5. Data Source Validation
+;;    - Check Unification: (org-supertag-db-check-data-sources)
+;;    - Check Duplicates: (org-supertag-db-check-duplicates)
+;;    - Fix Issues: (org-supertag-db-fix-duplicates)
+;;
+;; 6. Event System
+;;    - Add Listener: (org-supertag-db-add-listener event-type function)
+;;    - Trigger Event: (org-supertag-db-trigger-event event-type)
+;;
+;; 7. Cache System
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;    - Set Cache: (org-supertag-db-cache-set key value)
 ;;    - Get Cache: (org-supertag-db-cache-get key)
 ;;    - Remove Cache: (org-supertag-db-cache-remove key)
 ;;
+<<<<<<< HEAD
 ;; 7. Data Persistence
 ;;    - Save: (org-supertag-db-save)
 ;;    - Load: (org-supertag-db-load)
 ;;
 ;; 8. Backup
+=======
+;; 8. Data Persistence
+;;    - Save: (org-supertag-db-save)
+;;    - Load: (org-supertag-db-load)
+;;
+;; 9. Backup
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;    - Backup: (org-supertag-db-backup)
 ;;    - Restore: (org-supertag-db-restore)
 
@@ -57,19 +114,7 @@
 (require 'ht)
 (require 'cl-lib)
 (require 'org-element)
-(require 'f)
-
-(defun org-supertag-db--delete-duplicates (list)
-  "Return a new list with duplicate elements removed from LIST.
-This is a simple implementation to avoid compatibility issues with
-`delete-dups` or `cl-delete-duplicates` across Emacs versions."
-  (let ((result '())
-        (seen (make-hash-table :test 'equal)))
-    (dolist (item list)
-      (unless (gethash item seen)
-        (puthash item t seen)
-        (push item result)))
-    (nreverse result)))
+
 
 ;;------------------------------------------------------------------------------
 ;; Type System
@@ -112,11 +157,15 @@ This is a simple implementation to avoid compatibility issues with
                 :pos         ; Node position
                 :olp         ; Outline path (ancestor titles)
                 :level       ; Level (0 for file level)
-                )
+                )         
+<<<<<<< HEAD
      :optional (;; Hash Information
                 :hash        ; Node property hash
                 :content-hash ; Content hash
                 ;; Task Information
+=======
+     :optional (;; Task Information
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
                 :scheduled   ; Scheduled time
                 :deadline    ; Deadline time
                 :todo       ; Todo state
@@ -129,11 +178,15 @@ This is a simple implementation to avoid compatibility issues with
                 :ref-count  ; Reference count
                 ;; Event Information
                 :created-at  ; Creation time
+<<<<<<< HEAD
                 :modified-at ; Modification time
                 ;; Property Information
                 :properties  ; Property drawer contents
                 :raw-value  ; Raw headline value
                 ))
+=======
+                :modified-at)) ; Modification time
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
     
     (:type :tag
      :required (;; Basic Information
@@ -150,12 +203,17 @@ This is a simple implementation to avoid compatibility issues with
                 :description ; Description of tag purpose
                 :icon       ; Icon for visual identification
                 :color      ; Color scheme (background and foreground)
+<<<<<<< HEAD
                 :modified-at)) ; Modification time
-    
-    (:type :metadata
-     :required (:type :data)
-     :optional ()))
-  "Entity structure definitions.")
+=======
+                :modified-at ; Modification time
+                ;; Governance Information
+                :tag-status  ; Tag governance status
+                :tag-rules   ; Tag governance rules
+                :tag-history ; Tag governance history records
+                )) ; Modification time
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+  "Entity structure definitions."))
 
 ;; Link type definition
 (defconst org-supertag-db-link-type
@@ -189,8 +247,18 @@ This is a simple implementation to avoid compatibility issues with
      :optional (:relation-type          ; Relation type (:cooccurrence, :parent-child, :causal, etc.)
                 :strength               ; Strength (0.0-1.0)
                 :created-at             ; Creation time
+<<<<<<< HEAD
                 :updated-at)))           ; Update time
   "Entity link structure definitions.")
+=======
+                :updated-at             ; Update time
+                ;; Governance Information
+                :tag-rel-type           ; Tag relation governance type
+                :tag-rel-rules          ; Tag relation governance rules
+                :tag-rel-history        ; Tag relation governance history records
+                ))
+  "Entity link structure definitions."))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;; Behavior System Definitions
 (defconst org-supertag-behavior-timing
@@ -266,6 +334,7 @@ PROPS is the property list
 
 Returns:
 - t if valid
+<<<<<<< HEAD
 - Signals error if invalid"
   (let* ((struct (cl-find type org-supertag-db-object-structure
                          :key (lambda (x) (plist-get x :type))))
@@ -284,6 +353,25 @@ Returns:
             (org-supertag-db--validate-fields fields))))
      ;; Default: valid
      (t t))))
+=======
+- nil if invalid"
+  (let* ((struct (cl-find type org-supertag-db-object-structure
+                         :key (lambda (x) (plist-get x :type))))
+         (required (plist-get struct :required)))
+    (and
+     ;; Check required properties exist
+     (if required
+         (cl-every (lambda (key) (plist-member props key))
+                  required)
+       t)
+     ;; Special validation for tag type
+     (pcase type
+       (:tag
+        (let ((fields (plist-get props :fields)))
+          (or (null fields)  ; Fields can be empty
+              (org-supertag-db--validate-fields fields))))
+       (_ t)))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-db--validate-fields (fields)
   "Validate field definition list.
@@ -306,12 +394,8 @@ Returns:
       ;; Check if type is supported
       (let ((type (plist-get field :type)))
         (unless (alist-get type org-supertag-field-types)
-          (error "Unsupported field type: %s. This may be a deprecated field type. Supported types are: %s. Suggestion: Change the field type to 'string' fo4r plain text content"
-                 type
-                 (mapconcat (lambda (type-pair) (symbol-name (car type-pair)))
-                           org-supertag-field-types
-                           ", ")))))
-  t))
+          (error "Unsupported field type: %s" type)))))
+  t)
 
 ;; Link Type Validation
 (defun org-supertag-db-valid-link-type-p (type)
@@ -321,8 +405,7 @@ Valid types are :node-tag, :node-field, :tag-ref, and :tag-tag.
 Returns:
 - t if valid
 - nil if invalid"
-  (or (memq type '(:node-tag :node-field :tag-ref :tag-tag))
-      (and (stringp type) (> (length type) 0))))
+  (memq type '(:node-tag :node-field :tag-ref :tag-tag)))
 
 (defun org-supertag-db-valid-link-p (type from to props)
   "Validate link data.
@@ -416,39 +499,6 @@ Returns:
          (symbolp type)
          (alist-get type org-supertag-field-types))))
 
-(defun org-supertag-db--canonicalize-props (data)
-  "Recursively canonicalize any Lisp DATA into a standard plist format.
-This function is the single gatekeeper for data quality before writing to the DB.
-It converts alists to plists and cleans up keys and values."
-  (cond
-   ;; Atoms are returned as-is.
-   ((or (not (consp data)) (keywordp data)) data)
-
-   ;; An alist (list of cons pairs) is converted to a plist.
-   ((and (listp data) (consp (car data)) (not (keywordp (caar data))))
-    (let (plist)
-      (dolist (pair data)
-        (let* ((key (car pair))
-               (val (cdr pair))
-               (clean-val (org-supertag-db--canonicalize-props val)))
-          ;; Replace placeholder values "???" or :??? with nil
-          (when (or (and (stringp clean-val) (string= clean-val "???"))
-                    (and (symbolp clean-val) (string= (symbol-name clean-val) "???")))
-            (setq clean-val nil))
-
-          ;; Push KEY first so that after `nreverse` we retain key-value order.
-          (push (intern (concat ":" (if (symbolp key) (symbol-name key) key))) plist)
-          (push clean-val plist)))
-      (nreverse plist)))
-
-   ;; A regular list (or a plist already) is processed element by element.
-   ((listp data)
-    (mapcar #'org-supertag-db--canonicalize-props data))
-
-   ;; A dotted pair or other cons cell.
-   (t (cons (org-supertag-db--canonicalize-props (car data))
-            (org-supertag-db--canonicalize-props (cdr data))))))
-
 
 ;;------------------------------------------------------------------------------
 ;; Core Data Tables
@@ -473,27 +523,46 @@ It converts alists to plists and cleans up keys and values."
 ;;---------------------------------------------------------------------------------
 
 (defun org-supertag-db-add (id props)
+<<<<<<< HEAD
   "Add or update entity.
 ID is entity unique identifier
-PROPS is entity properties. It will be canonicalized before being saved.
+PROPS is entity properties
 
 Returns:
 - Success: entity ID
 - Error: throws error"
+=======
+  "Add an object with ID and PROPS to the database.
+If an object with the same ID already exists, it will be updated.
+This function now automatically adds or updates an 'updated-at' timestamp."
+  (org-supertag-db--add-object 
+   id 
+   (plist-put props :updated-at (format-time-string "%Y-%m-%dT%H:%M:%SZ" (current-time) t))))
+
+(defun org-supertag-db--add-object (id props)
+  "Private function to add object to database."
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   (condition-case err
-      (let* ((old-props (org-supertag-db-get id))
+      (let* ((type (plist-get props :type))
+             (old-props (org-supertag-db-get id))
              (is-update (not (null old-props)))
-             ;; The wrapped `normalize` function now performs full canonicalization.
+             ;; Normalize properties
              (clean-props (org-supertag-db--normalize-props props))
-             (type (plist-get clean-props :type))
              (new-props (if is-update
-                            ;; For updates, merge new props over old, preserving created-at.
-                            (append clean-props
-                                    (list :created-at (or (plist-get old-props :created-at) (current-time))
-                                          :modified-at (current-time)))
-                          ;; For new entries, add created-at.
-                          (append clean-props
-                                  (list :created-at (current-time))))))
+                           ;; Update: new props take precedence, but preserve content
+                           (org-supertag-db--normalize-props
+                            (append
+                             clean-props
+                             (when (and (not (plist-get clean-props :content))
+                                      (plist-get old-props :content))
+                               (list :content (plist-get old-props :content)))
+                             (list :created-at (plist-get old-props :created-at)
+                                   :modified-at (current-time))))
+                         ;; New creation
+                         (org-supertag-db--normalize-props
+                          (append
+                           clean-props
+                           (list :created-at (current-time)))))))
         ;; 1. Validation
         ;; 1.1 Validate type
         (unless (org-supertag-db-valid-object-type-p type)
@@ -501,12 +570,14 @@ Returns:
         ;; 1.2 Validate properties
         (unless (org-supertag-db-valid-object-p type new-props)
           (error "Invalid object properties"))
-
-        ;; 2. Pre-storage processing on type change
-        (when (and is-update (not (eq (plist-get old-props :type) type)))
-          (org-supertag-db--cache-clear-for-type (plist-get old-props :type) id))
-
-        ;; 3. Store the canonical entity
+        ;; 2. Pre-storage processing
+        (when is-update
+          ;; 2.1 Handle type changes
+          (let ((old-type (plist-get old-props :type)))
+            (unless (eq old-type type)
+              ;; Clear all related caches on type change
+              (org-supertag-db--cache-clear-for-type old-type id))))
+        ;; 3. Store entity
         (ht-set! org-supertag-db--object id new-props)
         ;; 4. Cache management
         ;; 4.1 Clear entity cache
@@ -557,6 +628,7 @@ Returns:
 ;; Data Operation: Link
 ;;---------------------------------------------------------------------------------
 
+<<<<<<< HEAD
 (defun org-supertag-db-link (type from to &optional props)
   "Create or update a link between entities.
 TYPE: Link type (:node-tag, :node-field, :tag-ref)
@@ -567,10 +639,10 @@ PROPS: Optional link properties
 Returns:
 - Link ID if successful
 - Signals error if validation fails"
-  (let* ((base-props (list :type type :from from :to to))
-         (full-props (if props
-                         (append props base-props)
-                       base-props))
+  (let* ((base-props (list :from from :to to))
+         (full-props (if props 
+                        (append base-props props)
+                      base-props))
          (rel-id (format "%s:%s->%s" type from to)))
     ;; 1. Validate
     ;; 1.1 Validate link type
@@ -613,6 +685,38 @@ Returns:
         (org-supertag-db--schedule-save)
         ;; 9. Return link ID
         rel-id))))
+=======
+(defun org-supertag-db-link (type from to &rest props)
+  "Create a link between two objects.
+TYPE is the link type symbol.
+FROM is the source object ID.
+TO is the target object ID.
+PROPS is a plist of additional link properties."
+  (let* ((link-id (format ":%s:%s->%s" type from to))
+         (base-props (list :type type :from from :to to))
+         ;; Intelligently handle props. If it's a list containing a plist, flatten it.
+         (flat-props (if (and (consp props) (plistp (car props)))
+                         (car props)
+                       props))
+         (all-props (append base-props flat-props)))
+    (puthash link-id all-props org-supertag-db--link)
+    (org-supertag-db--mark-dirty)
+    (org-supertag-db--schedule-save)
+    (org-supertag-db-emit 'link:created type from to all-props)
+    link-id))
+
+(defun org-supertag-db-remove-link (type from to)
+  "Remove a link between two objects.
+TYPE is the link type symbol.
+FROM is the source object ID.
+TO is the target object ID."
+  (let ((link-id (format ":%s:%s->%s" type from to)))
+    (when (gethash link-id org-supertag-db--link)
+      (remhash link-id org-supertag-db--link)
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db--schedule-save)
+      (org-supertag-db-emit 'link:removed type from to))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-db-unlink (type from to &optional dry-run)
   "Remove a link between entities.
@@ -715,13 +819,6 @@ Returns:
 ;;     ├── org-supertag-db-get-prop (Property Access)
 ;;     │       └── org-supertag-db-get-type (Type Access Helper)
 
-(defun org-supertag-db-get-pos (node-id)
-  "Get buffer position for node with NODE-ID.
-Returns position number or marker if found, nil otherwise."
-  (condition-case nil
-      (org-id-find node-id t)
-    (error nil)))
-
 (defun org-supertag-db-get-node-tags (node-id)
   "Get all tags for a node.
 NODE-ID is the node identifier."
@@ -894,16 +991,29 @@ Returns a list of node IDs."
     nodes))
 
 (defun org-supertag-get-all-files ()
-  "Get all unique file paths from database."
-  (let ((files '()))
-    (maphash (lambda (id props)
-               (when-let* ((file-path (plist-get props :file-path)))
-                 (when (and (eq (plist-get props :type) :node)
-                            file-path)
-                   (push file-path files))))
-             org-supertag-db--object)
-    (org-supertag-db--delete-duplicates (nreverse files))))
-
+  "Get list of all org files in database.
+Returns:
+- List of absolute file paths
+- nil if no files found
+
+Notes:
+1. Only returns files associated with nodes
+2. Removes duplicates
+3. Ensures paths exist"
+  (let ((files nil))
+    (maphash
+     (lambda (id props)
+       (when (and (eq (plist-get props :type) :node)
+                 (plist-get props :file-path))
+         (let ((file-path (plist-get props :file-path)))
+           (when (file-exists-p file-path)
+             (push file-path files)))))
+     org-supertag-db--object)
+    (delete-dups (nreverse files))))
+<<<<<<< HEAD
+
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;---------------------------------------------------------------------------------
 ;; Data Operation: Find
 ;;---------------------------------------------------------------------------------
@@ -976,6 +1086,7 @@ Returns:
       (mapcar #'car 
               (org-supertag-db-find base-pred)))))
 
+<<<<<<< HEAD
 (defun org-supertag-db-find-nodes-by-tag (tag)
   "Find all nodes that have TAG.
 TAG can be with or without # prefix.
@@ -992,6 +1103,20 @@ Returns a list of node IDs that have the specified tag."
                                         t))
                                     (plist-get node :tags))))
              collect id)))
+=======
+(defun org-supertag-db-find-nodes-by-tag (tag-id)
+  "Find nodes that use the specified tag.
+TAG-ID is the tag ID to find
+
+Returns:
+- List of node IDs that use the tag
+- nil if no matches found"
+  (let ((links (org-supertag-db-find-links :node-tag nil tag-id)))
+    (message "Found links: %S" links)  ; Debug info
+    (mapcar (lambda (link)
+              (plist-get link :from))
+            links)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-db-find-nodes-by-field-value (field-name value &optional tag-id)
   "Find nodes with the specified field value.
@@ -1022,6 +1147,7 @@ Returns:
      (and (eq (plist-get v :type) :tag)
           (assoc field-name (plist-get v :fields))))))
 
+<<<<<<< HEAD
 (defun org-supertag-db-find-links (type from to)
   "Find links matching the specified criteria.
 TYPE is the link type to find
@@ -1031,6 +1157,13 @@ TO is the optional target node ID
 Returns:
 - List of matching links
 - nil if no matches found"
+=======
+(defun org-supertag-db-find-links (type &key from to)
+  "Find links matching criteria.
+TYPE is the link type symbol.
+FROM is the optional source object ID.
+TO is the optional target object ID."
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   (let (results)
     (ht-map (lambda (k v)
               (when (and (or (null type) (eq (plist-get v :type) type))
@@ -1059,87 +1192,88 @@ Returns nil if entity does not exist"
         ;; 1. Get entity and check existence
         (let ((entity (org-supertag-db-get id)))
           (if (null entity)
-              (progn
-                ;; Even if the entity itself is missing, there may still be links referencing it.
-                (let ((removed-links-count 0))
-                  (ht-map (lambda (k v)
-                            (when (and (plist-get v :from)
-                                       (plist-get v :to)
-                                       (or (equal (plist-get v :from) id)
-                                           (equal (plist-get v :to) id)))
-                              ;; Remove link and caches
-                              (ht-remove! org-supertag-db--link k)
-                              (org-supertag-db--cache-remove 'link k)
-                              (pcase (plist-get v :type)
-                                (:node-field (org-supertag-db--cache-remove 'query (format "node-fields:%s" (plist-get v :from))))
-                                (:node-tag  (org-supertag-db--cache-remove 'query (format "node-tags:%s" (plist-get v :from))))
-                                (:tag-ref   (progn
-                                              (org-supertag-db--cache-remove 'query (format "tag-refs:%s" (plist-get v :from)))
-                                              (org-supertag-db--cache-remove 'query (format "tag-refs:%s" (plist-get v :to))))))
-                              (cl-incf removed-links-count)))
-                          org-supertag-db--link)
-                  (when (> removed-links-count 0)
-                    (message "Removed %d orphan links for non-existent entity %s" removed-links-count id)
+              (progn 
+                (message "Entity %s not found, nothing to remove" id)
+                nil) 
+            ;; 2. Entity exists, get type
+            (let ((type (plist-get entity :type)))
+              (if (null type)
+                  (message "Entity %s has no type, cannot remove properly" id)                
+                ;; 3. Get related links (use empty list as default)
+                (let ((outgoing-links (or (org-supertag-db-get-link nil id) '()))
+                      (incoming-links (or (org-supertag-db-get-link-reverse nil id) '()))
+                      (removed-data (list :entity entity)))
+                  
+                  ;; 4. Trigger events
+                  (org-supertag-db-emit 'entity:before-remove type id entity)
+                  
+                  ;; 5. Execute deletion (unless dry-run)
+                  (unless dry-run
+                    ;; 5.1 Delete all outgoing links
+                    (let ((link-count (org-supertag-db-unlink-all id)))
+                      (message "Removed %d outgoing links for entity %s" link-count id))
+                    
+                    ;; 5.2 Delete all incoming links
+                    (dolist (rev-link incoming-links)
+                      (when (car rev-link) ;; Ensure source exists
+                        (org-supertag-db-unlink nil (car rev-link) id)))
+                    
+                    ;; 5.3 Execute specific cleanup based on type
+                    (pcase type
+                      (:node
+                       (org-supertag-db--cache-remove 'query (format "node-tags:%s" id))
+                       (org-supertag-db--cache-remove 'query (format "node-fields:%s" id))
+                       (org-supertag-db--cache-remove 'query (format "node-refs:%s" id)))
+                      (:tag
+                       (org-supertag-db--cache-remove 'query (format "tag-fields:%s" id))
+                       (org-supertag-db--cache-remove 'query (format "tag-refs:%s" id))))
+                    
+                    ;; 5.4 Remove entity from database
+                    (message "Removing entity %s of type %s from database" id type)
+                    (ht-remove! org-supertag-db--object id)
+                    
+                    ;; 5.5 Clean up cache
+                    (org-supertag-db--cache-remove 'entity id)
+                    (org-supertag-db--cache-remove 'query (format "type:%s" type))
+                    
+<<<<<<< HEAD
+                    ;; 5.6 Trigger deletion completion event
+                    (org-supertag-db-emit 'entity:removed type id entity removed-data)
+                    
+                    ;; 5.7 Mark database as dirty and execute save
+=======
+                    ;; 5.6 **NEW: Update sync hash database to prevent sync issues**
+                    (when (featurep 'org-supertag-background-sync)
+                      (condition-case hash-err
+                          (progn
+                            ;; Remove hash record for deleted entity
+                            (when (boundp 'org-supertag-background-sync--last-sync-hashes)
+                              (remhash id org-supertag-background-sync--last-sync-hashes))
+                            ;; Save updated hashes to file
+                            (when (fboundp 'org-supertag-background-sync--save-hashes)
+                              (org-supertag-background-sync--save-hashes))
+                            (message "Updated sync hashes after removing entity %s" id))
+                        (error
+                         (message "Warning: Failed to update sync hashes for %s: %s" 
+                                  id (error-message-string hash-err)))))
+                    
+                    ;; 5.7 Trigger deletion completion event
+                    (org-supertag-db-emit 'entity:removed type id entity removed-data)
+                    
+                    ;; 5.8 Mark database as dirty and execute save
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+                    (message "Marking database as dirty")
                     (org-supertag-db--mark-dirty)
+                    (message "Scheduling database save")
                     (org-supertag-db--schedule-save))
-                  ;; Return cons with nil entity and maybe removed links count for consistency
-                  (cons nil removed-links-count)))
-               ;; 2. Entity exists, get type
-               (let ((type (plist-get entity :type)))
-                 (if (null type)
-                     (message "Entity %s has no type, cannot remove properly" id)                
-                   ;; 3. Get related links (use empty list as default)
-                   (let ((outgoing-links (or (org-supertag-db-get-link nil id) '()))
-                         (incoming-links (or (org-supertag-db-get-link-reverse nil id) '()))
-                         (removed-data (list :entity entity)))
-                     
-                     ;; 4. Trigger events
-                     (org-supertag-db-emit 'entity:before-remove type id entity)
-                     
-                     ;; 5. Execute deletion (unless dry-run)
-                     (unless dry-run
-                       ;; 5.1 Delete all outgoing links
-                       (let ((link-count (org-supertag-db-unlink-all id)))
-                         (message "Removed %d outgoing links for entity %s" link-count id))
-                       
-                       ;; 5.2 Delete all incoming links
-                       (dolist (rev-link incoming-links)
-                         (when (car rev-link) ;; Ensure source exists
-                           (org-supertag-db-unlink nil (car rev-link) id)))
-                       
-                       ;; 5.3 Execute specific cleanup based on type
-                       (pcase type
-                         (:node
-                          (org-supertag-db--cache-remove 'query (format "node-tags:%s" id))
-                          (org-supertag-db--cache-remove 'query (format "node-fields:%s" id))
-                          (org-supertag-db--cache-remove 'query (format "node-refs:%s" id)))
-                         (:tag
-                          (org-supertag-db--cache-remove 'query (format "tag-fields:%s" id))
-                          (org-supertag-db--cache-remove 'query (format "tag-refs:%s" id))))
-                       
-                       ;; 5.4 Remove entity from database
-                       (message "Removing entity %s of type %s from database" id type)
-                       (ht-remove! org-supertag-db--object id)
-                       
-                       ;; 5.5 Clean up cache
-                       (org-supertag-db--cache-remove 'entity id)
-                       (org-supertag-db--cache-remove 'query (format "type:%s" type))
-                       
-                       ;; 5.6 Trigger deletion completion event
-                       (org-supertag-db-emit 'entity:removed type id entity removed-data)
-                       
-                       ;; 5.7 Mark database as dirty and execute save
-                       (message "Marking database as dirty")
-                       (org-supertag-db--mark-dirty)
-                       (message "Scheduling database save")
-                       (org-supertag-db--schedule-save))
-                     
-                     ;; 6. Return deleted data
-                     (cons entity (append outgoing-links incoming-links))))))))
+                  
+                  ;; 6. Return deleted data
+                  (cons entity (append outgoing-links incoming-links))))))))
     
     ;; Error handling
     (error
      (message "Error removing entity %s: %s" id (error-message-string err))
+<<<<<<< HEAD
      nil))) ;; Return nil instead of signaling an error
 
 (defun org-supertag-db-remove-link (type from to &optional dry-run)
@@ -1198,6 +1332,9 @@ Returns:
      (message "Error removing link %s:%s->%s: %s"
               type from to (error-message-string err))
               (signal (car err) (cdr err)))))
+=======
+     nil)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;;------------------------------------------------------------------------------  
 ;; Property Operation
@@ -1229,107 +1366,38 @@ Returns property value if found, nil otherwise."
 
 (require 'org-element)
 
-(defun org-supertag-db-parse-node-properties ()
-  "Parse node properties at current point.
-Returns a plist of properties including :id, :title, :tags, etc.
-This version now also extracts inline tags (e.g., #tag) from the headline
-and parses node content for id-links."
-      (let* ((element (org-element-at-point))
-             (type (org-element-type element)))
-        (when (eq type 'headline)
-          (let* (;; Basic properties from org-element
-                 (id (org-element-property :ID element))
-                 (raw-value (org-element-property :raw-value element))
-                 (title (org-element-property :title element))
-                 (tags (org-element-property :tags element))
-                 (level (org-element-property :level element))
-                 (todo-type (org-element-property :todo-type element))
-                 (priority (org-element-property :priority element))
-                 (scheduled (org-element-property :scheduled element))
-                 (deadline (org-element-property :deadline element))
-                 (properties (org-element-property :properties element))
-                 (file-path (buffer-file-name))
-                 (pos (point))
-                 (olp (org-get-outline-path))
+(defun org-supertag-db--normalize-props (props)
+  "Normalize property list to ensure correct order and no duplicates.
+PROPS is the property list
+
+Returns:
+- Normalized property list"
+  (let ((result nil)
+        (seen-keys nil))
+    ;; 1. Ensure we have a type
+    (let ((type-value (or (plist-get props :type)
+                         ;; If no type but has file-path, assume it's a node
+                         (when (plist-get props :file-path) :node))))
+      (when type-value
+        (push :type seen-keys)
+        (setq result (list :type type-value))))
     
-                 ;; Reference and content parsing
-                 (refs-to '())
-                 (old-node (and id (org-supertag-db-get id)))
-                 (ref-from (if old-node (plist-get old-node :ref-from) '()))
-                 (content
-                  (save-excursion
-                    (org-end-of-meta-data t)
-                    (let ((beg (point))
-                          (end (save-excursion
-                                 (if (re-search-forward org-heading-regexp nil t)
-                                     (match-beginning 0)
-                                   (point-max)))))
-                      (when (> end beg)
-                        ;; Parse refs-to from content
-                        (save-excursion
-                          (goto-char beg)
-                          (while (re-search-forward org-link-any-re end t)
-                            (let* ((link-element (org-element-context))
-                                   (link-type (org-element-property :type link-element))
-                                   (path (org-element-property :path link-element)))
-                              (when (and path (equal link-type "id") (org-uuidgen-p path))
-                                (push path refs-to)))))
-                        ;; Extract content string
-                        (string-trim (buffer-substring-no-properties beg end))))))
-                 
-                 ;; Inline tag parsing
-                 (inline-tags '())
-                 (content-inline-tags '()))
+    ;; 2. Process other properties
+    (let ((rest-props props))
+      (while rest-props
+        (let ((key (car rest-props))
+              (value (cadr rest-props)))
+          (unless (memq key seen-keys)  ; Avoid duplicates
+            (push key seen-keys)
+            (setq result (append result (list key value)))))
+        (setq rest-props (cddr rest-props))))
     
-            ;; Extract inline tags (e.g., #some-tag) from the raw headline value.
-            (when raw-value
-              (let ((case-fold-search nil))
-                (save-match-data
-                  (let ((start 0))
-                    (while (string-match "#\\(\\w[-_[:alnum:]]*\\)" raw-value start)
-                      (push (match-string 1 raw-value) inline-tags)
-                      (setq start (match-end 0)))))))
-            
-            ;; Extract inline tags from the content body.
-            (when content
-              (let ((case-fold-search nil))
-                (save-match-data
-                  (let ((start 0))
-                    (while (string-match "#\\(\\w[-_[:alnum:]]*\\)" content start)
-                      (push (match-string 1 content) content-inline-tags)
-                      (setq start (match-end 0)))))))
+    ;; 3. Validate result
+    (let ((final-type (plist-get result :type)))
+      (unless (memq final-type '(:node :tag))
+        (error "Invalid or missing type in props: %S" props)))
     
-            (list
-             :type :node
-             :id id
-             :title title
-             :raw-value raw-value
-             :tags (org-supertag-db--delete-duplicates 
-                    (append tags (nreverse inline-tags) (nreverse content-inline-tags)))
-             :level level
-             :todo-type todo-type
-             :priority priority
-             :scheduled scheduled
-             :deadline deadline
-             :properties properties
-             :file-path file-path
-             :pos pos
-             :olp olp
-             :content content
-             :ref-to (org-supertag-db--delete-duplicates refs-to)
-             :ref-from ref-from
-             :ref-count (length ref-from)
-             :created-at (current-time)
-             :modified-at (current-time))))))
-
-(defun org-supertag-db-get-node-by-id (id)
-  "Get a node from the database by its ID."
-  (org-supertag-db-get id))
-
-(defun org-supertag-db--normalize-props (props)
-  "Normalize property list to ensure correct order and no duplicates.
-This function is now a wrapper around the canonicalization function."
-  (org-supertag-db--canonicalize-props props))
+    result))
 
 (defun org-supertag-db--clean-text (text)
   "Clean text by removing all text properties and ensuring it's a plain string.
@@ -1339,11 +1407,12 @@ Returns a clean string without any text properties."
     (let ((str (if (stringp text)
                    text
                  (format "%s" text))))
+<<<<<<< HEAD
       (substring-no-properties (string-trim str)))))
 
 (defun org-supertag-db--get-node-title ()
   "Get current node title in plain text format."
-  (condition-case err-info
+  (condition-case err
       (org-supertag-db--clean-text
        (or (org-get-heading t t t t)  ; Remove tags, TODO states etc
            (save-excursion
@@ -1356,14 +1425,104 @@ Returns a clean string without any text properties."
                 "" raw)))
            ""))
     (error
-     (message "Error getting node title: %s" (error-message-string err-info))
+     (message "Error getting node title: %s" (error-message-string err))
      "")))
 
-
 (defun org-supertag-db--parse-node-at-point ()
-  "Parse node data at current point.
-This is now a simple wrapper around the authoritative public parser."
-  (org-supertag-db-parse-node-properties))
+  "Parse node data at current point."
+  (save-excursion
+    (condition-case err
+        (let* ((element (org-element-at-point))
+               (type (org-element-type element)))
+          (message "Parsing node at point: type=%s" type)
+          (when (and (eq type 'headline)
+                     ;; Only process nodes with existing ID
+                     (org-entry-get nil "ID"))
+            ;; Parse headline
+            (let* ((title (org-supertag-db--get-node-title))
+                   (level (org-element-property :level element))
+                   (id (org-entry-get nil "ID"))
+                   ;; Task properties - Use more reliable retrieval methods
+                   (todo-state (org-entry-get nil "TODO"))
+                   (priority (org-entry-get nil "PRIORITY"))
+                   (scheduled (condition-case nil
+                                (org-get-scheduled-time (point))
+                              (error nil)))
+                   (deadline (condition-case nil
+                               (org-get-deadline-time (point))
+                             (error nil)))
+                   ;; Get outline path
+                   (olp (org-get-outline-path t))
+                   ;; Other properties
+                   (tags (org-get-tags))
+                   (properties (org-entry-properties nil 'standard))
+                   ;; parse reference relations
+                   (refs-to nil)
+                   (refs-from nil)
+                   ;; Get node content directly
+                   (content (save-excursion
+                             (org-end-of-meta-data t)
+                             (let* ((begin (point))
+                                    (end (org-element-property :contents-end element))
+                                    ;; Find next heading position
+                                    (next-heading (save-excursion
+                                                  (org-next-visible-heading 1)
+                                                  (point))))
+                               ;; go to begin
+                               (goto-char begin)
+                               ;; use more accurate boundary
+                               (let ((content-end (cond
+                                                 ;; if there is a next heading and in the current node content range
+                                                 ((and next-heading end (< next-heading end))
+                                                  (1- next-heading))
+                                                 ;; use the content end position of the node
+                                                 (end end)
+                                                 ;; if none, use the end position of the current node
+                                                 (t (org-entry-end-position)))))
+                                 ;; collect references
+                                 (while (re-search-forward org-link-any-re content-end t)
+                                   (let* ((link (org-element-context))
+                                          (link-type (org-element-property :type link))
+                                          (link-path (org-element-property :path link)))
+                                     (when (and (equal link-type "id")
+                                              (org-uuidgen-p link-path))
+                                       (push link-path refs-to))))
+                                 ;; only return content when begin and end are valid and have content
+                                 (when (and begin content-end (< begin content-end))
+                                   (org-supertag-db--clean-text
+                                    (buffer-substring-no-properties begin content-end)))))))
+                   ;; get other nodes that reference this node (keep existing reference relations)
+                   (existing-node (org-supertag-db-get id))
+                   (refs-from (when existing-node
+                              (plist-get existing-node :ref-from))))
+              
+              (message "Parsed headline: id=%s title=%s level=%s todo=%s priority=%s refs=%d" 
+                      id title level todo-state priority (length refs-to))
+              
+              (list :type :node
+                    :id id
+                    :title title
+                    :file-path (buffer-file-name)
+                    :pos (org-element-property :begin element)
+                    :olp olp
+                    :level level
+                    ;; Task properties
+                    :todo todo-state
+                    :priority priority
+                    :scheduled scheduled
+                    :deadline deadline
+                    ;; Other properties
+                    :properties properties
+                    :tags tags
+                    :content content
+                    ;; Reference relations
+                    :ref-to (delete-dups refs-to)
+                    :ref-from (or refs-from nil)
+                    :ref-count (+ (length refs-to) (length refs-from))
+                    :created-at (current-time)))))
+      (error
+       (message "Error parsing node at point: %s" (error-message-string err))
+       nil))))
 
 (defun org-supertag-db--validate-node-props (props)
   "Validate node property completeness."
@@ -1371,61 +1530,328 @@ This is now a simple wrapper around the authoritative public parser."
     (cl-every (lambda (key)
                 (when-let* ((value (plist-get props key)))
                   (not (string-empty-p (format "%s" value)))))
-                required)))  
-
+                required)))
+=======
+      ;; 强制创建新的字符串对象，避免重复引用
+      (copy-sequence (substring-no-properties (string-trim str))))))
+
+(defun org-supertag-db--remove-drawers-from-content (content)
+  "Remove all drawers from CONTENT string.
+Drawers are blocks between :NAME: and :END: lines.
+This includes PROPERTIES, LOGBOOK, and custom drawers."
+  (when content
+    (with-temp-buffer
+      (insert content)
+      (goto-char (point-min))
+      ;; Remove all drawers
+      (while (re-search-forward "^[ \t]*:\\([A-Za-z0-9_-]+\\):[ \t]*\n\\(?:.*\n\\)*?[ \t]*:END:[ \t]*$" nil t)
+        (replace-match ""))
+      ;; Clean up extra blank lines
+      (goto-char (point-min))
+      (while (re-search-forward "\n\n\n+" nil t)
+        (replace-match "\n\n"))
+      (string-trim (buffer-string)))))
+
+(defun org-supertag-db--test-drawer-removal ()
+  "Test the drawer removal function."
+  (interactive)
+  (let* ((test-content "This is the first line.
+:PROPERTIES:
+:ID: test-id-123
+:CREATED: [2024-01-01]
+:END:
+
+This is content after properties.
+
+:LOGBOOK:
+- State \"DONE\"       from \"TODO\"       [2024-01-02 10:00]
+- State \"TODO\"       from              [2024-01-01 09:00]
+:END:
+
+This is content after logbook.
+
+:CUSTOM_DRAWER:
+Some custom content here.
+Multiple lines.
+:END:
+
+This is the final content.")
+         (filtered (org-supertag-db--remove-drawers-from-content test-content)))
+    (message "=== Original Content ===\n%s\n\n=== Filtered Content ===\n%s" 
+             test-content filtered)
+    (with-current-buffer (get-buffer-create "*Drawer Filter Test*")
+      (erase-buffer)
+      (insert "=== Original Content ===\n" test-content "\n\n")
+      (insert "=== Filtered Content ===\n" filtered "\n")
+      (display-buffer (current-buffer)))))
+
+(defun org-supertag-db--get-unified-title (element)
+  "Get unified node title, prioritize org-element's raw-value.
+ELEMENT is the current org-element."
+  (org-supertag-db--clean-text
+   (or (org-element-property :raw-value element)
+       (org-get-heading t t t t)
+       "")))
+
+(defun org-supertag-db--filter-properties (properties file-path title)
+  "Filter property list, remove duplicate information with other fields.
+PROPERTIES is the original property list
+FILE-PATH is the file path
+TITLE is the node title"
+  (let ((filtered-props nil)
+        (exclude-keys '("FILE" "ITEM" "ALLTAGS" "TAGS"))) ; Exclude duplicate fields
+    (dolist (prop properties)
+      (let ((key (car prop))
+            (value (cdr prop)))
+        (unless (member key exclude-keys)
+          ;; Only keep non-duplicate properties
+          (push prop filtered-props))))
+    (nreverse filtered-props)))
 
+(defun org-supertag-db--parse-node-at-point ()
+  "Parse node data at current position - using unified data source strategy."
+  (save-excursion
+    (let* ((element (org-element-at-point))
+           (type (org-element-type element)))
+      (message "Parsing node at point: type=%s" type)
+      (when (and (eq type 'headline)
+                 ;; Only process nodes with existing ID
+                 (org-entry-get nil "ID"))
+        ;; Unified data source parsing
+        (let* (;; Basic information - single data source
+               (id (org-element-property :ID element))
+               (title (org-supertag-db--get-unified-title element))
+               (file-path (buffer-file-name))
+               (level (org-element-property :level element))
+               
+               ;; Position information - from element
+               (pos (org-element-property :begin element))
+               (olp (org-get-outline-path t))
+               
+               ;; Task properties - from org-entry (not in element)
+               (todo-state (org-entry-get nil "TODO"))
+               (priority (org-entry-get nil "PRIORITY"))
+               (scheduled (condition-case nil
+                            (org-get-scheduled-time (point))
+                          (error nil)))
+               (deadline (condition-case nil
+                           (org-get-deadline-time (point))
+                         (error nil)))
+               
+              ;; Tags - unified from org-get-tags
+               (tags (org-get-tags))
+
+               (raw-properties (org-entry-properties nil 'standard))
+               (properties (org-supertag-db--filter-properties 
+                           raw-properties file-path title))
+               
+               ;; Content and references
+               (refs-to nil)
+               (content (save-excursion
+                         (org-end-of-meta-data t)
+                         (let* ((begin (point))
+                                (end (org-element-property :contents-end element))
+                                (next-heading (save-excursion
+                                              (org-next-visible-heading 1)
+                                              (point))))
+                           (goto-char begin)
+                           (let ((content-end (cond
+                                             ((and next-heading end (< next-heading end))
+                                              (1- next-heading))
+                                             (end end)
+                                             (t (org-entry-end-position)))))
+                             ;; Collect references
+                             (while (re-search-forward org-link-any-re content-end t)
+                               (let* ((link (org-element-context))
+                                      (link-type (org-element-property :type link))
+                                      (link-path (org-element-property :path link)))
+                                 (when (and (equal link-type "id")
+                                          (org-uuidgen-p link-path))
+                                   (push link-path refs-to))))
+                             ;; Return content
+                             (when (and begin content-end (< begin content-end))
+                               (let ((raw-content (buffer-substring-no-properties begin content-end)))
+                                 (org-supertag-db--clean-text
+                                  (org-supertag-db--remove-drawers-from-content raw-content))))))))
+               
+               ;; Existing reference relationships (for backward compatibility)
+               (existing-node (org-supertag-db-get id))
+               (refs-from (when existing-node
+                          (plist-get existing-node :ref-from))))
+          
+          (message "Parsed headline: id=%s title=%s level=%s todo=%s priority=%s refs=%d" 
+                  id title level todo-state priority (length refs-to))
+          
+          ;; Return unified data structure
+          (list :type :node
+                :id id
+                :title title
+                :file-path file-path
+                :pos pos
+                :olp olp
+                :level level
+                ;; Task properties
+                :todo todo-state
+                :priority priority
+                :scheduled scheduled
+                :deadline deadline
+                ;; Filtered properties (no duplicate information)
+                :properties properties
+                :tags tags
+                :content content
+                ;; Reference relationships
+                :ref-to (delete-dups refs-to)
+                :ref-from (or refs-from nil)
+                :ref-count (+ (length refs-to) (length refs-from))
+                :created-at (current-time)))))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;;------------------------------------------------------------------------------
-;; Reference Reconciliation
+;; Batch Node Parsing
+;;------------------------------------------------------------------------------    
+
+<<<<<<< HEAD
+=======
+(defun org-supertag-db--collect-nodes-in-buffer ()
+  "Collect all heading nodes in current buffer.
+Returns a list of parsed nodes."
+  (let ((nodes nil))
+    ;; Only process file buffers
+    (when (buffer-file-name)
+      ;; Collect all heading nodes
+      (org-map-entries
+       (lambda ()
+         (let ((element (org-element-at-point)))
+           ;; Only process heading nodes
+           (when (and (eq (org-element-type element) 'headline)
+                      (org-at-heading-p))
+             (when-let* ((node (org-supertag-db--parse-node-at-point)))
+               (push node nodes)))))
+       t nil))
+    ;; Return collected nodes in original order
+    (nreverse nodes)))
+
+(defun org-supertag-db-update-buffer ()
+  "Update all nodes in current buffer.
+Returns number of nodes updated."
+  (let ((count 0)
+        (file-path (buffer-file-name)))
+    (when file-path
+      ;; Process all nodes
+      (dolist (node (org-supertag-db--collect-nodes-in-buffer))
+        ;; Add or update each node
+        (org-supertag-db-add (plist-get node :id) node)
+        (cl-incf count))
+      count)))
+
+(defun org-supertag-db-update-file (file)
+  "Update all nodes in specified file.
+FILE is the file path.
+Returns number of nodes updated."
+  (when (and file (file-exists-p file))
+    (with-current-buffer (find-file-noselect file)
+      (org-mode)
+      (org-supertag-db-update-buffer))))
+
+(defun org-supertag-db-update-directory (dir)
+  "Update all org files in directory.
+DIR is the directory path.
+Returns total number of nodes updated."
+  (let ((files (directory-files-recursively dir "\\.org$"))
+        (total 0))
+    (dolist (file files)
+      (cl-incf total (org-supertag-db-update-file file)))
+    (message "Updated %d nodes in %d files" total (length files))
+    total))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+(defun org-supertag-db-get-pos (node-id)
+  "Get buffer position for node with NODE-ID.
+Returns position number or marker if found, nil otherwise."
+  (condition-case nil
+      (org-id-find node-id t)
+    (error nil)))
+
 ;;------------------------------------------------------------------------------
+;; Node Reference Parsing
+;;------------------------------------------------------------------------------    
 
-(defun org-supertag-db-reconcile-references (node-id new-refs old-refs)
-  "Reconcile references for a node.
-NODE-ID: The ID of the node being updated.
-NEW-REFS: The new list of referenced node IDs.
-OLD-REFS: The old list of referenced node IDs."
-  (let ((old-refs (or old-refs '())))
-    ;; Add new references and trigger creation events
-    (dolist (ref-id new-refs)
-      (unless (member ref-id old-refs)
-        (org-supertag-db-emit 'ref:created node-id ref-id nil)
-        (when-let* ((ref-node (org-supertag-db-get ref-id)))
-          (let* ((new-ref-from (org-supertag-db--delete-duplicates (cons node-id (plist-get ref-node :ref-from))))
-                 (new-props (plist-put (plist-put ref-node :ref-from new-ref-from)
-                                       :ref-count (length new-ref-from))))
-            (org-supertag-db-add ref-id new-props)))))
-    
-    ;; Remove old references and trigger removal events
+(defun org-supertag-db--parse-node-all-ref ()
+  "Parse all references in current node.
+Returns list of referenced node IDs.
+References are parsed from org links in node content."
+  (let ((ref-to nil)
+        (node-id (org-supertag-db--get-current-node-id)))
+    (when node-id
+      ;; Parse links in entire node range
+      (let ((start (org-entry-beginning-position))
+            (end (org-entry-end-position)))
+        (save-excursion
+          (goto-char start)
+          (while (re-search-forward org-link-any-re end t)
+            (let* ((link (org-element-context))
+                   (type (org-element-property :type link))
+                   (path (org-element-property :path link)))
+              (when (and (equal type "id")
+                       (org-uuidgen-p path))
+                (push path ref-to)))))
+        ;; Update reference relationships
+        (let ((ref-to (delete-dups (nreverse ref-to))))
+          (org-supertag-db--update-node-all-ref node-id ref-to)
+          ref-to)))))
+
+(defun org-supertag-db--update-node-all-ref (node-id ref-to)
+  "Update all reference relationships for a node.
+NODE-ID is the current node ID
+REF-TO is a list of referenced node IDs"
+  (message "Starting to update node references - Node ID: %s" node-id)
+  (message "New reference list: %S" ref-to)
+  (let* ((node (org-supertag-db-get node-id))
+         (old-refs (plist-get node :ref-to)))
+    (message "Original reference list: %S" old-refs)
+    ;; Update current node's references
+    (org-supertag-db-add node-id
+                         (plist-put node :ref-to ref-to))
+    (message "Updated current node's references")
+    ;; Update back-references of referenced nodes and trigger new reference events
+    (dolist (ref-id ref-to)
+      (message "Processing referenced node: %s" ref-id)
+      (unless (member ref-id old-refs)  ; Only trigger events for new references
+        (message "Found new reference, triggering event")
+        (org-supertag-db-emit 'ref:created node-id ref-id nil))
+      (when-let* ((ref-node (org-supertag-db-get ref-id)))
+        (let* ((ref-from (plist-get ref-node :ref-from))
+               (new-ref-from (cons node-id (delete node-id ref-from))))
+          (message "Updating back-references of referenced node: %S" new-ref-from)
+          (org-supertag-db-add ref-id
+                              (plist-put ref-node 
+                                        :ref-from 
+                                        (delete-dups new-ref-from))))))
+    ;; Clean up old references and trigger removal events
     (dolist (old-ref old-refs)
-      (unless (member old-ref new-refs)
+      (unless (member old-ref ref-to)
+        (message "Cleaning up old reference: %s" old-ref)
         (org-supertag-db-emit 'ref:removed node-id old-ref nil)
         (when-let* ((ref-node (org-supertag-db-get old-ref)))
-          (let* ((ref-from (remove node-id (plist-get ref-node :ref-from) :test #'string=))
-                 (new-props (plist-put (plist-put ref-node :ref-from ref-from)
-                                       :ref-count (length ref-from))))
-            (org-supertag-db-add old-ref new-props)))))))
+          (let ((ref-from (delete node-id (plist-get ref-node :ref-from))))
+            (message "Updating back-references of previously referenced node: %S" ref-from)
+            (org-supertag-db-add old-ref
+                                (plist-put ref-node :ref-from ref-from))))))
+    (message "Node reference relationships update completed")))
+
+(defun org-supertag-db--get-current-node-id ()
+  "Get the ID of current node."
+  (save-excursion
+    (org-back-to-heading t)
+    (let ((id (or (org-id-get)
+                  (org-id-get-create))))
+      (message "Got current node ID: %s" id)
+      id)))
 
-;;------------------------------------------------------------------------------
-;; Field Relate System
-;;------------------------------------------------------------------------------      
-
-(defun org-supertag-db-get-links-by-type (type &key from to)
-  "Get all links of a specific type.
-TYPE is the link type to search for.
-FROM (optional) is the source object ID.
-TO (optional) is the target object ID."
-  (let ((results '()))
-    (maphash (lambda (id props)
-               (when (and (eq (plist-get props :type) type)
-                          (or (not from) (equal (plist-get props :from) from))
-                          (or (not to) (equal (plist-get props :to) to)))
-                  (push props results)))
-             org-supertag-db--link)
-    (nreverse results)))
 
 ;;------------------------------------------------------------------------------
 ;; Event System
-;;------------------------------------------------------------------------------
+;;------------------------------------------------------------------------------    
 
 ;; Event System Examples:
 ;; 1. Event Types
@@ -1457,7 +1883,7 @@ Maps event-name -> (handler1 handler2 ...)")
 EVENT: Event name, e.g. 'entity:changed
 HANDLER: Handler function that receives event-related parameters"
   (let ((handlers (ht-get org-supertag-db--events event)))
-    (ht-set! org-supertag-db--events event
+    (ht-set! org-supertag-db--events event 
              (cons handler handlers))))
 
 (defun org-supertag-db-off (event handler)
@@ -1475,10 +1901,47 @@ ARGS: Parameters passed to handlers"
   (dolist (handler (ht-get org-supertag-db--events event))
     (condition-case err
         (apply handler args)
-      (error
+      (error 
        (message "Event handler error [%s]: %S" event err)))))
 
 
+
+;;------------------------------------------------------------------------------
+;; Status
+;;------------------------------------------------------------------------------    
+
+(defun org-supertag-db-check-status ()
+  "Check database status."
+  (interactive)
+  (message "\n=== Database Status ===")
+  (message "Database file: %s" org-supertag-db-file)
+  (message "File exists: %s" (file-exists-p org-supertag-db-file))
+  (message "Current entities: %d" (ht-size org-supertag-db--object))
+  (message "Current links: %d" (ht-size org-supertag-db--link)))
+
+(defun org-supertag-db-test ()
+  "Test database operations."
+  (interactive)
+  (message "\n=== Testing Database Operations ===")
+  
+  ;; 1. Add a test entity
+  (org-supertag-db-add "test-tag" '(:type :tag :name "test"))
+  (message "Size after adding entity: %d" (ht-size org-supertag-db--object))
+  
+  ;; 2. Add a test link
+  (org-supertag-db-link :node-tag "test-node" "test-tag")
+  (message "Size after adding link: %d" (ht-size org-supertag-db--link))
+  
+  ;; 3. Save database
+  (org-supertag-db-save)
+  (message "Data saved")
+  
+  ;; 4. Reload
+  (org-supertag-db-load)
+  (message "After reload:")
+  (message "Entities: %d" (ht-size org-supertag-db--object))
+  (message "Links: %d" (ht-size org-supertag-db--link)))
+
 ;;-----------------------------------------------------------------------------
 ;; Cache System
 ;;-----------------------------------------------------------------------------
@@ -1527,7 +1990,6 @@ ID: Entity ID"
      (org-supertag-db--cache-remove 'query (format "tag-fields:%s" id))
      (org-supertag-db--cache-remove 'query (format "tag-refs:%s" id)))))
 
-
 ;;------------------------------------------------------------------------------
 ;; Dirty State
 ;;------------------------------------------------------------------------------    
@@ -1558,6 +2020,15 @@ Defaults to org-supertag subdirectory under Emacs config directory."
   :type 'directory
   :group 'org-supertag)
 
+<<<<<<< HEAD
+=======
+(defcustom org-supertag-db-use-print-circle t
+  "Whether to use print-circle when saving database.
+When t, uses circular structure detection to save space but creates #n= references.
+When nil, duplicates string data but makes saved file more readable."
+  :type 'boolean
+  :group 'org-supertag)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-db-ensure-data-directory ()
   "Ensure database and backup directories exist."
@@ -1650,7 +2121,11 @@ Returns t on success, nil on failure."
           (with-temp-file org-supertag-db-file
             (let ((print-level nil)
                   (print-length nil)
+<<<<<<< HEAD
                   (print-circle t))
+=======
+                  (print-circle org-supertag-db-use-print-circle))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
               (insert ";; -*- lexical-binding: t -*-\n")
               (insert ";;; Database file - Do not edit manually\n\n")
               (insert "(require 'ht)\n")
@@ -1794,11 +2269,8 @@ Steps:
   (unless (org-supertag-db-load)
     ;; Create empty database if load fails
     (setq org-supertag-db--object (ht-create)
-          org-supertag-db--link (ht-create))
-    (message "Created empty database, saving to disk...")
-    ;; Mark as dirty and save immediately to create the file
-    (org-supertag-db--mark-dirty)
-    (org-supertag-db-save))
+          org-supertag-db--link (ht-create))  
+    (message "Created empty database"))
   ;; Set up change listeners
   (org-supertag-db-on 'entity:changed
                     (lambda (type id props)
@@ -1809,7 +2281,11 @@ Steps:
                       (org-supertag-db--mark-dirty)
                       (org-supertag-db--schedule-save)))
   (org-supertag-db-on 'entity:removed
+<<<<<<< HEAD
                     (lambda (type id entity removed-data)
+=======
+                    (lambda (id entity)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
                       (org-supertag-db--mark-dirty)
                       (org-supertag-db--schedule-save)))
   (org-supertag-db-on 'link:created
@@ -1817,7 +2293,11 @@ Steps:
                       (org-supertag-db--mark-dirty)
                       (org-supertag-db--schedule-save)))
   (org-supertag-db-on 'link:removed
+<<<<<<< HEAD
                     (lambda (type from to props)
+=======
+                    (lambda (type from to)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
                       (org-supertag-db--mark-dirty)
                       (org-supertag-db--schedule-save)))
 
@@ -1836,51 +2316,841 @@ Steps:
 (add-hook 'org-supertag-db-before-load-hook #'org-supertag-db--cache-clear)
 
 
+
 ;;------------------------------------------------------------------------------
-;; Recompute all hashes
+;; Metadata Storage
+;;------------------------------------------------------------------------------  
+
+(defvar org-supertag-db--metadata (ht-create)
+  "Hash table for storing global metadata.
+Metadata is stored as key-value pairs and is used for configuration,
+statistics, and relationship data that doesn't fit the entity model.")
+
+(defun org-supertag-db-get-metadata (key &optional default)
+  "Get metadata value for KEY.
+If KEY does not exist, return DEFAULT or nil if DEFAULT is not provided.
+KEY should be a string or symbol identifying the metadata.
+Returns the stored value or DEFAULT if not found."
+  (ht-get org-supertag-db--metadata (if (symbolp key) key (intern key)) default))
+
+(defun org-supertag-db-set-metadata (key value)
+  "Set metadata KEY to VALUE.
+KEY should be a string or symbol identifying the metadata.
+VALUE can be any Lisp object.
+Returns VALUE if successful.
+
+This is used for storing configuration, statistics, and
+relationship data that doesn't fit into the entity model.
+Example uses:
+- Cooccurrence statistics
+- Tag frequency data
+- Usage history
+- Global settings"
+  (let ((k (if (symbolp key) key (intern key))))
+    ;; Store the value
+    (ht-set! org-supertag-db--metadata k value)
+    ;; Mark database as dirty and schedule save
+    (org-supertag-db--mark-dirty)
+    (org-supertag-db--schedule-save)
+    ;; Emit event for tracking
+    (org-supertag-db-emit 'metadata:changed k value)
+    ;; Return the value
+    value))
+
+(defun org-supertag-db-remove-metadata (key)
+  "Remove metadata entry with KEY.
+KEY should be a string or symbol identifying the metadata.
+Returns t if the key existed and was removed, nil otherwise."
+  (let ((k (if (symbolp key) key (intern key))))
+    (when (ht-contains-p org-supertag-db--metadata k)
+      ;; Remove the entry
+      (ht-remove! org-supertag-db--metadata k)
+      ;; Mark database as dirty and schedule save
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db--schedule-save)
+      ;; Emit event for tracking
+      (org-supertag-db-emit 'metadata:removed k)
+      t)))
+
+(defun org-supertag-db-list-metadata (&optional prefix)
+  "List all metadata keys, optionally filtered by PREFIX.
+If PREFIX is provided, only return keys that start with PREFIX.
+Returns an alist of (key . value) pairs."
+  (let ((results nil))
+    (ht-map (lambda (k v)
+              (when (or (null prefix)
+                        (string-prefix-p prefix (symbol-name k)))
+                (push (cons k v) results)))
+            org-supertag-db--metadata)
+    (nreverse results)))
+
+;; Add metadata save/load support to existing save/load functions
+(add-hook 'org-supertag-db-before-save-hook
+          (lambda ()
+            (ht-set! org-supertag-db--object "metadata"
+                   `(:type :metadata :data ,org-supertag-db--metadata))))
+
+(add-hook 'org-supertag-db-after-load-hook
+          (lambda ()
+            (when-let* ((metadata-entry (org-supertag-db-get "metadata")))
+              (setq org-supertag-db--metadata (or (plist-get metadata-entry :data)
+                                                 (ht-create))))))
+
+(defun org-supertag-db-test-metadata ()
+  "Test metadata storage functions."
+  (interactive)
+  (message "\n=== Testing Metadata Functions ===")
+  
+  ;; Set some test metadata
+  (org-supertag-db-set-metadata 'test-key "Test Value")
+  (org-supertag-db-set-metadata "numeric-key" 42)
+  (org-supertag-db-set-metadata 'list-key '(a b c))
+  
+  ;; Display current metadata
+  (message "Current metadata keys: %S" 
+           (mapcar #'car (org-supertag-db-list-metadata)))
+  
+  ;; Retrieve and verify values
+  (let ((test-val (org-supertag-db-get-metadata 'test-key))
+        (num-val (org-supertag-db-get-metadata "numeric-key"))
+        (list-val (org-supertag-db-get-metadata 'list-key))
+        (missing-val (org-supertag-db-get-metadata 'nonexistent "default")))
+    (message "test-key: %S (expected: \"Test Value\")" test-val)
+    (message "numeric-key: %S (expected: 42)" num-val)
+    (message "list-key: %S (expected: (a b c))" list-val)
+    (message "nonexistent: %S (expected: \"default\")" missing-val))
+  
+  ;; Remove a key
+  (org-supertag-db-remove-metadata 'test-key)
+  (message "After removal: %S" 
+           (mapcar #'car (org-supertag-db-list-metadata)))
+  
+  ;; Save and reload test
+  (when (yes-or-no-p "Save database to test persistence? ")
+    (org-supertag-db-save)
+    (setq org-supertag-db--metadata (ht-create)) ;; Clear in-memory
+    (message "Metadata table cleared. Current keys: %S" 
+             (mapcar #'car (org-supertag-db-list-metadata)))
+    (org-supertag-db-load)
+    (message "After reload: %S" 
+             (mapcar #'car (org-supertag-db-list-metadata)))))
+
+<<<<<<< HEAD
+(provide 'org-supertag-db)
+=======
+;;------------------------------------------------------------------------------
+;; Data Source Unification 
 ;;------------------------------------------------------------------------------
-(defun org-supertag-db-recompute-all-hashes ()
-  "Re-compute and update the hash for all objects in the database.
-This function iterates through every object, calculates its hash using the
-current `org-supertag-background-sync--calculate-object-hash` logic,
-and updates the object if the hash has changed or was missing.
-
-This is useful after changing the hashing logic. It assumes that
-`org-supertag-background-sync.el` is loaded."
+
+(defun org-supertag-db-check-data-sources ()
+  "Check data source unification and verify if there are any duplicate data source issues.
+Return a report of data source unification."
   (interactive)
-  (message "Starting to re-compute all object hashes...")
-  (let ((processed-count 0)
-        (updated-count 0)
-        ;; Get a snapshot of all object IDs to avoid issues with modifying the hash table while iterating
-        (all-ids (ht-keys org-supertag-db--object)))
-
-    (dolist (id all-ids)
-      (cl-incf processed-count)
-      ;; Get the latest props for the id
-      (when-let* ((props (org-supertag-db-get id)))
-          (let* ((old-hash (plist-get props :hash))
-                 ;; Use funcall to avoid circular dependency issues at compile time.
-                 ;; This requires org-supertag-background-sync.el to be loaded when run.
-                 (new-hash (funcall 'org-supertag-background-sync--calculate-object-hash props)))
-
-            ;; Update if hash is new or different, and the ID is valid.
-            (when (and new-hash (not (equal old-hash new-hash)))
-              (cl-incf updated-count)
-              (let ((new-props (plist-put (copy-sequence props) :hash new-hash)))
-                ;; Use org-supertag-db-add to ensure all caches and hooks are handled correctly.
-                (org-supertag-db-add id new-props))))))
-
-    (message "Hash re-computation complete. Processed: %d, Updated: %d."
-             processed-count updated-count)
-
-    (if (> updated-count 0)
-        (progn
-          (message "Saving updated hashes to the database...")
-          ;; The save is scheduled by db-add, but we can force it for immediate feedback.
-          (org-supertag-db-save)
-          (message "Database saved."))
-      (message "No hash updates were necessary."))))
+  (let ((total-objects 0)
+        (unified-objects 0)
+        (issues (make-hash-table :test 'equal))
+        (excluded-properties '("FILE" "ITEM" "ALLTAGS" "TAGS")))
+    
+    ;; Check object table
+    (ht-map (lambda (id props)
+              (cl-incf total-objects)
+              (let ((title (plist-get props :title))
+                    (raw-value (plist-get props :raw-value))
+                    (file-path (plist-get props :file-path))
+                    (properties (plist-get props :properties))
+                    (is-unified t))
+                
+                ;; Check if there are still raw-value fields (should have been removed)
+                (when raw-value
+                  (setq is-unified nil)
+                  (cl-incf (gethash "raw-value-exists" issues 0)))
+                
+                ;; Check if there are still excluded fields in properties
+                (when properties
+                  (dolist (prop properties)
+                    (when (member (car prop) excluded-properties)
+                      (setq is-unified nil)
+                      (cl-incf (gethash (format "excluded-property-%s" (car prop)) issues 0)))))
+                
+                ;; Check if there are duplicate string object references
+                (when (and title file-path properties)
+                  (let ((file-prop (alist-get "FILE" properties nil nil #'equal)))
+                    (when (and file-prop (eq file-path file-prop))
+                      (setq is-unified nil)
+                      (cl-incf (gethash "file-path-duplicate" issues 0)))))
+                
+                (when is-unified
+                  (cl-incf unified-objects))))
+            org-supertag-db--object)
+    
+    ;; Output report
+    (message "\n=== Data Source Unification Report ===")
+    (message "Total objects: %d" total-objects)
+    (message "Unified data source objects: %d" unified-objects)
+    (message "Unification rate: %.1f%%" (* 100.0 (/ (float unified-objects) total-objects)))
+    (message "Found issues:")
+    (if (= 0 (hash-table-count issues))
+        (message "  ✅ All data sources are unified, no duplicate issues")
+      (maphash (lambda (issue count)
+                 (message "  ❌ %s: %d objects" issue count))
+               issues))
+    
+    ;; Return statistics
+    (list :total-objects total-objects
+          :unified-objects unified-objects
+          :unification-rate (/ (float unified-objects) total-objects)
+          :issues issues)))
+
+(defun org-supertag-db-check-duplicates ()
+  "Check for duplicate references in the database.
+Return a report showing the statistics of duplicate references found."
+  (interactive)
+  (let ((duplicate-count 0)
+        (total-objects 0)
+        (duplicate-types (make-hash-table :test 'equal)))
+    
+    ;; Check object table
+    (ht-map (lambda (id props)
+              (cl-incf total-objects)
+              (let ((title (plist-get props :title))
+                    (raw-value (plist-get props :raw-value))
+                    (file-path (plist-get props :file-path))
+                    (properties (plist-get props :properties)))
+                
+                ;; Check if title and raw-value are the same object
+                (when (and title raw-value (eq title raw-value))
+                  (cl-incf duplicate-count)
+                  (cl-incf (gethash "title-raw-value" duplicate-types 0)))
+                
+                ;; Check if file-path and FILE in properties are the same
+                (when (and file-path properties)
+                  (let ((file-prop (alist-get "FILE" properties nil nil #'equal)))
+                    (when (and file-prop (eq file-path file-prop))
+                      (cl-incf duplicate-count)
+                      (cl-incf (gethash "file-path-property" duplicate-types 0)))))))
+            org-supertag-db--object)
+    
+    ;; Output report
+    (message "\n=== Database Duplicate Reference Check Report ===")
+    (message "Total objects: %d" total-objects)
+    (message "Found duplicate references: %d" duplicate-count)
+    (message "Duplicate type statistics:")
+    (maphash (lambda (type count)
+               (message "  %s: %d" type count))
+             duplicate-types)
+    
+    ;; Return statistics
+    (list :total-objects total-objects
+          :duplicate-count duplicate-count
+          :duplicate-types duplicate-types)))
+
+(defun org-supertag-db-fix-duplicates ()
+  "Fix duplicate references in the database.
+This will create new string objects to replace duplicate references."
+  (interactive)
+  (let ((fixed-count 0)
+        (total-objects 0))
+    
+    ;; Fix duplicate references in the object table
+    (ht-map (lambda (id props)
+              (cl-incf total-objects)
+              (let ((modified nil)
+                    (new-props (copy-sequence props)))
+                
+                ;; Fix duplicate title and raw-value
+                (let ((title (plist-get props :title))
+                      (raw-value (plist-get props :raw-value)))
+                  (when (and title raw-value (eq title raw-value))
+                    (setq new-props (plist-put new-props :raw-value (copy-sequence raw-value)))
+                    (setq modified t)))
+                
+                ;; Fix duplicate file-path and properties
+                (let ((file-path (plist-get props :file-path))
+                      (properties (plist-get props :properties)))
+                  (when (and file-path properties)
+                    (let ((file-prop (alist-get "FILE" properties nil nil #'equal)))
+                      (when (and file-prop (eq file-path file-prop))
+                        (let ((new-properties (mapcar (lambda (prop)
+                                                        (if (equal (car prop) "FILE")
+                                                            (cons (car prop) (copy-sequence (cdr prop)))
+                                                          prop))
+                                                      properties)))
+                          (setq new-props (plist-put new-props :properties new-properties))
+                          (setq modified t))))))
+                
+                ;; If there are modifications, update the object
+                (when modified
+                  (ht-set! org-supertag-db--object id new-props)
+                  (cl-incf fixed-count))))
+            org-supertag-db--object)
+    
+    ;; Mark database as dirty and save
+    (when (> fixed-count 0)
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db-save))
+    
+    ;; Output results
+    (message "\n=== Duplicate Reference Fix Report ===")
+    (message "Total objects: %d" total-objects)
+    (message "Fixed objects: %d" fixed-count)
+    (when (> fixed-count 0)
+      (message "Database saved"))
+    
+    fixed-count))
+
+(defun org-supertag-db-migrate-to-unified-format ()
+  "Migrate existing database to unified data source format.
+This will:
+1. Remove raw-value field (use title as unified source)
+2. Remove redundant fields from properties (FILE, ITEM, ALLTAGS, TAGS)
+3. Ensure all string fields are independent objects
+4. Create backup in case of migration issues"
+  (interactive)
+  (when (yes-or-no-p "This will migrate the entire database. Continue? It's recommended to backup first.")
+    (let ((migrated-count 0)
+          (total-objects 0)
+          (excluded-properties '("FILE" "ITEM" "ALLTAGS" "TAGS"))
+          (migration-stats (make-hash-table :test 'equal)))
+      
+      ;; Create backup
+      (message "Creating database backup...")
+      (org-supertag-db-backup)
+      (message "Backup created")
+      
+      ;; Start migration
+      (message "Starting data migration...")
+      (ht-map (lambda (id props)
+                (cl-incf total-objects)
+                (let ((original-props (copy-sequence props))
+                      (new-props (copy-sequence props))
+                      (migrated nil))
+                  
+                  ;; 1. Remove raw-value field (keep title as unified source)
+                  (when (plist-get props :raw-value)
+                    (setq new-props (org-supertag-db--plist-remove new-props :raw-value))
+                    (setq migrated t)
+                    (cl-incf (gethash "raw-value-removed" migration-stats 0)))
+                  
+                  ;; 2. Clean up redundant fields in properties
+                  (let ((properties (plist-get props :properties)))
+                    (when properties
+                      (let ((filtered-properties
+                             (cl-remove-if (lambda (prop)
+                                           (member (car prop) excluded-properties))
+                                         properties)))
+                        (when (not (equal properties filtered-properties))
+                          (setq new-props (plist-put new-props :properties filtered-properties))
+                          (setq migrated t)
+                          ;; Count removed fields
+                          (dolist (prop properties)
+                            (when (member (car prop) excluded-properties)
+                              (cl-incf (gethash (format "property-%s-removed" (car prop)) migration-stats 0))))))))
+                  
+                  ;; 3. Ensure string field independence (copy string objects)
+                  (dolist (field '(:title :file-path :content))
+                    (let ((value (plist-get new-props field)))
+                      (when (stringp value)
+                        (setq new-props (plist-put new-props field (copy-sequence value))))))
+                  
+                  ;; 4. Update object (if there are changes)
+                  (when migrated
+                    (ht-set! org-supertag-db--object id new-props)
+                    (cl-incf migrated-count))))
+              org-supertag-db--object)
+      
+      ;; Mark database as dirty and save
+      (when (> migrated-count 0)
+        (org-supertag-db--mark-dirty)
+        (org-supertag-db-save))
+      
+      ;; Output migration report
+      (message "\n🚀 === Data Migration Completion Report ===")
+      (message "📊 Total objects processed: %d" total-objects)
+      (message "🔄 Migrated objects: %d" migrated-count)
+      (message "📈 Migration rate: %.1f%%" (* 100.0 (/ (float migrated-count) total-objects)))
+      (message "\n📋 Migration statistics:")
+      (maphash (lambda (operation count)
+                 (message
+               migration-stats)
+      
+      (when (> migrated-count 0)
+        (message "\n💾 Database saved")
+        (message "🔍 Run (org-supertag-db-check-data-sources) to verify migration results"))
+      
+      ;; Return migration statistics
+      (list :total-objects total-objects
+            :migrated-objects migrated-count
+            :migration-rate (/ (float migrated-count) total-objects)
+            :statistics migration-stats))))))
+
+(defun org-supertag-db--plist-remove (plist key)
+  "Remove specified key-value pairs from plist.
+PLIST is the property list
+KEY is the key to remove"
+  (let ((result nil))
+    (while plist
+      (let ((current-key (car plist))
+            (current-value (cadr plist)))
+        (unless (eq current-key key)
+          (setq result (append result (list current-key current-value))))
+        (setq plist (cddr plist))))
+    result))
+
+(defun org-supertag-db-toggle-print-circle ()
+  "Toggle print-circle setting and save database.
+This can change the representation of duplicate references in the saved file."
+  (interactive)
+  (setq org-supertag-db-use-print-circle (not org-supertag-db-use-print-circle))
+  (message "Print-circle 设置已切换为: %s" org-supertag-db-use-print-circle)
+  (when (org-supertag-db--dirty-p)
+    (org-supertag-db-save)
+    (message "数据库已使用新设置重新保存")))
+
+(defun org-supertag-db-test-unified-data-sources ()
+  "Test data source unification effectiveness.
+This function will:
+1. Check data source unification
+2. Check duplicate references
+3. Validate data integrity
+4. Display comparison before and after unification"
+  (interactive)
+  (message "\n🔍 === Data Source Unification Test ===")
+  
+  ;; 1. Data source unification check
+  (message "\n📊 1. Data source unification check:")
+  (let ((unification-report (org-supertag-db-check-data-sources)))
+    (let ((rate (plist-get unification-report :unification-rate)))
+      (if (>= rate 1.0)
+          (message "✅ Data source unification: 100%% - Excellent!")
+        (message "⚠️  Data source unification: %.1f%% - Needs improvement" (* 100 rate)))))
+  
+  ;; 2. Duplicate reference check
+  (message "\n🔗 2. Duplicate reference check:")
+  (let ((duplicate-report (org-supertag-db-check-duplicates)))
+    (let ((dup-count (plist-get duplicate-report :duplicate-count)))
+      (if (= dup-count 0)
+          (message "✅ No duplicate references - Excellent!")
+        (message "⚠️  Found %d duplicate references - Suggest fixing" dup-count))))
+  
+  ;; 3. Data integrity check
+  (message "\n📋 3. Data integrity check:")
+  (let ((total-objects 0)
+        (valid-objects 0)
+        (required-fields '(:type :id :title :file-path)))
+    (ht-map (lambda (id props)
+              (cl-incf total-objects)
+              (when (cl-every (lambda (field) (plist-get props field)) required-fields)
+                (cl-incf valid-objects)))
+            org-supertag-db--object)
+    (let ((validity-rate (/ (float valid-objects) total-objects)))
+      (if (>= validity-rate 1.0)
+          (message "✅ Data integrity: 100%% (%d/%d) - Excellent!" valid-objects total-objects)
+        (message "⚠️  Data integrity: %.1f%% (%d/%d) - Needs checking" 
+                (* 100 validity-rate) valid-objects total-objects))))
+  
+  ;; 4. Unification strategy effectiveness summary
+  (message "\n📈 4. Unification strategy effectiveness:")
+  (message "🎯 Unification strategy benefits:")
+  (message "   • Eliminated title/raw-value duplicates")
+  (message "   • Removed redundant fields from properties (FILE, ITEM, ALLTAGS, TAGS)")
+  (message "   • Ensured each string field is an independent object")
+  (message "   • Simplified data structure, improved consistency")
+  
+  ;; 5. Suggested actions
+  (message "\n💡 5. Suggested actions:")
+  (let ((unification-report (org-supertag-db-check-data-sources))
+        (duplicate-report (org-supertag-db-check-duplicates)))
+    (let ((unif-rate (plist-get unification-report :unification-rate))
+          (dup-count (plist-get duplicate-report :duplicate-count)))
+      (cond
+       ((and (>= unif-rate 1.0) (= dup-count 0))
+        (message "🎉 Data source is fully unified! No further action needed."))
+       ((< unif-rate 0.5)
+        (message "🚀 Suggest running batch migration: (org-supertag-db-migrate-to-unified-format)")
+        (message "    This will automatically convert all old format data to unified format"))
+       ((< unif-rate 1.0)
+        (message "🔧 Suggest running: (org-supertag-db-fix-duplicates) to fix remaining issues"))
+       ((> dup-count 0)
+        (message "🔧 Suggest running: (org-supertag-db-fix-duplicates) to fix duplicate references"))
+       (t
+        (message "🎯 Database is in good condition!")))
+      
+      ;; Special note for low unification rate
+      (when (< unif-rate 0.2)
+        (message "⚠️  Data source unification rate is very low (%.1f%%), strongly suggest running batch migration" (* 100 unif-rate)))
+      
+      ;; Optional print-circle setting suggestion
+      (when org-supertag-db-use-print-circle
+        (message "💾 Currently using print-circle=t (compact storage)")
+        (message "    If you prefer a more readable save format, run: (setq org-supertag-db-use-print-circle nil)"))))
+  
+  (message "\n✨ Data source unification test completed!"))
+
+;; Tag Governance Status
+(defconst org-supertag-tag-status
+  '(:draft       ; 初始状态，标签刚创建或修改
+    :review      ; 审核状态，等待确认
+    :active      ; 活跃状态，正常使用
+    :deprecated  ; 废弃状态，不建议使用
+    :merged      ; 已合并到其他标签
+    :split       ; 已拆分为多个标签
+    :archived)   ; 已归档，不再使用
+  "Tag governance status types.")
+
+(defun org-supertag-tag-set-status (tag-id status &optional reason)
+  "Set tag governance status.
+TAG-ID: Tag identifier
+STATUS: New status (must be one of org-supertag-tag-status)
+REASON: Optional reason for status change"
+  (when (and tag-id status)
+    (unless (memq status org-supertag-tag-status)
+      (error "Invalid tag status: %s" status))
+    
+    (let* ((tag (org-supertag-db-get tag-id))
+           (old-status (plist-get tag :tag-status))
+           (history-entry
+            `(:timestamp ,(current-time)
+              :old-status ,old-status
+              :new-status ,status
+              :reason ,reason)))
+      
+      ;; Update tag status
+      (org-supertag-db-add :tag tag-id
+                          (plist-put
+                           (plist-put tag :tag-status status)
+                           :tag-history
+                           (cons history-entry
+                                 (plist-get tag :tag-history))))
+      
+      ;; Emit event
+      (org-supertag-db-emit 'tag:status-changed tag-id status old-status reason)
+      
+      ;; Return new status
+      status)))
+
+(defun org-supertag-tag-get-status (tag-id)
+  "Get tag governance status.
+TAG-ID: Tag identifier
+Returns current status or nil if not set."
+  (when tag-id
+    (org-supertag-db-get-prop tag-id :tag-status)))
+
+(defun org-supertag-tag-get-history (tag-id)
+  "Get tag governance history.
+TAG-ID: Tag identifier
+Returns list of history entries or nil if no history."
+  (when tag-id
+    (org-supertag-db-get-prop tag-id :tag-history)))
+
+;; Tag Governance Rules
+(defconst org-supertag-rule-types
+  '(:naming      ; 命名规则
+    :usage       ; 使用规则
+    :relation    ; 关系规则
+    :lifecycle   ; 生命周期规则
+    :custom)     ; 自定义规则
+  "Tag governance rule types.")
+
+(defun org-supertag-tag-add-rule (tag-id rule-type rule &optional description)
+  "Add a governance rule to tag.
+TAG-ID: Tag identifier
+RULE-TYPE: Rule type (must be one of org-supertag-rule-types)
+RULE: Rule definition (plist or function)
+DESCRIPTION: Optional rule description"
+  (when (and tag-id rule-type rule)
+    (unless (memq rule-type org-supertag-rule-types)
+      (error "Invalid rule type: %s" rule-type))
+    
+    (let* ((tag (org-supertag-db-get tag-id))
+           (rules (or (plist-get tag :tag-rules) (list)))
+           (rule-entry
+            `(:type ,rule-type
+              :rule ,rule
+              :description ,description
+              :created-at ,(current-time))))
+      
+      ;; Update tag rules
+      (org-supertag-db-add :tag tag-id
+                          (plist-put tag :tag-rules
+                                    (cons rule-entry rules)))
+      
+      ;; Emit event
+      (org-supertag-db-emit 'tag:rule-added tag-id rule-type rule)
+      
+      ;; Return rule entry
+      rule-entry)))
+
+(defun org-supertag-tag-remove-rule (tag-id rule-type)
+  "Remove a governance rule from tag.
+TAG-ID: Tag identifier
+RULE-TYPE: Rule type to remove"
+  (when (and tag-id rule-type)
+    (let* ((tag (org-supertag-db-get tag-id))
+           (rules (plist-get tag :tag-rules))
+           (filtered-rules
+            (cl-remove-if (lambda (rule)
+                           (eq (plist-get rule :type) rule-type))
+                         rules)))
+      
+      ;; Update tag rules
+      (when (not (equal rules filtered-rules))
+        (org-supertag-db-add :tag tag-id
+                            (plist-put tag :tag-rules filtered-rules))
+        
+        ;; Emit event
+        (org-supertag-db-emit 'tag:rule-removed tag-id rule-type)))))
+
+(defun org-supertag-tag-get-rules (tag-id &optional rule-type)
+  "Get governance rules for tag.
+TAG-ID: Tag identifier
+RULE-TYPE: Optional rule type to filter by
+Returns list of rule entries or nil if no rules."
+  (when tag-id
+    (let ((rules (org-supertag-db-get-prop tag-id :tag-rules)))
+      (if rule-type
+          (cl-remove-if-not
+           (lambda (rule)
+             (eq (plist-get rule :type) rule-type))
+           rules)
+        rules))))
+
+;; Tag Relation Governance
+(defconst org-supertag-relation-types
+  '(:semantic    ; 语义关联
+    :hierarchy   ; 层级关系
+    :synonym     ; 同义词
+    :antonym     ; 反义词
+    :part-whole  ; 部分-整体
+    :cause-effect ; 因果关系
+    :custom)     ; 自定义关系
+  "Tag relation governance types.")
+
+(defun org-supertag-relation-set-type (from-id to-id rel-type &optional reason)
+  "Set tag relation governance type.
+FROM-ID: Source tag identifier
+TO-ID: Target tag identifier
+REL-TYPE: Relation type (must be one of org-supertag-relation-types)
+REASON: Optional reason for type change"
+  (when (and from-id to-id rel-type)
+    (unless (memq rel-type org-supertag-relation-types)
+      (error "Invalid relation type: %s" rel-type))
+    
+    (let* ((link-id (org-supertag-db-get-link-id :tag-tag from-id to-id))
+           (link (org-supertag-db-get-link link-id))
+           (old-type (plist-get link :tag-rel-type))
+           (history-entry
+            `(:timestamp ,(current-time)
+              :old-type ,old-type
+              :new-type ,rel-type
+              :reason ,reason)))
+      
+      ;; Update relation type
+      (org-supertag-db-add-link :tag-tag from-id to-id
+                               (plist-put
+                                (plist-put link :tag-rel-type rel-type)
+                                :tag-rel-history
+                                (cons history-entry
+                                      (plist-get link :tag-rel-history))))
+      
+      ;; Emit event
+      (org-supertag-db-emit 'tag:relation-type-changed link-id rel-type old-type reason)
+      
+      ;; Return new type
+      rel-type)))
+
+(defun org-supertag-relation-add-rule (from-id to-id rule-type rule &optional description)
+  "Add a governance rule to tag relation.
+FROM-ID: Source tag identifier
+TO-ID: Target tag identifier
+RULE-TYPE: Rule type (must be one of org-supertag-rule-types)
+RULE: Rule definition (plist or function)
+DESCRIPTION: Optional rule description"
+  (when (and from-id to-id rule-type rule)
+    (unless (memq rule-type org-supertag-rule-types)
+      (error "Invalid rule type: %s" rule-type))
+    
+    (let* ((link-id (org-supertag-db-get-link-id :tag-tag from-id to-id))
+           (link (org-supertag-db-get-link link-id))
+           (rules (or (plist-get link :tag-rel-rules) (list)))
+           (rule-entry
+            `(:type ,rule-type
+              :rule ,rule
+              :description ,description
+              :created-at ,(current-time))))
+      
+      ;; Update relation rules
+      (org-supertag-db-add-link :tag-tag from-id to-id
+                               (plist-put link :tag-rel-rules
+                                         (cons rule-entry rules)))
+      
+      ;; Emit event
+      (org-supertag-db-emit 'tag:relation-rule-added link-id rule-type rule)
+      
+      ;; Return rule entry
+      rule-entry)))
+
+(defun org-supertag-relation-get-type (from-id to-id)
+  "Get tag relation governance type.
+FROM-ID: Source tag identifier
+TO-ID: Target tag identifier
+Returns current type or nil if not set."
+  (when (and from-id to-id)
+    (let ((link-id (org-supertag-db-get-link-id :tag-tag from-id to-id)))
+      (when link-id
+        (plist-get (org-supertag-db-get-link link-id) :tag-rel-type)))))
+
+(defun org-supertag-relation-get-rules (from-id to-id &optional rule-type)
+  "Get governance rules for tag relation.
+FROM-ID: Source tag identifier
+TO-ID: Target tag identifier
+RULE-TYPE: Optional rule type to filter by
+Returns list of rule entries or nil if no rules."
+  (when (and from-id to-id)
+    (let* ((link-id (org-supertag-db-get-link-id :tag-tag from-id to-id))
+           (link (when link-id (org-supertag-db-get-link link-id)))
+           (rules (when link (plist-get link :tag-rel-rules))))
+      (if rule-type
+          (cl-remove-if-not
+           (lambda (rule)
+             (eq (plist-get rule :type) rule-type))
+           rules)
+        rules))))
+
+;; Event System
+(defvar org-supertag-db-event-handlers (make-hash-table :test 'equal)
+  "Event handlers registry.")
+
+(defconst org-supertag-db-events
+  '(;; Tag Events
+    tag:created           ; 标签创建
+    tag:updated          ; 标签更新
+    tag:status-changed   ; 标签状态变更
+    tag:rule-added       ; 标签规则添加
+    tag:rule-removed     ; 标签规则移除
+    ;; Relation Events
+    tag:relation-created        ; 标签关系创建
+    tag:relation-updated       ; 标签关系更新
+    tag:relation-type-changed  ; 标签关系类型变更
+    tag:relation-rule-added    ; 标签关系规则添加
+    tag:relation-rule-removed) ; 标签关系规则移除
+  "Available event types.")
+
+(defun org-supertag-db-register-handler (event-type handler)
+  "Register an event handler.
+EVENT-TYPE: Event type symbol
+HANDLER: Handler function that takes event data as argument"
+  (when (and event-type handler)
+    (let ((handlers (gethash event-type org-supertag-db-event-handlers)))
+      (puthash event-type
+               (cons handler handlers)
+               org-supertag-db-event-handlers))))
+
+(defun org-supertag-db-unregister-handler (event-type handler)
+  "Unregister an event handler.
+EVENT-TYPE: Event type symbol
+HANDLER: Handler function to remove"
+  (when (and event-type handler)
+    (let ((handlers (gethash event-type org-supertag-db-event-handlers)))
+      (puthash event-type
+               (remove handler handlers)
+               org-supertag-db-event-handlers))))
+
+(defun org-supertag-db-emit (event-type &rest event-data)
+  "Emit an event to registered handlers.
+EVENT-TYPE: Event type symbol
+EVENT-DATA: Event data to pass to handlers"
+  (when event-type
+    (let ((handlers (gethash event-type org-supertag-db-event-handlers)))
+      (dolist (handler handlers)
+        (condition-case err
+            (apply handler event-data)
+          (error
+           (message "Error in event handler: %s" err)))))))
+
+;; Default Event Handlers
+(defun org-supertag-db--sync-handler (event-type &rest event-data)
+  "Default handler for syncing changes to Python backend.
+EVENT-TYPE: Event type symbol
+EVENT-DATA: Event data"
+  (let ((sync-data `(:event ,event-type :data ,event-data)))
+    (org-supertag-bridge-call-async 'sync_tag_event sync-data)))
+
+;; Register default handlers
+(dolist (event org-supertag-db-events)
+  (org-supertag-db-register-handler event #'org-supertag-db--sync-handler))
+
+(defun org-supertag-db-get-link-by-id (id)
+  "Get link properties by ID."
+  (gethash id org-supertag-db--link))
+
+;; === Hash-based snapshot functions for sync ===
+
+(defun org-supertag-db--calculate-object-hash (props)
+  "Calculate object hash from its property list."
+  (when (plistp props)
+    (let ((hash-content (format "%S" props)))
+      (secure-hash 'sha1 hash-content))))
+
+(defun org-supertag-db-get-all-hashes ()
+  "Get a hash table of all current objects (nodes, tags, links) in the DB."
+  (let ((all-hashes (make-hash-table :test 'equal)))
+    (maphash
+     (lambda (id props)
+       (puthash id (org-supertag-db--calculate-object-hash props) all-hashes))
+     org-supertag-db--object)
+    (maphash
+     (lambda (id props)
+       (puthash id (org-supertag-db--calculate-object-hash props) all-hashes))
+     org-supertag-db--link)
+    all-hashes))
+
+(defun org-supertag-db-get-snapshot-for-sync (last-sync-hashes)
+  "Compare the current DB state against the last sync hashes and return a snapshot of changes.
+The snapshot is a plist containing nodes and links to upsert, and IDs to delete."
+  (let ((nodes-to-upsert '())
+        (links-to-upsert '())
+        (ids-to-delete '())
+        (current-ids (make-hash-table :test 'equal))
+        (all-current-hashes (org-supertag-db-get-all-hashes)))
+
+    ;; 1. Find created/updated objects
+    (maphash
+     (lambda (id current-hash)
+       (puthash id t current-ids) ; Track all current IDs
+       (let ((last-hash (gethash id last-sync-hashes)))
+         (unless (and last-hash (string= current-hash last-hash))
+           ;; Change detected, add the full object to the correct list
+           (let ((props (or (gethash id org-supertag-db--object)
+                            (gethash id org-supertag-db--link))))
+             (when props
+               (let ((type (plist-get props :type)))
+                 (cond
+                  ((eq type :node) (push props nodes-to-upsert))
+                  ((eq type :link) (push props links-to-upsert))
+                  ;; Tags are handled as nodes
+                  ((eq type :tag) (push props nodes-to-upsert)) 
+                  (t nil))))))))
+     all-current-hashes)
+
+    ;; 2. Find deleted objects by comparing old hashes with current IDs
+    (maphash
+     (lambda (id _last-hash)
+       (unless (gethash id current-ids)
+         (push id ids-to-delete)))
+     last-sync-hashes)
+
+    ;; 3. Construct the snapshot plist
+    (let ((snapshot `(:nodes ,(reverse nodes-to-upsert)
+                      :links ,(reverse links-to-upsert)
+                      :ids-to-delete ,ids-to-delete)))
+      (message "[DB] Snapshot created: %d nodes, %d links, %d deletions."
+               (length (plist-get snapshot :nodes))
+               (length (plist-get snapshot :links))
+               (length (plist-get snapshot :ids-to-delete)))
+      snapshot)))
 
 (provide 'org-supertag-db)
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;; org-supertag-db.el ends here
   
diff --git a/org-supertag-field-operation.el b/org-supertag-field-operation.el
new file mode 100644
index 0000000..cf84599
--- /dev/null
+++ b/org-supertag-field-operation.el
@@ -0,0 +1,830 @@
+;;; org-supertag-field-operation.el --- Field operations for org-supertag -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; Provides field operation functionality for org-supertag
+;; Core principle: Manage field values and field editing interface
+
+(require 'cl-lib)
+(require 'org)
+(require 'org-element)
+(require 'org-supertag-db)
+(require 'org-supertag-field)
+
+
+
+;;----------------------------------------------------------------------
+;; Field Value Entity Operations
+;;----------------------------------------------------------------------
+
+(defun org-supertag-field-set-value (field-def value node-id tag-id)
+  "Set field value for node.
+FIELD-DEF is the field definition
+VALUE is the field value
+NODE-ID is the target node ID
+TAG-ID is the tag ID"
+  (condition-case err
+      (let* ((field-name (plist-get field-def :name))
+             (sanitized-name (org-supertag--sanitize-field-name field-name))
+             (field-type (plist-get field-def :type))
+             (type-def (org-supertag-get-field-type field-type))
+             (processed-value
+              (progn
+                (message "Debug - Setting field: name=%S, type=%S, initial-value=%S" 
+                         sanitized-name field-type value)
+                (if (and type-def value)
+                    (progn
+                      (message "Debug - Processing value with type-def: %S" type-def)
+                      (when-let* ((validator (plist-get type-def :validator)))
+                        (message "Debug - Validating with %S" validator)
+                        (unless (funcall validator value)
+                          (error "Value validation failed for %s: %S" sanitized-name value)))
+                      (if-let* ((formatter (plist-get type-def :formatter)))
+                          (progn
+                            (message "Debug - Formatting with %S" formatter)
+                            (funcall formatter value field-def))
+                        value))
+                  (progn
+                    (message "Debug - Using raw value (no processing): %S" value)
+                    value)))))
+        
+        ;; Use sanitized name for all operations
+        (unless (string= field-name sanitized-name)
+          (setq field-name sanitized-name)
+          (setq field-def (copy-sequence field-def))
+          (setf (plist-get field-def :name) sanitized-name))
+        
+        (message "Debug - Final processed value: %S" processed-value)
+        
+        (when (eq field-type 'tag-reference)
+          (message "Debug - Processing tag-reference: value=%S, processed=%S" 
+                   value processed-value)
+          (when-let* ((old-value (org-supertag-field-get-value field-def node-id tag-id)))
+            (org-supertag-node-db-remove-reference node-id old-value))
+          (when processed-value
+            (org-supertag-node-db-add-reference node-id processed-value)))
+        
+        (org-supertag-db-set-field-value node-id field-name processed-value tag-id)
+        
+        (when-let* ((pos (condition-case nil
+                           (org-id-find node-id t)
+                         (error nil))))
+          (org-with-point-at pos
+            (org-set-property field-name processed-value))))
+    (error
+     (message "Error in field set-value operation: %s"
+              (error-message-string err))
+     (signal (car err) (cdr err)))))
+
+(defun org-supertag-field-validate (field value)
+  "Validate field value.
+FIELD: Field definition
+VALUE: Value to validate
+Returns t if valid, otherwise throws error"
+  (let* ((name (plist-get field :name))
+         (type (plist-get field :type))
+         (required (plist-get field :required))
+         (type-def (org-supertag-get-field-type type))
+         (validator (plist-get type-def :validator)))
+    (message "Debug - Validating field: name=%S, type=%S, value=%S" name type value)
+    
+    (when (and required (null value))
+      (error "Field '%s' is required" name))
+    
+    (if (null value)
+        t
+      (progn
+        (when (and (eq type 'string) (not (stringp value)))
+          (error "Field '%s' requires string type, got %S" name (type-of value)))
+        
+        (when validator
+          (condition-case err
+              (if (not (funcall validator value))
+                (error "Value '%s' does not meet validation requirements" value))
+            (error
+             (error "Error validating field '%s': %s" name (error-message-string err)))))
+        t))))
+
+(defun org-supertag-field-get-value (field-def node-id tag-id)
+  "Get field value.
+FIELD-DEF: field definition
+NODE-ID: node ID
+TAG-ID: tag ID"
+  (let* ((field-name (plist-get field-def :name))
+         (type (plist-get field-def :type))
+         (type-spec (org-supertag-get-field-type type))
+         (value (org-supertag-db-get-field-value node-id field-name tag-id)))
+    (when (and value
+               (plist-get type-spec :formatter))
+      (setq value (funcall (plist-get type-spec :formatter) value field-def)))
+    value))
+
+(defun org-supertag-field-remove-value (field-def node-id tag-id)
+  "Remove field value.
+FIELD-DEF: field definition
+NODE-ID: node ID
+TAG-ID: tag ID"
+  (let ((field-name (plist-get field-def :name)))
+    (org-supertag-db-remove-link :node-field node-id field-name
+                                `(:tag-id ,tag-id))
+    (org-with-point-at (org-id-find node-id t)
+      (org-entry-delete nil field-name))))
+
+(defun org-supertag-field-batch-set-value (field-def-list value-list node-id tag-id)
+  "Set multiple field values in batch.
+FIELD-DEF-LIST: List of field definitions to set
+VALUE-LIST: List of values corresponding to the fields
+NODE-ID: ID of the target node
+TAG-ID: ID of the associated tag"
+  (cl-loop for field-def in field-def-list
+           for value in value-list
+           do (org-supertag-field-set-value field-def value node-id tag-id)))
+
+(defun org-supertag-field-find-nodes (field-name value &optional tag-id)
+  "Find nodes that have a specific field value.
+FIELD-NAME: Name of the field to search
+VALUE: Value to match against
+TAG-ID: Optional tag ID to limit search scope
+
+Returns a list of nodes that match the criteria."
+  (let ((links (org-supertag-db-get-links :node-field)))
+    (cl-remove-if-not
+     (lambda (link)
+       (and (equal (car link) field-name)  ; Match field name
+            (equal (plist-get (cadr link) :value) value)  ; Match value
+            (or (null tag-id)  ; If no tag ID specified
+                (equal (plist-get (cadr link) :tag-id) tag-id)))) ; Match tag ID
+     links)))
+
+;;----------------------------------------------------------------------
+;; Field Edit Interface 
+;;----------------------------------------------------------------------
+(defvar-local org-supertag--pending-operations nil
+  "Store pending operations.
+Format: ((operation . args) ...)
+operation can be 'remove-field, 'add-field, 'edit-field, etc.")
+
+(defun org-supertag-tag--refresh-field-table (context)
+  "Refresh the field table display.
+CONTEXT is the edit context plist containing:
+- :node-id        The node ID
+- :node-title     The node title
+- :tags           List of tags
+- :source-buffer  The source buffer
+- :source-point   The point in source buffer"
+  (let ((inhibit-read-only t)
+        (current-point (point))
+        (current-values (org-supertag-tag--collect-current-values)))
+    (erase-buffer)
+    (when org-supertag--pending-operations
+      (insert "Pending operations:\n")
+      (dolist (op org-supertag--pending-operations)
+        (pcase (car op)
+          ('remove-field
+           (insert (format "  - Remove field '%s'\n" (cddr op))))
+          (_ (insert (format "  - Unknown operation: %S\n" op)))))
+      (insert "\n"))
+    
+    (insert (format "Fields for Node: %s\n" 
+                    (plist-get context :node-title)))
+    (dolist (tag-id (plist-get context :tags))
+      (org-supertag-tag--insert-tag-fields tag-id
+                                          (plist-get context :source-buffer)
+                                          (plist-get context :source-point)))
+    
+    (insert "\nCommands:\n")
+    (insert "n: Next field    p: Previous field    RET: Edit field\n")
+    (insert "C-c C-c: Save and apply changes    C-c C-k: Cancel\n")
+    (org-supertag-tag--restore-field-values current-values)
+    (goto-char (min current-point (point-max)))))
+
+;;----------------------------------------------------------------------
+;; Tag Field Operation
+;;----------------------------------------------------------------------
+
+(defun org-supertag--sanitize-field-name (name)
+  "Convert field name to valid property name.
+NAME is the original field name.
+
+This function:
+1. Trims leading/trailing whitespace
+2. Replaces spaces with underscores
+3. Converts to uppercase"
+  (let ((sanitized (replace-regexp-in-string
+                   "\\s-+" "_"  ; Replace spaces with underscores
+                   (upcase (string-trim name)))))  ; Convert to uppercase and trim
+    sanitized))
+
+(defun org-supertag-tag-get-nodes (tag-id)
+  "Get all node IDs associated with a tag.
+TAG-ID: The tag identifier (without 'tag:' prefix)
+
+Returns a list of node IDs that have this tag attached."
+  (let (result)
+    (maphash
+     (lambda (link-id props)
+       (message "Checking link: %s with props: %s" link-id props)
+       (when (and (string-prefix-p ":node-tag:" link-id)
+                  (equal (plist-get props :to) tag-id))  ; Direct comparison, no prefix handling
+         (push (plist-get props :from) result)))
+     org-supertag-db--link)
+    (delete-dups result)))
+
+(defun org-supertag-tag-get-fields (tag-id)
+  "Get list of fields defined for a tag.
+TAG-ID: The tag identifier"
+  (plist-get (org-supertag-tag-get tag-id) :fields))
+
+(defun org-supertag-tag-add-field (tag-id field-def)
+  "Add a field to a tag.
+TAG-ID: The tag identifier
+FIELD-DEF: The field definition plist"
+  (let* ((tag (org-supertag-db-get tag-id))
+         (fields (plist-get tag :fields))
+         (sanitized-field-def 
+          (let ((field-name (plist-get field-def :name)))
+            (setq field-def 
+                  (plist-put (copy-sequence field-def)
+                            :name 
+                            (org-supertag--sanitize-field-name field-name)))))
+         (new-fields (append fields (list sanitized-field-def)))
+         (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
+    (org-supertag-db-add tag-id new-tag)))
+
+(defun org-supertag-tag-get-field-value (tag field-name)
+  "Get field value from TAG with FIELD-NAME.
+TAG is the tag entity
+FIELD-NAME is the name of the field to get"
+  (when-let* ((fields (plist-get tag :fields))
+              (field (cl-find field-name fields
+                             :key (lambda (f) (plist-get f :name))
+                             :test #'equal)))
+    (plist-get field :value)))
+
+(defun org-supertag-tag--set-field-value (tag-id node-id field-name value)
+  "Set field value for TAG-ID on NODE-ID.
+FIELD-NAME is the name of the field
+VALUE is the value to set"
+  (let ((tag (org-supertag-tag-get tag-id)))
+    (when tag
+      ;; Update field value in tag definition
+      (let* ((fields (plist-get tag :fields))
+             (field (cl-find field-name fields
+                            :key (lambda (f) (plist-get f :name))
+                            :test #'equal)))
+        (when field
+          (setf (plist-get field :value) value)
+          ;; Update tag in database
+          (org-supertag-db-add tag-id tag)
+          ;; Run hook
+          (run-hook-with-args 'org-supertag-node-field-updated-hook
+                             node-id field-name value))))))
+
+(defun org-supertag-tag--queue-remove-field (tag-id field-name)
+  "将字段删除操作加入队列。
+TAG-ID: 标签ID
+FIELD-NAME: 字段名称"
+  (push `(remove-field . (,tag-id . ,field-name))
+        org-supertag--pending-operations)
+  (message "Field '%s' marked for removal. Press C-c C-c to apply changes." 
+           field-name))
+
+;;----------------------------------------------------------------------
+;; Field Edit Mode
+;;----------------------------------------------------------------------  
+
+(defun org-supertag-tag--add-field (tag-id)
+  "Add a new field to TAG-ID."
+  (let* ((raw-field-name (read-string "Field name: "))
+         (sanitized-name (org-supertag--sanitize-field-name raw-field-name))
+         ;; 显示标准化后的名称
+         (_ (message "Field name will be standardized to: %s" sanitized-name))
+         (all-types (mapcar #'car org-supertag-field-types))
+         (type-help
+          (concat
+           "Available types:\n"
+           (mapconcat
+            (lambda (type)
+              (let ((desc (plist-get (cdr (assq type org-supertag-field-types))
+                                   :description)))
+                (format "  %-12s - %s" type desc)))
+            all-types
+            "\n")))
+         (field-type
+          (minibuffer-with-setup-hook
+              (lambda ()
+                (let ((inhibit-read-only t))
+                  (with-current-buffer (get-buffer-create "*Minibuf Help*")
+                    (erase-buffer)
+                    (insert type-help)
+                    (display-buffer (current-buffer)
+                                  '((display-buffer-in-side-window)
+                                    (side . bottom)
+                                    (window-height . fit-window-to-buffer))))))
+            (intern
+             (completing-read 
+              "Field type: "
+              (mapcar #'symbol-name all-types)
+              nil t))))
+         (field-def
+          (list :name sanitized-name  ; 使用标准化的名称
+                :type field-type)))
+
+    (when-let* ((help-buf (get-buffer "*Minibuf Help*")))
+      (kill-buffer help-buf))
+    
+    ;; 添加 options 类型的选项
+    (when (eq field-type 'options)
+      (let ((options
+             (split-string
+              (read-string "Options (comma separated): ")
+              "," t "[ \t]+")))
+        (setq field-def 
+              (plist-put field-def :options options))))
+    
+    ;; 添加描述
+    (let ((desc (read-string "Description (optional): ")))
+      (when (not (string-empty-p desc))
+        (setq field-def 
+              (plist-put field-def :description desc))))
+    
+    ;; 添加字段到标签
+    (org-supertag-tag-add-field tag-id field-def)))
+
+(defun org-supertag-tag--edit-field (tag-id field-name)
+  "Edit an existing field in TAG-ID with FIELD-NAME."
+  (let* ((tag (org-supertag-tag-get tag-id))
+         (fields (plist-get tag :fields))
+         (field (cl-find field-name fields
+                        :key (lambda (f) (plist-get f :name))
+                        :test #'equal))
+         (action (completing-read 
+                 "Action: "
+                 '("Edit type" "Edit options" "Edit description")
+                 nil t)))
+    (pcase action
+      ("Edit type"
+       (let* ((all-types (mapcar #'car org-supertag-field-types))
+              (type-help
+               (concat
+                "Available types:\n"
+                (mapconcat
+                 (lambda (type)
+                   (let ((desc (plist-get (cdr (assq type org-supertag-field-types))
+                                        :description)))
+                     (format "  %-12s - %s" type desc)))
+                 all-types
+                 "\n")))
+              (new-type
+               (minibuffer-with-setup-hook
+                   (lambda ()
+                     (let ((inhibit-read-only t))
+                       (with-current-buffer (get-buffer-create "*Minibuf Help*")
+                         (erase-buffer)
+                         (insert type-help)
+                         (display-buffer (current-buffer)
+                                       '((display-buffer-in-side-window)
+                                         (side . bottom)
+                                         (window-height . fit-window-to-buffer))))))
+                 (intern
+                  (completing-read 
+                   "New type: "
+                   (mapcar #'symbol-name all-types)
+                   nil t)))))
+
+         (when-let* ((help-buf (get-buffer "*Minibuf Help*")))
+           (kill-buffer help-buf))
+         
+         (setq field (plist-put field :type new-type))
+         (when (eq new-type 'options)
+           (let ((options
+                  (split-string
+                   (read-string "Options (comma separated): ")
+                   "," t "[ \t]+")))
+             (setq field (plist-put field :options options))))
+         ;; Update field value in tag definition
+         (let ((new-fields (cl-substitute field
+                                        (cl-find field-name fields
+                                                :key (lambda (f) (plist-get f :name))
+                                                :test #'equal)
+                                        fields
+                                        :test #'equal)))
+           ;; Update tag in database
+           (setq tag (plist-put tag :fields new-fields))
+           (org-supertag-db-add tag-id tag))))
+      
+      ("Edit options"
+       (when (eq (plist-get field :type) 'options)
+         (let ((new-options
+                (split-string
+                 (read-string "New options (comma separated): "
+                            (mapconcat #'identity 
+                                     (plist-get field :options)
+                                     ","))
+                 "," t "[ \t]+")))
+           (setq field (plist-put field :options new-options))
+           ;; Update field value in tag definition
+           (let ((new-fields (cl-substitute field
+                                          (cl-find field-name fields
+                                                  :key (lambda (f) (plist-get f :name))
+                                                  :test #'equal)
+                                          fields
+                                          :test #'equal)))
+             ;; Update tag in database
+             (setq tag (plist-put tag :fields new-fields))
+             (org-supertag-db-add tag-id tag)))))
+      
+      ("Edit description"
+       (let ((new-desc (read-string "New description: "
+                                   (plist-get field :description))))
+         (setq field (plist-put field :description new-desc))
+         ;; Update field value in tag definition
+         (let ((new-fields (cl-substitute field
+                                        (cl-find field-name fields
+                                                :key (lambda (f) (plist-get f :name))
+                                                :test #'equal)
+                                        fields
+                                        :test #'equal)))
+           ;; Update tag in database
+           (setq tag (plist-put tag :fields new-fields))
+           (org-supertag-db-add tag-id tag)))))))
+
+(defun org-supertag-field-edit-next-field ()
+  "Move to next editable field."
+  (interactive)
+  (forward-line)
+  (when (looking-at "^$")
+    (forward-line)))
+
+(defun org-supertag-field-edit-prev-field ()
+  "Move to previous editable field."
+  (interactive)
+  (forward-line -1)
+  (when (looking-at "^$")
+    (forward-line -1)))
+
+(defun org-supertag-field-edit-complete-field ()
+  "Edit the current field value."
+  (interactive)
+  (when (looking-at "^  \\([^[]+\\)\\[\\([^]]+\\)\\]: \\(.*\\)$")
+    (let* ((field-name (string-trim (match-string 1)))
+           (field-type (match-string 2))
+           (current-value (match-string 3))
+           ;; Fix tag ID extraction to only get the tag name
+           (tag-id (save-excursion
+                    (beginning-of-line)
+                    (re-search-backward "^Tag: \\([^ \n]+\\)" nil t)
+                    (match-string-no-properties 1)))
+           (tag (org-supertag-tag-get tag-id)))
+      
+      (unless tag
+        (error "Tag '%s' not found" tag-id))
+      
+      (let ((field (cl-find field-name 
+                           (plist-get tag :fields)
+                           :key (lambda (f) (plist-get f :name))
+                           :test #'equal)))
+        ;; Validate field exists and has a type defined
+        (unless field
+          (error "Field '%s' not found in tag '%s'" field-name tag-id))
+        (unless (plist-get field :type)
+          (error "Field '%s' has no type defined" field-name))
+        
+        ;; Read new value
+        (when-let* ((new-value (org-supertag-field-read-value field)))
+          (save-excursion
+            (beginning-of-line)
+            (when (re-search-forward ": .*$" nil t)
+              (replace-match (format ": %s" new-value)))))))))
+
+(defun org-supertag-field-edit-save ()
+  "Save all field values and close the edit buffer."
+  (interactive)
+  (let* ((context org-supertag--edit-context)
+         (source-buffer (plist-get context :source-buffer))
+         (source-point (plist-get context :source-point))
+         (modified-buffers (make-hash-table :test 'equal))
+         (node-id (plist-get context :node-id))
+         (tags (plist-get context :tags)))
+    
+    ;; 1. 收集需要修改的缓冲区并禁用 org-mode 功能
+    (dolist (op org-supertag--pending-operations)
+      (pcase (car op)
+        ('remove-field
+         (let* ((tag-id (cadr op))
+                (field-name (cddr op))
+                (nodes (org-supertag-tag-get-nodes tag-id)))
+           (dolist (node-id nodes)
+             (when-let* ((marker (org-id-find node-id t))
+                        (buf (if (markerp marker)
+                               (marker-buffer marker)
+                             (current-buffer))))
+               (puthash (buffer-file-name buf) buf modified-buffers)))))))
+    
+    ;; 2. 禁用 org-mode 功能
+    (maphash (lambda (_file buf)
+               (with-current-buffer buf
+                 (setq-local org-element-use-cache nil)))
+             modified-buffers)
+    
+    ;; 3. 执行待处理操作
+    (dolist (op org-supertag--pending-operations)
+      (pcase (car op)
+        ('remove-field
+         (let* ((tag-id (cadr op))
+                (field-name (cddr op))
+                (tag (org-supertag-db-get tag-id))
+                (fields (plist-get tag :fields))
+                (new-fields (cl-remove-if 
+                           (lambda (field)
+                             (equal (plist-get field :name) field-name))
+                           fields))
+                (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
+           ;; 更新数据库
+           (org-supertag-db-add tag-id new-tag)
+           
+           ;; 更新文件
+           (dolist (node-id (org-supertag-tag-get-nodes tag-id))
+             (org-supertag-db-delete-field-value node-id field-name tag-id)
+             (when-let* ((marker (org-id-find node-id t)))
+               (with-current-buffer (if (markerp marker)
+                                      (marker-buffer marker)
+                                    (current-buffer))
+                 (save-excursion
+                   (save-restriction
+                     (widen)
+                     (goto-char (if (markerp marker)
+                                  (marker-position marker)
+                                marker))
+                     (org-delete-property field-name))))))))))
+    
+    ;; 4. 保存当前显示的字段值
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^  \\([^[]+\\)\\[\\([^]]+\\)\\]: \\(.*\\)$" nil t)
+        (let* ((field-name (string-trim (match-string 1)))
+               (field-type (match-string 2))
+               (new-value (string-trim (match-string 3)))
+               ;; 获取当前字段所属的标签ID
+               (tag-id (save-excursion
+                        (re-search-backward "^Tag: \\([^ \n]+\\)" nil t)
+                        (match-string-no-properties 1)))
+               ;; 获取标签和字段定义
+               (tag (when tag-id (org-supertag-tag-get tag-id)))
+               (field (when tag
+                       (cl-find field-name
+                               (plist-get tag :fields)
+                               :key (lambda (f) (plist-get f :name))
+                               :test #'equal))))
+          (unless (string-empty-p new-value)
+            ;; 更新 org 文件
+            (with-current-buffer source-buffer
+              (save-excursion
+                (goto-char source-point)
+                (org-set-property field-name new-value)))
+            ;; 更新数据库
+            (when (and tag-id field)
+              (org-supertag-db-set-field-value node-id field-name new-value tag-id))))))
+    
+    ;; 5. 保存并重新加载修改过的缓冲区
+    (maphash (lambda (_file buf)
+               (with-current-buffer buf
+                 (save-buffer)
+                 ;; 重新启用缓存
+                 (setq-local org-element-use-cache t)
+                 ;; 重新加载 org-mode
+                 (when (derived-mode-p 'org-mode)
+                   (org-mode))))
+             modified-buffers)
+    
+    ;; 6. 清理并退出
+    (setq org-supertag--pending-operations nil)
+    (quit-window t)))
+
+(defun org-supertag-field-edit-cancel ()
+  "Cancel editing and close the buffer."
+  (interactive)
+  (when (yes-or-no-p "Cancel editing? Changes will be lost.")
+    (setq org-supertag--pending-operations nil)
+    (quit-window t)))
+
+(defvar org-supertag-field-edit-mode-map
+  (let ((map (make-sparse-keymap)))
+    ;; Basic navigation and editing
+    (define-key map (kbd "n") #'org-supertag-field-edit-next-field)
+    (define-key map (kbd "p") #'org-supertag-field-edit-prev-field)
+    (define-key map (kbd "RET") #'org-supertag-field-edit-complete-field)
+    (define-key map (kbd "C-c C-c") #'org-supertag-field-edit-save)
+    (define-key map (kbd "C-c C-k") #'org-supertag-field-edit-cancel)
+    
+    ;; Field operations
+    (define-key map (kbd "a")
+      (lambda ()
+        (interactive)
+        (let ((tag-id (save-excursion
+                       (re-search-backward "^Tag: \\([^ \n]+\\)" nil t)
+                       (match-string-no-properties 1))))
+          (when tag-id
+            (org-supertag-tag--add-field tag-id)
+            (org-supertag-tag--refresh-field-table org-supertag--edit-context)))))
+    
+    (define-key map (kbd "e")
+      (lambda ()
+        (interactive)
+        (save-excursion
+          (beginning-of-line)
+          (when (looking-at "^  \\([^ ]+\\) \\[")
+            (let* ((field-name (match-string 1))
+                   (tag-id (save-excursion
+                            (re-search-backward "^Tag: \\([^ \n]+\\)" nil t)
+                            (match-string-no-properties 1))))
+              (when (and tag-id field-name)
+                (org-supertag-tag--edit-field tag-id field-name)
+                (org-supertag-tag--refresh-field-table org-supertag--edit-context)))))))
+    
+    (define-key map (kbd "d")
+      (lambda ()
+        (interactive)
+        (org-supertag-field-edit-mode-remove-field)))
+    map))
+
+(define-derived-mode org-supertag-field-edit-mode special-mode "OrgSuperTag-FieldEdit"
+  "Major mode for editing org-supertag fields."
+  (setq-local buffer-read-only nil))
+
+(defun org-supertag-tag--collect-current-values ()
+  "Collect current field values from the edit buffer.
+Returns an alist of (field-name . value) pairs."
+  (let (result)
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^  \\([^[]+\\)\\[\\([^]]+\\)\\]: \\(.*\\)$" nil t)
+        (let ((field-name (string-trim (match-string 1)))
+              (value (string-trim (match-string 3))))
+          (push (cons field-name value) result))))
+    result))
+
+(defun org-supertag-tag--restore-field-values (values)
+  "Restore field values in the edit buffer.
+VALUES is an alist of (field-name . value) pairs."
+  (when values
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^  \\([^[]+\\)\\[\\([^]]+\\)\\]: " nil t)
+        (let* ((field-name (string-trim (match-string 1)))
+               (saved-value (cdr (assoc field-name values))))
+          (when saved-value
+            (insert saved-value)))))))
+
+(defun org-supertag-tag--insert-tag-fields (tag-id source-buffer source-point)
+  "Insert fields for a tag."
+  (let* ((tag (org-supertag-tag-get tag-id))
+         (base-tag (org-supertag-tag-get-base tag-id))
+         (fields (plist-get tag :fields))
+         (max-name-length 
+          (if fields
+              (apply #'max 
+                     (mapcar (lambda (f) 
+                             (length (plist-get f :name)))
+                            fields))
+            0))
+         (max-type-length
+          (if fields
+              (apply #'max 
+                     (mapcar (lambda (f)
+                             (length (symbol-name (plist-get f :type))))
+                            fields))
+            0)))
+    (insert (format "Tag: %s" tag-id))
+    (when base-tag
+      (insert (format " (extends %s)" base-tag)))
+    (insert "\n")
+    (insert "──────────────────────────────────────────────\n")
+    (if fields
+        (progn
+          (dolist (field fields)
+            (let* ((field-name (plist-get field :name))
+                   (field-type (plist-get field :type))
+                   (current-value 
+                    (with-current-buffer source-buffer
+                      (save-excursion
+                        (goto-char source-point)
+                        (org-entry-get (point) field-name nil t))))
+                   ;; 3. Use fixed width formatting for fields
+                   (formatted-line
+                    (format (format "  %%-%ds [%%-%ds]: %%s\n" 
+                                  max-name-length 
+                                  max-type-length)
+                           field-name
+                           (symbol-name field-type)
+                           (or current-value ""))))
+              (insert formatted-line)))
+          (insert "  [Press 'a' to add, 'e' to edit, 'd' to delete fields]\n"))
+      ;; If no fields defined
+      (insert "  No fields defined\n")
+      (insert "  [Press 'a' to add fields]\n"))))
+
+
+(defun org-supertag-tag--goto-first-field ()
+  "Move cursor to the first field in the OrgSuperTag-FieldEdit buffer.
+If no field exists, move to the first line after the separator line."
+  (let ((field-buffer (get-buffer "*Org SuperTag Fields*")))
+    (when field-buffer
+      (with-current-buffer field-buffer
+        (goto-char (point-min))
+        ;; Find first separator line
+        (if (re-search-forward "^─+$" nil t)
+            (progn
+              ;; Try to find first field
+              (forward-line 1)
+              (if (looking-at "^  \\([^[]+\\)\\[\\([^]]+\\)\\]: \\(.*\\)$")
+                  (beginning-of-line)
+                ;; If no field, stay at current line
+                t))
+          ;; If no separator found, go to beginning
+          (goto-char (point-min)))))))
+
+(defun org-supertag-tag-edit-fields ()
+  "Edit fields of current node's tags in a table format.
+Creates a dedicated buffer showing all fields of the node's tags in a table view,
+allowing for easy editing of field values."
+  (interactive)
+  (let* ((source-buffer (current-buffer))
+         (source-point (point))
+         (node-id (org-id-get))
+         (tags (org-supertag-node-get-tags node-id))
+         (edit-buffer (get-buffer-create "*Org SuperTag Fields*"))
+         (node-title (org-get-heading t t t t)))
+    
+    ;; Validate we have a node and tags
+    (unless node-id
+      (error "Not on a valid node"))
+    (unless tags
+      (error "No tags on current node"))
+    
+    ;; Prepare the edit buffer
+    (with-current-buffer edit-buffer
+      (let ((inhibit-read-only t))
+        (erase-buffer)
+        (org-supertag-field-edit-mode)
+        
+        ;; Store context
+        (setq-local org-supertag--edit-context
+                    (list :node-id node-id
+                          :node-title node-title
+                          :tags tags
+                          :source-buffer source-buffer
+                          :source-point source-point))
+        
+        ;; Initial content
+        (org-supertag-tag--refresh-field-table org-supertag--edit-context)))
+    
+    ;; Display buffer and position cursor
+    (let ((window (display-buffer edit-buffer
+                                '((display-buffer-below-selected)
+                                  (window-height . fit-window-to-buffer)
+                                  (preserve-size . (nil . t))
+                                  (select . t)))))
+      (select-window window)
+      (org-supertag-tag--goto-first-field))))
+
+(defun org-supertag-field-edit-mode-remove-field ()
+  "Queue the removal of the field at point."
+  (interactive)
+  (save-excursion
+    (beginning-of-line)
+    ;; 1. 首先尝试匹配已经标记为删除的行
+    ;; 2. 然后尝试匹配普通字段行
+    ;; 3. 使用更宽松的空白匹配
+    (when (or (looking-at "^.*\\[TO BE REMOVED\\].*\\([^ \n[]+\\).*\\[\\([^]]+\\)\\]")
+              (looking-at "^[[:blank:]]*\\([^ \n[]+\\)[[:blank:]]*\\[\\([^]]+\\)\\]"))
+      (let* ((field-name (match-string-no-properties 1))
+             (tag-id (save-excursion
+                      (re-search-backward "^Tag: \\([^ \n]+\\)" nil t)
+                      (match-string-no-properties 1))))
+        (when (and tag-id field-name)
+          ;; 检查字段是否已标记为删除
+          (unless (save-excursion
+                   (beginning-of-line)
+                   (looking-at ".*\\[TO BE REMOVED\\]"))
+            (when (yes-or-no-p (format "Mark field '%s' for removal? " field-name))
+              (org-supertag-tag--queue-remove-field tag-id field-name)
+              (let ((inhibit-read-only t))
+                (beginning-of-line)
+                ;; 保存当前行的内容
+                (let ((line-content (buffer-substring (line-beginning-position)
+                                                    (line-end-position))))
+                  ;; 删除当前行
+                  (delete-region (line-beginning-position)
+                               (line-end-position))
+                  ;; 插入带有删除标记的行
+                  (insert "  [TO BE REMOVED] " line-content))
+                ;; 移动到下一个可能的字段
+                (forward-line)
+                (while (and (not (eobp))
+                           (not (looking-at "[[:blank:]]*\\([^ \n[]+\\)[[:blank:]]*\\["))
+                           (not (looking-at ".*\\[TO BE REMOVED\\].*\\["))
+                           (not (looking-at "^Tag:")))
+                  (forward-line))))))))))
+
+
+(provide 'org-supertag-field-operation)
\ No newline at end of file
diff --git a/org-supertag-field.el b/org-supertag-field.el
index c05f02d..460d477 100755
--- a/org-supertag-field.el
+++ b/org-supertag-field.el
@@ -13,54 +13,48 @@
 (require 'org)
 (require 'org-element)
 (require 'org-supertag-db)
-(require 'org-supertag-util)
 
 ;;------------------------------------------------------------------------------
 ;; Field Value Database Operations
 ;;------------------------------------------------------------------------------
 
-(defun org-supertag-field-set-value (node-id field-name value &optional tag-id)
-  "Set a field value for a given node in the database.
-This function creates or updates the node-field link.
-If VALUE is nil or an empty string, the field is removed.
-TAG-ID is optional. If provided, the field is associated with a specific tag."
-  (let ((link-id (format ":node-field:%s->%s" node-id field-name)))
-    (if (and value (not (string-empty-p value)))
-        (let ((props (list :type :node-field
-                           :from node-id
-                           :to field-name
-                           :value value
-                           :created-at (current-time))))
-          (when tag-id
-            (setq props (plist-put props :tag-id tag-id)))
-          (puthash link-id props org-supertag-db--link))
-      ;; If value is nil or empty, remove the link.
-      (remhash link-id org-supertag-db--link))
-    ;; Mark the database as dirty and schedule a save.
+(defun org-supertag-db-set-field-value (node-id field-name value tag-id)
+  "Set field value to database.
+NODE-ID: Node identifier
+FIELD-NAME: Field name
+VALUE: Field value
+TAG-ID: Associated tag identifier"
+(let ((link-id (format ":node-field:%s->%s" node-id field-name))
+        (props (list :from node-id
+                    :to field-name
+                    :tag-id tag-id
+                    :value value
+                    :created-at (current-time))))
+    (message "Debug: Setting field value - link-id=%S props=%S" link-id props)
+    (puthash link-id props org-supertag-db--link)
     (org-supertag-db--mark-dirty)
-    (org-supertag-db--schedule-save)))
+    (org-supertag-db--schedule-save)
+    (message "Debug: Field value set - current value=%S"
+             (gethash link-id org-supertag-db--link))))
 
-(defun org-supertag-field-get-value (node-id field-name &optional tag-id)
+(defun org-supertag-db-get-field-value (node-id field-name tag-id)
   "Get field value from database.
-If TAG-ID is provided, it will ensure the field belongs to that tag."
+NODE-ID: Node identifier
+FIELD-NAME: Field name
+TAG-ID: Associated tag identifier"
   (let* ((link-id (format ":node-field:%s->%s" node-id field-name))
          (link (gethash link-id org-supertag-db--link)))
-    (when (and link
-               ;; If tag-id is provided, match it. If not, ignore tag-id.
-               (or (not tag-id) (equal (plist-get link :tag-id) tag-id)))
+    (when (and link 
+               (equal (plist-get link :tag-id) tag-id))
       (plist-get link :value))))
 
-(defun org-supertag-field-delete-value (node-id field-name &optional tag-id)
+(defun org-supertag-db-delete-field-value (node-id field-name tag-id)
   "Delete field value from database.
-If TAG-ID is provided, it will only delete if the field belongs to that tag."
-  (let* ((link-id (format ":node-field:%s->%s" node-id field-name))
-         (link (gethash link-id org-supertag-db--link)))
-    (when (and link
-               (or (not tag-id) (equal (plist-get link :tag-id) tag-id)))
-      (remhash link-id org-supertag-db--link)
-      (org-supertag-db--mark-dirty)
-      (org-supertag-db--schedule-save))))
-
+NODE-ID: Node identifier
+FIELD-NAME: Field name
+TAG-ID: Associated tag identifier"
+  (let ((link-id (format ":node-field:%s->%s" node-id field-name)))
+    (remhash link-id org-supertag-db--link)))
 
 ;;----------------------------------------------------------------------
 ;; Field Type System
@@ -87,23 +81,18 @@ If TAG-ID is provided, it will only delete if the field belongs to that tag."
              :formatter org-supertag-format-timestamp
              :reader org-supertag-read-timestamp-field
              :description "Timestamp"))
-    (url . (:validator org-supertag-validate-url
-            :formatter org-supertag-format-url
-            :reader org-supertag-read-url-field
-            :description "URL"))
-    (email . (:validator org-supertag-validate-email
-              :formatter org-supertag-format-email
-              :reader org-supertag-read-email-field
-              :description "Email"))
-    (tag . (:validator org-supertag-validate-tag
-            :formatter org-supertag-format-tag
-            :reader org-supertag-read-tag-field
-            :description "Tag Reference"))
-    ;; (range . (:validator org-supertag-validate-range
-    ;;           :formatter org-supertag-format-range
-    ;;           :reader org-supertag-read-range-field
-    ;;           :description "Number Range"))
-)
+    (list . (:validator org-supertag-validate-list
+             :formatter org-supertag-format-list
+             :reader org-supertag-read-list-field
+             :description "List"))
+    (range . (:validator org-supertag-validate-range
+              :formatter org-supertag-format-range
+              :reader org-supertag-read-range-field
+              :description "Number Range"))
+    (behavior . (:validator org-supertag-validate-behavior
+                 :formatter org-supertag-format-behavior
+                 :reader org-supertag-read-behavior-field
+                 :description "Tag Behavior")))
   "Field type definition.
 Each type is a cons cell, car is the type name (symbol), cdr is the type definition plist, containing:
 - :validator   Function to validate field value
@@ -131,8 +120,6 @@ Return the type definition plist, containing :validator, :formatter, :reader and
     (message "Debug - Field type definition: %S" type-def)
     type-def))
 
-
-
 ;;----------------------------------------------------------------------
 ;; Field Value Validators and Formatters
 ;;----------------------------------------------------------------------
@@ -247,42 +234,18 @@ VALUE can be either:
   (and (stringp value)
        (string-match-p "^[^@]+@[^@]+\\.[^@]+$" value)))
 
-(defun org-supertag-format-email (value &optional _field)
-  "Format email VALUE.
-VALUE is the email value to format.
-FIELD is the field definition (optional)."
-  (when (and value (stringp value))
-    (string-trim value)))
+(defun org-supertag-format-email (value)
+  "Format email VALUE."
+  (string-trim value))
 
 (defun org-supertag-validate-url (value)
   "Validate URL VALUE."
   (and (stringp value)
        (string-match-p "^https?://" value)))
 
-(defun org-supertag-format-url (value &optional _field)
-  "Format URL VALUE.
-VALUE is the URL value to format.
-FIELD is the field definition (optional)."
-  (when (and value (stringp value))
-    (string-trim value)))
-
-(defun org-supertag-validate-tag (value &optional _field)
-  "Validate tag VALUE. Can be a single tag string or a list of tags.
-VALUE can contain any characters except colons and whitespace."
-  (if (listp value)
-      ;; If it's a list, validate every element
-      (cl-every (lambda (v) (org-supertag-validate-tag v)) value)
-    ;; If it's a single value, perform the original validation
-    (when value
-      (and (stringp value)
-           (not (string-empty-p (string-trim value)))
-           (not (string-match-p "[:[:space:]]" (string-trim value)))))))
-
-(defun org-supertag-format-tag (value &optional _field-def)
-  "Format tag value to a single string (first tag from list if multiple)."
-  (if (listp value)
-      (car value)
-    value))
+(defun org-supertag-format-url (value)
+  "Format URL VALUE."
+  (string-trim value))
 
 (defun org-supertag-validate-options (value &optional field-def)
   "Validate options value.
@@ -306,7 +269,7 @@ Returns a trimmed string if VALUE is a string, otherwise returns nil."
   (when (and value (stringp value))  ; Only process if value is a non-nil string
     (string-trim value)))
 
-(defun org-supertag-validate-number (value &optional _field)
+(defun org-supertag-validate-number (value)
   "Validate numeric value.
 VALUE can be a number or numeric string."
   (or (numberp value)
@@ -325,28 +288,59 @@ FIELD is the field definition."
           value
         (number-to-string (string-to-number value))))))
 
+(defun org-supertag-validate-list (value)
+  "Validate list value.
+VALUE is the value to validate."
+  (message "Debug - Validating list value: %S" value)
+  (or (listp value)
+      (and (stringp value)
+           (string-match-p "^\\[.*\\]$" value))))
 
-
-;; (defun org-supertag-validate-range (value)
-;;   "Validate range value.
-;; VALUE should be in 'min-max' format."
-;;   (when value  ; Allow nil value
-;;     (condition-case nil
-;;         (let* ((parts (split-string value "-"))
-;;                (min (string-to-number (car parts)))
-;;                (max (string-to-number (cadr parts))))
-;;           (and (= (length parts) 2)     ; Must have two parts
-;;                (numberp min)             ; Min must be number
-;;                (numberp max)             ; Max must be number
-;;                (< min max)))            ; Min must be less than max
-;;       (error nil))))
-
-;; (defun org-supertag-format-range (value &optional _field)
-;;   "Format range VALUE.
-;; VALUE should be in 'min-max' format.
-;; FIELD is the field definition (optional)."
-;;   (when (and value (stringp value))
-;;     (string-trim value)))
+(defun org-supertag-format-list (value &optional _field)
+  "Format list value.
+VALUE is the value to format.
+FIELD is the field definition."
+  (message "Debug - Formatting list value: %S" value)
+  (cond
+   ((null value) "[]")
+   ((listp value)
+    (format "[%s]" 
+            (mapconcat (lambda (item) 
+                        (format "\"%s\"" item))
+                      value ",")))
+   ((stringp value)
+    (if (string-match-p "^\\[.*\\]$" value)
+        (let ((json-array-type 'list)
+              (json-false nil))
+          (condition-case err
+              (let ((parsed (json-read-from-string value)))
+                (org-supertag-format-list parsed))
+            (error value)))  ; If parsing fails, return original
+      (format "[%s]" value)))
+   (t (format "[%s]" value))))
+
+(defun org-supertag-read-list-field (prompt)
+  "Read list field value.
+PROMPT is the prompt message."
+  (message "Debug - Reading list field with prompt: %s" prompt)
+  (let* ((input (read-string (format "%s (comma separated): " prompt)))
+         (values (split-string input "," t "[ \t\n\r]+")))
+    (message "Debug - List field input: %S -> %S" input values)
+    values))
+
+(defun org-supertag-validate-range (value)
+  "Validate range value.
+VALUE should be in 'min-max' format."
+  (when value  ; Allow nil value
+    (condition-case nil
+        (let* ((parts (split-string value "-"))
+               (min (string-to-number (car parts)))
+               (max (string-to-number (cadr parts))))
+          (and (= (length parts) 2)     ; Must have two parts
+               (numberp min)             ; Min must be number
+               (numberp max)             ; Max must be number
+               (< min max)))            ; Min must be less than max
+      (error nil))))
 
 
 ;;---------------------------------------------------------------------------
@@ -354,19 +348,21 @@ FIELD is the field definition."
 ;;---------------------------------------------------------------------------
 
 (defun org-supertag-field-get-initial-value (field)
-  "Get initial value for FIELD.
-Only returns a value if explicitly specified in the field definition."
+  "Get initial value for FIELD."
   (let ((field-type (plist-get field :type))
         (field-value (plist-get field :value))
-        (field-default (plist-get field :default)))
+        (field-options (plist-get field :options)))
     (message "Getting initial value for field: %S" field)
     (let ((initial-value
            (cond
-            ;; If there's an explicit default value, use it
-            (field-default field-default)
-            ;; If there's a value in the field definition, use it
-            (field-value field-value)
-            ;; Otherwise return nil
+            ((eq field-type 'behavior)
+             field-value)
+            ((and (eq field-type 'options) field-options)
+             (car field-options))
+            ((eq field-type 'date)
+             (format-time-string "%Y-%m-%d"))
+            ((eq field-type 'string)
+             " ")  
             (t nil))))
       (message "Initial value for field %s (%s): %S" 
                (plist-get field :name)
@@ -374,7 +370,80 @@ Only returns a value if explicitly specified in the field definition."
                initial-value)
       initial-value)))
 
+;;------------------------------------------------------------------------------
+;; Behavior Field Type
+;;------------------------------------------------------------------------------
 
+(defun org-supertag-validate-behavior (value)
+  "Validate behavior field value.
+VALUE should be a plist with :trigger and either :action or :style."
+  (and (plistp value)
+       (plist-member value :trigger)
+       (memq (plist-get value :trigger) 
+             '(:on-add :on-remove :on-change :on-schedule :always))
+       (or (functionp (plist-get value :action))
+           (plistp (plist-get value :style)))))
+
+(defun org-supertag-format-behavior (value &optional _field)
+  "Format behavior field value for display.
+VALUE can be either:
+- A string (behavior name)
+- A behavior plist
+
+Format example:
+- Behavior[@important]
+- Behavior[:on-add +action +style:📦]
+- Behavior[:always +action]"
+  (cond
+   ((stringp value)
+    (if-let* ((behavior (gethash value org-supertag-behavior-registry)))
+        (let ((trigger (plist-get behavior :trigger))
+              (has-action (plist-get behavior :action))
+              (style (plist-get behavior :style)))
+          (format "Behavior[%s%s%s]"
+                  (or trigger "nil")
+                  (if has-action " +action" "")
+                  (if style 
+                      (format " +style:%s" 
+                              (or (plist-get style :prefix) ""))
+                    "")))
+      (format "Behavior[%s]" value)))
+   
+   ((and (listp value) (keywordp (car value)))
+    (let ((trigger (plist-get value :trigger))
+          (has-action (plist-get value :action))
+          (style (plist-get value :style)))
+      (format "Behavior[%s%s%s]"
+              (or trigger "nil")
+              (if has-action " +action" "")
+              (if style 
+                  (format " +style:%s" 
+                          (or (plist-get style :prefix) ""))
+                ""))))
+   
+   (t "Behavior[nil]")))
+
+(defun org-supertag-read-behavior-field (prompt &optional initial)
+  "Read behavior field value.
+PROMPT is the prompt string
+INITIAL is the initial value."
+  (let* ((trigger (intern 
+                  (completing-read 
+                   "Trigger: "
+                   '("on-add" "on-remove" "on-change" "on-schedule" "always")
+                   nil t nil nil "on-add")))
+         (has-action (y-or-n-p "Add action? "))
+         (action (when has-action
+                  (read-from-minibuffer "Action (lambda form): "
+                                      "(lambda (node-id)\n  )")))
+         (has-style (y-or-n-p "Add style? "))
+         (style (when has-style
+                 (list :face (read-from-minibuffer "Face properties: "
+                                                  "(:foreground \"blue\")")
+                       :prefix (read-string "Prefix: " "📋")))))
+    (list :trigger trigger
+          :action (when has-action (eval (read action)))
+          :style style)))
 
 ;;----------------------------------------------------------------------
 ;; Read Field Value
@@ -405,17 +474,14 @@ FIELD is the field definition."
                         (funcall reader name options)
                       (funcall reader name)))
                    (typed-value (org-supertag-field--convert-value type input-value)))
-              (let ((validator (plist-get type-def :validator)))
-                (if (or (not validator) 
-                        (not (fboundp validator))
-                        (funcall validator typed-value field))
-                    (throw 'done 
-                           (if formatter
-                               (funcall formatter typed-value field)
-                             typed-value))
-                  (when (or required
-                           (y-or-n-p (format "Field %s validation failed. Retry? " name)))
-                    (sit-for 1)))))
+              (if (org-supertag-field-validate field typed-value)
+                  (throw 'done 
+                         (if formatter
+                             (funcall formatter typed-value field)
+                           typed-value))
+                (when (or required
+                         (y-or-n-p (format "Field %s validation failed. Retry? " name)))
+                  (sit-for 1))))
           (error
            (let ((err-msg (error-message-string err)))
              (message "Error - Processing field %s: %s" name err-msg)
@@ -442,113 +508,100 @@ VALUE: Value to convert."
           (message "Debug - No formatter found, using raw value")
           value)))))
 
-(defun org-supertag-field--validate-and-format-value (value field-def)
-  "Private helper to validate and format a VALUE based on FIELD-DEF."
-  (let* ((type (plist-get field-def :type))
-         (type-spec (org-supertag-get-field-type type))
-         (validator (plist-get type-spec :validator))
-         (formatter (plist-get type-spec :formatter)))
-    (when (or (not validator) (funcall validator value))
-      (if formatter
-          (funcall formatter value field-def)
-        value))))
-
-
-;;----------------------------------------------------------------------
-;; Field Value Readers
-;;----------------------------------------------------------------------
-
-(defun org-supertag-read-string-field (prompt current-value)
-  "Read a string value from the user."
-  (read-string prompt current-value))
-
-(defun org-supertag-read-number-field (prompt current-value)
-  "Read a number value from the user."
-  (read-string prompt current-value))
-
-(defun org-supertag-read-date-field (prompt current-value)
-  "Read a date value from the user."
-  (let ((date (org-read-date nil t nil prompt
-                             (when (and current-value (not (string-empty-p current-value)))
-                               current-value))))
-    (format-time-string "<%Y-%m-%d %a>" date)))
-
-(defun org-supertag-read-timestamp-field (prompt current-value)
-  "Read a timestamp value from the user."
-  (let ((date-time (org-read-date t t nil prompt
-                                  (when (and current-value (not (string-empty-p current-value)))
-                                    current-value))))
-    (format-time-string "[%Y-%m-%d %a %H:%M]" date-time)))
-
-(defun org-supertag-read-options-field (prompt current-value field-def)
-  "Read an options value from the user."
-  (let ((options (plist-get field-def :options)))
-    (completing-read prompt options nil t current-value)))
-
-(defun org-supertag-read-tag-field (prompt current-value)
-  "Read a tag reference from the user."
-  (let ((tag-names (org-supertag-get-all-tags)))
-    (completing-read prompt tag-names nil t current-value)))
-
-(defun org-supertag-read-url-field (prompt current-value)
-  "Read a URL from the user."
-  (read-string prompt current-value))
-
-(defun org-supertag-read-email-field (prompt current-value)
-  "Read an email from the user."
-  (read-string prompt current-value))
-
+(defun org-supertag-read-string-field (prompt)
+  "Read string field value.
+PROMPT is the prompt message."
+  (message "Debug - Reading string field with prompt: %s" prompt)
+  (let* ((raw-input (read-string (format "%s: " prompt)))
+         (trimmed-input (string-trim raw-input))
+         (result (if (string-empty-p trimmed-input)
+                    nil
+                  trimmed-input)))
+    (message "Debug - String field: raw-input=%S, trimmed=%S, result=%S"
+             raw-input trimmed-input result)
+    result))
+
+(defun org-supertag-read-date-field (prompt &optional default)
+  "Read date field value using org-mode's date reader.
+PROMPT is the prompt message.
+DEFAULT is the default value."
+  (let* ((input (org-read-date nil t nil prompt nil default))
+         (formatted-date (format-time-string "<%Y-%m-%d %a>" input)))
+    (message "Debug - Date input from org-read-date: %S -> %S" input formatted-date)
+    formatted-date))
+
+(defun org-supertag-read-timestamp-field (prompt)
+  "Read timestamp field value using org-mode's date reader.
+PROMPT is the prompt message."
+  (let* ((time (org-read-date t t))  ; Use org-mode time reader with time
+         (formatted-ts (format-time-string "[%Y-%m-%d %a %H:%M]" time)))
+    formatted-ts))
+
+(defun org-supertag-read-email-field (prompt &optional default)
+  "Read email field value.
+PROMPT is the prompt message.
+DEFAULT is the default value."
+  (let ((input (read-string (format "%s (example@domain.com)%s: "
+                                   prompt
+                                   (if default
+                                       (format " (default: %s)" default)
+                                     ""))
+                           nil nil default)))
+    (if (org-supertag-validate-email input)
+        input
+      (progn
+        (message "Invalid email address, please try again")
+        (sit-for 1)
+        (org-supertag-read-email-field prompt default)))))
+
+(defun org-supertag-read-url-field (prompt &optional default)
+  "Read URL field value.
+PROMPT is the prompt message
+DEFAULT is the default value"
+  (let ((input (read-string (format "%s (https://example.com)%s: "
+                                   prompt
+                                   (if default
+                                       (format " (default: %s)" default)
+                                     ""))
+                           nil nil default)))
+    (if (org-supertag-validate-url input)
+        input
+      (progn
+        (message "Invalid URL, please try again")
+        (sit-for 1)
+        (org-supertag-read-url-field prompt default)))))
+
+(defun org-supertag-read-options-field (prompt options)
+  "Read options field value.
+PROMPT is the prompt message
+OPTIONS is the list of available options."
+  (unless options
+    (error "Options type field requires predefined options"))
+  (let ((input (completing-read (format "%s (%s): " 
+                                      prompt 
+                                      (mapconcat #'identity options "/"))
+                              options
+                              nil t)))
+    (if (member input options)
+        input
+      (progn
+        (message "Please select from the given options")
+        (sit-for 1)
+        (org-supertag-read-options-field prompt options)))))
+
+(defun org-supertag-read-number-field (prompt)
+  "Read numeric field value.
+PROMPT is the prompt message"
+  (let ((input-str (read-string (format "%s: " prompt))))
+    (if (string-match-p "^[0-9.]+$" input-str)
+        input-str  ; Return number as string
+      (progn
+        (message "Please enter a valid number")
+        (sit-for 1)
+        (org-supertag-read-number-field prompt)))))
 
 ;;----------------------------------------------------------------------
-;; High-Level Interactive Field Editor
-;;----------------------------------------------------------------------
-
-(defun org-supertag-field-read-and-validate-value (field-def &optional current-value)
-  "Interactively read and validate a field value based on its definition.
-This is the unified, high-level entry point for editing field values from any UI component.
-It shows a prompt, reads a value using the appropriate method for the field type,
-and then validates it. It will re-prompt on invalid input.
-
-FIELD-DEF is a property list defining the field (e.g., from `org-supertag-tag-get-field-def`).
-CURRENT-VALUE is the existing value of the field, if any.
-
-Returns the validated and formatted new value, or nil if the user cancels."
-  (let* ((field-name (plist-get field-def :name))
-         (field-type (plist-get field-def :type))
-         (type-info (or (org-supertag-get-field-type field-type)
-                        (org-supertag-get-field-type 'string))) ; Fallback to string
-         (reader-fn (plist-get type-info :reader))
-         (prompt (format "Edit %s (%s): " field-name (or (plist-get type-info :description) "Value")))
-         new-value-raw
-         validated-value
-         valid-input-p)
-
-    (unless reader-fn
-      (error "No reader function found for field type %s" field-type))
-
-    (while (not valid-input-p)
-      (setq new-value-raw
-            (condition-case nil
-                (cond
-                 ((eq field-type 'options) (funcall reader-fn prompt current-value field-def))
-                 (t (funcall reader-fn prompt current-value)))
-              (quit nil)))
-
-      (if (null new-value-raw) ; User cancelled input (e.g., C-g)
-          (progn
-            (message "Cancelled.")
-            (setq validated-value nil)
-            (setq valid-input-p t))
-        (setq validated-value (org-supertag-field--validate-and-format-value new-value-raw field-def))
-        (if validated-value
-            (setq valid-input-p t)
-          (message "Invalid value. Please try again.")
-          (sit-for 1)))) ; Pause to let user read the message
-
-    validated-value))
-
-;;----------------------------------------------------------------------
-;; Field Definition Management
+;; Field Modification
 ;;----------------------------------------------------------------------
 
 (defun org-supertag-tag-modify-field (tag-id field-name)
@@ -610,44 +663,12 @@ This will clear all existing values of this field across all nodes."
         
         (let ((nodes (org-supertag-tag-get-nodes tag-id)))
           (dolist (node-id nodes)
-            (org-supertag-field-delete-value node-id field-name tag-id)))
+            (org-supertag-field-remove-value field node-id tag-id)))
         
         (message "Field '%s' modified. You may now set new values." 
                  new-name)))))
 
-(defun org-supertag-field-get-tag-id (field)
-  (getf field :tag-id))
-
-(defun org-supertag-field-edit-cancel ()
-  "Cancel editing fields."
-  (when (get-buffer "*Org Supertag Fields*")
-    (kill-buffer "*Org Supertag Fields*")))
-
-(defun org-supertag-field-edit-save (fields)
-  "Save the edited fields."
-  (dolist (field fields)
-    (let ((name (org-supertag-field-get-name field))
-          (value (org-supertag-field-get-value field)))
-      (org-supertag-field-set-value (org-id-get-create) name value (org-supertag-field-get-tag-id field))))
-  (org-supertag-field-edit-cancel)
-  (message "Fields saved."))
-
-(defun org-supertag-field--get-existing-tags ()
-  "Get all existing tags for completion.
-Returns a list of tag names."
-  (let (tags)
-    (maphash (lambda (id props)
-               (let ((type (plist-get props :type)))
-                 (when (eq type :tag)
-                   (let ((name (or (plist-get props :name) id)))
-                     (push name tags)))))
-             org-supertag-db--object)
-    (sort tags #'string<)))
-
-
-
 
 
 (provide 'org-supertag-field)
-
 ;;; org-supertag-field.el ends here
diff --git a/org-supertag-inline.el b/org-supertag-inline.el
old mode 100755
new mode 100644
index 01b81e7..f26d39c
--- a/org-supertag-inline.el
+++ b/org-supertag-inline.el
@@ -1,18 +1,31 @@
 ;;; org-supertag-inline.el --- Support for inline tags in org-mode content -*- lexical-binding: t; -*-
 
+;; Copyright (C) 2023-2024
+
+;; Author: User <user@example.com>
+;; Keywords: org-mode, tags, inline
+
+;; This file is NOT part of GNU Emacs.
+
+;;; Commentary:
+
+;; This module adds support for inline tags within org-mode content.
+;; Inline tags are prefixed with a '#' symbol (like #hashtags) and are
+;; distinct from headline tags.
+;;
+;; Example:
+;; "This is a paragraph with an #inline-tag that can be tracked and queried."
+
 ;;; Code:
 
 (require 'org)
 (require 'org-supertag-db)
 (require 'org-supertag-node)
 (require 'org-supertag-tag)
-(require 'org-supertag-relation nil t)
+<<<<<<< HEAD
+=======
 (require 'seq)  ; For seq-remove function
 
-;;;----------------------------------------------------------------------
-;;; Compatibility Functions
-;;;----------------------------------------------------------------------
-
 ;; Compatibility function for org-at-commented-p
 (unless (fboundp 'org-at-commented-p)
   (defun org-at-commented-p ()
@@ -21,47 +34,173 @@
       (beginning-of-line)
       (looking-at-p "^[ \t]*#\\+"))))
 
-;;;----------------------------------------------------------------------
-;;; Compatibility Functions
-;;;----------------------------------------------------------------------
-
 ;; Ensure string-trim is available (for Emacs < 24.4 compatibility)
 (unless (fboundp 'string-trim)
   (defun string-trim (string)
     "Remove leading and trailing whitespace from STRING."
     (replace-regexp-in-string "\\`[ \t\n\r]+" ""
                               (replace-regexp-in-string "[ \t\n\r]+\\'" "" string))))
-
-(defconst org-supertag-inline--valid-tag-chars "a-zA-Z0-9_@%一-龥-"
-  "The set of characters considered valid for an inline supertag.
-This is used to construct regular expressions and includes ASCII
-alphanumeric characters and a direct UTF-8 CJK range for max compatibility.")
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defgroup org-supertag-inline-style nil
   "Customization options for org-supertag inline tag styling."
   :group 'org-supertag)
 
-(defcustom org-supertag-inline-manual-insert-add-newline nil
-  "When non-nil, add a newline after manually inserting an inline tag.
-This is useful for workflows where tags are expected to be on their own line,
-such as with the auto-tagging system."
+(defcustom org-supertag-inline-style-hide-prefix t
+  "Whether to hide the '#' prefix of inline tags."
   :type 'boolean
   :group 'org-supertag-inline-style)
 
+(defcustom org-supertag-inline-background 'unspecified
+  "Background color for inline tags.
+Can be:
+- A color string (e.g. \"#e8f0ff\")
+- 'unspecified for transparent
+- nil for transparent"
+  :type '(choice
+          (const :tag "Transparent" unspecified)
+          (const :tag "None" nil)
+          (color :tag "Color"))
+  :group 'org-supertag-inline-style)
+
+(defcustom org-supertag-inline-foreground 'unspecified
+  "Foreground color for inline tags.
+Can be:
+- A color string (e.g. \"#0066cc\")
+- 'unspecified for transparent
+- nil for transparent"
+  :type '(choice
+          (const :tag "Transparent" unspecified)
+          (const :tag "None" nil)
+          (color :tag "Color"))
+  :group 'org-supertag-inline-style)
+
+(defcustom org-supertag-inline-box '(:line-width 1 :color "#b0b0b0" :style nil)
+  "Box properties for inline tags.
+Can be t for a simple box, nil for no box, or a property list with
+:line-width, :color and :style attributes.
+
+The :line-width can be a positive or negative number:
+- Positive: draws the box around the text (increases text height)
+- Negative: draws the box within the text area (preserves text height)
+
+The :line-width can also be a cons cell (VWIDTH . HWIDTH) to specify
+different widths for vertical and horizontal lines.
+
+The :style can be:
+- released-button (3D button that is not pressed)
+- pressed-button (3D button that is being pressed)
+- nil (a simple 2D box, the default)"
+  :type '(choice
+          (const :tag "No box" nil)
+          (const :tag "Simple box" t)
+          (list :tag "Custom box"
+                :value (:line-width 1 :color "#b0b0b0")
+                (choice :tag "Line width"
+                       (cons :tag "Width as cons"
+                             :format "%t: %v"
+                             :value (:line-width (1 . 1))
+                             (const :format "" :line-width)
+                             (cons (number :tag "Vertical") (number :tag "Horizontal")))
+                       (cons :tag "Width as number"
+                             :format "%t: %v"
+                             :value (:line-width 1)
+                             (const :format "" :line-width)
+                             (number :format "%v")))
+                (cons :tag "Color" :format "%t: %v"
+                      :value (:color "#b0b0b0")
+                      (const :format "" :color)
+                      (color :format "%v"))
+                (cons :tag "Style" :format "%t: %v"
+                      :value (:style nil)
+                      (const :format "" :style)
+                      (choice :format "%v"
+                              (const :tag "None (regular 2D box)" nil)
+                              (const :tag "Released button" released-button)
+                              (const :tag "Pressed button" pressed-button)))))
+  :group 'org-supertag-inline-style)
+
+(defcustom org-supertag-inline-weight 'semi-bold
+  "Font weight for inline tags."
+  :type '(choice
+          (const :tag "Normal" normal)
+          (const :tag "Bold" bold)
+          (const :tag "Semi-bold" semi-bold))
+  :group 'org-supertag-inline-style)
+
 ;; Define the face for inline tags
 (defface org-supertag-inline-face
-  '((t :weight bold :foreground "deeppink"))
+  `((t (:background ,org-supertag-inline-background
+        :foreground ,org-supertag-inline-foreground
+        :box ,org-supertag-inline-box
+        :weight ,org-supertag-inline-weight)))
   "Face for org-supertag inline tags."
   :group 'org-supertag-inline-style)
 
+;; Function to create the display property for hiding the # prefix
+(defun org-supertag-inline-display-property (tag-name)
+  "Create a display property to hide the # prefix for TAG-NAME."
+  (when org-supertag-inline-style-hide-prefix
+    (let ((len (length tag-name)))
+      (propertize (substring tag-name 1) 
+                 'face 'org-supertag-inline-face
+                 'org-supertag-inline t))))
+
 ;; Compose font-lock keywords for highlighting inline tags
 (defvar org-supertag-inline-font-lock-keywords
-  `((,(concat "#[" org-supertag-inline--valid-tag-chars "]+")
-     (0 (if (and (not (org-in-src-block-p))
-                 (not (org-at-table-p))
-                 (not (org-at-commented-p))
-                 (not (eq (get-text-property (match-beginning 0) 'face) 'org-verbatim)))
-            'org-supertag-inline-face) t)))
+<<<<<<< HEAD
+  `((,(rx (group "#" (+ (any alnum "-_"))))
+      (0 (let* ((tag-name (match-string-no-properties 1))
+                (prefix "#")
+                (tag-text (substring tag-name 1)))
+           ;; Apply different properties to the prefix and the tag text
+           (when org-supertag-inline-style-hide-prefix
+             (add-text-properties
+              (match-beginning 0) (+ (match-beginning 0) 1)
+              '(invisible org-supertag-prefix display "")))
+           ;; Apply face to the entire tag including prefix if not hidden
+           (add-text-properties
+            (if org-supertag-inline-style-hide-prefix
+                (+ (match-beginning 0) 1)
+              (match-beginning 0))
+            (match-end 0)
+            `(face org-supertag-inline-face
+                  help-echo ,(format "Tag: %s" tag-text)
+                  org-supertag-inline t))
+           ;; Return nil to allow other fontification
+           nil))
+      ;; Don't apply in org src blocks, code blocks, or verbatim sections
+      (0 'org-supertag-inline-face nil
+         (and (not (org-in-src-block-p))
+              (not (org-at-table-p))
+              (not (org-at-commented-p))
+              (not (eq (get-text-property (match-beginning 0) 'face) 'org-verbatim))))))
+=======
+  `((,(rx "#" (+ (any alnum "-_")))
+     (0 (let* ((tag-name (match-string-no-properties 0))
+               (tag-text (substring tag-name 1)))
+          ;; Only apply if not in special contexts
+          (when (and (not (org-in-src-block-p))
+                     (not (org-at-table-p))
+                     (not (org-at-commented-p))
+                     (not (eq (get-text-property (match-beginning 0) 'face) 'org-verbatim)))
+            ;; Apply different properties to the prefix and the tag text
+            (when org-supertag-inline-style-hide-prefix
+              (add-text-properties
+               (match-beginning 0) (+ (match-beginning 0) 1)
+               '(invisible org-supertag-prefix display "")))
+            ;; Apply face to the entire tag including prefix if not hidden
+            (add-text-properties
+             (if org-supertag-inline-style-hide-prefix
+                 (+ (match-beginning 0) 1)
+               (match-beginning 0))
+             (match-end 0)
+             `(face org-supertag-inline-face
+                   help-echo ,(format "Tag: %s" tag-text)
+                   org-supertag-inline t)))
+          ;; Return nil to allow other fontification
+          nil))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   "Font-lock keywords for highlighting inline tags.")
 
 ;;;###autoload
@@ -82,14 +221,83 @@ such as with the auto-tagging system."
 ;; Enable the minor mode in org buffers
 (add-hook 'org-mode-hook 'org-supertag-inline-style-mode)
 
+;; Refresh styling when customizations change
+(defun org-supertag-inline-style-update ()
+  "Update the org-supertag-inline face based on current customizations."
+  (custom-set-faces
+   `(org-supertag-inline-face
+     ((t (:background ,org-supertag-inline-background
+          :foreground ,org-supertag-inline-foreground
+          :box ,org-supertag-inline-box
+          :weight ,org-supertag-inline-weight))))))
+
+(advice-add 'customize-save-variable :after
+            (lambda (&rest _)
+              (when (boundp 'org-supertag-inline-style-mode)
+                (org-supertag-inline-style-update))))
+
 ;; Override the tag insertion function to apply styling immediately
 (advice-add 'org-supertag-inline-insert-tag :after
             (lambda (&rest _)
               (font-lock-flush)))
 
-;;----------------------------------------------------------------------
-;; Helper functions for inline tag insertion
-;;----------------------------------------------------------------------
+<<<<<<< HEAD
+;;;###autoload
+(defun org-supertag-inline-insert-tag (tag-name)
+  "Insert an inline tag at point and establish proper relationships.
+When called with an active region, use the region text as the default tag name.
+TAG-NAME is the name of the tag to insert."
+  (interactive
+   (list (let* ((all-tags (org-supertag-get-all-tags))
+                (preset-names (mapcar #'car org-supertag-preset-tags))
+                ;; Remove existing preset tags to avoid duplicates
+                (user-tags (cl-remove-if (lambda (tag) (member tag preset-names)) all-tags))
+                (candidates (delete-dups
+                            (append 
+                             ;; Use [P] prefix to mark preset tags
+                             (mapcar (lambda (name) 
+                                     (format "[P] %s" name))
+                                   preset-names)
+                             ;; Regular tags are kept as is
+                             user-tags)))
+                ;; Get region content if active
+                (region-text (when (use-region-p)
+                             (buffer-substring-no-properties
+                              (region-beginning)
+                              (region-end))))
+                (input (completing-read
+                       "Inline tag: "
+                       candidates nil nil
+                       ;; Use region text as initial input if available
+                       region-text)))
+           ;; Process input, remove [P] prefix
+           (if (string-prefix-p "[P] " input)
+               (substring input 4)
+             input))))
+  
+  ;; Save original position for context analysis
+  (let* ((original-pos (point-marker))
+         ;; Basic parameter processing
+         (direct-create (string-suffix-p "#" tag-name))
+         (tag-name-clean (if direct-create
+                           (substring tag-name 0 -1)
+                         tag-name))
+         (sanitized-name (org-supertag-sanitize-tag-name tag-name-clean))
+         ;; Analyze context, check if in drawer, etc.
+         (context (org-element-context))
+         (context-type (org-element-type context))
+         (in-drawer (or (eq context-type 'drawer)
+                        (eq context-type 'property-drawer)
+                        (eq context-type 'node-property)
+                        (and (eq context-type 'keyword)
+                             (string= (org-element-property :key context) "END"))))
+         ;; ID lookup logic - prioritize forced ID, then existing ID
+         (force-id (and (boundp 'org-supertag-force-node-id)
+                      org-supertag-force-node-id))
+         ;; Reliably get node ID - try direct get first, then locate heading if failed
+=======
+;;; Helper functions for inline tag insertion
+
 (defun org-supertag-inline--read-tag-name ()
   "Read tag name from user input with completion.
 Returns the selected or entered tag name."
@@ -132,16 +340,73 @@ Returns a plist with context information:
          ;; ID lookup logic
          (force-id (and (boundp 'org-supertag-force-node-id)
                        org-supertag-force-node-id))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
          (existing-id (or force-id
                          (org-entry-get nil "ID")
                          (save-excursion
                            (ignore-errors
                              (org-back-to-heading t)
                              (org-entry-get nil "ID")))))
+<<<<<<< HEAD
+         ;; If no existing ID and on heading, create new ID
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
          (node-id (or existing-id
                      (save-excursion
                        (when (ignore-errors (org-back-to-heading t))
                          (org-supertag-node-create))))))
+<<<<<<< HEAD
+
+    ;; Debugging message
+    (message "inline-insert-tag: ID=%s force=%s drawer=%s ctx=%s pos=%d"
+             node-id force-id in-drawer context-type (marker-position original-pos))
+
+    ;; Handle tag creation
+    (let ((tag-id
+           (cond
+            ;; If tag exists, use it directly
+            ((org-supertag-tag-exists-p sanitized-name)
+             sanitized-name)
+            ;; Otherwise create a new tag
+            (t
+             (if (or direct-create
+                     (y-or-n-p (format "Create new tag '%s'? " sanitized-name)))
+                 (org-supertag-tag-create sanitized-name)
+               (user-error "Tag creation cancelled"))))))
+
+      ;; Delete region if any
+      (when (use-region-p)
+        (delete-region (region-beginning) (region-end)))
+
+      ;; Handle cursor position - if in drawer, move after drawer
+      (when in-drawer
+        (let ((end-pos (org-element-property :end context)))
+          (when end-pos
+            (goto-char end-pos))))
+
+      ;; Insert tag text
+      (unless (or (bobp) (eq (char-before) ? ))
+        (insert " "))
+      (insert (concat "#" tag-id))
+      (unless (or (eobp) (eq (char-after) ? ))
+        (insert " "))
+
+      ;; Apply tag relationship
+      (when node-id
+        (let ((org-supertag-tag-apply-skip-headline t)
+              (org-supertag-force-node-id node-id))
+          (org-supertag-tag-apply tag-id)))
+
+      ;; Cleanup
+      (set-marker original-pos nil)
+
+      ;; Return message
+      (message "Inserted inline tag #%s%s"
+              tag-id
+              (if node-id
+                  (format " and linked to node %s" node-id)
+                "")))))
+=======
     (list :context context
           :type context-type
           :in-drawer in-drawer
@@ -173,7 +438,7 @@ Returns a plist with :tag-id and :tag-name."
                    (t
                     (if (or direct-create
                             (y-or-n-p (format "Create new tag '%s'? " sanitized-name)))
-                        (org-supertag-tag--create sanitized-name)
+                        (org-supertag-tag-create sanitized-name)
                       (user-error "Tag creation cancelled"))))))
       ;; Return both tag-id and tag-name
       (list :tag-id tag-id :tag-name sanitized-name))))
@@ -198,37 +463,39 @@ Handles region deletion and smart drawer/content positioning."
 (defun org-supertag-inline--smart-position-for-insertion ()
   "Smartly position the cursor for inline tag insertion.
 Rules:
-- Position cursor at the beginning of a new line below the headline
-- If there's a property drawer, position below the drawer
-- If there's an existing tag line, move to the end of that line
-- Always ensure we're on a separate line, not on the headline itself"
+- If no drawer, create a new line below the heading or find an existing tag line
+- If there's a drawer, position below the drawer
+- If there's an existing tag line, move to the end of the tag line
+- If already in content, create a new line above the content"
   (org-back-to-heading t)
   (let ((drawer-end (org-supertag-inline--find-drawer-end))
         (existing-tag-line (org-supertag-inline--find-existing-tag-line)))
     (cond
-     ;; If there's already a tag line, position at the end of it
+     ;; 如果找到现有标签行，移动到行尾
      (existing-tag-line
       (goto-char existing-tag-line)
       (end-of-line))
-     
-     ;; If there's a drawer, position after it
+     ;; 如果有drawer，在drawer后智能定位
      (drawer-end
       (goto-char drawer-end)
-      ;; Ensure we are at the beginning of a line after drawer
-      (unless (bolp) (forward-line 1))
-      ;; Skip any empty lines after drawer
+      ;; 跳过空行到内容开始
       (while (and (not (eobp))
-                  (not (org-at-heading-p))
-                  (looking-at-p "^[ \t]*$"))
+                 (not (org-at-heading-p))
+                 (looking-at-p "^[ \t]*$"))
         (forward-line 1))
-      ;; Always position at the beginning of line for tag insertion
-      (beginning-of-line))
-     
-     ;; No drawer, position directly after headline
+      ;; 如果已经在内容行，在内容行上方创建新行插入
+      (if (and (not (eobp))
+              (not (org-at-heading-p))
+              (not (looking-at-p "^[ \t]*$")))
+          (progn
+            (beginning-of-line)
+            (open-line 1))
+        ;; 否则在当前位置创建新行
+        (unless (bolp) (insert "\n"))))
+     ;; 如果没有drawer，在标题下方新建一行
      (t
       (end-of-line)
-      (insert "\n")
-      (beginning-of-line)))))
+      (insert "\n")))))
 
 (defun org-supertag-inline--find-drawer-end ()
   "Find the end position of the current node's drawer."
@@ -277,7 +544,7 @@ Returns the tag line position, or nil if not found."
                  (not tag-line-pos))
         (beginning-of-line)
         ;; Check if the current line contains #tags (but not comment lines)
-        (when (and (looking-at-p (concat "^[ \t]*#[" org-supertag-inline--valid-tag-chars "]"))
+        (when (and (looking-at-p "^[ \t]*#[a-zA-Z0-9_-]+")
                   (not (looking-at-p "^[ \t]*#\\+")))  ; Exclude org keywords
           (setq tag-line-pos (point)))
         (forward-line 1))
@@ -308,6 +575,7 @@ Smart spacing rules:
                    (and (>= prev-char ?a) (<= prev-char ?z))
                    (and (>= prev-char ?A) (<= prev-char ?Z))
                    (and (>= prev-char ?0) (<= prev-char ?9))
+                   ;; 中文字符
                    (and (>= prev-char ?\u4e00) (<= prev-char ?\u9fff)))))
          ;; Need space after ONLY if:
          ;; 1. next character exists AND is alphanumeric
@@ -334,19 +602,7 @@ Smart spacing rules:
   "Check if the current line is a tag line."
   (save-excursion
     (beginning-of-line)
-    (looking-at-p (concat "^[ \t]*#[" org-supertag-inline--valid-tag-chars "]+"))))
-
-(defun org-supertag-inline--insert-tag-for-autotag (tag-name)
-  "Insert tag for auto-tag system with safe line insertion to preserve document structure.
-TAG-NAME is the tag name to insert.
-This function uses a two-line insertion strategy to avoid disrupting existing content."
-  ;; Always insert two newlines first to create safe space
-  (insert "\n\n")
-  ;; Move back to the first line and insert the tag
-  (forward-line -2)
-  (insert "#" tag-name)
-  ;; Move cursor to the end of the tag insertion area
-  (forward-line 1))
+    (looking-at-p "^[ \t]*#[a-zA-Z0-9_-]+")))
 
 (defun org-supertag-inline--establish-relationship (tag-result node-id)
   "Establish relationship between tag and node.
@@ -360,292 +616,154 @@ Only creates the relationship if NODE-ID is not nil."
       (org-supertag-tag-apply tag-id))))
 
 ;;;###autoload
-(defun org-supertag-inline-insert-tag (tag-name &optional pos)
-  "Insert an inline tag at POS, or at the current point.
-TAG-NAME: The name of the tag to insert (without '#').
-POS: The position to insert the tag. If nil, insert at point.
-The smart positioning logic has been removed to simplify the flow.
-The caller, `org-supertag-tag-apply`, is now responsible for positioning."
-  (let ((text-to-insert (concat " #" tag-name " ")))
-    (if pos (goto-char pos))
-    (insert text-to-insert)))
-
-(defun org-supertag-inline-insert-tag-for-autotag (node-id tag-name)
-  "Insert a tag for the auto-tag system, positioning it smartly.
-This function uses our custom node location finder to avoid dependency on org-id-find."
-  (when (and node-id tag-name)
-    (let ((found-location (org-supertag-find-node-location node-id)))
-      (if found-location
-          (let ((pos (car found-location))
-                (file-path (cdr found-location)))
-            (with-current-buffer (find-file-noselect file-path)
-              (save-excursion
-                (goto-char pos)
-                (org-back-to-heading t)
-                ;; Use the smart positioning logic for tag insertion
-                (org-supertag-inline--smart-position-for-insertion)
-                ;; Create tag and insert
-                (let* ((display-name (org-supertag-sanitize-tag-name tag-name))
-                       (tag-entity (or (org-supertag-tag-get display-name)
-                                       (progn
-                                         (org-supertag-tag--create display-name)
-                                         (org-supertag-tag-get display-name)))))
-                  ;; Insert tag with simple format for auto-tag system
-                  (org-supertag-inline--insert-tag-for-autotag display-name)
-                  (when tag-entity
-                    (org-supertag-node-db-add-tag node-id (plist-get tag-entity :id)))
-                  ;; Save the buffer to ensure tag is written to file
-                  (save-buffer)
-                  (message "Auto-inserted tag #%s for node %s at %s" display-name node-id file-path)))))
-        (error "Could not find node with ID: %s" node-id)))))
-
+(defun org-supertag-inline-insert-tag (tag-name)
+  "Insert an inline tag at point and establish proper relationships.
+When called with an active region, use the region text as the default tag name.
+TAG-NAME is the name of the tag to insert.
+
+This function follows a structured approach:
+1. Analyze the current context
+2. Ensure the tag exists (create if necessary)
+3. Adjust cursor position appropriately (for manual insertion, stay at cursor)
+4. Insert the tag with proper spacing
+5. Establish tag-node relationships"
+  (interactive (list (org-supertag-inline--read-tag-name)))
+  
+  (let* ((original-pos (point-marker))
+         (context-info (org-supertag-inline--analyze-context))
+         (node-id (plist-get context-info :node-id))
+         (tag-result (org-supertag-inline--ensure-tag tag-name))
+         (tag-id (plist-get tag-result :tag-id))
+         (display-name (plist-get tag-result :tag-name)))
+    
+    ;; Debug information
+    (message "inline-insert-tag: ID=%s drawer=%s ctx=%s pos=%d"
+             node-id
+             (plist-get context-info :in-drawer)
+             (plist-get context-info :type)
+             (marker-position original-pos))
+    
+    ;; Process the insertion - for manual insertion, don't change position
+    (org-supertag-inline--adjust-position context-info)
+    (org-supertag-inline--insert-tag-text display-name)
+    ;; Ensure tag line ends with newline if not at end of buffer
+    (unless (or (eobp) (looking-at-p "\n"))
+      (insert "\n"))
+    (org-supertag-inline--establish-relationship tag-result node-id)
+    
+    ;; Cleanup
+    (set-marker original-pos nil)
+    
+    ;; Return message
+    (message "Inserted inline tag #%s%s"
+             display-name
+             (if node-id
+                 (format " and linked to node %s" node-id)
+               ""))))
 
+;;;###autoload
 (defun org-supertag-inline-insert-tags-batch (tag-names)
-  "Insert multiple inline tags, respecting the user's choice for newlines."
-  (interactive (list (org-supertag-inline--read-multiple-tags)))
+  "Insert multiple inline tags in a single line and establish relationships.
+TAG-NAMES is a list of tag names."
   (when tag-names
-    (let* ((node-id (org-id-get-create))
-           (processed-tags (mapcar #'org-supertag-tag-get-or-create tag-names)))
-
-      ;; Do not reposition the cursor for manual batch insertion.
-      ;; The insertion will happen at the user's cursor position.
-      ;; (org-supertag-inline--position-for-batch-insert)
-
+    (let* ((context-info (org-supertag-inline--analyze-context))
+           (node-id (plist-get context-info :node-id))
+           (processed-tags '()))
+      
+      ;; Process and validate all tags
+      (dolist (tag-name tag-names)
+        (condition-case err
+            (let ((tag-result (org-supertag-inline--ensure-tag tag-name)))
+              (push tag-result processed-tags))
+          (error
+           (message "Failed to process tag: %s - %s" tag-name (error-message-string err)))))
+      
       ;; Insert tags in batch
-      (org-supertag-inline--insert-tags-batch processed-tags)
-
-      ;; Establish relationships
-      (dolist (tag-result processed-tags)
-        (org-supertag-node-db-add-tag node-id (plist-get tag-result :id)))
-
-      ;; Add newline only if explicitly configured
-      (when org-supertag-inline-manual-insert-add-newline
-        (insert "\n"))
-
-      (message "Inserted %d tags: %s"
-               (length processed-tags)
-               (mapconcat (lambda (r) (plist-get r :tag-name))
-                          processed-tags ", ")))))
+      (when processed-tags
+        (setq processed-tags (nreverse processed-tags))
+        (org-supertag-inline--insert-tags-batch processed-tags)
+        
+        ;; Establish relationships
+        (dolist (tag-result processed-tags)
+          (org-supertag-inline--establish-relationship tag-result node-id))
+        
+        (message "Inserted %d tags: %s" 
+                 (length processed-tags)
+                 (mapconcat (lambda (tr) (plist-get tr :tag-name)) processed-tags " "))))))
 
 (defun org-supertag-inline--insert-tags-batch (tag-results)
   "Insert multiple tags in a single line.
-TAG-RESULTS is a list of plists, each from `org-supertag-tag-get-or-create'."
-  (let ((first t))
-    (dolist (tag-result tag-results)
-      (unless first
-        (insert " ")) ; Space between tags
-      (org-supertag-inline--insert-tag-text (plist-get tag-result :tag-name))
-      (setq first nil))))
-
-;; The function org-supertag-inline-insert-tag-for-node is now obsolete
-;; and replaced by org-supertag-inline-insert-tag-for-autotag
-;; (defun org-supertag-inline-insert-tag-for-node (node-id tag-name) ... )
-
-;; The function org-supertag-inline-insert-tag-no-newline is now obsolete
-;; as its logic will be merged into the main org-supertag-inline-insert-tag.
-;; (defun org-supertag-inline-insert-tag-no-newline (tag-name) ... )
-
-(defun org-supertag-inline-remove-tag-at-point (tag-id)
-  "Remove an inline tag at or before point.
-TAG-ID is the tag name to remove."
-  (let ((tag-pattern (concat "#" (regexp-quote tag-id))))
-    (save-excursion
-      (when (re-search-backward tag-pattern nil t)
-        (replace-match "")))))
-
-(defun org-supertag-inline--remove-tag-in-current-node (tag)
-  "Remove all occurrences of #TAG inside the current node (headline + content)."
-  (save-excursion
-    (org-back-to-heading t)
-    (let ((subtree-end (save-excursion (org-end-of-subtree t t) (point)))
-          (regex (concat "#" (regexp-quote tag)))
-          (case-fold-search nil))
-      ;; Search headline line first (including todo / tags part)
-      (beginning-of-line)
-      (while (re-search-forward regex (line-end-position) t)
-        (replace-match ""))
-      ;; Now scan the subtree
-      (forward-line 1)
-      (while (re-search-forward regex subtree-end t)
-        (replace-match "")))))
-
-;;----------------------------------------------------------------------
-;; Interactive Commands
-;;----------------------------------------------------------------------
-
-(defun org-supertag-inline-add ()
-  "Interactively add a supertag to the current node.
-This command handles both database relations and text insertion."
-(interactive)
-  (message "DEBUG: org-supertag-inline-add called")
-  (let* ((context (org-supertag-inline--analyze-context))
-         (node-id (plist-get context :node-id))
-         (tag-name-raw (org-supertag-inline--read-tag-name))
-         (tag-info (org-supertag-inline--ensure-tag tag-name-raw))
-         (tag-id (plist-get tag-info :tag-id)))
-    (message "DEBUG: node-id=%s, tag-id=%s" node-id tag-id)
-    (when (and node-id tag-id)
-      ;; The single call to the unified apply function.
-      ;; Text insertion is handled within `org-supertag-tag-apply`.
-      (let ((org-supertag-force-node-id node-id))
-        (org-supertag-tag-apply tag-id))
-      (message "Tag '%s' applied to node %s." tag-id node-id))))
-(defalias 'org-supertag-tag-add-tag 'org-supertag-inline-add)
-
-(defun org-supertag-inline-remove ()
-  "Interactively remove a supertag from the current node and clean buffer text."
-  (interactive)
-  (let* ((node-id (org-id-get))
-         (tags (when node-id (org-supertag-node-get-tags node-id)))
-         (tag-to-remove (completing-read "Remove tag: " tags nil t)))
-    (when (and node-id (not (string-empty-p tag-to-remove)))
-      ;; 1. Update database first
-      (org-supertag-db-remove-link :node-tag node-id tag-to-remove)
-      ;; 2. Update co-occurrence relationships
-      (when (featurep 'org-supertag-relation)
-        (org-supertag-relation-unrecord-cooccurrence node-id tag-to-remove))
-      ;; 3. Update buffer text: remove every occurrence within the node
-      (org-supertag-inline--remove-tag-in-current-node tag-to-remove)
-      (message "Tag '%s' removed from node %s." tag-to-remove node-id))))
-(defalias 'org-supertag-tag-remove 'org-supertag-inline-remove)
-
-(defun org-supertag-inline-delete-all ()
-  "Interactively delete a tag definition and all its instances.
-This removes the tag from the database and from all org files."
-  (interactive)
-  (let* ((all-tags (org-supertag-get-all-tags))
-         (raw-input (completing-read "Delete tag permanently: " all-tags nil t))
-         (clean-tag (org-supertag-sanitize-tag-name (substring-no-properties raw-input))))
-    (when (and (not (string-empty-p clean-tag))
-               (yes-or-no-p (format "DELETE tag '%s' and all its uses? This is irreversible. " clean-tag)))
-      ;; First, find all nodes before deleting from DB
-      (let ((nodes (org-supertag-db-get-tag-nodes clean-tag)))
-        ;; 1. Delete from database (this is the function from org-supertag-tag.el)
-        (org-supertag-tag--delete-at-all clean-tag)
-        
-        ;; 2. Remove from all org buffers
-        (message "Removing inline tags from buffers...")
-        (let ((files (delete-dups (mapcar #'org-supertag-db-get-node-file nodes))))
-          (dolist (file files)
-            (when (file-exists-p file)
-              (with-current-buffer (find-file-noselect file)
-                (save-excursion
-                  (goto-char (point-min))
-                  (while (re-search-forward (concat "#" (regexp-quote clean-tag)) nil t)
-                    (replace-match "")))))
-          (message "Completed buffer cleanup for tag '%s'." clean-tag)))))))
-
-(defun org-supertag-inline-rename ()
-  "Interactively rename a tag across all files."
-  (interactive)
-  (let* ((all-tags (org-supertag-get-all-tags))
-         (old-name (completing-read "Tag to rename: " all-tags nil t))
-         (new-name (read-string (format "New name for '%s': " old-name))))
-    (when (and (not (string-empty-p old-name))
-               (not (string-empty-p new-name)))
-      ;; 1. Update database first
-      (org-supertag-tag--rename old-name new-name)
-      
-      ;; 2. Update all buffers
-      (message "Renaming tag in buffers...")
-      (let* ((nodes (org-supertag-db-get-tag-nodes new-name)) ; Get nodes by new name
-             (files (delete-dups (mapcar #'org-supertag-db-get-node-file nodes))))
-        (dolist (file files)
-          (when (file-exists-p file)
-            (with-current-buffer (find-file-noselect file)
-              (save-excursion
-                (goto-char (point-min))
-                (while (re-search-forward (concat "#" (regexp-quote old-name)) nil t)
-                  (replace-match (concat "#" new-name))))
-              (save-buffer))))
-        (message "Finished renaming tag '%s' to '%s' in %d files."
-                 old-name new-name (length files))))))
-(defalias 'org-supertag-tag-rename 'org-supertag-inline-rename)
-
-(defun org-supertag-inline-change-tag ()
-  "Interactively change a tag on the current node."
-  (interactive)
-  (let* ((node-id (org-id-get))
-         (current-tags (org-supertag-node-get-tags node-id))
-         (source-tag (completing-read "Select tag to change: " current-tags nil t))
-         (target-tag-raw (read-string "Change to new tag name: ")))
-    (when (and node-id
-               (not (string-empty-p source-tag))
-               (not (string-empty-p target-tag-raw)))
-      (let* ((tag-info (org-supertag-inline--ensure-tag target-tag-raw))
-             (target-tag-id (plist-get tag-info :tag-id)))
-        ;; 1. Update database
-        (org-supertag-db-remove-link :node-tag node-id source-tag)
-        (org-supertag-node-db-add-tag node-id target-tag-id)
-        ;; 2. Update buffer
-        (org-supertag-inline-remove-tag-at-point source-tag)
-        (org-supertag-inline-insert-tag target-tag-id)
-        (message "Changed tag '%s' to '%s' on node %s."
-                 source-tag target-tag-id node-id)))))
-(defalias 'org-supertag-tag-change-tag 'org-supertag-inline-change-tag)
-
-(defun org-supertag-debug-force-refontify ()
-  "DEBUG: Force removal of old face properties and re-fontify.
-This is a temporary command to fix stale highlighting."
-  (interactive)
-  (save-excursion
-    (goto-char (point-min))
-    (while (re-search-forward (concat "#[" org-supertag-inline--valid-tag-chars "]+") nil t)
-      (remove-text-properties (match-beginning 0) (match-end 0) '(face t))))
-  (font-lock-fontify-buffer)
-  (message "Forced re-fontification complete."))
-
-(defun org-supertag-inline-get-tag-at-point ()
-  "Get the inline supertag name at point using the official regex.
-The tag format is '#tagname'. This is a more robust version."
-  (when (derived-mode-p 'org-mode)
-    (save-excursion
-      (let* ((tag-re (and (boundp 'org-supertag-inline-font-lock-keywords)
-                         (car (car org-supertag-inline-font-lock-keywords))))
-             (start (point)))
-        (when (and tag-re (re-search-backward tag-re nil t))
-          ;; Check if the point is within the found match
-          (when (and (<= (match-beginning 0) start)
-                     (> (match-end 0) start))
-            ;; Extract the tag name, removing the leading '#' 
-            (substring (match-string 0) 1)))))))
-
-;;;###autoload
-(defun org-supertag-inline-relation-add-contextual ()
-  "Intelligently add a relation based on context.
-If the cursor is on a tag, use it as the source.
-Otherwise, find the current node and its tags. If there is one tag,
-use it. If there are multiple, prompt the user to select one."
-  (interactive)
-  (let* ((source-tag-name
-          (or (org-supertag-inline-get-tag-at-point)
-              (let* ((node-id (org-id-get))
-                     (node-tags (and node-id (org-supertag-node-get-tags node-id))))
-                (cond
-                 ((= (length node-tags) 1) (car node-tags))
-                   ((> (length node-tags) 1)
-                    (completing-read "Select source tag for this node: " node-tags nil t))
-                   (t nil))))))
-
-    (if (not source-tag-name)
-        (user-error "No source tag found at point or in the current node.")
-        (let* ((from-tag-id (org-supertag-tag-get-id-by-name source-tag-name))
-              (all-tags (org-supertag-get-all-tags))
-               (target-candidates (remove source-tag-name all-tags))
-               (other-tag-name (completing-read (format "Relate '%s' to: " source-tag-name) target-candidates nil t))
-               (other-tag-id (and (not (string-empty-p other-tag-name))
-                                  (org-supertag-tag-get-id-by-name other-tag-name))))
-          (if other-tag-id
-              (let* ((rel-choices (org-supertag-relation--get-relation-type-choices))
-                     (choice (completing-read "Select relation type: " rel-choices nil t))
-                     (rel-type (org-supertag-relation--get-type-from-choice choice)))
-                (when (and from-tag-id other-tag-id rel-type)
-                  (if (org-supertag-relation-has-complement-p rel-type)
-                      (org-supertag-relation-add-with-complement from-tag-id other-tag-id rel-type)
-                    (org-supertag-relation-add-relation from-tag-id other-tag-id rel-type)))
-                  ;; Inform the user once the relation has been added
-                  (message "Added relation: %s -[%s]-> %s" source-tag-name rel-type other-tag-name))
-            (user-error "Target tag not found or is empty: %s" other-tag-name))))))
+TAG-RESULTS is a list of tag result plists."
+  (when tag-results
+    (let* ((drawer-end (org-supertag-inline--find-drawer-end))
+           (existing-tag-line (org-supertag-inline--find-existing-tag-line)))
+      (cond
+       ;; If there's an existing tag line, add at the end
+       (existing-tag-line
+        (goto-char existing-tag-line)
+        (end-of-line)
+        (dolist (tag-result tag-results)
+          (insert (format " #%s" (plist-get tag-result :tag-name))))
+        ;; Ensure tag line ends with newline
+        (unless (looking-at-p "\n")
+          (insert "\n")))
+       ;; If there's a drawer, create a tag line after the drawer
+       (drawer-end
+        (goto-char drawer-end)
+        ;; Skip empty lines
+        (while (and (not (eobp))
+                   (not (org-at-heading-p))
+                   (looking-at-p "^[ \t]*$"))
+          (forward-line 1))
+        ;; If in content line, insert above
+        (if (and (not (eobp))
+                (not (org-at-heading-p))
+                (not (looking-at-p "^[ \t]*$")))
+            (progn
+              (beginning-of-line)
+              (open-line 1))
+          (unless (bolp) (insert "\n")))
+        ;; Insert all tags 
+        (insert (mapconcat (lambda (tag-result) 
+                            (format "#%s" (plist-get tag-result :tag-name)))
+                          tag-results " "))
+        (insert "\n"))
+       ;; If no drawer, create a tag line below the heading
+       (t
+        (end-of-line)
+        (insert "\n")
+        (insert (mapconcat (lambda (tag-result) 
+                            (format "#%s" (plist-get tag-result :tag-name)))
+                          tag-results " "))
+        (insert "\n"))))))
+
+(defun org-supertag-inline-insert-tag-for-node (node-id tag-name)
+  "Insert an inline tag for a specific node ID (used by auto-tag system).
+This finds the node's file and position, then inserts the tag below the drawer.
+NODE-ID: The ID of the node.
+TAG-NAME: The name of the tag to insert."
+  (when-let* ((node-data (org-supertag-db-get node-id))
+              (file-path (plist-get node-data :file-path))
+              (pos (plist-get node-data :pos)))
+    (if (and file-path (file-exists-p file-path))
+        (with-current-buffer (find-file-noselect file-path)
+          (goto-char pos)
+          (when (org-at-heading-p)
+            ;; Use the smart positioning logic for auto-tag insertion
+            (org-supertag-inline--smart-position-for-insertion)
+            ;; Create tag and insert
+            (let* ((tag-result (org-supertag-inline--ensure-tag tag-name))
+                   (display-name (plist-get tag-result :tag-name))
+                   (tag-id (plist-get tag-result :tag-id)))
+              ;; Insert the tag name (not UUID)
+              (insert "#" display-name " ")
+              ;; Establish relationship using tag-id
+              (let ((org-supertag-tag-apply-skip-headline t)
+                    (org-supertag-force-node-id node-id))
+                (org-supertag-tag-apply tag-id)))
+            (save-buffer)))
+      (message "Node file not found for ID: %s" node-id))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (provide 'org-supertag-inline)
 ;;; org-supertag-inline.el ends here 
diff --git a/org-supertag-migration.el b/org-supertag-migration.el
index 628560e..3a97cef 100644
--- a/org-supertag-migration.el
+++ b/org-supertag-migration.el
@@ -1,99 +1,149 @@
-;;; org-supertag-migration.el --- One-time migration scripts for org-supertag -*- lexical-binding: t; -*-
+;;; org-supertag-migration.el --- Migrate standard Org tags to supertags -*- lexical-binding: t; -*-
 
 ;;; Commentary:
-;; This file contains functions for performing one-time data migrations on the
-;; org-supertag database. These are intended to be run manually by the user
-;; to upgrade data structures to new formats.
+;; Provides functionality to migrate standard Org mode tags (e.g., :tag:)
+;; to org-supertag format (e.g., :#tag:) within the configured sync directories.
 
 ;;; Code:
 
-(require 'org-supertag-db)
-(require 'ht)
-
-(defun org-supertag-migration--canonicalize-props (data)
-  "Recursively canonicalize any Lisp DATA into a standard plist format.
-This function is a copy of the new logic intended for org-supertag-db.el,
-placed here to be self-contained for the migration process.
-It converts alists to plists and cleans up keys and values."
-  (cond
-   ;; Atoms are returned as-is.
-   ((or (not (consp data)) (keywordp data)) data)
-
-   ;; An alist (e.g., from properties drawer) is converted to a plist.
-   ((and (listp data) (consp (car data)) (not (keywordp (caar data))))
-    (let (plist)
-      (dolist (pair data)
-        (let ((key (car pair))
-              (val (cdr pair)))
-          (push (org-supertag-migration--canonicalize-props val) plist)
-          ;; Ensure key is a valid string/symbol before creating keyword
-          (let ((key-str (format "%s" key)))
-            (unless (string-empty-p key-str)
-              (push (intern (format ":%s" key-str)) plist)))))
-      (nreverse plist)))
-
-   ;; A regular list (or a plist already) is processed element by element.
-   ((listp data)
-    (mapcar #'org-supertag-migration--canonicalize-props data))
-
-   ;; A dotted pair or other cons cell.
-   (t (cons (org-supertag-migration--canonicalize-props (car data))
-            (org-supertag-migration--canonicalize-props (cdr data))))))
-
-(defun org-supertag-migrate-database-to-canonical-format ()
-  "Run a one-time migration to convert the entire database to the new canonical plist format.
-This process will:
-1. Ask for user confirmation.
-2. Back up the existing database file.
-3. Load the old database.
-4. Create a new, clean database in memory by converting every object.
-5. Overwrite the old database file with the new, clean data."
+(require 'org)
+(require 'org-element)
+(require 'org-supertag-sync) ; Need access to sync directories and helpers
+(require 'cl-lib) ; For cl-lib functions like cl-loop, cl-incf
+
+(defun org-supertag--get-files-in-scope ()
+  "Return a list of all .org files within the sync scope.
+Respects `org-supertag-sync-directories', `org-supertag-sync-exclude-directories',
+and `org-supertag-sync-file-pattern'."
+  (let ((files nil))
+    (dolist (dir org-supertag-sync-directories files)
+      (when (file-directory-p dir)
+        (let* ((abs-dir (expand-file-name dir))
+               (dir-files (directory-files-recursively abs-dir org-supertag-sync-file-pattern t)))
+          (dolist (file dir-files)
+            (when (and (file-regular-p file)
+                       (string= (file-name-extension file) "org") ; Ensure it's an org file
+                       (org-supertag-sync--in-sync-scope-p file)) ; Check includes/excludes
+              (push file files)))))))
+  (delete-dups files)) ; Return unique list
+
+(defun org-supertag--collect-standard-tags (files)
+  "Scan FILES and return a list of unique standard Org tags (without '#')."
+  (let ((standard-tags (make-hash-table :test 'equal))
+        (org-element-use-cache nil) ; Ensure fresh parsing
+        (errors nil))
+    (dolist (file files)
+      (condition-case err
+          (with-current-buffer (find-file-noselect file nil t) ; Visit quietly
+            (save-excursion
+              (goto-char (point-min))
+              (while (re-search-forward org-heading-regexp nil t)
+                (when (org-at-heading-p)
+                  (dolist (tag (org-get-tags))
+                    (unless (string-prefix-p "#" tag)
+                      (puthash tag t standard-tags)))))))
+        (error
+         (push (format "Error scanning %s: %s" file (error-message-string err)) errors))))
+    (when errors
+      (message "Warnings during tag collection:\n%s" (mapconcat #'identity errors "\n")))
+    (hash-table-keys standard-tags)))
+
+(defun org-supertag-migrate-standard-tags-to-supertag ()
+  "Migrate standard Org tags (:tag:) to supertags (:#tag:) globally.
+
+Scans files defined in `org-supertag-sync-directories`, respecting
+exclusions. Prompts the user to select which standard tags to
+migrate.
+
+WARNING: This function modifies Org files in place. BACK UP YOUR
+FILES before proceeding."
   (interactive)
-  (when (y-or-n-p "Really migrate database to the new canonical format? This will create a backup.")
-    ;; 1. Back up the existing database
-    (let ((db-file org-supertag-db-file)
-          (backup-file (concat org-supertag-db-file ".bak")))
-      (message "Backing up current database to %s" backup-file)
-      (copy-file db-file backup-file t))
-
-    ;; 2. Load the old database
-    (message "Loading existing database...")
-    (org-supertag-db-load)
-
-    ;; 3. Create new, clean hash tables
-    (let ((new-objects (ht-create))
-          (new-links (ht-create))
-          (processed-count 0))
-
-      (message "Starting migration of %d objects and %d links..."
-               (hash-table-count org-supertag-db--object)
-               (hash-table-count org-supertag-db--link))
-
-      ;; 4. Migrate all objects
-      (maphash (lambda (id props)
-                 (let ((clean-props (org-supertag-migration--canonicalize-props props)))
-                   (ht-set! new-objects id clean-props))
-                 (cl-incf processed-count))
-               org-supertag-db--object)
-
-      ;; 5. Migrate all links
-      (maphash (lambda (id props)
-                 (let ((clean-props (org-supertag-migration--canonicalize-props props)))
-                   (ht-set! new-links id clean-props))
-                 (cl-incf processed-count))
-               org-supertag-db--link)
-
-      (message "Migration complete. Processed %d total records." processed-count)
-
-      ;; 6. Overwrite the live database variables with the new, clean data
-      (setq org-supertag-db--object new-objects)
-      (setq org-supertag-db--link new-links)
-
-      ;; 7. Save the new database to disk
-      (message "Saving new, canonicalized database to %s..." org-supertag-db-file)
-      (org-supertag-db--mark-dirty) ; Mark as dirty to force save
-      (org-supertag-db-save)
-      (message "Database migration successful!"))))
+
+  (unless (and org-supertag-sync-directories (listp org-supertag-sync-directories))
+    (user-error "Please configure `org-supertag-sync-directories` first."))
+
+  (let* ((files (org-supertag--get-files-in-scope))
+         (standard-tags (org-supertag--collect-standard-tags files)))
+
+    (unless standard-tags
+      (message "No standard Org tags found in the sync scope.")
+      (cl-return-from org-supertag-migrate-standard-tags-to-supertag))
+
+    (let* ((tags-to-migrate (completing-read-multiple
+                             "Select standard tags to migrate to :#tag: format (SPC to mark, RET to confirm): "
+                             standard-tags
+                             nil t))) ; require-match=t
+
+      (unless tags-to-migrate
+        (message "No tags selected for migration. Aborting.")
+        (cl-return-from org-supertag-migrate-standard-tags-to-supertag))
+
+      (message "Selected tags for migration: %s" (mapconcat #'identity tags-to-migrate ", "))
+
+      (unless (yes-or-no-p (format "WARNING: This will modify %d files potentially.\nMigrate %d selected tags (%s) to :#tag: format?\n*** PLEASE BACK UP YOUR FILES FIRST! *** Proceed? "
+                                   (length files)
+                                   (length tags-to-migrate)
+                                   (mapconcat #'identity tags-to-migrate ", ")))
+        (message "Migration aborted by user.")
+        (cl-return-from org-supertag-migrate-standard-tags-to-supertag))
+
+      ;; --- Execution Phase ---
+      (let ((modified-files-count 0)
+            (migrated-instances-count 0)
+            (errors nil)
+            (tags-to-migrate-hash (make-hash-table :test 'equal)))
+        ;; Populate hash for quick lookup
+        (dolist (tag tags-to-migrate) (puthash tag t tags-to-migrate-hash))
+
+        (message "Starting migration...")
+        (cl-loop for file in files
+                 for idx from 1
+                 do
+                 (message "Processing file %d/%d: %s" idx (length files) (file-name-nondirectory file))
+                 (condition-case err
+                     (let ((buffer-modified nil))
+                       (with-current-buffer (find-file-noselect file nil nil) ; Need to modify
+                         (save-excursion
+                           (goto-char (point-min))
+                           ;; Use org-scan-tags for robust heading iteration and modification
+                           (org-scan-tags
+                            (lambda ()
+                              (let* ((current-tags (org-get-tags))
+                                     (new-tags nil)
+                                     (changed-p nil))
+                                (dolist (tag current-tags)
+                                  (if (gethash tag tags-to-migrate-hash)
+                                      (progn
+                                        (push (concat "#" tag) new-tags)
+                                        (cl-incf migrated-instances-count)
+                                        (setq changed-p t))
+                                    (push tag new-tags))) ; Keep existing #tags or non-selected tags
+                                (when changed-p
+                                  (org-set-tags (nreverse new-tags)) ; org-set-tags modifies buffer
+                                  (setq buffer-modified t))))
+                            ;; Scope: only tags in headings
+                            '(:scope headline :target plain))
+                           (when buffer-modified
+                             (basic-save-buffer)
+                             (cl-incf modified-files-count))))
+                       ;; Kill buffer after processing
+                       (unless (or (eq (current-buffer) (get-buffer file)) ; Don't kill if it was already open interactively
+                                   (memq (current-buffer) (buffer-list))) ; Check if buffer still exists
+                         (kill-buffer (current-buffer))))
+                   (error
+                    (push (format "Error processing %s: %s" file (error-message-string err)) errors))))
+
+        ;; --- Reporting Phase ---
+        (message "Migration complete.")
+        (message "Migrated %d tag instances for %d selected tags across %d files."
+                 migrated-instances-count
+                 (length tags-to-migrate)
+                 modified-files-count)
+        (when errors
+          (warn "Encountered %d errors during migration:" (length errors))
+          (dolist (err-msg errors) (message "- %s" err-msg))
+          (message "Please check the affected files manually."))))))
 
 (provide 'org-supertag-migration)
+
 ;;; org-supertag-migration.el ends here 
\ No newline at end of file
diff --git a/org-supertag-node.el b/org-supertag-node.el
old mode 100644
new mode 100755
index c4be761..5988b84
--- a/org-supertag-node.el
+++ b/org-supertag-node.el
@@ -16,6 +16,10 @@
 (require 'org-id)
 (require 'org-supertag-db)
 (require 'org-supertag-query)
+<<<<<<< HEAD
+=======
+(require 'cl-lib)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;;------------------------------------------------------------------------------
 ;; Database Operations
@@ -115,19 +119,13 @@ TAG-ID is the tag identifier"
   "Remove a tag from a node.
 NODE-ID is the node identifier
 TAG-ID is the tag identifier"
-  (let ((link-id (format ":node-tag:%s->%s" node-id tag-id))
-        (removed nil))
-    ;; Remove association from database
-    (when (ht-contains-p org-supertag-db--link link-id)
-      (remhash link-id org-supertag-db--link)
-      (setq removed t))
-    ;; Clear caches if something was removed
-    (when removed
-      (org-supertag-db--cache-remove 'query (format "node-tags:%s" node-id))
-      (org-supertag-db--cache-remove 'query (format "tag-nodes:%s" tag-id))
-      ;; Schedule save
-      (org-supertag-db-save))
-    removed))
+  (when (and (org-supertag-node-db-exists-p node-id)
+             (org-supertag-db-exists-p tag-id))
+    ;; Remove relationship
+    (org-supertag-db-remove-link :node-tag node-id tag-id)
+    ;; Trigger event
+    (run-hook-with-args 'org-supertag-node-tag-removed-hook
+                        node-id tag-id)))
 
 (defun org-supertag-node-db--get-candidates ()
   "Get list of all referenceable node candidates.
@@ -152,20 +150,27 @@ Notes:
           (lambda (a b)
             (string< (car a) (car b))))))
 
+<<<<<<< HEAD
+=======
+(defun org-supertag-node-get-all ()
+  "Get a list of all node IDs from the database."
+  (let ((nodes '()))
+    (maphash (lambda (id props)
+               (when (eq (plist-get props :type) :node)
+                 (push id nodes)))
+             org-supertag-db--object)
+    (nreverse nodes)))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 (defun org-supertag-node-get-tags (node-id)
-  "Get list of all valid tag IDs associated with a node.
-This function now filters out tags that don't have a corresponding
-tag definition in the database, preventing dangling links from being returned.
+  "Get list of all tag IDs associated with a node.
 NODE-ID is the node identifier"
   (let ((result nil))
     (maphash
      (lambda (link-id props)
        (when (and (string-prefix-p ":node-tag:" link-id)
                   (equal (plist-get props :from) node-id))
-         (let ((tag-id (plist-get props :to)))
-           ;; Verify that the tag actually exists before adding it.
-           (when (org-supertag-tag-get tag-id)
-             (push tag-id result)))))
+         (push (plist-get props :to) result)))
      org-supertag-db--link)
     (nreverse result)))
 
@@ -175,26 +180,16 @@ Returns the first tag ID found for the node."
   (when-let* ((node-tags (org-supertag-node-get-tags node-id)))
     (car node-tags)))
 
-(defun org-supertag-node-get-parent-tags (node-id)
-  "Get the parent tags of the node.
-NODE-ID: The ID of the current node"
+<<<<<<< HEAD
+=======
+(defun org-supertag-node-get-at-point ()
+  "Get the node plist from the database corresponding to the Org headline at point.
+Returns the node's property list, or nil if not at a valid node."
   (save-excursion
-    (message "DEBUG: org-supertag-node-get-parent-tags called with node-id: %s" node-id)
-    (when-let* ((pos (org-id-find node-id t)))
-      (goto-char pos)
-      (message "DEBUG: Child node (id: %s) found at pos %s" node-id pos)
-      (if (org-up-heading-safe)
-          (let* ((parent-id (org-id-get))
-                 (parent-tags (when parent-id (org-supertag-node-get-tags parent-id))))
-            (message "DEBUG: Parent ID found: %s" parent-id)
-            (message "DEBUG: Parent tags from DB: %S" parent-tags)
-            parent-tags)
-        ;; If no parent heading is found, log and return nil explicitly to avoid
-        ;; propagating a string value that breaks callers expecting a list.
-        (progn
-          (message "DEBUG: org-up-heading-safe returned nil. No parent found.")
-          nil)))))
-
+    (when (org-at-heading-p)
+      (when-let ((id (org-id-get)))
+        (org-supertag-db-get id)))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-node--ensure-sync ()
   "Ensure current node is properly synced with database.
@@ -215,15 +210,7 @@ Returns node ID if successful, nil otherwise."
               (org-supertag-node-sync-at-point))))
       ;; If we can't find a parent heading
       (error nil))))
-
-(defun org-supertag-node-get-all ()
-  "Get a list of all node IDs from the database."
-  (let ((nodes '()))
-    (maphash (lambda (id props)
-               (when (eq (plist-get props :type) :node)
-                 (push id nodes)))
-             org-supertag-db--object)
-    (nreverse nodes)))
+      
 ;;------------------------------------------------------------------------------
 ;; Node Field Relations
 ;;------------------------------------------------------------------------------
@@ -272,35 +259,68 @@ List of field values, each element is (field-id . value)"
 ;;------------------------------------------------------------------------------    
 
 (defun org-supertag-node-sync-at-point ()
-  "Synchronize current node with database, including full tag registration."
+<<<<<<< HEAD
+  "Synchronize current node with database."
   (when (org-at-heading-p)
     (let* ((node-id (org-id-get-create))
            (old-node (org-supertag-db-get node-id))
+           ;; use more accurate boundary
            (node-data (org-supertag-db--parse-node-at-point))
+           ;; keep existing reference relations
            (refs-from (when old-node (plist-get old-node :ref-from)))
+           ;; merge properties
            (props (append node-data
                          (list :ref-from (or refs-from nil)
-                               :created-at (or (and old-node (plist-get old-node :created-at))
-                                               (current-time)))))
-           (headline-tags (plist-get props :tags)))
+                               :created-at (or (and old-node 
+                                                 (plist-get old-node :created-at))
+                                            (current-time))))))
       
-      (message "Syncing node %s. Tags from file: %S" node-id headline-tags)
+      (message "Debug - props: %S" props)
+      ;; Update database
+=======
+  "Synchronize current node with database, including reference links."
+  (when (org-at-heading-p)
+    (let* ((node-id (org-id-get-create))
+           (old-node (org-supertag-db-get node-id))
+           (old-refs-to (or (plist-get old-node :ref-to) '()))
+           ;; Parse node data from buffer
+           (node-data (org-supertag-db--parse-node-at-point))
+           (content (plist-get node-data :content))
+           (new-refs-to (org-supertag-node--extract-links-from-content content))
+           ;; Get tag names from headline, convert to IDs
+           (tag-names (org-get-tags))
+           (tag-ids (mapcar #'org-supertag-tag-get-or-create tag-names))
+           ;; Determine added and removed references
+           (added-refs (seq-difference new-refs-to old-refs-to #'string=))
+           (removed-refs (seq-difference old-refs-to new-refs-to #'string=))
+           ;; Merge properties for the current node
+           (props (append node-data
+                         (list :ref-to new-refs-to ; Update with new refs
+                               :ref-from (or (plist-get old-node :ref-from) '())
+                               :tags tag-ids ; Store tag UUIDs
+                               :created-at (or (and old-node
+                                                 (plist-get old-node :created-at))
+                                            (current-time))))))
+
+      ;; --- Bidirectional Link Maintenance ---
+      ;; 1. For each newly added reference, update the target node's :ref-from
+      (dolist (target-id added-refs)
+        (when-let* ((target-node (org-supertag-db-get target-id)))
+          (let* ((current-ref-from (or (plist-get target-node :ref-from) '()))
+                 (new-ref-from (adjoin node-id current-ref-from :test #'string=)))
+            (org-supertag-db-add target-id (plist-put target-node :ref-from new-ref-from)))))
+
+      ;; 2. For each removed reference, update the target node's :ref-from
+      (dolist (target-id removed-refs)
+        (when-let* ((target-node (org-supertag-db-get target-id)))
+          (let* ((current-ref-from (plist-get target-node :ref-from))
+                 (new-ref-from (remove node-id current-ref-from :test #'string=)))
+            (org-supertag-db-add target-id (plist-put target-node :ref-from new-ref-from)))))
       
-      ;; 1. Update/create the node object itself.
+      (message "Debug - props: %S" props)
+      ;; Update the current node in the database
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
       (org-supertag-db-add node-id props)
-      
-      ;; 2. Process and register all tags found in the headline.
-      (if (null headline-tags)
-          (message "No headline tags to process for node %s." node-id)
-        (dolist (tag-name headline-tags)
-          (let ((sanitized-tag (org-supertag-sanitize-tag-name tag-name)))
-            (message "Processing tag: %s (sanitized: %s)" tag-name sanitized-tag)
-            ;; Ensure tag object exists
-            (unless (org-supertag-tag-get sanitized-tag)
-              (org-supertag-tag--create sanitized-tag))
-            ;; Ensure link exists
-            (org-supertag-node-db-add-tag node-id sanitized-tag))))
-      
       ;; Return node ID
       node-id)))
 
@@ -326,10 +346,15 @@ Notes:
       ;; Update properties drawer
       (org-set-property "ID" node-id)
       ;; Update other display states
+<<<<<<< HEAD
       (let ((tags (plist-get node :tags))
+=======
+      (let ((tag-ids (plist-get node :tags))
+            (tags (mapcar #'org-supertag-tag-get-name-by-id tag-ids))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
             (todo (plist-get node :todo)))
-        ;; (when tags
-        ;;   (org-set-tags tags))
+        (when tags
+          (org-set-tags tags))
         (when todo
           (org-todo todo))))))
 
@@ -740,6 +765,7 @@ Returns:
           (message "Node deleted: %s" node-id)))
     (user-error "No node found at current position")))
 
+<<<<<<< HEAD
 (defun org-supertag-node--ensure-id-system ()
   "Ensure org-id system is properly initialized."
   (require 'org-id)
@@ -750,6 +776,8 @@ Returns:
       (org-id-locations-load))))
 
 
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 (defun org-supertag-node-update ()
   "Update node at current position.
 Uses org-supertag-node-sync-at-point to perform a complete node synchronization."
@@ -768,6 +796,19 @@ Uses org-supertag-node-sync-at-point to perform a complete node synchronization.
           node-id)
       (user-error "Failed to sync node. Please ensure:\n1. You're on a valid heading\n2. The heading is not archived or commented\n3. The node ID is valid"))))
 
+(defun org-supertag-node-remove-tag (node-id tag-id)
+  "Remove tag association from a node.
+NODE-ID is the node identifier
+TAG-ID is the tag identifier"
+  (let ((link-id (format ":node-tag:%s->%s" node-id tag-id)))
+    ;; Remove association from database
+    (remhash link-id org-supertag-db--link)
+    ;; Clear caches
+    (org-supertag-db--cache-remove 'query (format "node-tags:%s" node-id))
+    (org-supertag-db--cache-remove 'query (format "tag-nodes:%s" tag-id))
+    ;; Schedule save
+    (org-supertag-db-save)))
+
 (defun org-supertag-node-at-valid-heading-p ()
   "Check if point is at a valid heading for tag operations.
 Returns t if:
@@ -894,75 +935,6 @@ filename / outline-path / title"
           (message "Jumped to node: %s" choice))))))
 
 
-;;------------------------------------------------------------------------------
-;; Node Location Functions
-;;------------------------------------------------------------------------------
-
-(defun org-supertag-find-node-location (node-id &optional file-path)
-  "Find the location of a node by ID using database information and file search.
-NODE-ID: The node identifier to find
-FILE-PATH: Optional file path, if nil will get from database
-
-Returns:
-- (POSITION . FILE-PATH) if found
-- nil if not found
-
-This function does NOT use org-id-find, instead it:
-1. Gets file path from database
-2. Searches for the ID in the file
-3. Returns the position of the headline"
-  (let* ((node-data (org-supertag-db-get node-id))
-         (target-file (or file-path (plist-get node-data :file-path))))
-    
-    (when (and target-file (file-exists-p target-file))
-      (with-temp-buffer
-        (org-mode)
-        (insert-file-contents target-file)
-        (save-excursion
-          (save-restriction
-            (widen)
-            (goto-char (point-min))
-            ;; Search for the ID property in the file
-            (when (re-search-forward
-                   (format "^[ \t]*:ID:[ \t]+%s[ \t]*$" (regexp-quote node-id))
-                   nil t)
-              ;; Found the ID property, now find the corresponding headline
-              (let ((id-pos (point)))
-                ;; Move backward to find the headline that contains this property
-                (when (re-search-backward "^\\*+ " nil t)
-                  (beginning-of-line)
-                  (when (org-at-heading-p)
-                    ;; Verify this is the correct headline by checking if the ID property
-                    ;; is within this headline's property drawer
-                    (let ((headline-start (point))
-                          (headline-end (save-excursion
-                                          (org-end-of-subtree t t)
-                                          (point))))
-                      (when (and (>= id-pos headline-start)
-                                 (<= id-pos headline-end))
-                        (cons (point) target-file)))))))))))))
-
-(defun org-supertag-goto-node (node-id)
-  "Go to a node by ID using database information.
-NODE-ID: The node identifier to navigate to
-
-Returns:
-- t if successfully navigated
-- nil if node not found"
-  (when-let* ((location (org-supertag-find-node-location node-id)))
-    (let ((pos (car location))
-          (file (cdr location)))
-      ;; Switch to the file and position
-      (find-file file)
-      (widen)
-      (goto-char pos)
-      ;; Ensure we're at the beginning of the headline
-      (org-back-to-heading t)
-      ;; Show the context
-      (org-show-context)
-      (org-show-subtree)
-      t)))
-
 ;;------------------------------------------------------------------------------
 ;; Node Relations 
 ;;------------------------------------------------------------------------------    
@@ -1035,6 +1007,25 @@ TO-ID is the target node"
 ;; Node Reference Functions
 ;;------------------------------------------------------------------------------  
 
+<<<<<<< HEAD
+=======
+(defun org-supertag-node--extract-links-from-content (content)
+  "Extract all [[id:...]] link UUIDs from a string.
+CONTENT is the string to parse.
+Returns a list of unique UUIDs found in the content."
+  (let ((links '())
+        (pos 0))
+    (with-temp-buffer
+      (insert content)
+      (goto-char (point-min))
+      (while (re-search-forward org-link-any-re nil t)
+        (let* ((element (org-element-context))
+               (type (org-element-property :type element))
+               (path (org-element-property :path element)))
+          (when (and (equal type "id") (org-uuidgen-p path))
+            (push path links)))))
+    (delete-dups (nreverse links))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-node--insert-reference (node-id)
   "Insert node reference at current position and update relationships.
@@ -1307,7 +1298,7 @@ This function will:
           (princ (format "%s\n" file))))
       
       (princ "\nRecommended Actions:\n")
-      (princ "------------------\n"
+      (princ "------------------\n")
       (princ "1. Check if problematic files still exist\n")
       (princ "2. Run org-id-update-id-locations\n")
       (princ "3. Consider removing invalid entries from the database\n")
@@ -1316,6 +1307,6 @@ This function will:
         (princ "\nTo remove invalid entries, evaluate:\n")
         (princ "(dolist (id '")
         (prin1 missing-ids (current-buffer))
-        (princ ") (org-supertag-db-remove id))"))))))
+        (princ ") (org-supertag-db-remove id))")))))
 
 (provide 'org-supertag-node)
diff --git a/org-supertag-proactive-engine.el b/org-supertag-proactive-engine.el
new file mode 100755
index 0000000..4b541a3
--- /dev/null
+++ b/org-supertag-proactive-engine.el
@@ -0,0 +1,141 @@
+;;; org-supertag-proactive-engine.el --- Proactive analysis engine for org-supertag -*- lexical-binding: t -*-
+
+(require 'org-supertag-db)
+(require 'org-supertag-api)
+(require 'org-supertag-ui-conversation)
+
+(defgroup org-supertag-proactive-engine nil
+  "Settings for the proactive intelligence engine."
+  :group 'org-supertag)
+
+(defcustom org-supertag-proactive-engine-enabled t
+  "Enable the proactive engine to analyze content automatically."
+  :type 'boolean
+  :group 'org-supertag-proactive-engine)
+
+(defcustom org-supertag-proactive-engine-idle-delay 2.5
+  "Idle time in seconds before triggering proactive analysis."
+  :type 'float
+  :group 'org-supertag-proactive-engine)
+
+(defvar org-supertag-proactive-engine--timer nil
+  "Timer for proactive analysis.")
+(defvar org-supertag-proactive-engine--last-node-id nil
+  "The ID of the node the cursor was last in.")
+(defvar org-supertag-proactive-engine--last-tick nil
+  "The buffer-modified-tick of the last check.")
+
+(defun org-supertag-proactive-engine--trigger-analysis ()
+  "Decide which analysis to run based on buffer state."
+  (when (and (derived-mode-p 'org-mode)
+             org-supertag-proactive-engine--last-node-id)
+    (let ((is-modified (and org-supertag-proactive-engine--last-tick
+                            (/= (buffer-modified-tick) org-supertag-proactive-engine--last-tick))))
+      (if is-modified
+          (org-supertag-proactive-engine--run-conceptual-resonance)
+        (org-supertag-proactive-engine--run-knowledge-archaeology)))))
+
+(defun org-supertag-proactive-engine--run-conceptual-resonance ()
+  "Run conceptual resonance analysis on the current node.
+This now involves sending the full node context to the backend for
+graph construction, vectorization, and analysis."
+  (message "Proactive Engine: Analyzing current node context...")
+  (let* ((node-id org-supertag-proactive-engine--last-node-id)
+         (node-props (org-supertag-db-get node-id))
+         (content (plist-get node-props :content))
+         (tags (plist-get node-props :tags)))
+    (when content
+      (let ((context-data `(:event_type "user_idle"
+                           :node_id ,node-id
+                           :buffer_content ,content
+                           :current_tags ,tags
+                           :dialogue_mode "normal" ;; Or get from a variable
+                           :timestamp ,(format-time-string "%Y-%m-%dT%H:%M:%SZ" (current-time) t))))
+        (org-supertag-api-analyze-node-context
+         context-data
+         (lambda (result)
+           (when (and result (plist-get result :insight))
+             (let* ((insight (plist-get result :insight))
+                    (resonant-title (plist-get result :resonant_note_title))
+                    (formatted-text (format "✨ *Context Analyzed*\n\n%s\n\n*Referenced:* `%s`" insight resonant-title)))
+               (org-supertag-ui-conversation-add-message "ai" formatted-text)))))))))
+
+(defun org-supertag-proactive-engine--run-knowledge-archaeology ()
+  "Run knowledge archaeology on the current node's concept."
+  (message "Proactive Engine: Running Knowledge Archaeology...")
+  (let* ((node-id org-supertag-proactive-engine--last-node-id)
+         (title (plist-get (org-supertag-db-get node-id) :title)))
+    (when title
+      (org-supertag-api-knowledge-archaeology-dig
+       title
+       (lambda (result)
+         (when result
+           (let* ((formatted-list (mapcar (lambda (node)
+                                            (format "- *%s* (%s)"
+                                                    (or (plist-get node :title) "No Title")
+                                                    (format-time-string "%Y-%m-%d" (plist-get node :document_date))))
+                                          result))
+                  (formatted-text (format " digging into the history of *%s*...\n\n%s"
+                                          title
+                                          (string-join formatted-list "\n"))))
+             (org-supertag-ui-conversation-add-message "ai" formatted-text))))))))
+
+(defun org-supertag-proactive-engine--post-command-hook-function ()
+  "Hook function to monitor cursor position and buffer modifications."
+  (when (and org-supertag-proactive-engine-enabled
+             (derived-mode-p 'org-mode)
+             (buffer-file-name)
+             (org-supertag--org-file-p (buffer-file-name)))
+    (let ((current-node-id (org-id-get)))
+      (when (and current-node-id (not (equal current-node-id org-supertag-proactive-engine--last-node-id)))
+        (setq org-supertag-proactive-engine--last-node-id current-node-id)
+        (setq org-supertag-proactive-engine--last-tick (buffer-modified-tick))
+        
+        (when org-supertag-proactive-engine--timer
+          (cancel-timer org-supertag-proactive-engine--timer))
+        
+        (setq org-supertag-proactive-engine--timer
+              (run-with-idle-timer
+               org-supertag-proactive-engine-idle-delay
+               nil
+               'org-supertag-proactive-engine--trigger-analysis))))))
+
+;;;###autoload
+(defun org-supertag-proactive-engine-activate ()
+  "Activate the proactive analysis engine."
+  (interactive)
+  (add-hook 'post-command-hook 'org-supertag-proactive-engine--post-command-hook-function nil t)
+  (when (called-interactively-p 'any)
+    (message "Org SuperTag Proactive Engine Activated.")))
+
+;;;###autoload
+(defun org-supertag-proactive-engine-deactivate ()
+  "Deactivate the proactive analysis engine."
+  (interactive)
+  (remove-hook 'post-command-hook 'org-supertag-proactive-engine--post-command-hook-function)
+  (when org-supertag-proactive-engine--timer
+    (cancel-timer org-supertag-proactive-engine--timer)
+    (setq org-supertag-proactive-engine--timer nil))
+  (message "Org SuperTag Proactive Engine Deactivated."))
+
+(defun org-supertag-proactive-engine--analyze-context ()
+  "Gather context and send it to the backend for analysis."
+  (let* ((node-id (org-id-get))
+         (context-data
+          `((:node_id . ,node-id)
+            (:file_path . ,(buffer-file-name))
+            (:buffer_content . ,(buffer-string))
+            (:current_tags . ,(org-get-tags))
+            (:cursor_pos . ,(point))
+            (:title . ,(org-get-heading t t t t)))))
+    (message "Proactive Engine: Sending context for node %s to backend..." node-id)
+    (org-supertag-api-analyze-node-context
+     context-data
+     (lambda (result)
+       (when (and result (plist-get result :insight))
+         (let* ((insight (plist-get result :insight))
+                (resonant-title (plist-get result :resonant_note_title))
+                (formatted-text (format "✨ *Context Analyzed*\n\n%s\n\n*Referenced:* `%s`" insight resonant-title)))
+           (org-supertag-ui-conversation-add-message "ai" formatted-text)))))))
+
+(provide 'org-supertag-proactive-engine) 
\ No newline at end of file
diff --git a/org-supertag-recovery.el b/org-supertag-recovery.el
old mode 100755
new mode 100644
index 340f81c..8f0dc6d
--- a/org-supertag-recovery.el
+++ b/org-supertag-recovery.el
@@ -15,9 +15,6 @@
 (require 'org-supertag-db)
 (require 'org-supertag-sync)
 (require 'org-supertag-relation)
-(require 'org-supertag-node)
-(require 'org-supertag-tag)
-(require 'cl-lib)
 
 ;; Recovery state tracking
 (defvar org-supertag-recovery--state (make-hash-table :test 'equal)
@@ -870,101 +867,6 @@ Select (1-9): "
 (message "Run: M-x org-supertag-recovery-full-suite to start recovery")
 (message "Run: M-x org-supertag-emergency-recovery for emergency recovery")
 
-(defun org-supertag-diagnose-dangling-links ()
-  "Diagnose and offer to fix dangling node-tag links.
-A dangling link is a :node-tag link where the 'to' part
-(the tag-id) does not exist as a defined tag."
-  (interactive)
-  (let ((dangling-links '())
-        (fixed-count 0))
-    ;; Collect all dangling links
-    (maphash
-     (lambda (link-id props)
-       (when (and (string-prefix-p ":node-tag:" link-id)
-                  (eq (plist-get props :type) :node-tag))
-         (let ((tag-id (plist-get props :to)))
-           (unless (org-supertag-tag-get tag-id)
-             (push (list link-id props) dangling-links)))))
-     org-supertag-db--link)
-
-    ;; Report and offer to fix
-    (if (not dangling-links)
-        (message "No dangling links found. Your database is clean.")
-      (progn
-        (message "Found %d dangling links:" (length dangling-links))
-        (dolist (link-info dangling-links)
-          (let* ((link-id (car link-info))
-                 (props (cadr link-info))
-                 (node-id (plist-get props :from))
-                 (tag-id (plist-get props :to)))
-            (message "  - Node '%s' is linked to non-existent tag '%s'."
-                     (or (plist-get (org-supertag-db-get node-id) :title) node-id)
-                     tag-id)))
-        (when (yes-or-no-p "Do you want to remove all these invalid links? ")
-          (dolist (link-info dangling-links)
-            (let ((link-id (car link-info)))
-              (remhash link-id org-supertag-db--link)
-              (setq fixed-count (1+ fixed-count))))
-          (org-supertag-db--mark-dirty)
-          (org-supertag-db--schedule-save)
-          (message "Removed %d dangling links. Database has been cleaned." fixed-count))))))
-
-;;; =================================================================
-;;; Corrupted Data Cleaning
-;;; =================================================================
-
-(defun org-supertag-recovery--value-is-corrupted-p (value)
-  "Check if a value has text properties, indicating corruption.
-This function recursively checks lists and does not use `cl-lib'."
-  (cond
-   ((stringp value) (not (null (text-properties-at 0 value))))
-   ((symbolp value) (not (null (text-properties-at 0 (symbol-name value)))))
-   ((listp value)
-    (let ((corrupted-found nil)
-          (remaining-list value))
-      (while (and remaining-list (not corrupted-found))
-        (when (org-supertag-recovery--value-is-corrupted-p (car remaining-list))
-          (setq corrupted-found t))
-        (setq remaining-list (cdr remaining-list)))
-      corrupted-found))
-   (t nil)))
-
-(defun org-supertag-recovery-delete-corrupted-links ()
-  "Find and delete corrupted entries from `org-supertag-db--link`.
-
-This is a destructive operation. It deletes any entry where the key
-or any part of the value plist contains text properties (often from
-completion frameworks). It will ask for confirmation before deleting.
-
-This function is designed to be simple and robust, with no
-dependencies on `cl-lib'."
-  (interactive)
-  (let ((to-delete '())
-        (scanned-count 0))
-    (message "Scanning for corrupted links to delete...")
-
-    (maphash
-     (lambda (key props)
-       (setq scanned-count (1+ scanned-count))
-       (when (or (org-supertag-recovery--value-is-corrupted-p key)
-                 (org-supertag-recovery--value-is-corrupted-p props))
-         (push key to-delete)))
-     org-supertag-db--link)
-
-    (if (not to-delete)
-        (message "Scan complete (scanned %d links). No corrupted links found." scanned-count)
-      (progn
-        (message "Found %d corrupted links to delete:" (length to-delete))
-        (dolist (key to-delete)
-          (message "  - Corrupted key: %s" key))
-        (when (yes-or-no-p "Do you want to permanently delete these invalid links? ")
-          (dolist (key to-delete)
-            (remhash key org-supertag-db--link))
-          (org-supertag-db--mark-dirty)
-          (org-supertag-db--schedule-save)
-          (message "Deletion complete. Deleted %d corrupted link(s). Database marked for saving." (length to-delete)))))))
-
-
 (provide 'org-supertag-recovery)
 
 ;;; org-supertag-recovery.el ends here
diff --git a/org-supertag-relation.el b/org-supertag-relation.el
old mode 100755
new mode 100644
index 5e68279..2a7f3b4
--- a/org-supertag-relation.el
+++ b/org-supertag-relation.el
@@ -10,10 +10,12 @@
 (require 'cl-lib)
 (require 'org-element)
 (require 'org-supertag-db)
-(require 'org-supertag-node nil t)  
-(require 'org-supertag-tag nil t)   
-
-
+<<<<<<< HEAD
+=======
+(require 'org-supertag-node nil t)  ;; 为 org-supertag-node-get-tags 函数
+(require 'org-supertag-tag nil t)   ;; 为 org-supertag-tag-get-id-by-name 等函数
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+(require 'org-supertag-sim-epc nil t)  ;; 如果可用则加载，否则静默跳过
 
 (defgroup org-supertag-relation nil
   "Customization for org-supertag tag relationships."
@@ -87,320 +89,6 @@ When set to t, adding a relation will automatically add its complementary relati
   :type 'boolean
   :group 'org-supertag-relation)
 
-(defcustom org-supertag-relation-cooccurrence-file
-  (org-supertag-data-file "relation-cooccurrence.el")
-  "File to store co-occurrence relationship data."
-  :type 'file
-  :group 'org-supertag-relation)
-
-(defvar org-supertag-relation--cooccurrence-db (make-hash-table :test 'equal)
-  "In-memory database for co-occurrence statistics.")
-
-(defvar org-supertag-relation--db-dirty nil
-  "Flag indicating if the co-occurrence database has unsaved changes.")
-
-(defvar org-supertag-relation--save-timer nil
-  "Timer for delayed saving of the co-occurrence database.")
-
-(defun org-supertag-relation--clean-completion (value)
-  "Safely extract a string from VALUE returned by `completing-read`."
-  (if (listp value)
-      (car value)
-    value))
-
-(defun org-supertag-relation--mark-dirty ()
-  "Mark the co-occurrence database as dirty."
-  (setq org-supertag-relation--db-dirty t))
-
-(defun org-supertag-relation-save-db ()
-  "Save the co-occurrence database to its file."
-  (when org-supertag-relation--db-dirty
-    (message "Attempting to save co-occurrence database to %s..." org-supertag-relation-cooccurrence-file)
-    ;; Ensure the data directory exists before saving.
-    (make-directory (file-name-directory org-supertag-relation-cooccurrence-file) t)
-    (with-temp-buffer
-      (let ((print-level nil) (print-length nil)
-            ;; Explicitly set coding system to prevent errors on write.
-            (coding-system-for-write 'utf-8-emacs-unix))
-        (prin1 org-supertag-relation--cooccurrence-db (current-buffer))
-        (write-region (point-min) (point-max) org-supertag-relation-cooccurrence-file)))
-    (setq org-supertag-relation--db-dirty nil)
-    (message "Co-occurrence database saved successfully.")))
-
-(defun org-supertag-relation--schedule-save ()
-  "Schedule a delayed save for the co-occurrence database."
-  (when org-supertag-relation--save-timer
-    (cancel-timer org-supertag-relation--save-timer))
-  (setq org-supertag-relation--save-timer
-        (run-with-idle-timer 2 nil #'org-supertag-relation-save-db)))
-
-(defun org-supertag-relation-load-db ()
-  "Load the co-occurrence database from its file."
-  (when (file-exists-p org-supertag-relation-cooccurrence-file)
-    (with-temp-buffer
-      ;; Explicitly read with UTF-8 to match the saving process.
-      (let ((coding-system-for-read 'utf-8))
-        (insert-file-contents org-supertag-relation-cooccurrence-file))
-      ;; Handle empty or invalid file case gracefully
-      (goto-char (point-min))
-      (unless (eobp)
-        (let ((data (condition-case nil
-                        (read (current-buffer))
-                      (error nil))))
-          (if (hash-table-p data)
-              (setq org-supertag-relation--cooccurrence-db data)
-            (message "Warning: org-supertag co-occurrence DB file is corrupt.")))))))
-
-(defun org-supertag-relation--migrate-from-old-db ()
-  "Try to find and extract co-occurrence data from the old DB format.
-The old format stored this data in a special ':metadata' object in the main DB."
-  (let* ((metadata-object (gethash "metadata" org-supertag-db--object)))
-    (when (and metadata-object (plist-get metadata-object :type) (eq (plist-get metadata-object :type) :metadata))
-      (let ((cooccurrence-data (plist-get metadata-object :data)))
-        (when (hash-table-p cooccurrence-data)
-          cooccurrence-data)))))
-
-(defun org-supertag-relation-init ()
-  "Initialize the relation module, loading its database or migrating old data."
-  ;; First, load whatever is in the new DB file, if anything.
-  (org-supertag-relation-load-db)
-
-  ;; If the newly loaded DB is empty, check if we need to migrate from the old format.
-  (when (= 0 (hash-table-count org-supertag-relation--cooccurrence-db))
-    (message "Co-occurrence DB is empty. Checking for old data to migrate...")
-    (let ((migrated-data (org-supertag-relation--migrate-from-old-db)))
-      (when migrated-data
-        (message "Found old data. Starting migration...")
-        (setq org-supertag-relation--cooccurrence-db migrated-data)
-        (org-supertag-relation--mark-dirty)
-        (org-supertag-relation-save-db)
-        (message "Successfully migrated %d co-occurrence records from old database." (hash-table-count migrated-data))))))
-
-;; Save on exit.
-(add-hook 'kill-emacs-hook #'org-supertag-relation-save-db)
-
-
-;;----------------------------------------------------------------------
-;; Co-occurrence relation management
-;;----------------------------------------------------------------------
-
-(defun org-supertag-relation--upsert-relation (from-tag to-tag rel-type strength)
-  "Add or update a tag relation with a specific strength."
-  (let ((rel-id (format ":tag-relation:%s->%s:%s" from-tag to-tag rel-type)))
-    (let ((props (list :from from-tag
-                       :to to-tag
-                       :type rel-type
-                       :strength strength
-                       :created-at (or (when-let (old-props (gethash rel-id org-supertag-db--link))
-                                         (plist-get old-props :created-at))
-                                       (current-time)))))
-      (puthash rel-id props org-supertag-db--link)
-      (org-supertag-db-emit 'link:updated :tag-relation from-tag to-tag props)
-      (org-supertag-db--mark-dirty)
-      (org-supertag-db--schedule-save))))
-
-(defun org-supertag-relation--update-cooccurrence-strength (from-tag to-tag)
-  "Update (or create) the co-occurrence relation using raw frequency only.
-方案 A: 不再计算归一化强度，也不再基于最小阈值删除关系，
-而是直接把累计出现次数写入 :strength 字段，供后续统计使用。" 
-  (let* ((freq (org-supertag-relation--get-cooccurrence-count from-tag to-tag))
-         (norm-factor org-supertag-relation-cooccurrence-normalization-factor)
-         (strength (if (> (+ freq norm-factor) 0)
-                        (/ freq (+ freq norm-factor))
-                      0)))
-    (if (< strength org-supertag-relation-min-strength)
-        (org-supertag-relation-remove-relation from-tag to-tag 'cooccurrence)
-      (org-supertag-relation--upsert-relation from-tag to-tag 'cooccurrence strength)))
-
-  ;; 方案 A: 直接使用频次作为 strength，永不自动删除。
-  (let ((freq (org-supertag-relation--get-cooccurrence-count from-tag to-tag)))
-    (org-supertag-relation--upsert-relation from-tag to-tag 'cooccurrence freq)))
-
-(defun org-supertag-relation-record-cooccurrence (node-id tag-id)
-  "Record the co-occurrence relationship of tags, unidirectionally propagated from parent node to child node.
-NODE-ID: Node ID
-TAG-ID: Tag ID
-
-This function will:
-1. Record the co-occurrence relationship between tags on the same node
-2. Record the unidirectional co-occurrence relationship from parent node tags"
-  ;; --- FIX: Sanitize all incoming and retrieved tag IDs at the source ---
-  ;; This prevents contaminated strings from being used to build DB keys.
-  (let* ((clean-tag-id (substring-no-properties (format "%s" tag-id)))
-         (existing-tags (org-supertag-node-get-tags node-id))
-         (clean-existing-tags (mapcar (lambda (t) (substring-no-properties (format "%s" t))) existing-tags))
-         (parent-tags (org-supertag-node-get-parent-tags node-id))
-         (clean-parent-tags (mapcar (lambda (t) (substring-no-properties (format "%s" t))) parent-tags)))
-
-    ;; 1. Process co-occurrence between same-level tags
-    (dolist (other-tag clean-existing-tags)
-      (unless (equal other-tag clean-tag-id)
-        (let* ((freq (org-supertag-relation--get-cooccurrence-count clean-tag-id other-tag))
-               (new-freq (1+ freq)))
-          (org-supertag-relation--set-cooccurrence-count clean-tag-id other-tag new-freq)
-          ;; Update strengths in both directions
-          (org-supertag-relation--update-cooccurrence-strength clean-tag-id other-tag)
-          (org-supertag-relation--update-cooccurrence-strength other-tag clean-tag-id))))
-
-    ;; 2. Process bidirectional co-occurrence from parent node tags
-    (when clean-parent-tags
-      (dolist (parent-tag clean-parent-tags)
-        (unless (equal parent-tag clean-tag-id)
-          (let* ((freq (org-supertag-relation--get-cooccurrence-count parent-tag clean-tag-id))
-                 (new-freq (+ freq 0.5)))  ; The co-occurrence weight from parent to child is 0.5
-            (org-supertag-relation--set-cooccurrence-count parent-tag clean-tag-id new-freq)
-            ;; Update strengths in both directions
-            (org-supertag-relation--update-cooccurrence-strength parent-tag clean-tag-id)
-            (org-supertag-relation--update-cooccurrence-strength clean-tag-id parent-tag)))))))
-
-(defun org-supertag-relation-unrecord-cooccurrence (node-id removed-tag-id)
-  "Decrement co-occurrence relationships when a tag is removed from a node.
-This is the reverse operation of `org-supertag-relation-record-cooccurrence'."
-  (let ((remaining-tags (org-supertag-node-get-tags node-id)) ; This is after the tag link is removed
-        (parent-tags (org-supertag-node-get-parent-tags node-id)))
-    ;; 1. Update relationships with same-level tags that are still on the node
-    (dolist (other-tag remaining-tags)
-      (let* ((current-freq (org-supertag-relation--get-cooccurrence-count removed-tag-id other-tag))
-             (new-freq (max 0 (- current-freq 1))))
-        (org-supertag-relation--set-cooccurrence-count removed-tag-id other-tag new-freq)
-        ;; Update relation strength based on new frequency
-        (org-supertag-relation--update-cooccurrence-strength removed-tag-id other-tag)
-        (org-supertag-relation--update-cooccurrence-strength other-tag removed-tag-id)))
-    ;; 2. Update relationships with parent tags
-    (when parent-tags
-      (dolist (parent-tag parent-tags)
-        (let* ((current-freq (org-supertag-relation--get-cooccurrence-count parent-tag removed-tag-id))
-               (new-freq (max 0 (- current-freq 0.5)))) ; The weight from parent to child is 0.5
-          (org-supertag-relation--set-cooccurrence-count parent-tag removed-tag-id new-freq)
-          ;; Update relation strength based on new frequency
-          (org-supertag-relation--update-cooccurrence-strength parent-tag removed-tag-id)
-          (org-supertag-relation--update-cooccurrence-strength removed-tag-id parent-tag))))))
-
-(defun org-supertag-relation--get-cooccurrence-count (tag1 tag2)
-  "Get the number of co-occurrences between TAG1 and TAG2.
-If there is no co-occurrence record, return 0."
-  (let* ((rel-id-key (format "tag-cooccur:%s:%s" tag1 tag2)))
-    (or (gethash rel-id-key org-supertag-relation--cooccurrence-db 0) 0)))
-
-(defun org-supertag-relation--set-cooccurrence-count (tag1 tag2 count)
-  "Set the co-occurrence count between TAG1 and TAG2 to COUNT."
-  (let* ((rel-id-key1 (format "tag-cooccur:%s:%s" tag1 tag2))
-         (rel-id-key2 (format "tag-cooccur:%s:%s" tag2 tag1)))
-    ;; Store the count in both directions for easy lookup
-    (puthash rel-id-key1 count org-supertag-relation--cooccurrence-db)
-    (puthash rel-id-key2 count org-supertag-relation--cooccurrence-db)
-    (org-supertag-relation--mark-dirty)
-    (org-supertag-relation--schedule-save)))
-
-(defun org-supertag-relation--set-metadata (key value)
-  "Set metadata KEY to VALUE in the co-occurrence database."
-  (puthash key value org-supertag-relation--cooccurrence-db)
-  (org-supertag-relation--mark-dirty)
-  (org-supertag-relation--schedule-save))
-
-(defun org-supertag-relation--get-metadata (key)
-  "Get metadata for KEY from the co-occurrence database."
-  (gethash key org-supertag-relation--cooccurrence-db))
-
-;; Function to analyze co-occurrence patterns across the entire database
-(defun org-supertag-relation-analyze-cooccurrence-patterns ()
-  "Analyze co-occurrence patterns across all tags and update relationship strength."
-  (interactive)
-  (let* ((nodes (org-supertag-db-find-by-type :node))
-         (tag-counts (make-hash-table :test 'equal))  ; Count of nodes per tag
-         (cooccur-counts (make-hash-table :test 'equal))  ; Co-occurrence counts
-         (total-nodes (length nodes)))
-    
-    ;; Calculate the number of times each tag appears and co-occurrence counts
-    (dolist (node-id nodes)
-      (let ((tags (org-supertag-node-get-tags node-id)))
-        ;; Calculate the number of times a single tag appears
-        (dolist (tag tags)
-          (puthash tag (1+ (or (gethash tag tag-counts) 0)) tag-counts))
-        
-        ;; Calculate the co-occurrence count between tags
-        (dolist (tag1 tags)
-          (dolist (tag2 tags)
-            (unless (equal tag1 tag2)
-              (let ((key (format "%s:%s" tag1 tag2)))
-                (puthash key (1+ (or (gethash key cooccur-counts) 0)) cooccur-counts)))))))
-    
-    ;; Save the global tag frequency data to metadata
-    (org-supertag-relation--set-metadata 'tag-frequency-data tag-counts)
-    (org-supertag-relation--set-metadata 'tag-total-nodes total-nodes)
-    (org-supertag-relation--set-metadata 'tag-analysis-timestamp (current-time))
-    
-    ;; Calculate the point mutual information (PMI) for each co-occurrence
-    (maphash
-     (lambda (key count)
-       (let* ((tags (split-string key ":"))
-              (tag1 (car tags))
-              (tag2 (cadr tags))
-              (prob-t1 (/ (float (gethash tag1 tag-counts)) total-nodes))
-              (prob-t2 (/ (float (gethash tag2 tag-counts)) total-nodes))
-              (prob-t1t2 (/ (float count) total-nodes))
-              (pmi (log (/ prob-t1t2 (* prob-t1 prob-t2)) 2))
-              (norm-pmi (/ (1+ pmi) 2))  ; Normalize to 0-1 range
-              (strength (min 1.0 (max 0.1 norm-pmi))))  ; Limit to 0.1-1.0 range
-         
-         ;; Update co-occurrence count
-         (org-supertag-relation--set-cooccurrence-count tag1 tag2 count)
-         
-         ;; Store PMI value in metadata for statistical access
-         (org-supertag-relation--set-metadata (format "tag-pmi:%s:%s" tag1 tag2) norm-pmi)
-         
-         ;; If strength is meaningful, update the relation with the new strength
-         (when (>= strength org-supertag-relation-min-strength)
-           (org-supertag-relation-add-relation tag1 tag2 'cooccurrence)
-           (org-supertag-relation-add-relation tag2 tag1 'cooccurrence))))
-     cooccur-counts)
-    
-    (message "Co-occurrence analysis completed. Analyzed %d nodes and %d tag pairs. Saving results..."
-             total-nodes (hash-table-count cooccur-counts))
-    ;; Force an immediate save after this heavy operation.
-    (org-supertag-relation-save-db)))
-
-(defun org-supertag-relation-get-strength (tag1 tag2)
-  "Get the strength of the relationship between TAG1 and TAG2.
-If the relation does not exist, return nil."
-  (let ((rel (car (cl-remove-if
-                  (lambda (r) (equal (plist-get r :to) tag2))
-                  (org-supertag-relation-get-all tag1)))))
-    (when rel
-      (plist-get rel :strength))))
-
-;; Add a command to display co-occurrence relations
-(defun org-supertag-relation-find-cooccurrence-tags ()
-  "Find tags co-occurring with the current tag."
-  (interactive)
-  (let ((tag-id org-supertag-relation--current-tag))
-    (if (not tag-id)
-        (user-error "Current tag not set. Please re-select tag")
-      (let* ((relations (cl-remove-if-not
-                        (lambda (rel) (eq (plist-get rel :type) 'cooccurrence))
-                        (org-supertag-relation-get-all tag-id)))
-             (related-tags '()))
-        
-        ;; Collect related tags
-        (dolist (rel relations)
-          (let* ((other-tag-id (plist-get rel :to))
-                 (other-tag-name (org-supertag-tag-get-name-by-id other-tag-id))
-                 (strength (or (plist-get rel :strength) 0.0)))
-            (when other-tag-name
-              (push (cons other-tag-name (format "%.2f" strength)) related-tags))))
-        
-        ;; Display results
-        (if (null related-tags)
-            (message "No tags co-occurring with the current tag")
-          (with-output-to-temp-buffer "*Org-Supertag Co-occurrence*"
-            (princ "Tags co-occurring with the current tag (sorted by strength):\n\n")
-            (dolist (tag (sort related-tags (lambda (a b) (> (string-to-number (cdr a)) (string-to-number (cdr b))))))
-              (princ (format "- %s (Strength: %s)\n" (car tag) (cdr tag))))))))))
-
-;;----------------------------------------------------------------------
-;; Relation Type Management
-;;----------------------------------------------------------------------
-
 (defun org-supertag-relation--get-all-relation-types ()
   "Get all available relation types."
   org-supertag-relation-types)
@@ -416,9 +104,9 @@ Return the formatted option list."
              (complement-type (org-supertag-relation-get-complement rel-type))
              (formatted-option
               (if complement-type
-                  (let ((complement-desc
+                  (let ((complement-desc 
                          (cdr (assq complement-type relation-types))))
-                    (format "%s - %s ⟷ [Automatically adds bidirectional relation: %s]"
+                    (format "%s - %s ⟷ [Automatically adds bidirectional relation: %s]" 
                             rel-type description complement-type))
                 (format "%s - %s" rel-type description))))
         (push formatted-option choices)))
@@ -435,6 +123,31 @@ FROM-TAG: Source tag ID
 TO-TAG: Target tag ID
 REL-TYPE: Relation type
 STRENGTH: Relation strength (optional, default is 1.0)"
+  (interactive
+<<<<<<< HEAD
+   (let* ((from-tag (completing-read "Source tag: " (org-supertag-get-all-tags) nil t))
+          (to-tag (completing-read "Target tag: " (org-supertag-get-all-tags) nil t))
+          (rel-choices (org-supertag-relation--get-relation-type-choices))
+          (rel-type (org-supertag-relation--get-type-from-choice 
+                    (completing-read "Relation type: " rel-choices nil t))))
+     (list 
+      (org-supertag-tag-get-id-by-name from-tag)
+      (org-supertag-tag-get-id-by-name to-tag)
+      rel-type 
+=======
+   (let* ((from-tag-name (completing-read "Source tag: " (org-supertag-get-all-tags) nil t))
+          (to-tag-name (completing-read "Target tag: " (org-supertag-get-all-tags) nil t))
+          (from-tag-id (org-supertag-tag-get-id-by-name from-tag-name))
+          (to-tag-id (org-supertag-tag-get-id-by-name to-tag-name))
+          (rel-choices (org-supertag-relation--get-relation-type-choices))
+          (rel-type (org-supertag-relation--get-type-from-choice
+                     (completing-read "Relation type: " rel-choices nil t))))
+     (list
+      from-tag-id
+      to-tag-id
+      rel-type
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+      1.0)))
   ;; Check if the tags are valid
   (cond
    ((not from-tag)
@@ -443,6 +156,39 @@ STRENGTH: Relation strength (optional, default is 1.0)"
     (user-error "Target tag cannot be empty"))
    ((equal from-tag to-tag)
     (user-error "Source tag and target tag cannot be the same")))
+<<<<<<< HEAD
+  
+  (let* ((from-id (if (org-supertag-db-exists-p from-tag)
+                      from-tag
+                    (org-supertag-tag-get-id-by-name from-tag)))
+         (to-id (if (org-supertag-db-exists-p to-tag)
+                   to-tag
+                 (org-supertag-tag-get-id-by-name to-tag)))
+         ;; Include rel-type in the relation ID to support multiple relations
+         (rel-id (format ":tag-relation:%s->%s:%s" from-id to-id rel-type)))
+    
+    ;; Check if relation already exists
+    (when (gethash rel-id org-supertag-db--link)
+      (user-error "Relation already exists: %s -[%s]-> %s"
+                  (org-supertag-tag-get-name-by-id from-id)
+                  rel-type
+                  (org-supertag-tag-get-name-by-id to-id)))
+    
+    ;; Add the relation
+    (let ((props (list :from from-id
+                      :to to-id
+                      :type rel-type
+                      :strength (or strength 1.0)
+                      :created-at (current-time))))
+      (puthash rel-id props org-supertag-db--link)
+         (org-supertag-db-emit 'link:created :tag-relation from-id to-id props)
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db--schedule-save)
+      (message "Added relation: %s -[%s]-> %s" 
+               (org-supertag-tag-get-name-by-id from-id)
+               rel-type 
+               (org-supertag-tag-get-name-by-id to-id)))))
+=======
 
   (let ((rel-id (format ":tag-relation:%s->%s:%s" from-tag to-tag rel-type)))
     ;; Check if relation already exists
@@ -461,26 +207,51 @@ STRENGTH: Relation strength (optional, default is 1.0)"
                  (org-supertag-tag-get-name-by-id from-tag)
                  rel-type
                  (org-supertag-tag-get-name-by-id to-tag))))))
-
-(defun org-supertag-relation-add-relation-interactive ()
-  "Interactively add a tag relation."
-  (interactive)
-  (let* ((all-tags (org-supertag-get-all-tags))
-         (from-tag-name (org-supertag-relation--clean-completion
-                         (completing-read "Source tag: " all-tags nil t)))
-         (to-tag-name (org-supertag-relation--clean-completion
-                       (completing-read "Target tag: " all-tags nil t)))
-         (from-tag-id (org-supertag-tag-get-id-by-name from-tag-name))
-         (to-tag-id (org-supertag-tag-get-id-by-name to-tag-name))
-         (rel-choices (org-supertag-relation--get-relation-type-choices))
-         (rel-type (org-supertag-relation--get-type-from-choice
-                    (completing-read "Relation type: " rel-choices nil t))))
-    (org-supertag-relation-add-relation from-tag-id to-tag-id rel-type 1.0)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-relation-remove-relation (from-tag to-tag &optional rel-type)
   "Remove the relation between FROM-TAG and TO-TAG.
 If REL-TYPE is provided, only remove that specific relation type,
 otherwise remove all relations between the tags."
+  (interactive
+<<<<<<< HEAD
+   (let* ((from-tag (completing-read "Source tag: " (org-supertag-get-all-tags) nil t))
+          (to-choices (mapcar
+                     (lambda (rel)
+                       (format "%s (%s)" (car rel) (cdr rel)))
+                     (org-supertag-relation-get-all-from from-tag)))
+          (choice (completing-read "Relation to remove: " to-choices nil t))
+          (to-tag (car (split-string choice " ("))))
+     (list from-tag to-tag)))
+  
+  (when (and from-tag to-tag)
+    (let* ((from-id (if (org-supertag-db-exists-p from-tag)
+                       from-tag
+                     (org-supertag-tag-get-id-by-name from-tag)))
+           (to-id (if (org-supertag-db-exists-p to-tag)
+                     to-tag
+                   (org-supertag-tag-get-id-by-name to-tag)))
+           (from-name (org-supertag-tag-get-name-by-id from-id))
+           (to-name (org-supertag-tag-get-name-by-id to-id)))
+
+      ;; If rel-type is provided, only remove that specific relation
+      (if rel-type
+          (let ((rel-id (format ":tag-relation:%s->%s:%s" from-id to-id rel-type)))
+            (when (gethash rel-id org-supertag-db--link)
+              (remhash rel-id org-supertag-db--link)
+              (org-supertag-db-emit 'link:removed :tag-relation from-id to-id)))
+=======
+   (let* ((from-tag-name (completing-read "Source tag: " (org-supertag-get-all-tags) nil t))
+          (from-tag-id (org-supertag-tag-get-id-by-name from-tag-name))
+          (to-choices (mapcar
+                       (lambda (rel)
+                         (format "%s (%s)" (org-supertag-tag-get-name-by-id (car rel)) (cdr rel)))
+                       (org-supertag-relation-get-all-from from-tag-id)))
+          (choice (completing-read "Relation to remove: " to-choices nil t))
+          (to-tag-name (car (split-string choice " (")))
+          (to-tag-id (org-supertag-tag-get-id-by-name to-tag-name)))
+     (list from-tag-id to-tag-id)))
+
   (when (and from-tag to-tag)
     (let* ((from-name (org-supertag-tag-get-name-by-id from-tag))
            (to-name (org-supertag-tag-get-name-by-id to-tag)))
@@ -491,68 +262,79 @@ otherwise remove all relations between the tags."
             (when (gethash rel-id org-supertag-db--link)
               (remhash rel-id org-supertag-db--link)
               (org-supertag-db-emit 'link:removed :tag-relation from-tag to-tag)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
         ;; Otherwise, remove all relations between these tags
         (let ((removed-count 0))
           (maphash
            (lambda (key _)
              (when (and (string-prefix-p ":tag-relation:" key)
-                        (string-match (format ":tag-relation:%s->%s:" from-tag to-tag) key))
+<<<<<<< HEAD
+                       (string-match (format ":tag-relation:%s->%s:" from-id to-id) key))
+               (remhash key org-supertag-db--link)
+               (org-supertag-db-emit 'link:removed :tag-relation from-id to-id)
+               (cl-incf removed-count)))
+           org-supertag-db--link)))
+ 
+=======
+                       (string-match (format ":tag-relation:%s->%s:" from-tag to-tag) key))
                (remhash key org-supertag-db--link)
                (org-supertag-db-emit 'link:removed :tag-relation from-tag to-tag)
                (cl-incf removed-count)))
-           org-supertag-db--link)
-          (message "Removed relation: %s -> %s" from-name to-name)
-          (org-supertag-db--mark-dirty)
-          (org-supertag-db--schedule-save))))))
+           org-supertag-db--link)))
 
-(defun org-supertag-relation-remove-relation-interactive ()
-  "Interactively remove a tag relation."
-  (interactive)
-  (let* ((from-tag-name (org-supertag-relation--clean-completion
-                         (completing-read "Source tag: " (org-supertag-get-all-tags) nil t)))
-         (from-tag-id (org-supertag-tag-get-id-by-name from-tag-name))
-         (to-choices (mapcar
-                      (lambda (rel)
-                        (format "%s (%s)" (org-supertag-tag-get-name-by-id (car rel)) (cdr rel)))
-                      (org-supertag-relation-get-all-from from-tag-id)))
-         (choice (completing-read "Relation to remove: " to-choices nil t))
-         (cleaned-choice (org-supertag-relation--clean-completion choice))
-         (to-tag-name (car (split-string cleaned-choice " (")))
-         (to-tag-id (org-supertag-tag-get-id-by-name to-tag-name)))
-    (org-supertag-relation-remove-relation from-tag-id to-tag-id)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+      (org-supertag-db--mark-dirty)
+      (org-supertag-db--schedule-save)
+      (message "Removed relation: %s -> %s" from-name to-name))))
 
 (defun org-supertag-relation-get (from-tag to-tag &optional rel-type)
   "Get the relation data between FROM-TAG and TO-TAG.
+<<<<<<< HEAD
+=======
 FROM-TAG and TO-TAG can be names or IDs.
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 If REL-TYPE is provided, get the specific relation type,
 otherwise return all relations between the tags.
 Returns a list of relation property lists."
   (when (and from-tag to-tag)
     (let ((result nil)
+<<<<<<< HEAD
+          (from-id (if (org-supertag-db-exists-p from-tag)
+                      from-tag
+                    (org-supertag-tag-get-id-by-name from-tag)))
+          (to-id (if (org-supertag-db-exists-p to-tag)
+                    to-tag
+                  (org-supertag-tag-get-id-by-name to-tag))))
+=======
           (from-id (or (org-supertag-db-get from-tag) (org-supertag-tag-get-id-by-name from-tag)))
           (to-id (or (org-supertag-db-get to-tag) (org-supertag-tag-get-id-by-name to-tag))))
       (unless from-id (error "Source tag not found: %s" from-tag))
       (unless to-id (error "Target tag not found: %s" to-tag))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
       (maphash
        (lambda (key props)
          (when (and (string-prefix-p ":tag-relation:" key)
-                    (string-match (format ":tag-relation:%s->%s:\(.+\)" from-id to-id) key))
+                   (string-match (format ":tag-relation:%s->%s:\\(.+\\)" from-id to-id) key))
            (when (or (null rel-type)
-                     (eq rel-type (plist-get props :type)))
+                    (eq rel-type (plist-get props :type)))
              (push props result))))
        org-supertag-db--link)
       result)))
 
 (defun org-supertag-relation-get-all-from (tag-id)
   "Get all relations from the TAG-ID.
+<<<<<<< HEAD
+Return the list of (other-tag . rel-type)."
+=======
 Return the list of (other-tag-id . rel-type)."
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   (if (null tag-id)
       nil
     (let (result)
       (maphash
        (lambda (rel-id props)
          (when (and (string-prefix-p ":tag-relation:" rel-id)
-                    (string-match (format ":tag-relation:%s->\([^:]+\):\(.+\)" tag-id) rel-id))
+                   (string-match (format ":tag-relation:%s->\\([^:]+\\):\\(.+\\)" tag-id) rel-id))
            (let ((to-tag (match-string 1 rel-id))
                  (rel-type (plist-get props :type)))
              (push (cons to-tag rel-type) result))))
@@ -561,14 +343,18 @@ Return the list of (other-tag-id . rel-type)."
 
 (defun org-supertag-relation-get-all-to (tag-id)
   "Get all relations to the TAG-ID.
+<<<<<<< HEAD
+Return the list of (other-tag . rel-type)."
+=======
 Return the list of (other-tag-id . rel-type)."
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   (if (null tag-id)
       nil
     (let (result)
       (maphash
        (lambda (rel-id props)
          (when (and (string-prefix-p ":tag-relation:" rel-id)
-                    (equal (plist-get props :to) tag-id))
+                   (equal (plist-get props :to) tag-id))
            (let ((from-tag (plist-get props :from))
                  (rel-type (plist-get props :type)))
              (push (cons from-tag rel-type) result))))
@@ -581,11 +367,11 @@ Return the list of relations, each relation is a plist with :from, :to and :type
 Only returns outgoing relations (where TAG-ID is the :from tag)."
   (if (null tag-id)
       nil
-    (let ((result nil))
+    (let (result)
       (maphash
        (lambda (rel-id props)
          (when (and (string-prefix-p ":tag-relation:" rel-id)
-                    (equal (plist-get props :from) tag-id))
+                   (equal (plist-get props :from) tag-id))  
            (push props result)))
        org-supertag-db--link)
       result)))
@@ -599,7 +385,7 @@ Return the number of nodes associated with this tag."
       (maphash
        (lambda (link-id props)
          (when (and (string-prefix-p ":node-tag:" link-id)
-                    (equal (plist-get props :to) tag-id))
+                   (equal (plist-get props :to) tag-id))
            (cl-incf count)))
        org-supertag-db--link)
       count)))
@@ -611,7 +397,14 @@ Return the number of nodes associated with this tag."
 (defvar org-supertag-relation--current-tag nil
   "The ID of the current tag being edited.")
 
-(defvar org-supertag-relation-manage--buffer-name "*Org-Supertag Relation Management*"
+(defvar org-supertag-relation--recommendations-cache (make-hash-table :test 'equal)
+  "Cache the recommendation results for each tag.
+The key is the tag ID, and the value is the recommendation list.")
+
+(defvar org-supertag-relation--updating-recommendations nil
+  "Mark whether the recommendations are being updated to prevent duplicate calls.")
+
+(defvar org-supertag-relation-manage--buffer-name "*Org-Supertag Relation*"
   "The name of the buffer for the relation management interface.")
 
 (defvar org-supertag-relation--current-section 'existing
@@ -627,6 +420,7 @@ Can be 'existing, 'cooccurrence, or 'recommended.")
 (defun org-supertag-relation-manage ()
   "Manage tag relations interface entry point."
   (interactive)
+  (clrhash org-supertag-relation--recommendations-cache)
   (let* ((tags (org-supertag-get-all-tags))
          (tag-name (completing-read "Select a tag to manage relations: " tags nil t)))
     (if (or (null tag-name) (string-empty-p tag-name))
@@ -636,6 +430,22 @@ Can be 'existing, 'cooccurrence, or 'recommended.")
             (user-error "No tag ID found for: %s" tag-name)
           (org-supertag-relation--show-management-interface tag-id))))))
 
+<<<<<<< HEAD
+(defun org-supertag-tag-get-id-by-name (tag-name)
+  "Get the tag ID by TAG-NAME.
+TAG-NAME: Tag name."
+  (when (and tag-name (not (string-empty-p tag-name)))
+    (let (result)
+      (maphash
+       (lambda (id entity)
+         (when (and (eq (plist-get entity :type) :tag)
+                   (string= (plist-get entity :name) tag-name))
+           (setq result id)))
+       org-supertag-db--object)
+      result)))
+
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 (defun org-supertag-relation--show-management-interface (tag-id)
   "Show the tag relations management interface.
 TAG-ID: The ID of the current tag being edited."
@@ -648,7 +458,7 @@ TAG-ID: The ID of the current tag being edited."
     (org-supertag-relation--refresh-display))
   
   (let ((display-buffer-alist
-         '(( "\\*Org-Supertag Relation\\*"
+         '(("\\*Org-Supertag Relation\\*"
             (display-buffer-in-side-window)
             (side . right)
             (window-width . 0.5)
@@ -666,16 +476,83 @@ TAG-ID: The ID of the current tag being edited."
           (insert (propertize (format "Tag Relation Management - %s\n\n" tag-name)
                              'face '(:height 1.5 :weight bold))))
 
-        ;; Existing relations
-        (insert (propertize "\nExisting Relations (Manually set):\n" 'face '(:weight bold)))
+        (insert (propertize "Shortcuts:\n" 'face '(:weight bold)))
+        (insert " Navigation:\n")
+        (insert "   [n] - Next    [p] - Previous\n")
+        (insert " Operations:\n")
+        (insert "   [RET] - Select/Remove Item\n")
+        (insert "   [q] - Quit    [r] - Refresh\n")
+        (insert "   [g] - Find by Group    [f] - Find by Relation\n")
+        (insert "   [i] - Show Isolated Tags\n\n")
+        
+        ;; Display complementary relation information
+        (insert (propertize "Relation Types:\n" 'face '(:weight bold)))
+        (insert " " (propertize "◎" 'face '(:foreground "green")) " - bidirectional relation (Automatically adds reverse relation)\n")
+        (insert " " (propertize "→" 'face '(:foreground "blue")) " - unidirectional relation\n\n")
+
+        ;; 显示共现关系
+        (insert (propertize "\nCo-occurrence Tags:\n" 'face '(:weight bold)))
+        (let* ((all-relations (org-supertag-relation-get-all org-supertag-relation--current-tag))
+               ;; 过滤共现关系
+               (relations (cl-remove-duplicates
+                         (cl-remove-if
+                          (lambda (rel)
+                            (or (not (eq (plist-get rel :type) 'cooccurrence))
+                                (equal (plist-get rel :to) org-supertag-relation--current-tag)))
+                          all-relations)
+                         :test (lambda (a b)
+                                (and (equal (plist-get a :to) (plist-get b :to))
+                                     (eq (plist-get a :type) (plist-get b :type)))))))
+          (if relations
+              (dolist (rel relations)
+                (let* ((other-tag-id (plist-get rel :to))
+                       (other-tag-name (org-supertag-tag-get-name-by-id other-tag-id))
+                       (strength (or (plist-get rel :strength) 0.0))
+                       (strength-display (format " %.2f" strength))
+                       (add-button-text (propertize "[+]" 'face '(:foreground "green"))))
+                  (insert " ")
+                  (insert-text-button add-button-text
+                                    'action (lambda (button)
+                                              (let* ((tag-id org-supertag-relation--current-tag)
+                                                     (other-tag-id (button-get button 'other-tag-id))
+                                                     ;; 只允许选择非 cooccurrence 的关系类型
+                                                     (rel-choices (cl-remove-if (lambda (c)
+                                                                                  (string-match-p "^cooccurrence" c))
+                                                                                (org-supertag-relation--get-relation-type-choices)))
+                                                     (choice (completing-read "Select relation type: " rel-choices nil t))
+                                                     (rel-type (org-supertag-relation--get-type-from-choice choice)))
+                                                (when (and tag-id other-tag-id rel-type)
+                                                  (if (org-supertag-relation-has-complement-p rel-type)
+                                                      (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
+                                                    (org-supertag-relation-add-relation tag-id other-tag-id rel-type))
+                                                  (org-supertag-relation--refresh-display)
+                                                  (message "Added relation: %s -[%s]-> %s"
+                                                           (org-supertag-tag-get-name-by-id tag-id)
+                                                           rel-type
+                                                           (org-supertag-tag-get-name-by-id other-tag-id)))))
+                                    'other-tag-id other-tag-id
+                                    'follow-link t
+                                    'help-echo "Click to add an explicit relation to this tag")
+                  (insert (format " %s " 
+                                (propertize "cooccurrence" 'face '(:foreground "white" :background "black"))))
+                  (insert (format " %s%s\n" 
+                                other-tag-name
+                                (propertize strength-display 'face '(:foreground "gray50"))))))
+            (insert "  (No cooccurrence relations)\n")))
+
+        ;; Display existing relations
+        (insert (propertize "\nExisting relations:\n" 'face '(:weight bold)))
         (let* ((all-relations (org-supertag-relation-get-all org-supertag-relation--current-tag))
+               ;; Filter non-cooccurrence relations
                (relations (cl-remove-duplicates
                          (cl-remove-if
                           (lambda (rel)
                             (or (eq (plist-get rel :type) 'cooccurrence)
                                 (equal (plist-get rel :to) org-supertag-relation--current-tag)))
                           all-relations)
-                         :test #'equal)))
+                         :test (lambda (a b)
+                                (and (equal (plist-get a :to) (plist-get b :to))
+                                     (eq (plist-get a :type) (plist-get b :type)))))))
           (if relations
               (dolist (rel relations)
                 (let* ((other-tag-id (plist-get rel :to))
@@ -690,28 +567,113 @@ TAG-ID: The ID of the current tag being edited."
                                     'action 'org-supertag-relation--remove-button-action
                                     'other-tag-id other-tag-id
                                     'follow-link t
-                                    'help-echo "Click or press RET to remove this relation")
+                                    'help-echo "Click to remove this relation")
                   (insert " ")
                   (insert (propertize relation-symbol 'face relation-face))
-                  (insert (format " %s " (propertize (format "%s" rel-type) 'face '(:foreground "white" :background "black"))))
+                  (insert (format " %s " 
+                                (propertize (format "%s" rel-type) 'face '(:foreground "white" :background "black"))))
                   (insert (format " %s" other-tag-name))
+                  
+                  ;; Display bidirectional relationship
                   (when is-complementary
                     (let ((complement-type (org-supertag-relation-get-complement rel-type)))
                       (insert (format " (bidirectional: %s)" complement-type))))
+                  
                   (insert "\n")))
             (insert "  (No existing relations)\n")))
-
-        ;; Shortcuts help moved to bottom
-        (insert (propertize "\nShortcuts:\n" 'face '(:weight bold)))
-        (insert " [a] - Batch add relations to other tags\n")
-        (insert " [RET] on a line to Remove an existing relation\n")
-        (insert " [q] - Quit    [r] - Refresh\n\n")
-
-        ;; Relation Types help moved to bottom
-        (insert (propertize "Relation Types:\n" 'face '(:weight bold)))
-        (insert " " (propertize "◎" 'face '(:foreground "green")) " - Bidirectional (adds reverse relation automatically)\n")
-        (insert " " (propertize "→" 'face '(:foreground "blue")) " - Unidirectional\n\n")
-
+        
+        ;; Update the recommended tag area
+        (insert "\n")
+        (insert (propertize "Recommended similar tags:\n" 'face '(:weight bold)))
+<<<<<<< HEAD
+        (if (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache)
+            (progn
+              (dolist (rec (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache))
+                (let* ((tag-name (car rec))
+                       (similarity (cdr rec))
+                       (tag-id (org-supertag-tag-get-id-by-name tag-name))
+                       (is-selected (member tag-id org-supertag-relation--selected-tags))
+                       (selection-mark (if is-selected 
+                                          (propertize "[✓]" 'face '(:foreground "green"))
+                                        "[ ]")))
+                  (insert " ")
+                  (insert selection-mark)
+                  (insert " ")
+                  (insert-text-button (propertize "[Select]" 'face '(:foreground "green"))
+                                    'action 'org-supertag-relation--select-tag-action
+                                    'other-tag-id tag-id
+                                    'follow-link t
+                                    'help-echo "Click to select this tag to establish a relationship")
+                  (insert " ")
+                  (insert-text-button (propertize "[Mark]" 'face '(:foreground "blue"))
+                                    'action 'org-supertag-relation-toggle-tag-selection
+                                    'other-tag-id tag-id
+                                    'follow-link t
+                                    'help-echo "Mark/unmark this tag for batch operations")
+                  (insert (format " %s (%.2f)\n" tag-name similarity)))))
+          ;; No recommendations, trigger recommendation search
+          (progn
+            (insert "  (Getting similar tags...)\n")
+            (let* ((all-tags (org-supertag-get-all-tags))
+                   (other-tag-ids (mapcar #'org-supertag-tag-get-id-by-name 
+                                        (remove (org-supertag-tag-get-name-by-id org-supertag-relation--current-tag) all-tags))))
+              (run-with-timer 0 nil
+                            (lambda ()
+                              (org-supertag-relation--get-recommendations 
+                               org-supertag-relation--current-tag other-tag-ids))))))
+=======
+        (message "Checking recommendations cache for tag: %s" org-supertag-relation--current-tag)
+        (let ((cached-recommendations (gethash org-supertag-relation--current-tag 
+                                               org-supertag-relation--recommendations-cache)))
+          (if (and cached-recommendations (not (null cached-recommendations)))
+              (progn
+                (message "Found %d recommendations in cache" (length cached-recommendations))
+                (dolist (rec cached-recommendations)
+                  (message "Processing recommendation: %S" rec)
+                  (let* ((tag-name (car rec))
+                         (similarity (cdr rec))
+                         (tag-id (org-supertag-tag-get-id-by-name tag-name)))
+                    (message "Tag %s -> ID: %s, Similarity: %s" tag-name tag-id similarity)
+                    (let* ((is-selected (member tag-id org-supertag-relation--selected-tags))
+                          (selection-mark (if is-selected 
+                                             (propertize "[✓]" 'face '(:foreground "green"))
+                                           "[ ]")))
+                      (insert " ")
+                      (insert selection-mark)
+                      (insert " ")
+                      (insert-text-button (propertize "[Select]" 'face '(:foreground "green"))
+                                        'action 'org-supertag-relation--select-tag-action
+                                        'other-tag-id tag-id
+                                        'follow-link t
+                                        'help-echo "Click to select this tag to establish a relationship")
+                      (insert " ")
+                      (insert-text-button (propertize "[Mark]" 'face '(:foreground "blue"))
+                                        'action 'org-supertag-relation-toggle-tag-selection
+                                        'other-tag-id tag-id
+                                        'follow-link t
+                                        'help-echo "Mark/unmark this tag for batch operations")
+                      (insert (format " %s (%.2f)\n" tag-name similarity))))))
+            ;; No recommendations, trigger recommendation search
+            (progn
+              (message "No recommendations in cache (or empty list), starting search...")
+              (insert "  (Getting similar tags...)\n")
+              (let* ((all-tags (org-supertag-get-all-tags))
+                     (other-tag-ids (mapcar #'org-supertag-tag-get-id-by-name 
+                                          (remove (org-supertag-tag-get-name-by-id org-supertag-relation--current-tag) all-tags))))
+                (message "Starting recommendation search with %d other tags" (length other-tag-ids))
+                ;; Use lower timer delay to be more responsive
+                (run-with-timer 0.05 nil
+                              (lambda ()
+                                (message "Timer triggered to get recommendations")
+                                (org-supertag-relation--get-recommendations 
+                                 org-supertag-relation--current-tag other-tag-ids)))))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+        
+        (insert "\n")
+        (insert (propertize "Explanation: " 'face '(:weight bold)))
+        (insert "Click [Select] to add a recommended relationship, click [-] to remove an existing relationship\n")
+        (insert "When selecting a relationship type with a [bidirectional] mark, the system will automatically add a complementary relationship\n")
+        
         (goto-char current-point)))))
 
 ;;----------------------------------------------------------------------
@@ -740,53 +702,176 @@ GROUP: The group symbol, such as 'default, 'knowledge, etc."
 ;; Main Functions
 ;;----------------------------------------------------------------------  
 
+(defun org-supertag-relation-find-tags-by-group (tag-id group &optional direction)
+  "Find all tags related to the specified tag through a specific group relation.
+TAG-ID: The ID of the tag.
+GROUP: The group symbol, such as 'default, 'knowledge, etc.
+DIRECTION: The direction, nil for both, 'from for from, 'to for to."
+  (let* ((all-rels (cond
+                   ((eq direction 'from) (org-supertag-relation-get-all-from tag-id))
+                   ((eq direction 'to) (org-supertag-relation-get-all-to tag-id))
+                   (t (org-supertag-relation-get-all tag-id))))
+         (group-types (org-supertag-relation--get-types-in-group group))
+         (filtered-rels (cl-remove-if-not
+                        (lambda (rel)
+                          (memq (plist-get rel :type) group-types))
+                        all-rels))
+         (result '()))
+    (dolist (rel filtered-rels)
+      (let ((other-id (if (eq direction 'from)
+                          (plist-get rel :to)
+                        (if (eq direction 'to)
+                            (plist-get rel :from)
+                          (if (string= (plist-get rel :from) tag-id)
+                              (plist-get rel :to)
+                            (plist-get rel :from))))))
+        (when other-id
+          (push other-id result))))
+    (cl-remove-duplicates result :test 'string=)))
+
+(defun org-supertag-relation-find-related-by-group (group)
+  "Find related tags by group.
+GROUP: The group symbol, such as 'default, 'knowledge, etc."
+  (interactive
+   (list (completing-read "Choose group: "
+                         (mapcar (lambda (g) (symbol-name (car g)))
+                                 org-supertag-relation-groups)
+                         nil t)))
+  (let* ((tag-id org-supertag-relation--current-tag)
+         (group-sym (intern group)))
+    (if (not tag-id)
+        (user-error "Current tag not set. Please re-select tag")
+      (let* ((related-ids (org-supertag-relation-find-tags-by-group tag-id group-sym))
+             (related-names (mapcar 'org-supertag-tag-get-name-by-id related-ids)))
+        (if (null related-names)
+            (message "Can't find related tags through %s group relation" group)
+          (message "Related tags: %s" (mapconcat 'identity related-names ", ")))))))
+
+(defun org-supertag-relation-find-by-group ()
+  "Find related tags by group."
+  (interactive)
+  (let ((groups (mapcar (lambda (g) (symbol-name (car g))) org-supertag-relation-groups)))
+    (if (null groups)
+        (message "No relation groups defined")
+      (let* ((group (completing-read "Select relation group: " groups nil t)))
+        (org-supertag-relation-find-related-by-group group)))))
+
+(defun org-supertag-relation-find-related-tags ()
+  "Find related tags to the current tag."
+  (interactive)
+  (let ((tag-id org-supertag-relation--current-tag))
+    (if (not tag-id)
+        (user-error "Current tag not set. Please re-select tag")
+      (let ((relations (org-supertag-relation-get-all tag-id))
+            (related-tags '()))
+        (dolist (rel relations)
+          (when rel ; make sure the relation is not nil
+            (let* ((other-tag-id (plist-get rel :to))
+                   (rel-type (plist-get rel :type)))
+              (when other-tag-id ; make sure the other tag id is not nil
+                (let ((other-tag-name (org-supertag-tag-get-name-by-id other-tag-id)))
+                  (when other-tag-name ; make sure the other tag name is not nil
+                    (push (cons other-tag-name rel-type) related-tags)))))))
+        (if (null related-tags)
+            (message "Can't find related tags")
+          (with-output-to-temp-buffer "*Org-Supertag Relations*"
+            (princ "Related tags:\n\n")
+            (dolist (tag related-tags)
+              (princ (format "- %s (%s)\n" (car tag) (cdr tag))))))))))
+
 (defvar org-supertag-relation-mode-map
   (let ((map (make-sparse-keymap)))
-      (define-key map (kbd "a") 'org-supertag-relation-batch-add)
-      (define-key map (kbd "d") 'org-supertag-relation-remove)
-      (define-key map (kbd "r") 'org-supertag-relation-refresh-display)
-      (define-key map (kbd "q") 'org-supertag-relation-quit)
-      (define-key map (kbd "n") 'org-supertag-relation--next-line)
-      (define-key map (kbd "p") 'org-supertag-relation--prev-line)
-      (define-key map (kbd "RET") 'org-supertag-relation--select-current-line)
-  map))
+    (define-key map (kbd "a") 'org-supertag-relation-add)
+    (define-key map (kbd "d") 'org-supertag-relation-remove)
+    (define-key map (kbd "r") 'org-supertag-relation-refresh)
+    (define-key map (kbd "q") 'org-supertag-relation-quit)
+    (define-key map (kbd "g") 'org-supertag-relation-find-by-group)
+    (define-key map (kbd "f") 'org-supertag-relation-find-related-tags)
+    (define-key map (kbd "i") 'org-supertag-relation-show-isolated-tags)
+    (define-key map (kbd "n") 'org-supertag-relation--next-line)
+    (define-key map (kbd "p") 'org-supertag-relation--prev-line)
+    (define-key map (kbd "RET") 'org-supertag-relation--select-current-line)
+    (define-key map (kbd "m") 'org-supertag-relation-toggle-tag-selection)
+    (define-key map (kbd "M") 'org-supertag-relation-select-all-tags)
+    (define-key map (kbd "u") 'org-supertag-relation-unselect-all-tags)
+    (define-key map (kbd "b") 'org-supertag-relation-batch-add-selected)
+<<<<<<< HEAD
+=======
+    (define-key map (kbd "R") 'org-supertag-relation-force-refresh-recommendations)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+    map))
 
 (define-derived-mode org-supertag-relation-mode special-mode "Org-Supertag-Relation"
   :group 'org-supertag
   (setq buffer-read-only t)
   (setq truncate-lines t)
-  (setq header-line-format
+  (setq header-line-format 
         (propertize " Org-Supertag Relation Management" 'face '(:weight bold))))
 
-(defun org-supertag-relation-batch-add ()
-  "Batch add relations from the current tag to multiple other tags."
-  (interactive)
-  (let* ((tag-id org-supertag-relation--current-tag)
-         (tag-name (org-supertag-tag-get-name-by-id tag-id)))
-    (if (not tag-id)
-        (user-error "Current tag not set. Please re-open the relation view.")
-      (let* ((all-tags (remove tag-name (org-supertag-get-all-tags)))
-             (target-names (completing-read-multiple (format "Batch relate '%s' to: " tag-name) all-tags nil t)))
-        (if (not target-names)
-            (message "No target tags selected.")
+(defun org-supertag-relation-add (void-ok)
+  "Add a relation to the current tag.
+If VOID-OK is non-nil, allow missing tag IDs."
+  (interactive "P")
+  (let* ((tag-id org-supertag-relation--current-tag))
+    (if (and (not tag-id) (not void-ok))
+        (user-error "Current tag not set. Please re-select tag")
+      (let* ((all-tags (org-supertag-get-all-tags))
+             (other-tag-name (completing-read "Select related tag: " all-tags nil t))
+             (other-tag-id (org-supertag-tag-get-id-by-name other-tag-name)))
+        (if (not other-tag-id)
+            (user-error "Can't find tag with ID: %s" other-tag-name)
+          (unless tag-id
+            (setq tag-id (org-supertag-tag-get-id-by-name
+                         (completing-read "Select source tag: " all-tags nil t))))
+        
           (let* ((rel-choices (org-supertag-relation--get-relation-type-choices))
                  (choice (completing-read "Select relation type: " rel-choices nil t))
-                 (rel-type (org-supertag-relation--get-type-from-choice choice))
-                 (added-count 0))
-            (dolist (target-name target-names)
-              (let ((other-tag-id (org-supertag-tag-get-id-by-name target-name)))
-                (when (and other-tag-id rel-type)
-                  (if (org-supertag-relation-has-complement-p rel-type)
-                      (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
-                    (org-supertag-relation-add-relation tag-id other-tag-id rel-type))
-                  (cl-incf added-count))))
-            (org-supertag-relation--refresh-display)
-            (message "Batch added %d relations to '%s'." added-count tag-name)))))))
+                 (rel-type (org-supertag-relation--get-type-from-choice choice)))
+            (when (and tag-id other-tag-id rel-type)
+              (if (org-supertag-relation-has-complement-p rel-type)
+                  (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
+                (org-supertag-relation-add-relation tag-id other-tag-id rel-type))
+              (org-supertag-relation--refresh-display)
+              (message "Added relation: %s %s %s" 
+                       (org-supertag-tag-get-name-by-id tag-id)
+                       rel-type 
+                       (org-supertag-tag-get-name-by-id other-tag-id)))))))))
 
 (defun org-supertag-relation-remove ()
   "Remove the relation between the current tag and the selected tag."
   (interactive)
-  (org-supertag-relation-remove-relation-interactive))
+  (let ((tag-id org-supertag-relation--current-tag))
+    (if (not tag-id)
+        (user-error "Current tag not set. Please re-select tag")
+      (let* ((relations (org-supertag-relation-get-all tag-id))
+             (rel-choices nil)
+             (rel-to-id-map (make-hash-table :test 'equal)))        
+        ;; Create choices and build mapping
+        (dolist (rel relations)
+          (let* ((other-tag-id (plist-get rel :to))
+                 (other-tag-name (when other-tag-id
+                                 (or (org-supertag-tag-get-name-by-id other-tag-id)
+                                     "(Unnamed Tag)")))
+                 (rel-type (plist-get rel :type))
+                 (choice (when (and other-tag-name rel-type)
+                          (format "%s (%s)" other-tag-name rel-type))))
+            (when choice
+              (push choice rel-choices)
+              (puthash choice other-tag-id rel-to-id-map))))
+        ;; Make sure the choices are in reverse order to make it easier to navigate
+        (setq rel-choices (nreverse rel-choices))
+        (if (null rel-choices)
+            (message "There are no relations to remove")
+          (let* ((choice (completing-read "Choose relation to remove:" rel-choices nil t))
+                 (other-tag-id (gethash choice rel-to-id-map)))
+            (if other-tag-id
+                (progn
+                  (org-supertag-relation-remove-relation tag-id other-tag-id)
+                  (org-supertag-relation--refresh-display)
+                  (message "Removed relation: %s <-> %s" 
+                           (or (org-supertag-tag-get-name-by-id tag-id) tag-id)
+                           (or (org-supertag-tag-get-name-by-id other-tag-id) other-tag-id)))
+              (message "Can't find target tag ID. Please try again."))))))))
 
 (defun org-supertag-relation-refresh ()
   (interactive)
@@ -795,9 +880,408 @@ GROUP: The group symbol, such as 'default, 'knowledge, etc."
 (defun org-supertag-relation-quit ()
   "Close the relation management buffer."
   (interactive)
-  (kill-buffer org-supertag-relation-manage--buffer-name)
-  (kill-buffer))
+  (kill-buffer org-supertag-relation-manage--buffer-name))
+
+;;------------------------------------------------------------------------
+;; Tag recommendation algorithm
+;;------------------------------------------------------------------------
+
+<<<<<<< HEAD
+=======
+(defun org-supertag-relation--parse-python-data (data)
+  "Parse data returned from Python server into Emacs Lisp format.
+DATA is the raw data returned from the Python server.
+Returns a list of (tag-name . score) cons cells."
+  (cond
+   ;; 空数据情况
+   ((null data) nil)
+   
+   ;; 已经是正确格式的情况 (tag . score)
+   ((and (consp (car data)) 
+         (stringp (caar data))
+         (numberp (cdar data)))
+    data)
+   
+   ;; Python列表的列表，如 [["tag", score], ...]
+   ((and (listp (car data))
+         (>= (length (car data)) 2))
+    (mapcar (lambda (item)
+              (cons (if (stringp (nth 0 item))
+                       (nth 0 item)
+                      (format "%s" (nth 0 item)))
+                    (float (nth 1 item))))
+            data))
+   
+   ;; 字符串或其他格式
+   (t
+    (message "Unknown data format: %S" data)
+    nil)))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+(defun org-supertag-relation--get-recommendations (tag-id other-tags)
+  "Get tag relation recommendations.
+TAG-ID: Current tag ID
+OTHER-TAGS: List of other possible related tags"
+  (if (or (null tag-id)
+          (null other-tags)
+          org-supertag-relation--updating-recommendations)  ; Prevent duplicate calls
+<<<<<<< HEAD
+      (gethash tag-id org-supertag-relation--recommendations-cache)  ; Return cached recommendations
+    (let ((tag-name (org-supertag-tag-get-name-by-id tag-id)))
+      (when (and tag-name (fboundp 'org-supertag-sim-find-similar))
+        (setq org-supertag-relation--updating-recommendations t)
+        ;; Use similarity search to get recommendations
+        (org-supertag-sim-find-similar
+         tag-name
+         10  ; Get the top 10 similar tags
+         (lambda (similar-tags)
+           ;; Filter out existing relations
+           (let* ((existing-relations (mapcar 
+                                     (lambda (rel) (plist-get rel :to))
+                                     (org-supertag-relation-get-all tag-id)))
+                  (recommendations
+                   (cl-remove-if
+                    (lambda (tag)
+                      (let ((other-id (org-supertag-tag-get-id-by-name (car tag))))
+                        (or (null other-id)  ; Ignore non-existent tags
+                            (equal other-id tag-id)  ; Ignore self
+                            (member other-id existing-relations))))  ; Ignore existing relations
+                    similar-tags)))
+             ;; Update the recommendation cache
+             (puthash tag-id recommendations org-supertag-relation--recommendations-cache)
+             (setq org-supertag-relation--updating-recommendations nil)
+             ;; Use run-with-timer to delay refreshing the display, avoiding recursion
+             (run-with-timer 0.1 nil
+                            (lambda ()
+                              (when (get-buffer org-supertag-relation-manage--buffer-name)
+                                (with-current-buffer org-supertag-relation-manage--buffer-name
+                                  (let ((inhibit-read-only t))
+                                    (org-supertag-relation--refresh-display))))))))))
+      ;; Return cached recommendations
+      (gethash tag-id org-supertag-relation--recommendations-cache))))
+=======
+      (progn
+        (message "返回缓存的推荐，tag=%s (updating=%s)" 
+                 tag-id org-supertag-relation--updating-recommendations)
+        (gethash tag-id org-supertag-relation--recommendations-cache))  ; Return cached recommendations
+    (let ((tag-name (org-supertag-tag-get-name-by-id tag-id)))
+      (message "获取标签推荐: %s (name: %s)" tag-id tag-name)
+      (when (and tag-name (fboundp 'org-supertag-sim-find-similar))
+        (message "开始为 %s 查找相似标签" tag-name)
+        (setq org-supertag-relation--updating-recommendations t)
+        
+        ;; 使用更清晰的回调函数，增强调试
+        (condition-case err
+            (org-supertag-sim-find-similar
+             tag-name
+             10  ; Get the top 10 similar tags
+             (lambda (similar-tags)
+               (message "收到相似标签回调，类型=%s, 数量=%d, 数据=%S"
+                        (type-of similar-tags) 
+                        (length similar-tags) 
+                        similar-tags)
+               
+               ;; 确保数据格式正确
+               (let ((parsed-data (org-supertag-relation--parse-python-data similar-tags)))
+                 (message "解析后的数据: %S" parsed-data)
+                 
+                 ;; 处理数据 - 过滤已存在的关系
+                 (let* ((existing-relations (mapcar 
+                                           (lambda (rel) (plist-get rel :to))
+                                           (org-supertag-relation-get-all tag-id)))
+                        (recommendations
+                         (cl-remove-if
+                          (lambda (tag-pair)
+                            (let* ((tag-name (car tag-pair))
+                                   (other-id (org-supertag-tag-get-id-by-name tag-name)))
+                              (message "处理标签: %s -> ID: %s" tag-name other-id)
+                              (or (null other-id)  ; 忽略不存在的标签
+                                  (equal other-id tag-id)  ; 忽略自身
+                                  (member other-id existing-relations))))  ; 忽略已存在的关系
+                          parsed-data)))
+                   
+                   (message "过滤后剩余 %d 个推荐" (length recommendations))
+                   ;; 更新推荐缓存
+                   (puthash tag-id recommendations org-supertag-relation--recommendations-cache)
+                   (message "已将推荐缓存到 tag-id=%s: %S" tag-id recommendations)
+                   (setq org-supertag-relation--updating-recommendations nil)
+                   ;; 延迟刷新显示，避免递归
+                   (run-with-timer 0.1 nil
+                                  (lambda ()
+                                    (message "刷新关系显示...")
+                                    (when (get-buffer org-supertag-relation-manage--buffer-name)
+                                      (with-current-buffer org-supertag-relation-manage--buffer-name
+                                        (when (eq org-supertag-relation--current-tag tag-id)
+                                          (let ((inhibit-read-only t))
+                                            (org-supertag-relation--refresh-display)))))))))))
+          (error
+           (message "查找相似标签时出错: %s" (error-message-string err))
+           (setq org-supertag-relation--updating-recommendations nil))))
+      
+      ;; 不返回缓存值，让回调处理UI更新
+      nil)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
+(defun org-supertag-relation--format-recommendations ()
+  "Format the recommendation list for display.
+Only show similar tags without suggesting relation types."
+  (when (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache)
+    (mapconcat
+     (lambda (rec)
+       (let* ((tag-name (car rec))
+              (similarity (cdr rec)))
+         (format "%-30s (%.2f)" tag-name similarity)))
+     (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache)
+     "\n")))
+
+(defun org-supertag-relation--suggest-relation-type (tag1 tag2)
+  "Recommend appropriate relation types for TAG1 and TAG2 based on tag characteristics."
+  (when (and tag1 tag2)
+    (let* ((cooccur-count (org-supertag-relation--get-cooccurrence-count tag1 tag2)))
+      (if (> cooccur-count 0)
+          'cooccurrence  ; If there is a cooccurrence record, prioritize recommending 'cooccurrence' relation
+        (let* ((relation-types (mapcar #'car org-supertag-relation-types))
+               (hash-val (abs (sxhash (concat (format "%s" tag1) (format "%s" tag2)))))
+               (idx (mod hash-val (length relation-types))))
+          (nth idx relation-types))))))
+
+;; Co-occurrence relation management
+(defun org-supertag-relation-record-cooccurrence (node-id tag-id)
+  "Record the co-occurrence relationship of tags, unidirectionally propagated from parent node to child node.
+NODE-ID: Node ID
+TAG-ID: Tag ID
+
+This function will:
+1. Record the co-occurrence relationship between tags on the same node
+2. Record the unidirectional co-occurrence relationship from parent node tags"
+  (let ((existing-tags (org-supertag-node-get-tags node-id))
+        (parent-tags (org-supertag-node-get-parent-tags node-id)))
+    
+    ;; 1. Process co-occurrence between same-level tags
+    (dolist (other-tag existing-tags)
+      (unless (equal other-tag tag-id)
+        ;; Compute the co-occurrence relationship strength based on frequency
+        (let* ((freq (org-supertag-relation--get-cooccurrence-count tag-id other-tag))
+               (new-freq (1+ freq))
+               ;; Apply a sigmoid-like normalization to get diminishing returns
+               (norm-factor org-supertag-relation-cooccurrence-normalization-factor)
+               (strength (/ new-freq (+ new-freq norm-factor))))
+          ;; Store the updated frequency count for future calculations
+          (org-supertag-relation--set-cooccurrence-count tag-id other-tag new-freq)
+          ;; Create a bidirectional relationship with the calculated strength
+          (org-supertag-relation-add-relation tag-id other-tag 'cooccurrence)
+          (org-supertag-relation-add-relation other-tag tag-id 'cooccurrence))))
+    
+<<<<<<< HEAD
+    ;; 2. Process unidirectional co-occurrence from parent node tags
+    (when parent-tags
+      (dolist (parent-tag parent-tags)
+        (unless (equal parent-tag tag-id)
+          ;; Only create a unidirectional co-occurrence relationship from parent tag to child tag
+=======
+    ;; 2. Process bidirectional co-occurrence from parent node tags
+    (when parent-tags
+      (dolist (parent-tag parent-tags)
+        (unless (equal parent-tag tag-id)
+          ;; Create bidirectional co-occurrence relationship between parent tag and child tag
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+          (let* ((freq (org-supertag-relation--get-cooccurrence-count parent-tag tag-id))
+                 (new-freq (+ freq 0.5))  ; The co-occurrence weight from parent to child is 0.5
+                 (norm-factor org-supertag-relation-cooccurrence-normalization-factor)
+                 (strength (/ new-freq (+ new-freq norm-factor))))
+            (org-supertag-relation--set-cooccurrence-count parent-tag tag-id new-freq)
+<<<<<<< HEAD
+            ;; Only add a unidirectional relationship from parent to child
+            (org-supertag-relation-add-relation parent-tag tag-id 'cooccurrence)))))))
+=======
+            ;; Add bidirectional relationship between parent and child tags
+            (org-supertag-relation-add-relation parent-tag tag-id 'cooccurrence)
+            (org-supertag-relation-add-relation tag-id parent-tag 'cooccurrence)))))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
+(defun org-supertag-relation--get-cooccurrence-count (tag1 tag2)
+  "Get the number of co-occurrences between TAG1 and TAG2.
+If there is no co-occurrence record, return 0."
+  (let* ((rel-id-key (format "tag-cooccur:%s:%s" tag1 tag2)))
+    (or (org-supertag-db-get-metadata rel-id-key 0) 0)))
+
+(defun org-supertag-relation--set-cooccurrence-count (tag1 tag2 count)
+  "Set the co-occurrence count between TAG1 and TAG2 to COUNT."
+  (let* ((tags (sort (list tag1 tag2) #'string<))
+         (tag1-sorted (car tags))
+         (tag2-sorted (cadr tags))
+         (rel-id-key1 (format "tag-cooccur:%s:%s" tag1 tag2))
+         (rel-id-key2 (format "tag-cooccur:%s:%s" tag2 tag1)))
+    ;; Store the count in both directions for easy lookup
+    (org-supertag-db-set-metadata rel-id-key1 count)
+    (org-supertag-db-set-metadata rel-id-key2 count)))
+
+;; Function to analyze co-occurrence patterns across the entire database
+(defun org-supertag-relation-analyze-cooccurrence-patterns ()
+  "Analyze co-occurrence patterns across all tags and update relationship strength."
+  (interactive)
+  (let* ((nodes (org-supertag-db-find-by-type :node))
+         (tag-counts (make-hash-table :test 'equal))  ; Count of nodes per tag
+         (cooccur-counts (make-hash-table :test 'equal))  ; Co-occurrence counts
+         (total-nodes (length nodes)))
+    
+    ;; Calculate the number of times each tag appears and co-occurrence counts
+    (dolist (node-id nodes)
+      (let ((tags (org-supertag-node-get-tags node-id)))
+        ;; Calculate the number of times a single tag appears
+        (dolist (tag tags)
+          (puthash tag (1+ (or (gethash tag tag-counts) 0)) tag-counts))
+        
+        ;; Calculate the co-occurrence count between tags
+        (dolist (tag1 tags)
+          (dolist (tag2 tags)
+            (unless (equal tag1 tag2)
+              (let ((key (format "%s:%s" tag1 tag2)))
+                (puthash key (1+ (or (gethash key cooccur-counts) 0)) cooccur-counts)))))))
+    
+    ;; Save the global tag frequency data to metadata
+    (org-supertag-db-set-metadata 'tag-frequency-data tag-counts)
+    (org-supertag-db-set-metadata 'tag-total-nodes total-nodes)
+    (org-supertag-db-set-metadata 'tag-analysis-timestamp (current-time))
+    
+    ;; Calculate the point mutual information (PMI) for each co-occurrence
+    (maphash
+     (lambda (key count)
+       (let* ((tags (split-string key ":"))
+              (tag1 (car tags))
+              (tag2 (cadr tags))
+              (prob-t1 (/ (float (gethash tag1 tag-counts)) total-nodes))
+              (prob-t2 (/ (float (gethash tag2 tag-counts)) total-nodes))
+              (prob-t1t2 (/ (float count) total-nodes))
+              (pmi (log (/ prob-t1t2 (* prob-t1 prob-t2)) 2))
+              (norm-pmi (/ (1+ pmi) 2))  ; Normalize to 0-1 range
+              (strength (min 1.0 (max 0.1 norm-pmi))))  ; Limit to 0.1-1.0 range
+         
+         ;; Update co-occurrence count
+         (org-supertag-relation--set-cooccurrence-count tag1 tag2 count)
+         
+         ;; Store PMI value in metadata for statistical access
+         (org-supertag-db-set-metadata (format "tag-pmi:%s:%s" tag1 tag2) norm-pmi)
+         
+         ;; If strength is meaningful, update the relation with the new strength
+         (when (>= strength org-supertag-relation-min-strength)
+           (org-supertag-relation-add-relation tag1 tag2 'cooccurrence)
+           (org-supertag-relation-add-relation tag2 tag1 'cooccurrence))))
+     cooccur-counts)
+    
+    (message "Co-occurrence analysis completed. Analyzed %d nodes and %d tag pairs."
+             total-nodes (hash-table-count cooccur-counts))))
+
+;; Co-occurrence record incremental relation update
+(defun org-supertag-relation-update-on-tag-add (node-id tag-id)
+  "When NODE-ID adds TAG-ID as a tag, update the co-occurrence relationship.
+This is an incremental update feature that only updates affected relations."
+  (when org-supertag-relation-enable-incremental-updates
+    ;; Get all existing tags on the node (excluding the newly added tag)
+    (let ((existing-tags (cl-remove tag-id (org-supertag-node-get-tags node-id) :test #'equal)))
+      (when existing-tags
+        ;; Update each existing tag
+        (dolist (existing-tag existing-tags)
+          ;; Get the current co-occurrence count
+          (let* ((current-count (org-supertag-relation--get-cooccurrence-count existing-tag tag-id))
+                 (new-count (1+ current-count)))
+            ;; Update co-occurrence count
+            (org-supertag-relation--set-cooccurrence-count existing-tag tag-id new-count)
+            
+            ;; If co-occurrence is frequent enough, update or create the relation
+            (when (>= new-count org-supertag-relation-cooccurrence-normalization-factor)
+              (let* ((strength (/ new-count (+ new-count org-supertag-relation-cooccurrence-normalization-factor))))
+                (when (>= strength org-supertag-relation-min-strength)
+                  (org-supertag-relation-add-relation existing-tag tag-id 'cooccurrence)
+                  (org-supertag-relation-add-relation tag-id existing-tag 'cooccurrence)))))))))
+    
+    ;; Record incremental update
+    (org-supertag-db-set-metadata 'tag-incremental-updates-count 
+                                 (1+ (org-supertag-db-get-metadata 'tag-incremental-updates-count 0))))
+
+(defun org-supertag-relation-update-on-tag-remove (node-id tag-id)
+  "When NODE-ID deletes TAG-ID as a tag, update the co-occurrence relationship.
+This is an incremental update feature that only updates affected relations."
+  (when org-supertag-relation-enable-incremental-updates
+    ;; Get all existing tags on the node (excluding the newly deleted tag)
+    (let ((remaining-tags (cl-remove tag-id (org-supertag-node-get-tags node-id) :test #'equal)))
+      (when remaining-tags
+        ;; If the deleted tag has related tags, update each existing tag
+        (dolist (related-tag-id (mapcar #'car (org-supertag-relation-get-all tag-id)))
+          (let* ((should-decrement (not (member related-tag-id remaining-tags)))
+                 (current-count (org-supertag-relation--get-cooccurrence-count related-tag-id tag-id)))
+            
+            (when (and should-decrement (> current-count 0))
+              (let ((new-count (1- current-count)))
+                (org-supertag-relation--set-cooccurrence-count related-tag-id tag-id new-count)
+                
+                ;; If co-occurrence is not frequent enough, delete the relation
+                (when (< new-count (/ org-supertag-relation-cooccurrence-normalization-factor 2))
+                  ;; Use a more complex logic to decide whether to delete the relation
+                  ;; For simplicity, we only delete the relation when the strength is below a threshold
+                  (let ((cur-strength (org-supertag-relation-get-strength related-tag-id tag-id)))
+                    (when (and cur-strength (< cur-strength org-supertag-relation-min-strength))  
+                      ;; Delete the relation
+                      (org-supertag-relation-remove-relation related-tag-id tag-id))))))))
+        
+        ;; Record incremental update
+        (org-supertag-db-set-metadata 'tag-incremental-updates-count 
+                                    (1+ (org-supertag-db-get-metadata 'tag-incremental-updates-count 0)))))))
+
+;; Register hooks
+(add-hook 'org-supertag-node-tag-added-hook #'org-supertag-relation-update-on-tag-add)
+(add-hook 'org-supertag-node-tag-removed-hook #'org-supertag-relation-update-on-tag-remove)
+
+(defun org-supertag-relation-get-strength (tag1 tag2)
+  "Get the strength of the relationship between TAG1 and TAG2.
+If the relation does not exist, return nil."
+  (let ((rel (car (cl-remove-if
+                  (lambda (r) (equal (plist-get r :to) tag2))
+                  (org-supertag-relation-get-all tag1)))))
+    (when rel
+      (plist-get rel :strength))))
+
+;; Add a command to display co-occurrence relations
+(defun org-supertag-relation-find-cooccurrence-tags ()
+  "Find tags co-occurring with the current tag."
+  (interactive)
+  (let ((tag-id org-supertag-relation--current-tag))
+    (if (not tag-id)
+        (user-error "Current tag not set. Please re-select tag")
+      (let* ((relations (cl-remove-if-not
+                        (lambda (rel) (eq (plist-get rel :type) 'cooccurrence))
+                        (org-supertag-relation-get-all tag-id)))
+             (related-tags '()))
+        
+        ;; Collect related tags
+        (dolist (rel relations)
+          (let* ((other-tag-id (plist-get rel :to))
+                 (other-tag-name (org-supertag-tag-get-name-by-id other-tag-id))
+                 (strength (or (plist-get rel :strength) 0.0)))
+            (when other-tag-name
+              (push (cons other-tag-name (format "%.2f" strength)) related-tags))))
+        
+        ;; Display results
+        (if (null related-tags)
+            (message "No tags co-occurring with the current tag")
+          (with-output-to-temp-buffer "*Org-Supertag Co-occurrence*"
+            (princ "Tags co-occurring with the current tag (sorted by strength):\n\n")
+            (dolist (tag (sort related-tags (lambda (a b) (> (string-to-number (cdr a)) (string-to-number (cdr b))))))
+              (princ (format "- %s (Strength: %s)\n" (car tag) (cdr tag))))))))))  
 
+;; Button action handlers for the relation management interface
+(defun org-supertag-relation--add-button-action (button)
+  "Handle the action of clicking the add relation button."
+  (let* ((tag-id org-supertag-relation--current-tag)
+         (other-tag-id (button-get button 'other-tag-id))
+         (rel-type (button-get button 'rel-type)))
+    (when (and tag-id other-tag-id rel-type)
+      (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
+      (org-supertag-relation--refresh-display)
+      (message "Relation added: %s -[%s]-> %s" 
+               (org-supertag-tag-get-name-by-id tag-id)
+               rel-type
+               (org-supertag-tag-get-name-by-id other-tag-id)))))
 
 (defun org-supertag-relation--remove-button-action (button)
   "Handle the action of clicking the remove relation button."
@@ -811,11 +1295,38 @@ GROUP: The group symbol, such as 'default, 'knowledge, etc."
           (org-supertag-relation-remove-with-complement tag-id other-tag-id rel-type)
         (org-supertag-relation-remove-relation tag-id other-tag-id rel-type))
       (org-supertag-relation--refresh-display)
-      (message "Relation is removed: %s -[%s]-> %s" 
+      (message "关系已移除: %s -[%s]-> %s" 
                (org-supertag-tag-get-name-by-id tag-id)
                rel-type
                (org-supertag-tag-get-name-by-id other-tag-id)))))
 
+(defun org-supertag-relation--select-tag-action (button)
+  "Handle the action of selecting a similar tag.
+This will prompt user to choose a relation type for the selected tag."
+  (let* ((tag-id org-supertag-relation--current-tag)
+         (other-tag-id (button-get button 'other-tag-id))
+         (rel-choices (org-supertag-relation--get-relation-type-choices))
+         (choice (completing-read "Select relation type: " rel-choices nil t))
+         (rel-type (org-supertag-relation--get-type-from-choice choice)))
+    (when (and tag-id other-tag-id rel-type)
+      (if (org-supertag-relation-has-complement-p rel-type)
+          (let ((complement (org-supertag-relation-get-complement rel-type)))
+            (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
+            (org-supertag-relation--refresh-display)
+            (message "Added bidirectional relation: %s -[%s]-> %s and %s -[%s]-> %s" 
+                     (org-supertag-tag-get-name-by-id tag-id)
+                     rel-type
+                     (org-supertag-tag-get-name-by-id other-tag-id)
+                     (org-supertag-tag-get-name-by-id other-tag-id)
+                     complement
+                     (org-supertag-tag-get-name-by-id tag-id)))
+        (org-supertag-relation-add-relation tag-id other-tag-id rel-type)
+        (org-supertag-relation--refresh-display)
+        (message "Added relation: %s -[%s]-> %s" 
+                 (org-supertag-tag-get-name-by-id tag-id)
+                 rel-type
+                 (org-supertag-tag-get-name-by-id other-tag-id))))))
+
 (defun org-supertag-relation--next-line ()
   "Move to the next line."
   (interactive)
@@ -836,15 +1347,62 @@ GROUP: The group symbol, such as 'default, 'knowledge, etc."
   (with-current-buffer org-supertag-relation-manage--buffer-name
     (save-excursion
       (beginning-of-line)
-      (when (re-search-forward "\\(\[-\\]\\|\[Select\]\\)" (line-end-position) t)
+      (when (re-search-forward "\\(\\[-\\]\\|\\[Select\\]\\)" (line-end-position) t)
         (let ((button (button-at (match-beginning 0))))
           (when button
             (push-button button)))))))
 
-(defun org-supertag-relation-refresh-display ()
-  "Interactive wrapper for org-supertag-relation--refresh-display."
-  (interactive)
-  (org-supertag-relation--refresh-display))
+(defun org-supertag-relation--highlight-current-item ()
+  "Highlight the current item in the buffer."
+  (with-current-buffer org-supertag-relation-manage--buffer-name
+    (let ((inhibit-read-only t))
+      (remove-overlays (point-min) (point-max) 'org-supertag-relation-highlight t)
+      (let* ((all-items (org-supertag-relation--get-all-items))
+             (current-item (nth org-supertag-relation--current-item-index all-items)))
+        (when current-item
+          (let ((tag-name (cond
+                          ((eq (car current-item) 'recommended)
+                           (car (cdr current-item)))
+                          (t
+                           (org-supertag-tag-get-name-by-id 
+                            (plist-get (cdr current-item) :to))))))
+            (save-excursion
+              (goto-char (point-min))
+              (let ((found nil))
+                (while (and (not found)
+                           (search-forward tag-name nil t))
+                  (save-excursion
+                    (beginning-of-line)
+                    (when (looking-at ".*\\(\\[-\\]\\|\\[Select\\]\\).*")
+                      (setq found t)
+                      (let* ((line-start (line-beginning-position))
+                             (line-end (line-end-position))
+                             (ov (make-overlay line-start line-end)))
+                        (overlay-put ov 'face 'highlight)
+                        (overlay-put ov 'org-supertag-relation-highlight t)))))))))))))  
+
+(defun org-supertag-relation--get-all-items ()
+  "Get all items from all sections in display order."
+  (let ((items nil))
+    ;; Co-occurrence section
+    (dolist (rel (cl-remove-if-not
+                 (lambda (rel) (eq (plist-get rel :type) 'cooccurrence))
+                 (org-supertag-relation-get-all org-supertag-relation--current-tag)))
+      (push (cons 'cooccurrence rel) items))
+    
+    ;; Existing relations section
+    (dolist (rel (cl-remove-if
+                 (lambda (rel) (eq (plist-get rel :type) 'cooccurrence))
+                 (org-supertag-relation-get-all org-supertag-relation--current-tag)))
+      (push (cons 'existing rel) items))
+    
+    ;; Recommended section
+    (when-let* ((recommendations (gethash org-supertag-relation--current-tag
+                                       org-supertag-relation--recommendations-cache)))
+      (dolist (rec recommendations)
+        (push (cons 'recommended rec) items)))
+    
+    (nreverse items)))
     
 (defun org-supertag-relation-get-complement (relation-type)
   "Get the complementary relationship of a relation type.
@@ -877,6 +1435,345 @@ also remove the complementary relation from OTHER-TAG-ID to TAG-ID."
     (when complement
       (org-supertag-relation-remove-relation other-tag-id tag-id complement))))
 
+;;----------------------------------------------------------------------
+;; Isolated Tags Management
+;;----------------------------------------------------------------------
+
+(defun org-supertag-relation-find-isolated-tags ()
+  "Find all tags with no relations.
+Returns a list of tag IDs."
+  (let ((all-tags (org-supertag-db-find-by-type :tag))
+        (isolated-tags nil))
+    (dolist (tag-id all-tags)
+      ;; Check if there are outgoing relations from this tag
+      (let ((outgoing-relations (org-supertag-relation-get-all tag-id))
+            (incoming-relations nil))
+        
+        ;; Check if there are incoming relations to this tag
+        (maphash
+         (lambda (rel-id props)
+           (when (and (string-prefix-p ":tag-relation:" rel-id)
+                      (equal (plist-get props :to) tag-id))
+             (push props incoming-relations)))
+         org-supertag-db--link)
+        
+        ;; If there are no outgoing or incoming relations, it is an isolated tag
+        (when (and (null outgoing-relations) (null incoming-relations))
+          (push tag-id isolated-tags))))
+    
+    isolated-tags))
+
+(defun org-supertag-relation-show-isolated-tags ()
+  "Show a list of all isolated tags and provide processing options."
+  (interactive)
+  (let* ((isolated-tags (org-supertag-relation-find-isolated-tags))
+         (tags-with-names nil))
+    
+    ;; Collect tag names
+    (dolist (tag-id isolated-tags)
+      (let ((tag-name (org-supertag-tag-get-name-by-id tag-id)))
+        (when tag-name
+          (push (cons tag-id tag-name) tags-with-names))))
+    
+    (if (null tags-with-names)
+        (message "No isolated tags found")
+      ;; Create isolated tags browsing buffer
+      (with-current-buffer (get-buffer-create "*Org-Supertag Isolated Tags*")
+        (let ((inhibit-read-only t))
+          (erase-buffer)
+          (org-supertag-relation-mode) ;; Use the same mode
+          
+          ;; Title
+          (insert (propertize "List of Isolated Tags (Tags with no relations)\n\n" 
+                             'face '(:height 1.5 :weight bold)))
+          
+          ;; Help information
+          (insert "Instructions:\n")
+          (insert " [m] - Select/Unselect tag\n")
+          (insert " [M] - Select all tags    [u] - Unselect all tags\n")
+          (insert " [RET] - Manage relations of selected tag\n")
+          (insert " [b] - Batch add relations to selected tags\n")
+          (insert " [r] - Refresh list\n")
+          (insert " [q] - Quit\n\n")
+          
+          ;; Display the list of isolated tags
+          (insert (propertize (format "Found %d isolated tags:\n" (length tags-with-names))
+                             'face '(:weight bold)))
+          
+          ;; Sort tags and insert
+          (let ((sorted-tags (sort tags-with-names 
+                                  (lambda (a b) 
+                                    (string< (cdr a) (cdr b))))))
+            (dolist (tag sorted-tags)
+              (let* ((tag-id (car tag))
+                     (tag-name (cdr tag))
+                     (is-selected (member tag-id org-supertag-relation--selected-tags))
+                     (selection-mark (if is-selected 
+                                        (propertize "[✓]" 'face '(:foreground "green"))
+                                      "[ ]")))
+                (insert " ")
+                (insert selection-mark)
+                (insert " ")
+                (insert-text-button (propertize "[Manage]" 'face '(:foreground "green"))
+                                   'action 'org-supertag-relation--manage-isolated-tag-action
+                                   'tag-id tag-id
+                                   'follow-link t
+                                   'help-echo (format "Manage relations for %s tag" tag-name))
+                (insert " ")
+                (insert-text-button (propertize "[Mark]" 'face '(:foreground "blue"))
+                                   'action 'org-supertag-relation-toggle-isolated-tag-selection
+                                   'tag-id tag-id
+                                   'follow-link t
+                                   'help-echo "Mark/Unmark this tag for batch operations")
+                (insert (format " %s\n" tag-name))))))
+        
+        ;; Bind special keys
+        (local-set-key (kbd "m") 'org-supertag-relation-toggle-isolated-tag-at-point)
+        (local-set-key (kbd "M") 'org-supertag-relation-select-all-isolated-tags)
+        (local-set-key (kbd "u") 'org-supertag-relation-unselect-all-tags)
+        (local-set-key (kbd "RET") 'org-supertag-relation-manage-selected-isolated-tag)
+        (local-set-key (kbd "r") 'org-supertag-relation-refresh-isolated-tags)
+        (local-set-key (kbd "q") 'org-supertag-relation-quit-isolated-tags)
+        (local-set-key (kbd "n") 'next-line)
+        (local-set-key (kbd "p") 'previous-line)
+        (local-set-key (kbd "j") 'next-line)
+        (local-set-key (kbd "k") 'previous-line)
+        (local-set-key (kbd "b") 'org-supertag-relation-batch-relate-isolated-tags)
+        
+        ;; Show buffer
+        (pop-to-buffer (current-buffer))))))
+
+(defun org-supertag-relation--manage-isolated-tag-action (button)
+  "Respond to clicks on the [Manage] button in the isolated tags list."
+  (let ((tag-id (button-get button 'tag-id)))
+    (when tag-id
+      (org-supertag-relation--show-management-interface tag-id))))
+
+(defun org-supertag-relation-toggle-isolated-tag-selection (button)
+  "Toggle the selection state of an isolated tag for batch operations."
+  (let ((tag-id (button-get button 'tag-id)))
+    (when tag-id
+      (if (member tag-id org-supertag-relation--selected-tags)
+          ;; Unselect
+          (setq org-supertag-relation--selected-tags
+                (delete tag-id org-supertag-relation--selected-tags))
+        ;; Add selection
+        (push tag-id org-supertag-relation--selected-tags))
+      
+      ;; Refresh display to update selection status
+      (org-supertag-relation-refresh-isolated-tags)
+      
+      ;; Show current selection count
+      (message "Selected %d tags" 
+               (length org-supertag-relation--selected-tags)))))
+
+(defun org-supertag-relation-toggle-isolated-tag-at-point ()
+  "Toggle the selection state of the isolated tag at the current line."
+  (interactive)
+  (let ((button (save-excursion 
+                 (beginning-of-line)
+                 (re-search-forward "\\[Mark\\]" (line-end-position) t)
+                 (button-at (match-beginning 0)))))
+    (when button
+      (org-supertag-relation-toggle-isolated-tag-selection button))))
+
+(defun org-supertag-relation-select-all-isolated-tags ()
+  "Select all displayed isolated tags."
+  (interactive)
+  (setq org-supertag-relation--selected-tags nil) ; First clear
+  
+  ;; Collect all isolated tags
+  (with-current-buffer "*Org-Supertag Isolated Tags*"
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "\\[Manage\\]" nil t)
+        (let ((button (button-at (match-beginning 0))))
+          (when button
+            (push (button-get button 'tag-id) 
+                  org-supertag-relation--selected-tags))))))
+  
+  ;; Refresh display and prompt
+  (org-supertag-relation-refresh-isolated-tags)
+  (message "Selected all isolated tags (%d tags)" 
+           (length org-supertag-relation--selected-tags)))
+
+(defun org-supertag-relation-manage-selected-isolated-tag ()
+  "Manage the relations of the isolated tag on the current line."
+  (interactive)
+  (let ((button (save-excursion 
+                 (beginning-of-line)
+                 (re-search-forward "\\[Manage\\]" (line-end-position) t)
+                 (button-at (match-beginning 0)))))
+    (when button
+      (org-supertag-relation--manage-isolated-tag-action button))))
+
+(defun org-supertag-relation-refresh-isolated-tags ()
+  "Refresh the display of the isolated tags list."
+  (interactive)
+  (when (get-buffer "*Org-Supertag Isolated Tags*")
+<<<<<<< HEAD
+    (let ((point (point)))
+      (org-supertag-relation-show-isolated-tags)
+      (goto-char (min point (point-max))))))
+=======
+    (with-current-buffer "*Org-Supertag Isolated Tags*"
+      (let ((inhibit-read-only t)
+            (current-point (point)))
+        (erase-buffer)
+        (insert (propertize "Isolated Tags (No Relations)\n\n" 'face '(:height 1.2 :weight bold)))
+        (insert "Shortcuts: [m] - Manage selected tag, [q] - Quit, [s] - Select/Unselect, [S] - Select All, [U] - Unselect All\n\n")
+
+        (let ((isolated-tags (org-supertag-relation--get-isolated-tags)))
+          (if (null isolated-tags)
+              (insert "No isolated tags found.\n")
+            (dolist (tag-id isolated-tags)
+              (let* ((tag-name (org-supertag-tag-get-name-by-id tag-id))
+                     (is-selected (member tag-id org-supertag-relation--selected-tags)) ; check selected status
+                     (selection-mark (if is-selected
+                                        (propertize "[✓]" 'face '(:foreground "green"))
+                                      "[ ]")))
+                (insert selection-mark) ; Use the mark
+                (insert " ")
+                (insert-text-button (propertize "[Manage]" 'face '(:foreground "blue"))
+                                  'action 'org-supertag-relation--manage-isolated-tag-action
+                                  'tag-id tag-id
+                                  'follow-link t
+                                  'help-echo "Manage relations for this tag")
+                (insert " ")
+                (insert-text-button (propertize "[Select]" 'face (if is-selected '(:foreground "orange") '(:foreground "green")))
+                                  'action 'org-supertag-relation-toggle-isolated-tag-selection
+                                  'tag-id tag-id
+                                  'follow-link t
+                                  'help-echo (if is-selected "Unselect this tag" "Select this tag"))
+                (insert (format " %s\n" tag-name))))))
+        (goto-char current-point)))))
+
+(defun org-supertag-relation-toggle-isolated-tag-selection (button)
+  "Toggle the selection state of an isolated tag for batch operations."
+  (let ((tag-id (button-get button 'tag-id)))
+    (when tag-id
+      (if (member tag-id org-supertag-relation--selected-tags)
+          ;; Unselect
+          (setq org-supertag-relation--selected-tags
+                (delete tag-id org-supertag-relation--selected-tags))
+        ;; Add selection
+        (push tag-id org-supertag-relation--selected-tags))
+      
+      ;; Refresh display to update selection status
+      (org-supertag-relation-refresh-isolated-tags)
+      
+      ;; Show current selection count
+      (message "Selected %d tags" 
+               (length org-supertag-relation--selected-tags)))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
+(defun org-supertag-relation-batch-relate-isolated-tags ()
+  "Batch create relations for the selected isolated tags."
+  (interactive)
+  (let ((selected-tags org-supertag-relation--selected-tags))
+    (if (< (length selected-tags) 2)
+        (message "Need to select at least two tags to create batch relations")
+      
+      ;; Select the central tag
+      (let* ((tag-names (mapcar (lambda (id) 
+                               (cons id (org-supertag-tag-get-name-by-id id)))
+                             selected-tags))
+             (choice (completing-read "Select the central tag (all other tags will be related to it): " 
+                                     (mapcar #'cdr tag-names) nil t))
+             (center-tag-id (car (rassoc choice tag-names)))
+             (other-tags (remove center-tag-id selected-tags)))
+        
+        ;; Select the relation type
+        (let* ((rel-choices (org-supertag-relation--get-relation-type-choices))
+               (choice (completing-read "Select the relation type to apply: " rel-choices nil t))
+               (rel-type (org-supertag-relation--get-type-from-choice choice))
+               (added-count 0))
+          
+          ;; Apply relations in batch
+          (dolist (other-tag-id other-tags)
+            (if (org-supertag-relation-has-complement-p rel-type)
+                (org-supertag-relation-add-with-complement center-tag-id other-tag-id rel-type)
+              (org-supertag-relation-add-relation center-tag-id other-tag-id rel-type))
+            (cl-incf added-count))
+          
+          ;; Clear selection and refresh display
+          (setq org-supertag-relation--selected-tags nil)
+          (org-supertag-relation-refresh-isolated-tags)
+          (message "Added %d relations (central tag: %s)" 
+                   added-count (org-supertag-tag-get-name-by-id center-tag-id)))))))
+
+(defun org-supertag-relation-toggle-tag-selection (button)
+  "Toggle the selection state of a tag for batch operations.
+This function is used to mark/unmark tags in the standard relation management interface."
+  (interactive)
+  (when (or button (get-text-property (point) 'button))
+    (let* ((btn (or button (button-at (point))))
+           (tag-id (when btn (button-get btn 'other-tag-id)))
+           (current-tag org-supertag-relation--current-tag))
+      
+      (when (and tag-id (not (equal tag-id current-tag)))
+        (if (member tag-id org-supertag-relation--selected-tags)
+            ;; Unselect
+            (setq org-supertag-relation--selected-tags
+                  (delete tag-id org-supertag-relation--selected-tags))
+          ;; Add selection
+          (push tag-id org-supertag-relation--selected-tags))
+        
+        ;; Refresh display to update selection status
+        (org-supertag-relation--refresh-display)
+        
+        ;; Show current selection count
+        (message "Selected %d tags" 
+                 (length org-supertag-relation--selected-tags))))))
+
+(defun org-supertag-relation-batch-add-selected ()
+  "Batch add relations to all selected tags.
+Used to batch add relations in the standard relation management interface."
+  (interactive)
+  (let* ((tag-id org-supertag-relation--current-tag)
+         (selected-tags org-supertag-relation--selected-tags))
+    
+    (if (null selected-tags)
+        (message "No tags selected")
+      
+      ;; When there are selected tags, select the relation type
+      (let* ((rel-choices (org-supertag-relation--get-relation-type-choices))
+             (choice (completing-read "Select the relation type to apply: " rel-choices nil t))
+             (rel-type (org-supertag-relation--get-type-from-choice choice))
+             (added-count 0))
+        
+        ;; Apply relations in batch
+        (dolist (other-tag-id selected-tags)
+          ;; Avoid creating relations with itself
+          (unless (equal tag-id other-tag-id)
+            (if (org-supertag-relation-has-complement-p rel-type)
+                (org-supertag-relation-add-with-complement tag-id other-tag-id rel-type)
+              (org-supertag-relation-add-relation tag-id other-tag-id rel-type))
+            (cl-incf added-count)))
+        
+        ;; Clear selection and refresh display
+        (setq org-supertag-relation--selected-tags nil)
+        (org-supertag-relation--refresh-display)
+        (message "Added %d relations" added-count)))))
+
+(defun org-supertag-relation-select-all-tags ()
+  "Select all recommended tags currently displayed."
+  (interactive)
+  (setq org-supertag-relation--selected-tags nil) ; First clear
+  
+  ;; Collect tags from the recommendation area
+  (when (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache)
+    (dolist (rec (gethash org-supertag-relation--current-tag org-supertag-relation--recommendations-cache))
+      (let* ((tag-name (car rec))
+             (tag-id (org-supertag-tag-get-id-by-name tag-name)))
+        (when tag-id
+          (push tag-id org-supertag-relation--selected-tags)))))
+  
+  ;; Refresh display and prompt
+  (org-supertag-relation--refresh-display)
+  (message "Selected all recommended tags (%d tags)" 
+           (length org-supertag-relation--selected-tags)))
+
 (defun org-supertag-relation-unselect-all-tags ()
   "Unselect all tags."
   (interactive)
@@ -884,22 +1781,120 @@ also remove the complementary relation from OTHER-TAG-ID to TAG-ID."
   (org-supertag-relation--refresh-display)
   (message "Unselected all tags"))
 
+;;;###autoload
+(defun org-supertag-relation-manage-isolated-tags ()
+  "Find and manage all isolated tags that have no relations.
+This function serves as an independent entry point, directly displaying the isolated tag management interface."
+  (interactive)
+  (org-supertag-relation-show-isolated-tags))
+
+(defun org-supertag-relation-quit-isolated-tags ()
+  "Close the isolated tags buffer."
+  (interactive)
+  (kill-buffer "*Org-Supertag Isolated Tags*"))
+
+<<<<<<< HEAD
+=======
+(defun org-supertag-relation-force-refresh-recommendations ()
+  "Debug function to force refresh the recommendations display.
+This is useful if recommendations are found but not showing in the UI."
+  (interactive)
+  (message "Force refreshing recommendations display...")
+  (when (get-buffer org-supertag-relation-manage--buffer-name)
+    (with-current-buffer org-supertag-relation-manage--buffer-name
+      (let ((inhibit-read-only t))
+        (message "Current tag: %s" org-supertag-relation--current-tag)
+        (let ((cached-recs (gethash org-supertag-relation--current-tag 
+                                    org-supertag-relation--recommendations-cache)))
+          (message "Cached recommendations: %S" cached-recs)
+          (org-supertag-relation--refresh-display)
+          (message "Display refreshed"))))))
+
+;; Add the function to the mode map
+(define-key org-supertag-relation-mode-map (kbd "R") 'org-supertag-relation-force-refresh-recommendations)
+
+
+
 ;;----------------------------------------------------------------------
-;; Contextual and Batch Relation Management (New Features)
+;; Compatibility functions for org-supertag-auto-tag.el
 ;;----------------------------------------------------------------------
 
-(defun org-supertag-relation-toggle-tag-selection-at-point ()
-  "Toggle selection for the tag on the current line in the relation view."
-  (interactive)
-  (with-current-buffer org-supertag-relation-manage--buffer-name
-    (let ((button (save-excursion
-                    (beginning-of-line)
-Æ`                    (when (re-search-forward "\\[Mark\\]" (line-end-position) t)
-                      (button-at (match-beginning 0))))))
-      (when button
-        (org-supertag-relation-toggle-tag-selection button)))))
+(defun org-supertag-relation-set-type (from-tag to-tag rel-type)
+  "为 org-supertag-auto-tag.el 提供的兼容函数，设置标签关系类型.
+FROM-TAG: 源标签ID
+TO-TAG: 目标标签ID
+REL-TYPE: 关系类型"
+  (org-supertag-relation-add-relation from-tag to-tag rel-type))
+
+(defun org-supertag-relation-add-rule (from-tag to-tag rule-type rule-data)
+  "为 org-supertag-auto-tag.el 提供的兼容函数，添加标签关系规则.
+FROM-TAG: 源标签ID
+TO-TAG: 目标标签ID
+RULE-TYPE: 规则类型
+RULE-DATA: 规则数据"
+  ;; 目前只是存储关系，可以后续扩展规则功能
+  (let* ((rel-id (format ":tag-relation:%s->%s:rule" from-tag to-tag))
+         (props (list :from from-tag
+                     :to to-tag
+                     :type rule-type
+                     :rule-data rule-data
+                     :created-at (current-time))))
+    (puthash rel-id props org-supertag-db--link)
+    (org-supertag-db-emit 'link:created :tag-rule from-tag to-tag props)
+    (org-supertag-db--mark-dirty)
+    (org-supertag-db--schedule-save)))
 
+;;----------------------------------------------------------------------
+;; Tag status and rules management functions for org-supertag-auto-tag.el
+;;----------------------------------------------------------------------
 
-(provide 'org-supertag-relation)
+(defun org-supertag-tag-get-status (tag-id)
+  "获取标签状态.
+TAG-ID: 标签ID
+返回标签状态，如果没有设置则返回 nil"
+  (when tag-id
+    (let ((entity (gethash tag-id org-supertag-db--object)))
+      (when entity
+        (plist-get entity :status)))))
 
+(defun org-supertag-tag-set-status (tag-id new-status)
+  "设置标签状态.
+TAG-ID: 标签ID
+NEW-STATUS: 新状态"
+  (when tag-id
+    (let ((entity (gethash tag-id org-supertag-db--object)))
+      (when entity
+        (plist-put entity :status new-status)
+        (puthash tag-id entity org-supertag-db--object)
+        (org-supertag-db-emit 'object:updated :tag tag-id entity)
+        (org-supertag-db--mark-dirty)
+        (org-supertag-db--schedule-save)))))
+
+(defun org-supertag-tag-get-rules (tag-id)
+  "获取标签规则.
+TAG-ID: 标签ID
+返回标签规则列表"
+  (when tag-id
+    (let ((entity (gethash tag-id org-supertag-db--object)))
+      (when entity
+        (plist-get entity :rules)))))
+
+(defun org-supertag-tag-add-rule (tag-id rule-name rule-data)
+  "添加标签规则.
+TAG-ID: 标签ID
+RULE-NAME: 规则名称
+RULE-DATA: 规则数据"
+  (when tag-id
+    (let ((entity (gethash tag-id org-supertag-db--object)))
+      (when entity
+        (let* ((current-rules (or (plist-get entity :rules) '()))
+               (new-rules (cons (cons rule-name rule-data) current-rules)))
+          (plist-put entity :rules new-rules)
+          (puthash tag-id entity org-supertag-db--object)
+          (org-supertag-db-emit 'object:updated :tag tag-id entity)
+          (org-supertag-db--mark-dirty)
+          (org-supertag-db--schedule-save))))))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+(provide 'org-supertag-relation)
 ;;; org-supertag-relation.el ends here
diff --git a/org-supertag-resonance.el b/org-supertag-resonance.el
new file mode 100755
index 0000000..013454d
--- /dev/null
+++ b/org-supertag-resonance.el
@@ -0,0 +1,76 @@
+;;; org-supertag-resonance.el --- Proactive resonance discovery for org-supertag  -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;;
+;; This package implements the "Core Resonance Loop" for org-supertag.
+;; It provides different interaction modes for proactively discovering
+;; connections between the current note and the existing knowledge base.
+;;
+;; The user can choose between several 'trigger modes':
+;; - 'manual: Suggestions are only fetched when explicitly requested.
+;; - 'whisper: The system indicates potential resonance silently in the mode-line.
+;; - 'proactive: The system automatically fetches and displays suggestions on idle.
+
+;;; Code:
+
+(require 'org-supertag-util)
+(require 'org-supertag-api)
+
+(defgroup org-supertag-resonance nil
+  "Settings for the proactive resonance engine."
+  :group 'org-supertag)
+
+(defcustom org-supertag-resonance-trigger-mode 'manual
+  "The trigger mode for the conceptual resonance feature.
+This variable determines how and when resonance suggestions are fetched.
+
+Possible values are:
+- 'manual:     Suggestions are only fetched when you run the command
+              `org-supertag-resonance-discover`. This is the least
+              intrusive mode.
+- 'whisper:    When idle, the system checks for resonance potential. If
+              found, a small indicator appears in the mode-line. You can
+              then run a command to see the results. (Not yet implemented)
+- 'proactive:  When idle, the system automatically fetches suggestions
+              and displays them in the sidebar. This is the most
+              automated mode."
+  :type '(choice (const :tag "Manual" manual)
+                 (const :tag "Whisper" whisper)
+                 (const :tag "Proactive" proactive))
+  :group 'org-supertag-resonance)
+
+(defcustom org-supertag-resonance-idle-delay 2
+  "The idle delay in seconds before triggering resonance discovery.
+This only applies to 'whisper' and 'proactive' modes."
+  :type 'integer
+  :group 'org-supertag-resonance)
+
+(defvar org-supertag-resonance--timer nil
+  "The timer object for idle-based resonance discovery.")
+
+(defun org-supertag-resonance--fetch-and-display ()
+  "Fetch and display resonance for the current node."
+  (let* ((node (org-supertag-node-get-at-point))
+         (node-id (and node (plist-get node :id)))
+         (content (and node-id (org-supertag-get-node-content node-id))))
+    (if (and node-id content)
+        (progn
+          (message "Resonance: Analyzing current node for connections...")
+          (org-supertag-api-proactive-get-resonance
+           node-id
+           content
+           (lambda (response)
+             (if (and response (eq (plist-get response :status) 'success))
+                 (message "Resonance Found: %s" response) ; Placeholder UI
+               (message "Resonance: No significant connections found or an error occurred.")))))
+      (message "Resonance: Could not find a valid org-supertag node at point."))))
+
+;;;###autoload
+(defun org-supertag-resonance-discover ()
+  "Manually trigger the resonance discovery for the current node."
+  (interactive)
+  (org-supertag-resonance--fetch-and-display))
+
+(provide 'org-supertag-resonance)
+
+;;; org-supertag-resonance.el ends here 
diff --git a/org-supertag-scheduler.el b/org-supertag-scheduler.el
deleted file mode 100644
index 740ac9f..0000000
--- a/org-supertag-scheduler.el
+++ /dev/null
@@ -1,206 +0,0 @@
-;;; org-supertag-scheduler.el --- A unified task scheduler for org-supertag -*- lexical-binding: t; -*-
-
-;;; Commentary:
-;;
-;; This module provides a unified, robust scheduler for managing all background
-;; and periodic tasks within org-supertag. It replaces the scattered and
-;; inconsistent timer implementations in various modules.
-;;
-;; Features:
-;; - Centralized task registry.
-;; - A single, master timer to reduce overhead.
-;; - Support for two distinct task types:
-;;   - :interval: For tasks that run every N seconds (relative time).
-;;   - :daily: For tasks that run once per day at a specific time (absolute time).
-;; - Handles missed executions for daily tasks (e.g., if Emacs is not running).
-;; - Persists state for daily tasks across Emacs sessions.
-
-;;; Code:
-
-(require 'cl-lib)
-
-;;; === Customization ===
-
-(defcustom org-supertag-scheduler-check-interval 300
-  "The interval in seconds for the master timer to check for pending tasks.
-A shorter interval provides more accuracy for scheduled tasks but uses
-more resources. 300 seconds (5 minutes) is a reasonable default."
-  :type 'integer
-  :group 'org-supertag)
-
-;;; === Core Variables ===
-
-(defvar org-supertag-scheduler--tasks (make-hash-table :test 'equal)
-  "The central registry for all scheduled tasks.
-Key: A unique task ID (symbol).
-Value: A plist defining the task, e.g.:
-'(:type :daily
-  :time \"03:00\"
-  :funtion #'my-task-function
-  :last-run \"2023-10-28\")")
-
-(defvar org-supertag-scheduler--master-timer nil
-  "The single master timer that periodically calls `org-supertag-scheduler--check-tasks`.")
-
-(defvar org-supertag-scheduler--state-file
-  (expand-file-name "scheduler-state.json" org-supertag-data-directory)
-  "File to persist the state of daily tasks (e.g., last run times).")
-
-
-;;; === State Management ===
-
-(defun org-supertag-scheduler--save-state ()
-  "Save the state of all tasks (specifically last-run times) to a file."
-  (require 'json)
-  (let ((state-to-save (make-hash-table :test 'equal)))
-    (maphash (lambda (id task-plist)
-               (when-let ((last-run (plist-get task-plist :last-run)))
-                 (puthash (symbol-name id) last-run state-to-save)))
-             org-supertag-scheduler--tasks)
-    (with-temp-buffer
-      (json-encode state-to-save)
-      (write-file org-supertag-scheduler--state-file nil))))
-
-(defun org-supertag-scheduler--load-state ()
-  "Load task states from the state file and update the in-memory registry."
-  (when (file-exists-p org-supertag-scheduler--state-file)
-    (require 'json)
-    (let ((json-string (with-temp-buffer
-                         (insert-file-contents org-supertag-scheduler--state-file)
-                         (buffer-string))))
-      (when (and json-string (> (length (string-trim json-string)) 0))
-        (let ((state (json-read-from-string json-string)))
-          (maphash (lambda (id task-plist)
-                     (let* ((id-str (symbol-name id))
-                            (last-run (cdr (assoc-string id-str state))))
-                       (when last-run
-                         (plist-put task-plist :last-run last-run)
-                         (puthash id task-plist org-supertag-scheduler--tasks))))
-                   org-supertag-scheduler--tasks))))))
-
-
-;;; === Public API ===
-
-(defun org-supertag-scheduler-register-task (id type function &rest args)
-  "Register a task with the central scheduler.
-ID: A unique symbol to identify the task (e.g., 'auto-tag-scan).
-TYPE: The type of task, either :interval or :daily.
-FUNCTION: The function to call when the task runs.
-ARGS: A plist of options depending on TYPE.
-  - For :interval: :interval (seconds)
-  - For :daily: :time (string in \"HH:MM\" format)
-
-If a task with the same ID already exists, it will be updated."
-  (let ((task-plist (list :type type :function function)))
-    (pcase type
-      (:interval
-       (let ((interval (plist-get args :interval)))
-         (unless (and (integerp interval) (> interval 0))
-           (error "Invalid interval for task %s" id))
-         (setq task-plist (plist-put task-plist :interval interval))))
-      (:daily
-       (let ((time (plist-get args :time)))
-         (unless (and (stringp time) (string-match "^[0-2][0-9]:[0-5][0-9]$" time))
-           (error "Invalid time format for task %s. Must be HH:MM" id))
-         (setq task-plist (plist-put task-plist :time time)))))
-    (puthash id task-plist org-supertag-scheduler--tasks)
-    (message "[Org-supertag-scheduler] Task '%s' registered." id)))
-
-(defun org-supertag-scheduler-deregister-task (id)
-  "Remove a task from the scheduler.
-ID: The unique symbol of the task to remove."
-  (remhash id org-supertag-scheduler--tasks)
-  (message "[Org-supertag-scheduler] Task '%s' deregistered." id))
-
-(defun org-supertag-scheduler-start ()
-  "Start the master scheduler timer.
-This function should be called once when org-supertag initializes."
-  (interactive)
-  (unless (timerp org-supertag-scheduler--master-timer)
-    (org-supertag-scheduler--load-state)
-    (setq org-supertag-scheduler--master-timer
-          (run-with-timer 0
-                          org-supertag-scheduler-check-interval
-                          #'org-supertag-scheduler--check-tasks))
-    (message "Org SuperTag Scheduler started.")))
-
-(defun org-supertag-scheduler-stop ()
-  "Stop the master scheduler timer."
-  (interactive)
-  (when (timerp org-supertag-scheduler--master-timer)
-    (cancel-timer org-supertag-scheduler--master-timer)
-    (setq org-supertag-scheduler--master-timer nil)
-    (org-supertag-scheduler--save-state)
-    (message "Org SuperTag Scheduler stopped.")))
-
-(defun org-supertag-scheduler-list-tasks ()
-  "Display a list of all registered tasks and their status."
-  (interactive)
-  (let ((tasks-info '()))
-    (maphash (lambda (id task)
-               (push (format "- %s: %s" id task) tasks-info))
-             org-supertag-scheduler--tasks)
-    (message "[Org-supertag-scheduler] Registered tasks:\n%s" (mapconcat #'identity (nreverse tasks-info) "\n"))))
-
-
-;;; === Internal Functions ===
-
-(defun org-supertag-scheduler--check-tasks ()
-  "The core function called by the master timer.
-Iterates through all registered tasks and runs them if their conditions are met.
-To prevent a 'thundering herd' on startup, it will only run ONE missed
-daily task per cycle, ensuring a smoother startup experience."
-  (message "[Scheduler-Debug] Master timer running check-tasks at %s" (format-time-string "%H:%M:%S"))
-  (let ((now (current-time))
-        (daily-task-ran-p nil)) ; Flag to ensure only one daily task runs per cycle
-    (maphash
-     (lambda (id task)
-       ;; Only check remaining tasks if a daily task hasn't already run in this cycle.
-       ;; Interval tasks are lightweight and can always run.
-       (pcase (plist-get task :type)
-         (:interval
-          (let ((interval (plist-get task :interval))
-                (last-run (plist-get task :last-run)))
-            (when (or (not last-run)
-                      (>= (time-to-seconds (time-subtract now last-run)) interval))
-              (org-supertag-scheduler--run-task id task now))))
-         (:daily
-          (unless daily-task-ran-p
-            (let* ((today-str (format-time-string "%Y-%m-%d" now))
-                   (current-time-str (format-time-string "%H:%M" now))
-                   (scheduled-time-str (plist-get task :time))
-                   (last-run-date (plist-get task :last-run)))
-              (message "[Scheduler-Debug] Checking daily task '%s': Now=%s, Scheduled=%s, LastRun=%s, Today=%s"
-                       id current-time-str scheduled-time-str (or last-run-date "never") today-str)
-              (when (and (string-greaterp current-time-str scheduled-time-str)
-                         (not (equal last-run-date today-str)))
-                (message "[Scheduler-Debug] -> Conditions MET for '%s'. Running..." id)
-                (org-supertag-scheduler--run-task id task now)
-                (setq daily-task-ran-p t))))))) ; Set flag to stop further daily tasks this cycle
-     org-supertag-scheduler--tasks)))
-
-(defun org-supertag-scheduler--run-task (id task now)
-  "Execute a task and update its state."
-  (let ((func (plist-get task :function)))
-    (message "[Org-supertag-scheduler] Running task '%s'..." id)
-    (if (org-supertag-bridge-ready-p)
-        (progn
-          (condition-case err
-              (funcall func)
-            (error
-             (message "[Org-supertag-scheduler] Error running task '%s': %s" id (error-message-string err))))
-          ;; Update last-run time
-          (pcase (plist-get task :type)
-            (:interval
-             (plist-put task :last-run now))
-            (:daily
-             (plist-put task :last-run (format-time-string "%Y-%m-%d" now))))
-          (puthash id task org-supertag-scheduler--tasks)
-          ;; Persist state immediately after a daily task runs
-          (when (eq (plist-get task :type) :daily)
-            (org-supertag-scheduler--save-state)))
-      (message "[Org-supertag-scheduler] Skipped task '%s' because Python server is not running." id))))
-
-(provide 'org-supertag-scheduler)
-
-;;; org-supertag-scheduler.el ends here
\ No newline at end of file
diff --git a/org-supertag-sim-epc-test.el b/org-supertag-sim-epc-test.el
new file mode 100644
index 0000000..07fb543
--- /dev/null
+++ b/org-supertag-sim-epc-test.el
@@ -0,0 +1,236 @@
+;;; org-supertag-sim-epc-test.el --- Test for SimTag EPC connection
+
+;; Copyright (C) 2023 
+
+;; Author: 
+;; Keywords: org-mode, nlp, epc, test
+
+;;; Commentary:
+;; 
+;; This file provides the functionality to test the Org-SuperTag SimTag EPC connection.
+;; It can help diagnose connection issues with the EPC server.
+;;
+
+;;; Code:
+
+(require 'cl-lib)
+(require 'epc)
+(require 'json)
+
+;; Global variable to save test results
+(defvar org-supertag-sim-epc-test-results nil
+  "Save test results.")
+
+(defun org-supertag-sim-epc-test-display-log ()
+  "Display test log."
+  (interactive)
+  (with-current-buffer (get-buffer-create "*simtag-test-log*")
+    (display-buffer (current-buffer))))
+
+(defun org-supertag-sim-epc-test-log (format-string &rest args)
+  "Record test log information.
+FORMAT-STRING is the format string
+ARGS are the format arguments"
+  (let ((msg (apply #'format format-string args)))
+    (with-current-buffer (get-buffer-create "*simtag-test-log*")
+      (goto-char (point-max))
+      (insert (format-time-string "[%Y-%m-%d %H:%M:%S] "))
+      (insert msg)
+      (insert "\n"))
+    (message "SimTag Test: %s" msg)))
+(defun org-supertag-sim-epc-test-connect-port (port)
+  "Test connection to EPC server at specified port.
+PORT is the port number to connect to"
+  (interactive "nPort: ")
+  (org-supertag-sim-epc-test-log "Testing connection to port %d" port)
+  (condition-case err
+      (let ((epc-process nil))
+        (org-supertag-sim-epc-test-log "Attempting to create EPC connection...")
+        (setq epc-process (epc:start-epc "127.0.0.1" port))
+        (if (epc:live-p epc-process)
+            (progn 
+              (org-supertag-sim-epc-test-log "EPC connection successful")
+              (let ((result (org-supertag-sim-epc-test-echo epc-process "Test message")))
+                (org-supertag-sim-epc-test-log "Echo test result: %S" result))
+              (epc:stop-epc epc-process)
+              t)
+          (org-supertag-sim-epc-test-log "EPC connection failed")
+          nil))
+    (error
+     (org-supertag-sim-epc-test-log "Connection error: %s" (error-message-string err))
+     nil)))
+
+(defun org-supertag-sim-epc-test-echo (epc-process msg)
+  "Test EPC server echo functionality.
+EPC-PROCESS is the EPC connection
+MSG is the message to send"
+  (org-supertag-sim-epc-test-log "Testing Echo functionality, sending message: %s" msg)
+  (condition-case err
+      (let ((result (epc:call-sync epc-process 'echo (list msg) 3)))
+        (org-supertag-sim-epc-test-log "Echo response: %S" result)
+        result)
+    (error
+     (org-supertag-sim-epc-test-log "Echo test error: %s" (error-message-string err))
+     nil)))
+
+(defun org-supertag-sim-epc-test-status (epc-process)
+  "Get EPC server status.
+EPC-PROCESS is the EPC connection"
+  (org-supertag-sim-epc-test-log "Getting server status...")
+  (condition-case err
+      (let ((result (epc:call-sync epc-process 'get_server_status nil 3)))
+        (org-supertag-sim-epc-test-log "Server status: %S" result)
+        result)
+    (error
+     (org-supertag-sim-epc-test-log "Get status error: %s" (error-message-string err))
+     nil)))
+
+(defun org-supertag-sim-epc-test-extract-entities (epc-process text)
+  "Test entity extraction functionality.
+EPC-PROCESS is the EPC connection
+TEXT is the text to extract entities from"
+  (org-supertag-sim-epc-test-log "Testing entity extraction, text: %s" text)
+  (condition-case err
+      (let ((result (epc:call-sync epc-process 'extract_entities (list text) 5)))
+        (org-supertag-sim-epc-test-log "Extraction result: %S" result)
+        result)
+    (error
+     (org-supertag-sim-epc-test-log "Entity extraction error: %s" (error-message-string err))
+     nil)))
+
+;;;###autoload
+(defun org-supertag-sim-epc-test-check-python-script ()
+  "Check if Python script exists."
+  (interactive)
+  (let ((script-path (expand-file-name "simtag_epc.py"
+                                      (file-name-directory (locate-library "org-supertag-sim-epc")))))
+    (if (file-exists-p script-path)
+        (progn
+          (org-supertag-sim-epc-test-log "Python script exists: %s" script-path)
+          (let ((size (nth 7 (file-attributes script-path)))
+                (mod-time (format-time-string "%Y-%m-%d %H:%M:%S"
+                                             (nth 5 (file-attributes script-path)))))
+            (org-supertag-sim-epc-test-log "Script size: %d bytes, modification time: %s" size mod-time)))
+      (org-supertag-sim-epc-test-log "Python script does not exist: %s" script-path))))
+
+;;;###autoload
+(defun org-supertag-sim-epc-test-check-env-script ()
+  "Check if environment script exists."
+  (interactive)
+  (let ((script-path (expand-file-name "run_simtag_epc_venv.sh"
+                                       (file-name-directory (locate-library "org-supertag-sim-epc")))))
+    (if (file-exists-p script-path)
+        (progn
+          (org-supertag-sim-epc-test-log "Environment script exists: %s" script-path)
+          (let ((size (nth 7 (file-attributes script-path)))
+                (mod-time (format-time-string "%Y-%m-%d %H:%M:%S"
+                                             (nth 5 (file-attributes script-path))))
+                (executable (file-executable-p script-path)))
+            (org-supertag-sim-epc-test-log "Script size: %d bytes, modification time: %s, executable: %s" 
+                                          size mod-time executable)
+            (unless executable
+              (org-supertag-sim-epc-test-log "Warning: Script is not executable"))))
+      (org-supertag-sim-epc-test-log "Environment script does not exist: %s" script-path))))
+
+;;;###autoload
+(defun org-supertag-sim-epc-test-python-version ()
+  "Check Python version."
+  (interactive)
+  (org-supertag-sim-epc-test-log "Checking Python version...")
+  (let ((output (shell-command-to-string "python3 --version 2>&1")))
+    (org-supertag-sim-epc-test-log "Python version: %s" output)
+    ;; Check if Python is available
+    (if (string-match "Python 3\\.[0-9]+" output)
+        (org-supertag-sim-epc-test-log "Python 3 is available")
+      (org-supertag-sim-epc-test-log "Warning: Python 3 not detected"))))
+
+;;;###autoload
+(defun org-supertag-sim-epc-test-port (port)
+  "Check if port is in use.
+PORT is the port number to check"
+  (interactive "nPort: ")
+  (org-supertag-sim-epc-test-log "Checking if port %d is in use..." port)
+  (let ((output (shell-command-to-string (format "lsof -i :%d" port))))
+    (if (string-empty-p output)
+        (org-supertag-sim-epc-test-log "Port %d is not in use" port)
+      (org-supertag-sim-epc-test-log "Port %d is in use:\n%s" port output))))
+
+;;;###autoload
+(defun org-supertag-sim-epc-run-tests ()
+  "Run all EPC server tests."
+  (interactive)
+  ;; Create or clear log buffer
+  (with-current-buffer (get-buffer-create "*simtag-test-log*")
+    (erase-buffer)
+    (display-buffer (current-buffer)))
+  
+  (setq org-supertag-sim-epc-test-results nil)
+  
+  (org-supertag-sim-epc-test-log "Starting SimTag EPC tests...")
+  
+  ;; Check script files
+  (org-supertag-sim-epc-test-check-python-script)
+  (org-supertag-sim-epc-test-check-env-script)
+  
+  ;; Check Python version
+  (org-supertag-sim-epc-test-python-version)
+  
+  ;; Check port
+  (org-supertag-sim-epc-test-port 21278)
+  
+  ;; Test EPC connection
+  (let ((port 21278))
+    (org-supertag-sim-epc-test-log "===============================================")
+    (org-supertag-sim-epc-test-log "Testing EPC connection to port %d" port)
+    (org-supertag-sim-epc-test-log "===============================================")
+    
+    (condition-case err
+        (let ((epc-process nil))
+          (org-supertag-sim-epc-test-log "Attempting to create EPC connection...")
+          (setq epc-process (epc:start-epc "127.0.0.1" port))
+          (if (epc:live-p epc-process)
+              (progn 
+                (org-supertag-sim-epc-test-log "EPC connection successful")
+                
+                ;; Test echo functionality
+                (let ((echo-result (org-supertag-sim-epc-test-echo epc-process "Test message")))
+                  (push `(:test "echo" :result ,echo-result) org-supertag-sim-epc-test-results))
+                
+                ;; Test get status
+                (let ((status-result (org-supertag-sim-epc-test-status epc-process)))
+                  (push `(:test "status" :result ,status-result) org-supertag-sim-epc-test-results))
+                
+                ;; Test entity extraction
+                (let ((extract-result (org-supertag-sim-epc-test-extract-entities epc-process "This is a test text for testing entity extraction functionality.")))
+                  (push `(:test "extract" :result ,extract-result) org-supertag-sim-epc-test-results))
+                
+                ;; Stop EPC process
+                (epc:stop-epc epc-process)
+                (org-supertag-sim-epc-test-log "EPC connection closed"))
+            (org-supertag-sim-epc-test-log "EPC connection failed")))
+      (error
+       (org-supertag-sim-epc-test-log "Connection error: %s" (error-message-string err)))))
+  
+  ;; Display test results summary
+  (org-supertag-sim-epc-test-log "===============================================")
+  (org-supertag-sim-epc-test-log "Test results summary")
+  (org-supertag-sim-epc-test-log "===============================================")
+  
+  (let ((all-passed t))
+    (dolist (result org-supertag-sim-epc-test-results)
+      (let ((test-name (plist-get result :test))
+            (test-result (plist-get result :result)))
+        (if test-result
+            (org-supertag-sim-epc-test-log "Test '%s': Passed" test-name)
+          (org-supertag-sim-epc-test-log "Test '%s': Failed" test-name)
+          (setq all-passed nil))))
+    
+    (org-supertag-sim-epc-test-log "===============================================")
+    (if all-passed
+        (org-supertag-sim-epc-test-log "All tests passed!")
+      (org-supertag-sim-epc-test-log "Some tests failed, please check the log for details.")))
+  
+  (org-supertag-sim-epc-test-log "Tests completed")
+)
+(provide 'org-supertag-sim-epc-test)
+;;; org-supertag-sim-epc-test.el ends here
diff --git a/org-supertag-sim-epc.el b/org-supertag-sim-epc.el
new file mode 100644
index 0000000..f07cd9e
--- /dev/null
+++ b/org-supertag-sim-epc.el
@@ -0,0 +1,849 @@
+;;; org-supertag-sim-epc.el --- tag similarity service based on EPC -*- lexical-binding: t; -*-
+
+;; 依赖
+(require 'json)
+(require 'org-supertag-db)
+(require 'cl-lib)
+(require 'epc)  
+
+(defgroup org-supertag-sim-epc nil
+  "Tag similarity service based on EPC."
+  :group 'org-supertag)
+
+(defcustom org-supertag-sim-epc-python-path 
+  (or (executable-find "python3")
+      (executable-find "python"))
+  "Python interpreter path."
+  :type 'string
+  :group 'org-supertag-sim-epc)
+
+(defcustom org-supertag-sim-epc-script-path
+  (expand-file-name "simtag_epc.py"
+                   (file-name-directory
+                    (or load-file-name buffer-file-name)))
+  "simtag_epc.py script path."
+  :type 'string
+  :group 'org-supertag-sim-epc)
+
+(defcustom org-supertag-sim-epc-vector-file
+  (expand-file-name "tag_vectors.json"
+                   (file-name-directory
+                    (if (boundp 'org-supertag-db-file)
+                        org-supertag-db-file
+                      (expand-file-name "supertag-db.el" org-supertag-data-directory))))
+  "Tag vector file path."
+  :type 'string
+  :group 'org-supertag-sim-epc)
+
+(defcustom org-supertag-sim-epc-request-timeout 30
+  "Request timeout (seconds)."
+  :type 'integer
+  :group 'org-supertag-sim-epc)
+
+(defvar org-supertag-sim-epc-manager nil
+  "EPC manager object.")
+
+(defvar org-supertag-sim-epc-initialized nil
+  "Whether the tag system has been initialized.")
+
+(defvar org-supertag-sim-epc--startup-timer nil
+  "Server startup timer.")
+
+(defcustom org-supertag-sim-epc-dir
+  (file-name-directory (or load-file-name buffer-file-name))
+  "SimTag EPC server directory path."
+  :type 'directory
+  :group 'org-supertag-sim-epc)
+
+(defcustom org-supertag-sim-epc-data-dir
+  (expand-file-name "data" org-supertag-sim-epc-dir)
+  "SimTag data directory path."
+  :type 'directory
+  :group 'org-supertag-sim-epc)
+
+(defcustom org-supertag-sim-epc-venv-dir
+  (expand-file-name ".venv" org-supertag-sim-epc-dir)
+  "Python virtual environment directory."
+  :type 'directory
+  :group 'org-supertag-sim-epc)
+
+;; Ensure the data directory exists
+(unless (file-exists-p org-supertag-sim-epc-data-dir)
+  (make-directory org-supertag-sim-epc-data-dir t))
+
+;; Log function
+(defun org-supertag-sim-epc-log (format-string &rest args)
+  "Log information.
+FORMAT-STRING is the format string
+ARGS is the format parameters"
+  (let ((msg (apply #'format format-string args)))
+    (with-current-buffer (get-buffer-create "*simtag-epc-log*")
+      (goto-char (point-max))
+      (insert (format-time-string "[%Y-%m-%d %H:%M:%S] "))
+      (insert msg)
+      (insert "\n"))
+    (message "SimTag EPC: %s" msg)))
+
+(defun org-supertag-sim-epc--ensure-db-file ()
+  "Ensure the database file exists and returns its path and tag data."
+  (unless (boundp 'org-supertag-db-file)
+    (error "Database file path is not defined"))
+  (let* ((db-file org-supertag-db-file)
+         (tags (org-supertag-db-find-by-type :tag)))
+    ;; Ensure the database file exists
+    (unless (file-exists-p db-file)
+      (org-supertag-db-save))
+    
+    ;; Convert tag data to list format expected by Python
+    (let ((tag-list
+           (delq nil
+                 (cl-loop for tag in tags
+                          for props = (org-supertag-db-get tag)
+                          for name = (and props (plist-get props :name))
+                          when name
+                          collect (list (cons "id" tag) (cons "name" name))))))
+      (list db-file tag-list))))
+
+(defun org-supertag-sim-epc-debug-env ()
+  "Debug environment variables."
+  (interactive)
+  (let ((debug-info (format "=== Environment Debug ===
+PYTHONPATH: %s
+VIRTUAL_ENV: %s
+PATH: %s
+Python Path: %s
+Base Dir: %s
+Venv Dir: %s
+=== End Debug ==="
+                           (getenv "PYTHONPATH")
+                           (getenv "VIRTUAL_ENV")
+                           (getenv "PATH")
+                           org-supertag-sim-epc-python-path
+                           org-supertag-sim-epc-dir
+                           org-supertag-sim-epc-venv-dir)))
+    (org-supertag-sim-epc-log debug-info)
+    (message debug-info)))
+
+(defun org-supertag-sim-epc-start-server ()
+  "Start SimTag EPC server."
+  (interactive)
+  ;; Cancel existing startup timer
+  (when org-supertag-sim-epc--startup-timer
+    (cancel-timer org-supertag-sim-epc--startup-timer))
+  
+  ;; Set a new startup timer to execute after 1 second of idle time in Emacs
+  (setq org-supertag-sim-epc--startup-timer
+        (run-with-idle-timer 
+         1 nil  ; Execute after 1 second
+         (lambda ()
+           (condition-case err
+               (org-supertag-sim-epc--start-server-internal)
+             (error
+              (org-supertag-sim-epc-log "服务器启动失败: %s" err)
+              (message "SimTag EPC服务器启动失败，将在下次空闲时重试")))))))
+
+(defun org-supertag-sim-epc-setup-venv ()
+  "Setup the Python virtual environment."
+  (interactive)
+  (let ((venv-dir (file-name-as-directory org-supertag-sim-epc-venv-dir))
+        (base-dir (file-name-as-directory org-supertag-sim-epc-dir)))
+    ;; 设置环境变量
+    (setq process-environment 
+          (append process-environment
+                  (list (format "PYTHONPATH=%s" base-dir)
+                        (format "VIRTUAL_ENV=%s" venv-dir)
+                        (format "PATH=%s/bin:%s" venv-dir (getenv "PATH")))))
+    
+    (unless (file-exists-p venv-dir)
+      (org-supertag-sim-epc-log "Creating virtual environment...")
+      (make-directory venv-dir t)
+      (shell-command-to-string 
+       (format "python3 -m venv %s" venv-dir)))
+    
+    ;; 安装依赖
+    (let ((pip (expand-file-name "bin/pip" venv-dir)))
+      (org-supertag-sim-epc-log "Installing dependencies...")
+      (shell-command-to-string 
+       (format "%s install epc sentence-transformers torch numpy requests" pip)))
+    
+    ;; 更新 Python 解释器路径
+    (let ((python-path (expand-file-name "bin/python" venv-dir)))
+      (when (file-exists-p python-path)
+        (setq org-supertag-sim-epc-python-path python-path)
+        (org-supertag-sim-epc-log "Python path updated: %s" python-path)))
+    
+    ;; 调试输出
+    (org-supertag-sim-epc-debug-env)))
+
+(defun org-supertag-sim-epc--start-server-internal ()
+  "Internal function: Start SimTag EPC server."
+  (let* ((venv-dir (file-name-as-directory org-supertag-sim-epc-venv-dir))
+         (base-dir (file-name-as-directory org-supertag-sim-epc-dir))
+         (python-exe (expand-file-name "bin/python" venv-dir))  ;; 使用虚拟环境中的 Python
+         (python-file org-supertag-sim-epc-script-path)
+         (vector-file org-supertag-sim-epc-vector-file)
+         (db-file org-supertag-db-file)
+         (default-directory base-dir)
+         (process-buffer (get-buffer-create "*simtag-epc-process*")))
+    
+    ;; 确保环境设置正确
+    (org-supertag-sim-epc-setup-venv)
+    (org-supertag-sim-epc-check-module-structure)
+    
+    ;; 记录启动信息
+    (org-supertag-sim-epc-log "Starting server...")
+    (org-supertag-sim-epc-log "Working directory: %s" default-directory)
+    (org-supertag-sim-epc-debug-env)
+    
+    (org-supertag-sim-epc-log "Starting EPC server...")
+    (org-supertag-sim-epc-log "Python path: %s" python-exe)
+    (org-supertag-sim-epc-log "Script path: %s" python-file)
+    (org-supertag-sim-epc-log "Startup parameters: %S" (list python-file "--vector-file" vector-file "--db-file" db-file "--debug"))
+    
+    ;; Clear the process buffer
+    (with-current-buffer process-buffer
+      (erase-buffer))
+    
+    ;; Create a process
+    (make-process
+     :name "simtag-epc"
+     :buffer process-buffer
+     :command (cons python-exe (list python-file "--vector-file" vector-file "--db-file" db-file "--debug"))
+     :filter (lambda (proc output)
+               (org-supertag-sim-epc-log "Process output: %s" output)
+               (with-current-buffer (process-buffer proc)
+                 (goto-char (point-max))
+                 (insert output))
+               ;; Check the port number
+               (when (string-match "\\([0-9]+\\)[\n\r]" output)
+                 (let ((port (string-to-number (match-string 1 output))))
+                   (org-supertag-sim-epc-log "Found port number: %d" port)
+                   ;; Create EPC connection
+                   (condition-case err
+                       (progn
+                         (setq org-supertag-sim-epc-manager 
+                               (epc:start-epc python-exe (list python-file "--vector-file" vector-file "--db-file" db-file "--debug")))
+                         (org-supertag-sim-epc-log "EPC connection created"))
+                     (error
+                      (org-supertag-sim-epc-log "EPC connection failed: %s" 
+                                               (error-message-string err))))))))))
+
+(defun org-supertag-sim-epc-stop-server ()
+  "Stop SimTag EPC server."
+  (when (and org-supertag-sim-epc-manager
+             (epc:live-p org-supertag-sim-epc-manager))
+    (org-supertag-sim-epc-log "Stopping EPC server...")
+    (epc:stop-epc org-supertag-sim-epc-manager)
+    (org-supertag-sim-epc-log "EPC server stopped"))
+  
+  (setq org-supertag-sim-epc-manager nil
+        org-supertag-sim-epc-initialized nil))
+
+(defun org-supertag-sim-epc-server-running-p ()
+  "Check if the EPC server is running."
+  (and org-supertag-sim-epc-manager
+       (epc:live-p org-supertag-sim-epc-manager)))
+
+(defun org-supertag-sim-epc-ensure-server ()
+  "Ensure the EPC server is running, start it if not running."
+  (unless (org-supertag-sim-epc-server-running-p)
+    (org-supertag-sim-epc-start-server)))
+
+(defun org-supertag-sim-epc-check-ollama-installed ()
+  "Check if Ollama is installed, similar to the check_ollama_installed function in ollama_bridge.py."
+  (org-supertag-sim-epc-log "Checking if Ollama is installed...")
+  (let ((result 
+         (cond
+          ;; Windows system
+          ((string-match-p "windows" (symbol-name system-type))
+           (let ((possible-paths '("C:/Program Files/Ollama/ollama.exe"
+                                   "C:/Program Files (x86)/Ollama/ollama.exe"
+                                   "~/AppData/Local/Programs/Ollama/ollama.exe"
+                                   "~/scoop/apps/ollama/current/ollama.exe"))
+                 (found nil))
+             (dolist (path possible-paths)
+               (when (file-exists-p (expand-file-name path))
+                 (setq found t)))
+             found))
+          ;; Unix system (Linux/macOS)
+          (t
+           (= 0 (call-process "which" nil nil nil "ollama"))))))
+    
+    (if result
+        (progn
+          (org-supertag-sim-epc-log "Ollama is installed")
+          t)
+      (org-supertag-sim-epc-log "Ollama is not installed")
+      nil)))
+
+(defun org-supertag-sim-epc-get-ollama-install-instruction ()
+  "Get the installation command for Ollama, similar to get_install_command in ollama_bridge.py."
+  (let ((system-type (symbol-name system-type)))
+    (cond
+     ((string-match-p "darwin" system-type)  ;; macOS
+      "curl -fsSL https://ollama.com/install.sh | sh")
+     ((string-match-p "gnu/linux" system-type)  ;; Linux
+      "curl -fsSL https://ollama.com/install.sh | sh")
+     ((string-match-p "windows" system-type)  ;; Windows
+      "Windows installation options:
+1. Use winget (recommended):
+   winget install Ollama.Ollama
+
+2. Use Scoop:
+   scoop bucket add main
+   scoop install ollama
+
+3. Download the installation package directly:
+   Visit https://ollama.com/download")
+     (t
+      "Please visit https://ollama.com for installation guide"))))
+
+(defun org-supertag-sim-epc-check-ollama ()
+  "Check if the Ollama service is running.
+Check方式类似于 ollama_bridge.py中的is_service_running方法."
+  (org-supertag-sim-epc-log "Checking the Ollama service...")
+  (let ((result (condition-case nil
+                    (with-timeout (3)  ;; Set 3 second timeout
+                      (let ((output (shell-command-to-string "curl -s --connect-timeout 2 http://localhost:11434/api/tags")))
+                        (with-temp-buffer
+                          (insert output)
+                          (goto-char (point-min))
+                          ;; Verify if a valid JSON response is returned
+                          (and (> (length output) 2)
+                               (or (looking-at "\\[")    ;; Should start with a JSON array
+                                   (looking-at "{"))))))  ;; Or start with a JSON object
+                  (error nil))))
+    (if result
+        (progn
+          (org-supertag-sim-epc-log "Ollama service is running")
+          t)
+      (org-supertag-sim-epc-log "Failed to connect to the Ollama service! Please ensure Ollama is started")
+      (message "Warning: Failed to connect to the Ollama service! Please ensure Ollama is started")
+      nil)))
+
+(defun org-supertag-sim-epc-start-ollama ()
+  "Start the Ollama service, select the appropriate startup method based on the current system type."
+  (org-supertag-sim-epc-log "Attempting to start the Ollama service...")
+  (let ((system-type (symbol-name system-type)))
+    (cond
+     ((or (string-match-p "darwin" system-type)
+          (string-match-p "gnu/linux" system-type))
+      (start-process "ollama-start" nil "ollama" "serve"))
+     ((string-match-p "windows" system-type)
+      (start-process "ollama-start" nil "ollama.exe" "serve"))
+     (t (message "Unsupported system type: %s" system-type)
+        nil))))
+
+(defun org-supertag-sim-epc-ensure-ollama-running ()
+  "Ensure the Ollama service is running, similar to the _ensure_service process in ollama_bridge.py.
+1. Check if Ollama is installed
+2. If not installed, provide installation instructions
+3. Check if the service is running
+4. If not running, start the service and wait for it to be ready"
+  ;; 1. Check if Ollama is installed
+  (unless (org-supertag-sim-epc-check-ollama-installed)
+    (let ((install-instruction (org-supertag-sim-epc-get-ollama-install-instruction)))
+      (org-supertag-sim-epc-log "Ollama is not installed, please install it first")
+      (message "Ollama is not installed, please run the following command to install:\n%s" install-instruction)
+      (error "Ollama is not installed, cannot continue")))
+  
+  ;; 2. Check if the service is running
+  (unless (org-supertag-sim-epc-check-ollama)
+    (message "Ollama service is not running, starting...")
+    
+    ;; 3. Try to start the service
+    (let ((system-type (symbol-name system-type)))
+      (cond
+       ((or (string-match-p "darwin" system-type)
+            (string-match-p "gnu/linux" system-type))
+        (start-process "ollama-start" nil "ollama" "serve"))
+       ((string-match-p "windows" system-type)
+        (start-process "ollama-start" nil "ollama.exe" "serve"))
+       (t (message "Unsupported system type: %s" system-type))))
+    
+    ;; 4. Wait for the service to start
+    (let ((max-attempts 5)
+          (attempt 0)
+          (success nil))
+      (while (and (< attempt max-attempts) (not success))
+        (setq attempt (1+ attempt))
+        (message "Waiting for Ollama service to start... Attempt %d/%d" attempt max-attempts)
+        (sleep-for 2)  ;; Wait 2 seconds each time
+        (setq success (org-supertag-sim-epc-check-ollama))
+        (when success
+          (message "Ollama service has been successfully started"))
+        (when (and (not success) (= attempt max-attempts))
+          (message "Warning: Ollama service failed to start, please start it manually")
+          (error "Ollama service failed to start, please start it manually"))))))
+
+(defun org-supertag-sim-epc-verify-ollama-model (model-name)
+  "Verify if the Ollama model exists, similar to ensure_model_exists in ollama_bridge.py.
+MODEL-NAME is the name of the model to verify." 
+  (org-supertag-sim-epc-log "Verifying if model %s exists..." model-name)
+  (let ((result (condition-case err
+                    (with-timeout (3)
+                      (let ((output (shell-command-to-string 
+                                     "curl -s --connect-timeout 2 http://127.0.0.1:11434/api/tags")))
+                        (org-supertag-sim-epc-log "API response: %s" (truncate-string-to-width output 100))
+                        (with-temp-buffer
+                          (insert output)
+                          (goto-char (point-min))
+                          ;; Check if the returned JSON contains the specified model
+                          (and (looking-at "{")  ;; Should be a JSON object
+                               (condition-case err
+                                   (let* ((json-object-type 'hash-table)
+                                          (json-array-type 'list)
+                                          (json-key-type 'string)
+                                          (json-data (json-read))
+                                          (models (gethash "models" json-data))
+                                          (found nil))
+                                     ;; Iterate through the models array, check if it contains the specified model
+                                     (when models
+                                       (dolist (model models)
+                                         (let ((name (gethash "name" model)))
+                                           (org-supertag-sim-epc-log "Checking model: %s" name)
+                                           (when (and name (string= name model-name))
+                                             (setq found t)))))
+                                     found)
+                                 (error
+                                  (org-supertag-sim-epc-log "JSON parsing error: %s" (error-message-string err))
+                                  nil))))))
+                  (error
+                   (org-supertag-sim-epc-log "Error verifying model: %s" (error-message-string err))
+                   nil))))
+    (if result
+        (progn
+          (org-supertag-sim-epc-log "Model %s exists" model-name)
+          t)
+      (org-supertag-sim-epc-log "Model %s does not exist" model-name)
+      nil)))
+
+(defun org-supertag-sim-epc-pull-ollama-model (model-name)
+  "Pull the Ollama model, similar to pull_model in ollama_bridge.py.
+MODEL-NAME is the name of the model to pull."
+  (org-supertag-sim-epc-log "Pulling model %s..." model-name)
+  (message "Pulling Ollama model %s, this may take some time..." model-name)
+  (let ((process (start-process "ollama-pull" nil "ollama" "pull" model-name)))
+    (set-process-sentinel 
+     process
+     (lambda (proc event)
+       (if (string-match-p "finished" event)
+           (progn
+             (org-supertag-sim-epc-log "Model %s pulled successfully" model-name)
+             (message "Ollama model %s pulled successfully" model-name))
+         (org-supertag-sim-epc-log "Model %s pull failed: %s" model-name event)
+         (message "Warning: Ollama model %s pull failed: %s" model-name event))))
+    ;; Return the process so it can be tracked
+    process))
+
+(defun org-supertag-sim-epc-ensure-ollama-model (model-name)
+  "Ensure the Ollama model exists and is available, pull it if it doesn't exist.
+MODEL-NAME is the name of the model to ensure, default is gemma-3-4b."
+  (let ((model (or model-name "hf.co/unsloth/gemma-3-4b-it-GGUF:latest")))
+    ;; First ensure the service is running
+    (org-supertag-sim-epc-ensure-ollama-running)
+    
+    ;; Check if the model exists
+    (unless (org-supertag-sim-epc-verify-ollama-model model)
+      (message "Model %s does not exist, pulling..." model)
+      (org-supertag-sim-epc-pull-ollama-model model)
+      ;; Don't wait for the pull to complete, as it may take a long time
+      ;; In actual application, some callback mechanism may be needed
+      )))
+
+;; The complete Ollama check and initialization process
+(defun org-supertag-sim-epc-setup-ollama ()
+  "Complete setup of the Ollama environment, including checking installation, starting the service, and preparing the default model."
+  (interactive)
+  (message "Setting up the Ollama environment...")
+  (condition-case err
+      (progn
+        ;; 1. Ensure Ollama is installed
+        (unless (org-supertag-sim-epc-check-ollama-installed)
+          (let ((install-instruction (org-supertag-sim-epc-get-ollama-install-instruction)))
+            (message "Ollama is not installed, please run the following command to install:\n%s" install-instruction)
+            (error "Ollama is not installed")))
+        
+        ;; 2. Ensure the service is running
+        (org-supertag-sim-epc-ensure-ollama-running)
+        
+        ;; 3. Prepare the default model (optional)
+        ;; Here you can choose whether to check and prepare the default model
+        ;; Since model download may take a long time, the default is to check without downloading
+        (when (org-supertag-sim-epc-verify-ollama-model "hf.co/unsloth/gemma-3-4b-it-GGUF:latest")
+          (org-supertag-sim-epc-log "Default model is ready"))
+        
+        (message "Ollama environment setup completed"))
+    (error
+     (message "Ollama environment setup failed: %s" (error-message-string err))
+     nil)))
+
+;; Function interface
+(defun org-supertag-sim-epc-init ()
+  "Initialize the EPC service and Ollama service for tag similarity engine.
+This is an internal function called by org-supertag-sim-init.
+For normal users, use org-supertag-sim-init as the main entry point."
+  (org-supertag-sim-epc-log "Starting initialization...")
+  (org-supertag-sim-epc-ensure-server)
+  
+  ;; Ensure the Ollama service is set up and running (no error capture)
+  (org-supertag-sim-epc-log "Ensuring Ollama service is running...")
+  (org-supertag-sim-epc-setup-ollama)
+  
+  (let* ((db-info (org-supertag-sim-epc--ensure-db-file))
+         (db-file (car db-info))
+         (tag-list (cadr db-info)))
+    
+    (org-supertag-sim-epc-log "Database file: %s" db-file)
+    (org-supertag-sim-epc-log "Vector file: %s" org-supertag-sim-epc-vector-file)
+    
+    (condition-case err
+        (progn
+          (org-supertag-sim-epc-log "Calling the initialize method...")
+          ;; Pass the file path string directly
+          (let ((response (epc:call-sync org-supertag-sim-epc-manager 
+                                        'initialize 
+                                        (list org-supertag-sim-epc-vector-file 
+                                              db-file))))
+            (org-supertag-sim-epc-log "Initialization return result: %S" response)
+            (if (string= (plist-get response :status) "success")
+                (progn
+                  (setq org-supertag-sim-epc-initialized t)
+                  (org-supertag-sim-epc-log "Initialization successful")
+                  (plist-get response :result))
+              (error "Initialization failed: %S" response))))
+      (error
+       (org-supertag-sim-epc-log "Initialization process error: %s" (error-message-string err))
+       (error "Initialization process error: %s" (error-message-string err)))))) ;; Modify here, throw an error directly
+
+
+(defun org-supertag-sim-epc-restart-server ()
+  "Restart the SimTag EPC server."
+  (interactive)
+  (org-supertag-sim-epc-stop-server)
+  (sleep-for 1)  ; Wait for the process to fully terminate
+  (org-supertag-sim-epc-start-server)
+  (when (org-supertag-sim-epc-server-running-p)
+    (message "SimTag EPC server has been restarted")))
+
+(defun org-supertag-sim-epc-show-vector-file-info ()
+  "Show the information of the vector file."
+  (interactive)
+  (message "Vector file path: %s" org-supertag-sim-epc-vector-file)
+  (when (boundp 'org-supertag-db-file)
+    (message "Database file path: %s" org-supertag-db-file))
+  (if (file-exists-p org-supertag-sim-epc-vector-file)
+      (let ((size (nth 7 (file-attributes org-supertag-sim-epc-vector-file)))
+            (mod-time (format-time-string "%Y-%m-%d %H:%M:%S"
+                                          (nth 5 (file-attributes org-supertag-sim-epc-vector-file)))))
+        (message "Vector file exists (size: %d bytes, update time: %s)" size mod-time))
+    (message "Vector file does not exist, will be created during initialization")))
+
+(defun org-supertag-sim-epc-clean-python-cache ()
+  "Clean the Python cache file."
+  (interactive)
+  (let ((script-dir (file-name-directory org-supertag-sim-epc-script-path)))
+    (message "Cleaning Python cache file...")
+    (shell-command (format "find %s -name '__pycache__' -type d -exec rm -rf {} +; find %s -name '*.pyc' -delete" 
+                           script-dir script-dir))
+    (message "Python cache file has been cleaned")))
+
+(defun org-supertag-sim-epc-force-kill ()
+  "Force terminate all SimTag EPC related processes."
+  (interactive)
+  (message "Force terminating SimTag EPC processes...")
+  
+  ;; Terminate known processes
+  (when (org-supertag-sim-epc-server-running-p)
+    (org-supertag-sim-epc-stop-server))
+  
+  ;; Clean Python processes
+  (message "Terminating related Python processes...")
+  (shell-command "pkill -f 'python.*simtag_epc\\.py' || true")
+  
+  ;; Reset status
+  (setq org-supertag-sim-epc-manager nil)
+  (setq org-supertag-sim-epc-initialized nil)
+  
+  (message "SimTag EPC processes have been terminated"))
+
+(defun org-supertag-sim-epc-emergency-restart ()
+  "Emergency restart the EPC server."
+  (interactive)
+  (message "Performing emergency restart...")
+  (org-supertag-sim-epc-force-kill)
+  (sit-for 1)
+  (org-supertag-sim-epc-start-server)
+  (sit-for 2)
+  (if (org-supertag-sim-epc-server-running-p)
+      (message "EPC server has been restarted")
+    (message "EPC server restart failed")))
+
+(defun org-supertag-sim-epc-echo-test ()
+  "Test the EPC connection."
+  (interactive)
+  (org-supertag-sim-epc-ensure-server)
+  (condition-case err
+      (let ((result (epc:call-sync org-supertag-sim-epc-manager 'echo '("test"))))
+        (message "Echo test successful: %S" result)
+        t)
+    (error
+     (message "Echo test failed: %s" (error-message-string err))
+     nil)))
+
+(defun org-supertag-sim-epc-show-log ()
+  "Show the SimTag EPC log buffer."
+  (interactive)
+  (let ((log-buffer (get-buffer-create "*simtag-epc-log*")))
+    (with-current-buffer log-buffer
+      (special-mode)  ; Make the buffer read-only
+      (goto-char (point-max)))
+    (display-buffer log-buffer)))
+
+(defun org-supertag-sim-epc-check-module-structure ()
+  "Check and create the necessary module structure."
+  (interactive)
+  (let* ((base-dir org-supertag-sim-epc-dir)
+         (simtag-dir (expand-file-name "simtag" base-dir))
+         (init-file (expand-file-name "__init__.py" simtag-dir)))
+    
+    ;; Create simtag directory
+    (unless (file-exists-p simtag-dir)
+      (make-directory simtag-dir t))
+    
+    ;; Create __init__.py
+    (unless (file-exists-p init-file)
+      (with-temp-file init-file
+        (insert "# SimTag package\n")))
+    
+    ;; Check necessary Python files
+    (dolist (file '("config.py" "epc_server.py"))
+      (let ((file-path (expand-file-name file simtag-dir)))
+        (unless (file-exists-p file-path)
+          (org-supertag-sim-epc-log "Missing necessary file: %s" file-path))))
+    
+    (org-supertag-sim-epc-log "Module structure check completed")))
+
+(defun org-supertag-sim-epc-test-server ()
+  "Test the server functionality comprehensively."
+  (interactive)
+  (org-supertag-sim-epc-log "Starting server test...")
+  
+  (condition-case err
+      (let ((result (epc:call-sync org-supertag-sim-epc-manager 'echo '("test"))))
+        (org-supertag-sim-epc-log "Echo test successful: %S" result))
+    (error
+     (org-supertag-sim-epc-log "Echo test failed: %s" (error-message-string err))
+     (error "Echo test failed")))
+  
+  (condition-case err
+      (let ((result (epc:call-sync org-supertag-sim-epc-manager 'check_imports '())))
+        (org-supertag-sim-epc-log "Module import test successful: %S" result))
+    (error
+     (org-supertag-sim-epc-log "Module import test failed: %s" (error-message-string err))
+     (error "Module import test failed")))
+  
+  (condition-case err
+      (let ((result (epc:call-sync org-supertag-sim-epc-manager 'get_config '())))
+        (org-supertag-sim-epc-log "Configuration test successful: %S" result))
+    (error
+     (org-supertag-sim-epc-log "Configuration test failed: %s" (error-message-string err))
+     (error "Configuration test failed")))
+  
+  (org-supertag-sim-epc-log "Server test completed"))
+
+(defun org-supertag-sim-epc-extract-entities-async (text callback)
+  "Asynchronously extract entities from TEXT and pass the result to the CALLBACK function.
+TEXT is the text to analyze
+CALLBACK is the callback function that receives the entity list"
+  (org-supertag-sim-epc-log "Asynchronous entity extraction, text length: %d" (length text))
+  (deferred:$
+    (deferred:try
+      (deferred:$
+        (org-supertag-sim-epc-ensure-server)
+        (deferred:nextc it
+          (lambda (_)
+            (epc:call-deferred org-supertag-sim-epc-manager
+                              'extract_entities
+                              (list text))))
+        (deferred:nextc it
+          (lambda (response)
+            (let ((status (plist-get response :status))
+                  (result (plist-get response :result)))
+              (if (string= status "success")
+                  (progn
+                    (org-supertag-sim-epc-log "Entity extraction successful, found %d entities" 
+                                              (length result))
+                    (funcall callback result))
+                (error "Entity extraction failed: %s" 
+                       (or (plist-get response :message) "Unknown error")))))))
+      :catch
+      (lambda (err)
+        (org-supertag-sim-epc-log "Entity extraction error: %s" (error-message-string err))
+        (message "Entity extraction error: %s" (error-message-string err))
+        (funcall callback nil)))))
+
+(defun org-supertag-sim-epc-get-tag-suggestions-async (text limit callback)
+  "Asynchronously get tag suggestions for TEXT.
+TEXT is the text to analyze
+LIMIT is the maximum number of suggestions to return
+CALLBACK is the callback function that receives the suggestions"
+  (org-supertag-sim-epc-log "Asynchronous tag suggestion, text length: %d, limit: %d" 
+                           (length text) limit)
+  (deferred:$
+    (deferred:try
+      (deferred:$
+        (org-supertag-sim-epc-ensure-server)
+        (deferred:nextc it
+          (lambda (_)
+            (epc:call-deferred org-supertag-sim-epc-manager
+                              'suggest_tags
+                              (list text limit))))
+        (deferred:nextc it
+          (lambda (response)
+            (let ((status (plist-get response :status))
+                  (result (plist-get response :result)))
+              (if (string= status "success")
+                  (progn
+                    (org-supertag-sim-epc-log "Tag suggestion successful, found %d suggestions" 
+                                              (length result))
+                    (funcall callback result))
+                (error "Tag suggestion failed: %s" 
+                       (or (plist-get response :message) "Unknown error")))))))
+      :catch
+      (lambda (err)
+        (org-supertag-sim-epc-log "Tag suggestion error: %s" (error-message-string err))
+        (message "Tag suggestion error: %s" (error-message-string err))
+        (funcall callback nil)))))
+
+(defun org-supertag-sim-epc-find-similar-async (tag-name limit callback)
+  "Asynchronously find similar tags to TAG-NAME.
+LIMIT is the maximum number of similar tags to return
+CALLBACK is the callback function that receives the similar tags"
+  (org-supertag-sim-epc-log "Asynchronous similar tag search: %s, limit: %d" tag-name limit)
+  (deferred:$
+    (deferred:try
+      (deferred:$
+        (org-supertag-sim-epc-ensure-server)
+        (deferred:nextc it
+          (lambda (_)
+            (epc:call-deferred org-supertag-sim-epc-manager
+                              'find_similar
+                              (list tag-name limit))))
+        (deferred:nextc it
+          (lambda (response)
+            (let ((status (plist-get response :status))
+                  (result (plist-get response :result)))
+              (if (string= status "success")
+                  (progn
+                    (org-supertag-sim-epc-log "Similar tag search successful, found %d similar tags" 
+                                              (length result))
+                    (funcall callback result))
+                (error "Similar tag search failed: %s" 
+                       (or (plist-get response :message) "Unknown error")))))))
+      :catch
+      (lambda (err)
+        (org-supertag-sim-epc-log "Similar tag search error: %s" (error-message-string err))
+        (message "Similar tag search error: %s" (error-message-string err))
+        (funcall callback nil)))))
+
+(defun org-supertag-sim-epc-interactive-ollama ()
+  "Create a buffer where users can interact with Ollama."
+  (interactive)
+  (require 'org-supertag-sim)
+  (unless (and org-supertag-sim--initialized org-supertag-sim-epc-initialized)
+    (org-supertag-sim-init))
+  (let ((buffer (get-buffer-create "*org-supertag-ollama-chat*")))
+    (switch-to-buffer buffer)
+    (when (= (buffer-size) 0)
+      (insert "===== Ollama 交互式对话 =====\n\n")
+      (insert "在此输入消息，按 C-c C-c 发送\n\n")
+      (insert "系统提示 (可选):\n")
+      (insert "--------------------------\n")
+      (insert "你是一个专注于Emacs和org-mode的AI助手，专为org-supertag项目提供帮助。\n")
+      (insert "--------------------------\n\n")
+      (insert "用户消息:\n")
+      (insert "请输入您的问题...\n")
+      (local-set-key (kbd "C-c C-c") 'org-supertag-sim-epc--send-message)
+      (message "输入您的消息，然后按 C-c C-c 发送给Ollama"))))
+
+(defun org-supertag-sim-epc--send-message ()
+  "Send a message from the interactive Ollama buffer."
+  (interactive)
+  (with-current-buffer "*org-supertag-ollama-chat*"
+    ;; Extract user message and system prompt
+    (let ((system-prompt nil)
+          (user-message nil))
+      
+      ;; Get system prompt
+      (save-excursion
+        (goto-char (point-min))
+        (when (search-forward "--------------------------\n" nil t)
+          (let ((start (point)))
+            (when (search-forward "--------------------------\n" nil t)
+              (setq system-prompt (buffer-substring-no-properties 
+                                  start
+                                  (- (point) 
+                                     (length "--------------------------\n"))))))))
+      
+      ;; Get user message
+      (save-excursion
+        (goto-char (point-min))
+        (when (search-forward "用户消息:\n" nil t)
+          (setq user-message (buffer-substring-no-properties 
+                             (point)
+                             (point-max)))))
+      
+      ;; Clear user message area and prepare to receive reply
+      (let ((inhibit-read-only t))
+        (save-excursion
+          (goto-char (point-min))
+          (when (search-forward "User message:\n" nil t)
+            (delete-region (point) (point-max))
+            (insert user-message)
+            (insert "\n\nWaiting for Ollama response...\n"))))
+      
+      ;; Send message to Ollama
+      (when user-message
+        (deferred:$
+          (deferred:next
+            (lambda ()
+              (let ((args (if system-prompt
+                              (list user-message system-prompt)
+                            (list user-message))))
+                (epc:call-deferred org-supertag-sim-epc-manager
+                                  'run_ollama
+                                  args))))
+          
+          (deferred:nextc it
+            (lambda (response)
+              (with-current-buffer "*org-supertag-ollama-chat*"
+                (save-excursion
+                  (goto-char (point-max))
+                  (let ((status (plist-get response :status))
+                        (result (plist-get response :result))
+                        (message (plist-get response :message)))
+                    (delete-region (- (point-max) 
+                                     (length "\n\nWaiting for Ollama response...\n")) 
+                                  (point-max))
+                    (if (string= status "success")
+                        (progn
+                          (insert "\n\nOllama reply:\n")
+                          (insert result)
+                          (insert "\n\n------\n\nUser message:\n"))
+                      (insert "\n\nError: " (or message "Unknown error") "\n\nUser message:\n"))))
+                (goto-char (point-max)))))
+          
+          (deferred:error it
+            (lambda (err)
+              (with-current-buffer "*org-supertag-ollama-chat*"
+                (save-excursion
+                  (goto-char (point-max))
+                  (delete-region (- (point-max) 
+                                   (length "\n\nWaiting for Ollama response...\n")) 
+                                (point-max))
+                  (insert "\n\nError: " (error-message-string err) "\n\nUser message:\n"))
+                (goto-char (point-max))))))))))
+
+(provide 'org-supertag-sim-epc)
+;;; org-supertag-sim-epc.el ends here
diff --git a/org-supertag-sim.el b/org-supertag-sim.el
new file mode 100644
index 0000000..af05cce
--- /dev/null
+++ b/org-supertag-sim.el
@@ -0,0 +1,509 @@
+;;; org-supertag-sim.el --- Semantic similarity support for org-supertag -*- lexical-binding: t; -*-
+
+;; Author: Your Name
+;; Keywords: outlines, org-mode, tags
+;; Package-Requires: ((emacs "28.1") (org-supertag "0.1"))
+
+;;; Commentary:
+;; Provides semantic 1similarity support for org-supertag.
+;; Uses sim-tag.py as the backend to provide tag similarity calculation and recommendation functionality.
+
+;;; Code:
+
+(require 'json)
+(require 'org-supertag-db)
+(require 'org-supertag-sim-epc)  
+
+(defgroup org-supertag-sim nil
+  "Semantic similarity support for org-supertag."
+  :group 'org-supertag)
+
+(defcustom org-supertag-sim-vector-file
+  (org-supertag-data-file "tag_vectors.json")
+  "Tag vector storage file path."
+  :type 'file
+  :group 'org-supertag-sim)
+
+(defcustom org-supertag-sim-sync-interval 3600
+  "Vector library synchronization interval (seconds)."
+  :type 'integer
+  :group 'org-supertag-sim)
+
+(defvar org-supertag-sim--initialized nil
+  "Flag indicating whether the system has been initialized.")
+
+(defvar org-supertag-sim--sync-timer nil
+  "Timer for periodic synchronization.")
+
+(defvar org-supertag-sim-tag-select-mode-map
+  (let ((map (make-sparse-keymap)))
+    (define-key map (kbd "n") #'next-line)
+    (define-key map (kbd "p") #'previous-line)
+    (define-key map (kbd "SPC") #'org-supertag-sim-toggle-tag-selection)
+    (define-key map (kbd "RET") #'org-supertag-sim-toggle-tag-selection)
+    (define-key map (kbd "C-c C-c") #'org-supertag-sim-apply-selected-tags)
+    (define-key map (kbd "C-c C-k") #'org-supertag-sim-cancel-tag-selection)
+    (define-key map (kbd "q") #'org-supertag-sim-cancel-tag-selection)
+    (define-key map (kbd "a") #'org-supertag-sim-select-all-tags)
+    (define-key map (kbd "A") #'org-supertag-sim-unselect-all-tags)
+    (define-key map (kbd "r") #'org-supertag-sim-regenerate-tags)
+    map)
+  "Tag select mode keybindings.")
+
+(define-derived-mode org-supertag-sim-tag-select-mode special-mode "OrgSuperTag-TagSelect"
+  "Major mode for tag selection in org-supertag."
+  :group 'org-supertag
+  (setq-local buffer-read-only t))
+
+(defun org-supertag-sim--get-all-tags ()
+  "Get the names and IDs of all tags."
+  (let ((tags '()))
+    (dolist (tag (org-supertag-db-find-by-type :tag))
+      (when-let* ((tag-props (org-supertag-db-get tag))
+                  (tag-name (plist-get tag-props :name)))
+        (push (cons tag tag-name) tags)))
+    tags))
+
+(defun org-supertag-sim--ensure-db-file ()
+  "Ensure the database file exists and return its path and tag data."
+  (unless (boundp 'org-supertag-db-file)
+    (error "Database file path is not defined"))
+  (let* ((db-file org-supertag-db-file)
+         (tags (org-supertag-db-find-by-type :tag)))
+    ;; Ensure the database file exists
+    (unless (file-exists-p db-file)
+      (org-supertag-db-save))
+    
+    (let ((tag-data
+           (delq nil
+                 (mapcar (lambda (tag)
+                          (when-let* ((props (org-supertag-db-get tag)))
+                            ;; Only keep tag ID and name
+                            (list
+                             (cons "id" tag)
+                             (cons "name" (plist-get props :name)))))
+                        tags))))
+      (list db-file tag-data))))
+
+(defun org-supertag-sim--ensure-vector-dir ()
+  "Ensure the vector file directory exists."
+  (let ((vector-dir (file-name-directory org-supertag-sim-vector-file)))
+    (unless (file-exists-p vector-dir)
+      (make-directory vector-dir t))
+    vector-dir))
+
+(defun org-supertag-sim-init ()
+  "Initialize the tag similarity engine, including EPC server and Ollama service.
+As a unified entry point, it will initialize the entire system, including the underlying EPC service and similarity engine."
+  (interactive)
+  (message "Initializing tag similarity system...")
+  (condition-case err
+      (progn
+        (unless (org-supertag-sim-epc-server-running-p)
+          (org-supertag-sim-epc-start-server)
+          (sit-for 1))
+        (when (org-supertag-sim-epc-server-running-p)
+          (org-supertag-sim-epc-init)
+
+          (unless org-supertag-sim-epc-initialized
+            (error "EPC service initialization failed"))
+          (org-supertag-db-on 'entity:created #'org-supertag-sim--on-tag-created)
+          (org-supertag-db-on 'entity:removed #'org-supertag-sim--on-tag-removed)
+          (org-supertag-sim--start-sync-timer)
+          (setq org-supertag-sim--initialized t)
+          (message "Tag similarity system initialized")))
+    (error
+     (message "Tag similarity system initialization failed: %s" (error-message-string err))
+     nil)))
+
+(defun org-supertag-sim--on-tag-created (tag &rest _)
+  "Handle tag creation events.
+TAG is the created tag information."
+  (when org-supertag-sim--initialized
+    (let ((tag-props (org-supertag-db-get tag)))
+      (when (eq (plist-get tag-props :type) :tag)
+        (org-supertag-sim--update-tag tag)))))
+
+(defun org-supertag-sim--on-tag-removed (tag-id &rest rest-args)
+  "Handle tag removal events.
+TAG-ID is the ID of the removed tag."
+  (message "org-supertag-sim--on-tag-removed received: tag-id=%s, rest=%S" tag-id rest-args)
+  (when org-supertag-sim--initialized
+    ;; Assuming the tag type is known or irrelevant for removal
+    (org-supertag-sim--remove-tag tag-id)))
+
+(defun org-supertag-sim--update-tag (tag)
+  "Update the vector of a single tag.
+TAG is the tag information."
+  (when org-supertag-sim--initialized
+    (let ((tag-props (org-supertag-db-get tag)))
+      (condition-case err
+          (epc:call-deferred org-supertag-sim-epc-manager
+                            'update_tag
+                            (list (list (cons "id" tag)
+                                      (cons "name" (plist-get tag-props :name)))))
+        (error
+         (message "Update tag vector error: %s" (error-message-string err)))))))
+
+(defun org-supertag-sim--remove-tag (tag-id)
+  "Remove a tag from the vector library.
+TAG-ID is the tag ID."
+  (when org-supertag-sim--initialized
+    (condition-case err
+        (epc:call-deferred org-supertag-sim-epc-manager
+                          'remove_tag
+                          (list tag-id))
+      (error
+       (message "Delete tag vector error: %s" (error-message-string err))))))
+
+(defun org-supertag-sim--sync-library ()
+  "Synchronize the tag vector library."
+  (when org-supertag-sim--initialized
+    (let* ((db-info (org-supertag-sim--ensure-db-file))
+           (db-file (car db-info))
+           (tag-data (cadr db-info)))
+      (condition-case err
+          (epc:call-deferred org-supertag-sim-epc-manager
+                            'sync_library
+                            (list db-file tag-data))
+        (error
+         (message "Sync tag library error: %s" (error-message-string err)))))))
+
+(defun org-supertag-sim--start-sync-timer ()
+  "Start the periodic synchronization timer."
+  (when org-supertag-sim--sync-timer
+    (cancel-timer org-supertag-sim--sync-timer))
+  (setq org-supertag-sim--sync-timer
+        (run-with-timer 
+         org-supertag-sim-sync-interval
+         org-supertag-sim-sync-interval
+         #'org-supertag-sim--sync-library)))
+
+(defun org-supertag-sim--ensure-initialized ()
+  "Ensure the system is initialized, return a deferred object.
+If the system is not initialized, it will try to initialize automatically.
+
+Returns:
+- A deferred object that will be resolved when the system is initialized
+- nil if the system is already initialized"
+  (if (and org-supertag-sim--initialized org-supertag-sim-epc-initialized)
+      ;; If both systems are initialized, return success directly
+      (deferred:next (lambda () t))
+    ;; Otherwise, try to initialize
+    (deferred:$
+      (deferred:try
+        (deferred:$
+          (deferred:next
+            (lambda ()
+              (message "System not initialized, initializing...")
+              (org-supertag-sim-init)))
+          (deferred:nextc it
+            (lambda (_)
+              ;; Check initialization status again
+              (if (and org-supertag-sim--initialized org-supertag-sim-epc-initialized)
+                  t
+                (error "System initialization failed: EPC server not ready")))))
+        :catch
+        (lambda (err)
+          (message "Initialization error: %s" (error-message-string err))
+          nil)))))
+
+(defun org-supertag-sim-find-similar (tag-name &optional top-k callback)
+  "Find tags similar to the specified tag.
+TAG-NAME is the tag name.
+TOP-K is the number of similar tags to return, default is 5.
+CALLBACK is an optional callback function that receives the result as a parameter."
+  (let ((buffer (current-buffer))
+        (limit (or top-k 5)))
+    (message "Searching for similar tags...")
+    (deferred:$
+      (deferred:try
+        (deferred:$
+          (org-supertag-sim--ensure-initialized)
+          (deferred:nextc it
+            (lambda (_)
+              (epc:call-deferred org-supertag-sim-epc-manager
+                                'find_similar
+                                (list tag-name limit))))
+          (deferred:nextc it
+            (lambda (response)
+              (let ((status (plist-get response :status))
+                    (result (plist-get response :result)))
+                (if (string= status "success")
+                    (when (buffer-live-p buffer)
+                      (with-current-buffer buffer
+                        (let ((formatted-result
+                               (mapcar (lambda (item)
+                                       (cons (car item)
+                                             (float (if (listp (cdr item))
+                                                      (car (cdr item))
+                                                    (cdr item)))))
+                                     result)))
+                          (if callback
+                              (funcall callback formatted-result)
+                            ;; Display results only when there is no callback
+                            (message "Found %d similar tags" (length formatted-result)))
+                          formatted-result)))
+                  (error "Failed to find similar tags: %s" 
+                         (or (plist-get response :message) "Unknown error")))))))
+        :catch
+        (lambda (err)
+          (message "Failed to find similar tags: %s" (error-message-string err))
+          nil)))))
+
+(defun org-supertag-sim-search-tags (query-tags &optional weights callback)
+  "Search for tag combinations.
+QUERY-TAGS is the query tag list.
+WEIGHTS is the weight list.
+CALLBACK is an optional callback function that receives the result as a parameter."
+  (org-supertag-sim--ensure-initialized)
+  (let ((buffer (current-buffer)))
+    (message "Searching for tag combinations...")
+    (condition-case err
+        (epc:call-deferred org-supertag-sim-epc-manager
+                          'search_tags
+                          (list query-tags weights))
+      (deferred:nextc it
+        (lambda (result)
+          (when (buffer-live-p buffer)
+            (with-current-buffer buffer
+              (if callback
+                  (funcall callback result)
+                (message "Found %d related tags" (length result)))))))
+      (error
+       (message "Failed to search tags: %s" (error-message-string err))
+       nil))))
+
+(defun org-supertag-sim-extract-entities (text &optional callback)
+  "Extract named entities from TEXT asynchronously.
+Optional CALLBACK will be called with the results."
+  (org-supertag-sim--ensure-initialized)
+  (let ((buffer (current-buffer)))
+    (message "Analyzing entities...")
+    (condition-case err
+        (epc:call-deferred org-supertag-sim-epc-manager
+                          'extract_entities
+                          (list text))
+      (deferred:nextc it
+        (lambda (result)
+          (when (buffer-live-p buffer)
+            (with-current-buffer buffer
+              (if callback
+                  (funcall callback result)
+                ;; 默认处理
+                (if result
+                    (progn
+                      (message "Found %d entities:" (length result))
+                      (dolist (entity result)
+                        (let ((entity-text (cdr (assoc 'entity entity)))
+                              (type (cdr (assoc 'type entity)))
+                              (start (cdr (assoc 'start entity)))
+                              (end (cdr (assoc 'end entity))))
+                          (message "  %s [%s] (%d-%d)" entity-text type start end))))
+                  (message "No entities found")))))))
+      (error
+       (message "Failed to extract entities: %s" (error-message-string err))
+       nil))))
+
+(defun org-supertag-sim-suggest-tags-from-text (text &optional callback)
+  "According to the text content, generate tag suggestions.
+Optional CALLBACK will be called with the results."
+  (let ((buffer (current-buffer)))
+    (message "Analyzing text content...")
+    
+    ;; Build JSON request data
+    (let ((request-data (list :content text)))
+      
+      ;; Record request information
+      (message "Text length: %d" (length text))
+      
+      (deferred:$
+        (deferred:try
+          (deferred:$
+            ;; Ensure the system is initialized first
+            (org-supertag-sim--ensure-initialized)
+            (deferred:nextc it
+              (lambda (_)
+                ;; Ensure the text is not empty
+                (if (string-empty-p text)
+                    (error "Text content is empty")
+                  ;; Use JSON format to send the request
+                  (epc:call-deferred org-supertag-sim-epc-manager
+                                  'suggest_tags_json
+                                  (list (json-encode request-data))))))
+            (deferred:nextc it
+              (lambda (response)
+                (when (buffer-live-p buffer)
+                  (with-current-buffer buffer
+                    (let ((status (plist-get response :status))
+                          (result (plist-get response :result))
+                          (error-msg (plist-get response :message)))
+                      (cond
+                       ;; Success case
+                       ((string= status "success")
+                        (if callback
+                            (funcall callback result)
+                          (message "Found %d related tags" (length result))))
+                       ;; Error case
+                       (t
+                        (message "Failed to generate tag suggestions: %s" 
+                                 (or error-msg "Unknown error"))
+                        (if callback (funcall callback nil)))))))))
+          :catch
+          (lambda (err)
+            (message "Failed to generate tag suggestions: %s" (error-message-string err))
+            (when callback (funcall callback nil)))))))))
+
+(defun org-supertag-sim-toggle-tag-selection ()
+  "Toggle the selection state of the current tag."
+  (interactive)
+  (let ((inhibit-read-only t))
+    (beginning-of-line)
+    (when (looking-at "^\\([[:space:]]*\\)\\(\\[[ X]\\]\\) \\(.*\\)$")
+      (let* ((tag-name (match-string-no-properties 3))
+             (current-state (match-string-no-properties 2))
+             (new-state (if (string= current-state "[X]") "[ ]" "[X]")))
+        (replace-match (concat (match-string 1) new-state " " tag-name))
+        (forward-line 1)))))
+
+(defun org-supertag-sim-apply-selected-tags ()
+  "Apply all selected tags to the current node.
+This function will:
+1. Collect selected tags from the selection buffer
+2. Move to the correct position in the source buffer
+3. Insert tags in inline format (#tag) after the properties drawer
+4. Ensure proper spacing between tags"
+  (interactive)
+  (let* ((context (buffer-local-value 'org-supertag-sim--select-context (current-buffer)))
+         (source-buffer (plist-get context :source-buffer))
+         (node-id (plist-get context :node-id))
+         (selected-tags '()))
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^\\([[:space:]]*\\)\\(\\[X\\]\\) \\(.*\\)$" nil t)
+        (push (match-string-no-properties 3) selected-tags)))
+    (if (null selected-tags)
+        (message "No tags selected")
+      ;; Apply tags
+      (message "Applying %d selected tags..." (length selected-tags))
+      (with-current-buffer source-buffer
+        (save-excursion
+          ;; 1. Move to the correct position
+          (org-back-to-heading t)
+          (org-end-of-meta-data t)
+          ;; 2. Insert tags with proper spacing
+          (dolist (tag selected-tags)
+            (insert "#" tag " "))
+          (message "Successfully applied %d tags: %s" 
+                   (length selected-tags)
+                   (mapconcat (lambda (tag)
+                              (propertize (concat "#" tag) 'face 'font-lock-keyword-face))
+                            selected-tags " ")))))
+    (quit-window t)))
+
+(defun org-supertag-sim-cancel-tag-selection ()
+  "Cancel the tag selection operation."
+  (interactive)
+  (message "Tag selection canceled")
+  (quit-window t))
+
+(defun org-supertag-sim-select-all-tags ()
+  "Select all tags."
+  (interactive)
+  (let ((inhibit-read-only t))
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^\\([[:space:]]*\\)\\(\\[[ X]\\]\\) \\(.*\\)$" nil t)
+        (replace-match (concat (match-string 1) "[X] " (match-string 3))))
+      (message "All tags selected"))))
+
+(defun org-supertag-sim-unselect-all-tags ()
+  "Unselect all tags."
+  (interactive)
+  (let ((inhibit-read-only t))
+    (save-excursion
+      (goto-char (point-min))
+      (while (re-search-forward "^\\([[:space:]]*\\)\\(\\[[ X]\\]\\) \\(.*\\)$" nil t)
+        (replace-match (concat (match-string 1) "[ ] " (match-string 3))))
+      (message "All tags unselected"))))
+
+(defun org-supertag-sim-auto-tag-node ()
+  "According to the content of the current node, automatically suggest and apply tags.
+Use semantic analysis to extract relevant tags from the node content, and display them in a dedicated buffer,
+allowing users to select and apply the tags they need by pressing the space key."
+  (interactive)
+  (require 'org-supertag-inline)
+  (let* ((node-title (org-get-heading t t t t))
+         (content (org-get-entry))
+         (node-id (org-id-get-create))
+         ;; Combine title and content for better semantic understanding
+         (full-text (concat node-title "\n\n" content))
+         (progress-reporter (make-progress-reporter "Analyzing content..." 0 100)))
+    
+    ;; Display initial progress
+    (progress-reporter-update progress-reporter 10)
+    
+    ;; Temporarily display a prompt
+    (run-with-timer 0.5 nil (lambda () 
+                             (progress-reporter-update progress-reporter 30)
+                             (message "Generating tag suggestions...")))
+    
+    ;; Tag generation is asynchronous
+    (org-supertag-sim-suggest-tags-from-text
+     full-text
+     (lambda (suggested-tags)
+       ;; Update progress
+       (progress-reporter-update progress-reporter 80)
+       (progress-reporter-done progress-reporter)
+       
+       (if suggested-tags
+           ;; Display the tag selection interface in a dedicated buffer
+           (let* ((select-buffer (get-buffer-create "*Org SuperTag Selection*"))
+                  (source-buffer (current-buffer)))
+             
+             (with-current-buffer select-buffer
+               (let ((inhibit-read-only t))
+                 (erase-buffer)
+                 (org-supertag-sim-tag-select-mode)
+                 
+                 ;; Store context information
+                 (setq-local org-supertag-sim--select-context
+                             (list :node-id node-id
+                                   :node-title node-title
+                                   :source-buffer source-buffer))
+
+                 (insert (format "Suggested %d tags for the node:\n" (length suggested-tags)))
+                 (insert "──────────────────────────────────────────────\n")
+                 (insert (format "Node: %s\n\n" node-title))
+                 (dolist (tag suggested-tags)
+                   (insert (format "[ ] %s\n" tag)))
+                 (insert "\nOperation instructions:\n")
+                 (insert "Space/Enter: Select/Unselect tag  n/p: Move up/down\n")
+                 (insert "a: Select all  A: Unselect all\n")
+                 (insert "C-c C-c: Apply selected tags  C-c C-k/q: Cancel\n")))
+             (select-window 
+              (display-buffer select-buffer
+                              '((display-buffer-below-selected)
+                                (window-height . fit-window-to-buffer)
+                                (preserve-size . (nil . t))
+                                (select . t))))
+             (with-current-buffer select-buffer
+               (goto-char (point-min))
+               (re-search-forward "^\\[ \\]" nil t)
+               (beginning-of-line)))
+         (message "No suitable tag suggestions found"))))))
+
+(defun org-supertag-sim-regenerate-tags ()
+  "Regenerate tag suggestions for the current node."
+  (interactive)
+  (let* ((context (buffer-local-value 'org-supertag-sim--select-context (current-buffer)))
+         (source-buffer (plist-get context :source-buffer))
+         (select-buffer (current-buffer)))
+    (when source-buffer
+      ;; First close the current selection buffer
+      (quit-window)
+      ;; Then regenerate tags in the source buffer
+      (with-current-buffer source-buffer
+        (org-supertag-sim-auto-tag-node)))))
+
+(provide 'org-supertag-sim)
diff --git a/org-supertag-smart-companion.el b/org-supertag-smart-companion.el
deleted file mode 100644
index cc342ea..0000000
--- a/org-supertag-smart-companion.el
+++ /dev/null
@@ -1,471 +0,0 @@
-;;; org-supertag-smart-companion.el --- Smart Knowledge Companion for org-supertag -*- lexical-binding: t; -*-
-
-;; Copyright (C) 2024 Yibie
-
-;; Author: Yibie
-;; Keywords: org-mode, tags, ai, knowledge-companion
-;; Version: 1.0.0
-
-;;; Commentary:
-
-;; This module provides an intelligent knowledge companion that analyzes
-;; tag context and provides smart suggestions to help users discover
-;; related concepts and expand their knowledge graph.
-
-;; Key features:
-;; - Seamless integration with existing tag operations
-;; - Unobtrusive, context-aware suggestions
-;; - Intelligent analysis of tag relationships
-;; - Progressive disclosure of information
-;; - Sync scope awareness (only suggests tags for files in org-supertag-sync scope)
-;; - Non-intrusive notification system (shows hints in echo area)
-
-;; Usage:
-;; - C-c t s - Show tag suggestions for current node
-;; - M-x org-supertag-smart-companion-suggest-here - Manually trigger suggestions
-;; - M-x org-supertag-smart-companion-toggle - Toggle companion functionality
-;; - M-x org-supertag-smart-companion-toggle-quiet-mode - Toggle quiet mode
-
-;; Configuration:
-;; - org-supertag-smart-companion-sync-scope-only - When t, only suggests tags for files in sync scope
-;; - org-supertag-smart-companion-auto-popup - When t, automatically shows suggestion window
-;; - org-supertag-smart-companion-quiet-mode - When t, reduces visual notifications
-
-;;; Code:
-
-(require 'org-supertag-api)
-(require 'org-supertag-bridge)
-(require 'org-supertag-node)
-(require 'org-supertag-auto-tag)
-(require 'org-supertag-sync)
-
-;;; Customization
-
-(defgroup org-supertag-smart-companion nil
-  "Smart knowledge companion for org-supertag."
-  :group 'org-supertag
-  :prefix "org-supertag-smart-companion-")
-
-(defcustom org-supertag-smart-companion-enabled t
-  "Whether to enable the smart knowledge companion functionality."
-  :type 'boolean
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-delay 2.0
-  "Delay in seconds before analyzing tag context after tag addition."
-  :type 'number
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-context-size 5
-  "Number of related concepts to analyze for context."
-  :type 'integer
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-suggestion-threshold 0.5
-  "Minimum confidence threshold for showing suggestions."
-  :type 'number
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-auto-popup nil
-  "Whether to automatically popup the suggestion window when suggestions are found."
-  :type 'boolean
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-quiet-mode nil
-  "When enabled, reduces visual and audible notifications to be less intrusive."
-  :type 'boolean
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-idle-interval 10.0
-  "Interval in seconds for idle detection. Longer intervals are less intrusive."
-  :type 'number
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-session-limit 3
-  "Maximum number of suggestions per session to avoid overwhelming the user."
-  :type 'integer
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-sync-scope-only t
-  "When non-nil, only provide tag suggestions for files in org-supertag-sync scope.
-When nil, provide suggestions for all org files regardless of sync status."
-  :type 'boolean
-  :group 'org-supertag-smart-companion)
-
-(defcustom org-supertag-smart-companion-suggest-chat-on-active-node t
-  "When non-nil, proactively suggest opening the Chat View on active nodes."
-  :type 'boolean
-  :group 'org-supertag-smart-companion)
-
-;;; Faces
-
-(defface org-supertag-smart-companion-hint-face
-  '((t :inherit shadow :weight normal))
-  "Face for smart companion hint messages."
-  :group 'org-supertag-smart-companion)
-
-(defface org-supertag-smart-companion-suggestion-face
-  '((t :inherit font-lock-comment-face :weight bold))
-  "Face for smart companion suggestions."
-  :group 'org-supertag-smart-companion)
-
-;;; Variables
-
-(defvar-local org-supertag-smart-companion--last-suggestion nil
-  "The most recent suggestion data from the smart companion.")
-
-(defvar-local org-supertag-smart-companion--analysis-timer nil
-  "Timer for delayed analysis after tag addition.")
-
-(defvar org-supertag-smart-companion--suggestion-buffer "*Org SuperTag Smart Suggestions*"
-  "Buffer name for displaying detailed suggestions.")
-
-(defvar org-supertag-smart-companion--analysis-status nil
-  "Current analysis status: nil, 'analyzing, 'completed, 'error.")
-
-(defvar org-supertag-smart-companion--status-message-timer nil
-  "Timer for status message display.")
-
-(defvar org-supertag-smart-companion--suggestion-position nil
-  "Position where suggestions were generated, for applying tags.")
-
-(defvar org-supertag-smart-companion--session-suggestion-count 0
-  "Number of suggestions made in current session to avoid overwhelming the user.")
-
-(defvar org-supertag-smart-companion--last-suggestion-time nil
-  "Time of last suggestion to avoid repeated suggestions.")
-
-(defvar org-supertag-smart-companion--suggested-nodes nil
-  "List of node IDs that have already been suggested to avoid repetition.")
-
-(defvar org-supertag-smart-companion--last-suggestion-type nil
-  "The type of the last suggestion made (:tag or :chat).")
-
-;;; Core Functions
-
-(defun org-supertag-smart-companion--analyze-tag-async (tag-name node-id)
-  "Asynchronously analyze tag context for TAG-NAME at NODE-ID."
-  (when (and org-supertag-smart-companion-enabled
-             (org-supertag-bridge-ready-p))
-    (setq org-supertag-smart-companion--analysis-status 'analyzing)
-    (org-supertag-smart-companion--show-status-briefly "🔍 Analyzing tag context...")
-    (let ((payload `(("tag_name" . ,tag-name)
-                     ("node_id" . ,node-id)
-                     ("context_size" . ,org-supertag-smart-companion-context-size))))
-      (org-supertag-bridge-call-async
-       "smart_companion/analyze_tag_context"
-       (list payload)
-       #'org-supertag-smart-companion--handle-suggestion))))
-
-(defun org-supertag-smart-companion--show-status-briefly (message &optional duration)
-  "Show a brief status MESSAGE for DURATION seconds (default 1.5)."
-  (when org-supertag-smart-companion--status-message-timer
-    (cancel-timer org-supertag-smart-companion--status-message-timer))
-  (let ((propertized-message (propertize message 'face 'org-supertag-smart-companion-hint-face)))
-    (message propertized-message)
-    (setq org-supertag-smart-companion--status-message-timer
-          (run-with-timer (or duration 5.0) nil
-                          (lambda ()
-                            (when (equal (current-message) propertized-message)
-                              (message nil)))))))
-
-(defun org-supertag-smart-companion--show-tag-suggestion-hint (count)
-  "Show a gentle hint about COUNT tag suggestions."
-  (let ((msg (format "💡 Found %d tag suggestions for this node. Press C-c t s to show." count)))
-    (org-supertag-smart-companion--show-status-briefly msg)))
-
-(defun org-supertag-smart-companion--handle-tag-suggestions (results)
-  "Handle tag suggestions from auto-tag backend and display them."
-  (message "Smart Companion: Processing tag suggestion results...")
-  (when results
-    (setq org-supertag-smart-companion--last-suggestion results)
-    (let ((count (length results)))
-      (when (> count 0)
-        (org-supertag-smart-companion--show-tag-suggestion-hint count)))))
-
-;;; Smart Context Detection
-
-(defun org-supertag-smart-companion--at-headline-p ()
-  "Check if cursor is at org headline or in subtree."
-  (and (derived-mode-p 'org-mode)
-       (save-excursion
-         (condition-case nil
-             (progn
-               (org-back-to-heading t)
-               t)
-           (error nil)))))
-
-(defun org-supertag-smart-companion--has-inline-tags-p ()
-  "Check if current headline's subtree contains inline tags."
-  (save-excursion
-    (org-back-to-heading t)
-    (let ((subtree-end (save-excursion (org-end-of-subtree t t) (point))))
-      (re-search-forward "#[a-zA-Z0-9_@%一-龥-]+" subtree-end t))))
-
-(defun org-supertag-smart-companion--has-sufficient-content-p ()
-  "Check if current node has sufficient content for tag suggestions."
-  (save-excursion
-    (org-back-to-heading t)
-    (let* ((node-data (list :file-path (buffer-file-name)
-                           :pos (point)
-                           :title (org-get-heading t t t t)))
-           (content (org-supertag-auto-tag--get-node-content node-data)))
-      (and content
-           (>= (length content) org-supertag-auto-tag-batch-min-content-length)))))
-
-(defun org-supertag-smart-companion--should-suggest-p (context)
-  "Determine if suggestions should be provided based on anti-distraction logic."
-  (let* ((current-time (current-time))
-         (node-id (plist-get context :node-id))
-         (time-since-last (if org-supertag-smart-companion--last-suggestion-time
-                             (float-time (time-subtract current-time
-                                                       org-supertag-smart-companion--last-suggestion-time))
-                           most-positive-fixnum)))
-    (and
-     (< org-supertag-smart-companion--session-suggestion-count
-        org-supertag-smart-companion-session-limit)
-     (> time-since-last 30.0)
-     (not (member node-id org-supertag-smart-companion--suggested-nodes))
-     (or (not org-supertag-smart-companion-quiet-mode)
-         (> time-since-last 120.0)))))
-
-(defun org-supertag-smart-companion--provide-chat-suggestion (context)
-  "Provide a chat suggestion for the given CONTEXT."
-  (let ((node-id (plist-get context :node-id)))
-    (setq org-supertag-smart-companion--last-suggestion-time (current-time))
-    (setq org-supertag-smart-companion--session-suggestion-count
-          (1+ org-supertag-smart-companion--session-suggestion-count))
-    (setq org-supertag-smart-companion--last-suggestion-type :chat)
-    (when node-id
-      (push node-id org-supertag-smart-companion--suggested-nodes))
-    (org-supertag-smart-companion--show-status-briefly
-     "💡 Smart Companion has ideas for this node. Press C-c t s to explore.")))
-
-(defun org-supertag-smart-companion--provide-context-suggestions (context)
-  "Provide tag suggestions based on context for untagged nodes."
-  (setq org-supertag-smart-companion--last-suggestion-type :tag)
-  (setq org-supertag-smart-companion--last-suggestion-time (current-time))
-  (setq org-supertag-smart-companion--session-suggestion-count
-        (1+ org-supertag-smart-companion--session-suggestion-count))
-  (let ((node-id (plist-get context :node-id)))
-    (when node-id
-      (push node-id org-supertag-smart-companion--suggested-nodes)))
-  (unless org-supertag-smart-companion-quiet-mode
-    (org-supertag-smart-companion--show-status-briefly "🔍 Analyzing content for tag suggestions..."))
-  (org-supertag-smart-companion--suggest-tags-for-current-headline))
-
-(defun org-supertag-smart-companion--check-and-suggest ()
-  "Check current context and provide suggestions if appropriate."
-  (when (and org-supertag-smart-companion-enabled
-             (derived-mode-p 'org-mode))
-    (let ((context (org-supertag-smart-companion--analyze-current-context)))
-      (when (org-supertag-smart-companion--should-suggest-p context)
-        (cond
-         ((plist-get context :should-suggest-chat)
-          (org-supertag-smart-companion--provide-chat-suggestion context))
-         ((plist-get context :should-suggest)
-          (org-supertag-smart-companion--provide-context-suggestions context)))))))
-
-(defun org-supertag-smart-companion--analyze-current-context ()
-  "Analyze current context to determine if suggestions should be provided."
-  (save-excursion
-    (let* ((at-headline (org-supertag-smart-companion--at-headline-p))
-           (node-id (when at-headline
-                      (org-back-to-heading t)
-                      (org-entry-get nil "ID")))
-           (existing-tags (when node-id (org-supertag-node-get-tags node-id)))
-           (has-inline-tags (when at-headline
-                              (org-supertag-smart-companion--has-inline-tags-p)))
-           (is-untagged (and at-headline
-                             (not existing-tags)
-                             (not has-inline-tags)))
-           (has-sufficient-content (when at-headline
-                                     (org-supertag-smart-companion--has-sufficient-content-p)))
-           (file-in-sync-scope (and buffer-file-name
-                                    (fboundp 'org-supertag-sync--in-sync-scope-p)
-                                    (org-supertag-sync--in-sync-scope-p buffer-file-name)))
-           (enforce-sync-scope org-supertag-smart-companion-sync-scope-only)
-           (is-active-node (and at-headline node-id existing-tags has-sufficient-content)))
-      (list :at-headline at-headline
-            :node-id node-id
-            :existing-tags existing-tags
-            :has-inline-tags has-inline-tags
-            :is-untagged is-untagged
-            :has-sufficient-content has-sufficient-content
-            :file-in-sync-scope file-in-sync-scope
-            :should-suggest (and is-untagged
-                                 has-sufficient-content
-                                 (or (not enforce-sync-scope)
-                                     file-in-sync-scope))
-            :should-suggest-chat (and is-active-node
-                                      org-supertag-smart-companion-suggest-chat-on-active-node
-                                      (or (not enforce-sync-scope)
-                                          file-in-sync-scope))))))
-
-(defun org-supertag-smart-companion--suggest-tags-for-current-headline ()
-  "Suggest tags for current headline using auto-tag infrastructure."
-  (save-excursion
-    (org-back-to-heading t)
-    (let* ((node-data (list :file-path (buffer-file-name)
-                           :pos (point)
-                           :title (org-get-heading t t t t)))
-           (content (org-supertag-auto-tag--get-node-content node-data)))
-      (when (and content
-                 (>= (length content) org-supertag-auto-tag-batch-min-content-length))
-        (let* ((temp-id (format "temp-%d-%d" (point) (random 10000)))
-               (node-dict `(("id" ,temp-id)
-                           ("content" ,content)))
-               (model-config (org-supertag-auto-tag--get-model-config))
-               (payload `(("nodes" ,(list node-dict))
-                         ("model_config" ,model-config))))
-          (setq org-supertag-smart-companion--suggestion-position
-                (list :file (buffer-file-name) :pos (point)))
-          (org-supertag-api-batch-generate-tags
-           (list payload)
-           #'org-supertag-smart-companion--handle-tag-suggestions))))))
-
-;;; Legacy Tag Addition Hook (保留兼容性)
-
-(defun org-supertag-smart-companion--on-tag-added (tag-name)
-  "Handle tag addition event for TAG-NAME with intelligent analysis."
-  (when org-supertag-smart-companion-enabled
-    (run-with-timer 1.0 nil
-                    (lambda ()
-                      (org-supertag-smart-companion--check-and-suggest)))))
-
-;;; Interactive Commands
-
-;;;###autoload
-(defun org-supertag-smart-companion-show-suggestions ()
-  "Show the last suggestion, whether it was for tags or chat."
-  (interactive)
-  (cond
-   ((eq org-supertag-smart-companion--last-suggestion-type :chat)
-    (require 'org-supertag-view-chat)
-    (org-supertag-view-chat-open))
-   ((eq org-supertag-smart-companion--last-suggestion-type :tag)
-    (if org-supertag-smart-companion--last-suggestion
-        (org-supertag-auto-tag-show-suggestion-ui org-supertag-smart-companion--last-suggestion)
-      (message "No tag suggestions available. Run M-x org-supertag-smart-companion-suggest-here to get new suggestions.")))
-   (t (message "No suggestions available at the moment."))))
-
-;;;###autoload
-(defun org-supertag-smart-companion-suggest-here ()
-  "Analyze current location and provide intelligent tag suggestions for untagged nodes."
-  (interactive)
-  (if (not (derived-mode-p 'org-mode))
-      (message "Smart Companion: Can only be used in org-mode")
-    (let ((context (org-supertag-smart-companion--analyze-current-context)))
-      (message "Smart Companion DEBUG: at-headline=%s, node-id=%s, is-untagged=%s, has-sufficient-content=%s, file-in-sync-scope=%s"
-               (plist-get context :at-headline)
-               (plist-get context :node-id)
-               (plist-get context :is-untagged)
-               (plist-get context :has-sufficient-content)
-               (plist-get context :file-in-sync-scope))
-      (cond
-       ((not (plist-get context :at-headline))
-        (message "Smart Companion: Please move to an org headline or its subtree"))
-       ((not (plist-get context :is-untagged))
-        (message "Smart Companion: This node already has tags. Smart Companion suggests tags for untagged nodes."))
-       ((not (plist-get context :has-sufficient-content))
-        (message "Smart Companion: Node content is too short for meaningful tag suggestions"))
-       ((and org-supertag-smart-companion-sync-scope-only
-             (not (plist-get context :file-in-sync-scope)))
-        (message "Smart Companion: Current file is not in org-supertag-sync scope (set org-supertag-smart-companion-sync-scope-only to nil to override)"))
-       (t
-        (message "Smart Companion: Analyzing node content for tag suggestions...")
-        (org-supertag-smart-companion--provide-context-suggestions context))))))
-
-;;;###autoload
-(defun org-supertag-smart-companion-toggle ()
-  "Toggle the smart companion functionality."
-  (interactive)
-  (setq org-supertag-smart-companion-enabled
-        (not org-supertag-smart-companion-enabled))
-  (message "Smart Companion %s"
-           (if org-supertag-smart-companion-enabled "enabled" "disabled")))
-
-;;;###autoload
-(defun org-supertag-smart-companion-reset-session ()
-  "Reset Smart Companion session counters and suggested node tracking."
-  (interactive)
-  (setq org-supertag-smart-companion--session-suggestion-count 0
-        org-supertag-smart-companion--last-suggestion-time nil
-        org-supertag-smart-companion--suggested-nodes nil)
-  (message "Smart Companion session reset. All nodes can receive suggestions again."))
-
-;;;###autoload
-(defun org-supertag-smart-companion-toggle-quiet-mode ()
-  "Toggle quiet mode for Smart Companion to reduce distractions."
-  (interactive)
-  (setq org-supertag-smart-companion-quiet-mode
-        (not org-supertag-smart-companion-quiet-mode))
-  (message "Smart Companion quiet mode %s"
-           (if org-supertag-smart-companion-quiet-mode "enabled" "disabled")))
-
-;;; Integration with org-supertag-tag
-
-(defun org-supertag-smart-companion--advice-tag-add (orig-fun &rest args)
-  "Advice function to hook into tag addition for smart companion analysis."
-  (let ((result (apply orig-fun args)))
-    (when (and result org-supertag-smart-companion-enabled)
-      (run-with-timer 0.1 nil
-                      (lambda ()
-                        (org-supertag-smart-companion--analyze-recent-tag-addition))))
-    result))
-
-(defun org-supertag-smart-companion--analyze-recent-tag-addition ()
-  "Analyze the most recently added tag for smart companion suggestions."
-  (when (derived-mode-p 'org-mode)
-    (let* ((node-id (org-id-get))
-           (tags (when node-id (org-supertag-node-get-tags node-id))))
-      (when (and tags (> (length tags) 0))
-        (let ((trigger-tag (car tags)))
-          (org-supertag-smart-companion--on-tag-added trigger-tag))))))
-
-;;; Key Bindings
-
-(defun org-supertag-smart-companion-setup-keybindings (&optional prefix)
-  "Optionally set up key bindings for smart companion."
-  (interactive)
-  (define-key org-mode-map (kbd "C-c t s") 'org-supertag-smart-companion-show-suggestions))
-
-;;; Setup and Activation
-
-;;;###autoload
-(defun org-supertag-smart-companion-setup ()
-  "Set up the smart companion functionality."
-  (interactive)
-  (advice-add 'org-supertag-inline-add :around
-              #'org-supertag-smart-companion--advice-tag-add)
-  (when org-supertag-smart-companion-enabled
-    (org-supertag-smart-companion--setup-idle-detection))
-  (org-supertag-smart-companion-setup-keybindings)
-  (message "Smart Companion functionality has been set up. Use C-c t s to show suggestions."))
-
-(defun org-supertag-smart-companion--setup-idle-detection ()
-  "Set up idle detection to provide suggestions at appropriate times."
-  (run-with-idle-timer org-supertag-smart-companion-idle-interval t
-                       (lambda ()
-                         (when (and org-supertag-smart-companion-enabled
-                                    (derived-mode-p 'org-mode))
-                           (org-supertag-smart-companion--check-and-suggest)))))
-
-;;;###autoload
-(defun org-supertag-smart-companion-cleanup ()
-  "Clean up the smart companion functionality."
-  (interactive)
-  (advice-remove 'org-supertag-inline-add
-                 #'org-supertag-smart-companion--advice-tag-add)
-  (when org-supertag-smart-companion--analysis-timer
-    (cancel-timer org-supertag-smart-companion--analysis-timer)
-    (setq org-supertag-smart-companion--analysis-timer nil))
-  (when (get-buffer org-supertag-smart-companion--suggestion-buffer)
-    (kill-buffer org-supertag-smart-companion--suggestion-buffer))
-  (message "Smart Companion functionality has been cleaned up."))
-
-
-
-(provide 'org-supertag-smart-companion)
-
-;;; org-supertag-smart-companion.el ends here
\ No newline at end of file
diff --git a/org-supertag-sync.el b/org-supertag-sync.el
index 3b891f7..bc75567 100755
--- a/org-supertag-sync.el
+++ b/org-supertag-sync.el
@@ -33,7 +33,10 @@
 (require 'org)
 (require 'org-element)
 (require 'org-supertag-db)
+<<<<<<< HEAD
+=======
 (require 'cl-lib)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;;; Customization
 
@@ -77,7 +80,7 @@ Enabling this increases accuracy but reduces performance."
   :group 'org-supertag-sync)
 
 (defcustom org-supertag-sync-hash-props
-  '(:raw-value :tags :todo-type :priority)
+  '(:raw-value :tags :todo-type :priority :properties)
   "Properties to include when calculating node hash values."
   :type '(repeat symbol)
   :group 'org-supertag-sync)
@@ -107,51 +110,15 @@ Value: last sync time")
 (defvar org-supertag-sync--timer nil
   "Timer for periodic sync checks.")
 
+<<<<<<< HEAD
+=======
 ;; Add a variable to track initialization status
 (defvar org-supertag-sync--initialized nil
   "Flag indicating whether org-supertag-sync has been initialized.")
 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;; Core Functions - File State Tracking
 
-(defun org-supertag-sync--in-sync-scope-p (file)
-  "Check if FILE is within synchronization scope.
-Returns t if file should be synchronized based on configured directories.
-If no directories are configured, returns t for all org files."
-  (when (and file (file-exists-p file))
-    (let* ((expanded-file (expand-file-name file))
-           (file-dir (file-name-directory expanded-file))
-           (excluded (and org-supertag-sync-exclude-directories
-                         (cl-some (lambda (dir)
-                                   (let ((expanded-exclude-dir (expand-file-name dir)))
-                                     (string-prefix-p expanded-exclude-dir file-dir)))
-                                 org-supertag-sync-exclude-directories)))
-           (included (if org-supertag-sync-directories
-                        (cl-some (lambda (dir)
-                                  (let ((expanded-dir (expand-file-name dir)))
-                                    (string-prefix-p expanded-dir file-dir)))
-                                org-supertag-sync-directories)
-                      t)))
-      (and included
-           (not excluded)
-           (string-match-p org-supertag-sync-file-pattern file)))))
-
-(defun org-supertag-scan-sync-directories (&optional all-files-p)
-  "Scan sync directories for org files.
-If ALL-FILES-P is non-nil, return all files in scope.
-Otherwise, returns a list of new files that are not yet in sync state."
-  (let ((files nil))
-    (dolist (dir org-supertag-sync-directories)
-      (when (file-exists-p dir)
-        (let ((dir-files (directory-files-recursively
-                         dir org-supertag-sync-file-pattern t)))
-          (dolist (file dir-files)
-            (when (and (file-regular-p file)
-                       (org-supertag-sync--in-sync-scope-p file)
-                       (or all-files-p
-                           (not (gethash file org-supertag-sync--state))))
-              (push file files))))))
-    files))
-
 (defun org-supertag-sync-update-state (file)
   "Update sync state for FILE."
   (when (file-exists-p file)
@@ -169,24 +136,20 @@ Returns t if file has been modified since last sync."
                      (file-attributes file))))
     (time-less-p last-sync mtime)))
 
-(defun org-supertag-get-modified-files ()
-  "Get list of files that need synchronization.
-Returns files that have been modified since last sync."
-  (let ((files nil))
-    (maphash
-     (lambda (file state)
-       (when (and (file-exists-p file)
-                  (org-supertag-sync--in-sync-scope-p file)
-                  (org-supertag-sync-check-state file))
-         (push file files)))
-     org-supertag-sync--state)
-    files))
-
 ;;; Core Functions - Node Hash Support
 
 (defun org-supertag-node-hash (node)
   "Calculate hash value for NODE.
 Only includes stable properties, excludes position information."
+<<<<<<< HEAD
+  (secure-hash 'sha1
+               (format "%s%s%s%s%s"
+                      (plist-get node :raw-value)    ; title
+                      (plist-get node :tags)         ; tags
+                      (plist-get node :todo-type)    ; TODO state
+                      (plist-get node :priority)     ; priority
+                      (plist-get node :properties)))) ; property drawer
+=======
   (let* ((raw-value (or (plist-get node :raw-value) ""))
          (tags (let ((tag-list (plist-get node :tags)))
                  (if (listp tag-list)
@@ -194,30 +157,19 @@ Only includes stable properties, excludes position information."
                    (or tag-list ""))))
          (todo-type (or (plist-get node :todo-type) ""))
          (priority (or (plist-get node :priority) ""))
-         (properties (let ((props-plist (plist-get node :properties)))
-                       (if (plistp props-plist)
-                           (let (props-alist)
-                             ;; 1. Convert plist to alist for safe sorting.
-                             (let ((temp-plist props-plist))
-                               (while temp-plist
-                                 (let ((key (pop temp-plist))
-                                       (val (pop temp-plist)))
-                                   (when key ; Handle odd-length or malformed plists
-                                     (push (cons key val) props-alist)))))
-                             
-                             ;; 2. Sort the alist by key (keys are keywords).
-                             (setq props-alist (sort props-alist (lambda (a b) (string< (symbol-name (car a)) (symbol-name (car b))))))
-                             
-                             ;; 3. Create the flattened string representation.
-                             (mapconcat (lambda (pair)
-                                          ;; Format value as empty string if it's nil.
-                                          (format "%s=%s" (car pair) (or (cdr pair) "")))
-                                        props-alist
-                                        "|"))
-                         ""))))
+         (properties (let ((props (plist-get node :properties)))
+                      (if (listp props)
+                          ;; Sort properties by key for consistent hashing
+                          (mapconcat (lambda (prop)
+                                      (format "%s=%s" (car prop) (cdr prop)))
+                                    (sort (copy-sequence props)
+                                          (lambda (a b) (string< (car a) (car b))))
+                                    "|")
+                        ""))))
     (secure-hash 'sha1
                  (format "%s|%s|%s|%s|%s"
                         raw-value tags todo-type priority properties))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-node-content-hash (node)
   "Calculate hash value for NODE's content.
@@ -252,17 +204,6 @@ If OLD-NODE doesn't have a hash value, calculate it on the fly."
             (and old-content-hash new-content-hash
                  (not (string= old-content-hash new-content-hash))))))))
 
-(defun org-supertag-sync--normalize-properties (props)
-  "Normalize property list PROPS for consistent comparison.
-Converts all keys and values to strings."
-  (when props
-    (cl-loop for (k v) on props by #'cddr
-            when (and k v)
-            collect (cons (if (keywordp k)
-                              (substring (symbol-name k) 1)
-                            (format "%s" k))
-                          (format "%s" v)))))
-
 ;;-------------------------------------------------------------------
 ;; Core Functions - Node Scanning and Update
 ;;-------------------------------------------------------------------
@@ -271,6 +212,25 @@ Converts all keys and values to strings."
   "Scan current buffer and collect node information.
 Only process headings with IDs or that meet creation criteria.
 Returns a hash table mapping node IDs to their properties."
+<<<<<<< HEAD
+  (let ((nodes (make-hash-table :test 'equal)))
+    (when (org-supertag-sync--in-sync-scope-p buffer-file-name)
+      (org-with-wide-buffer
+       (goto-char (point-min))
+       (while (re-search-forward org-heading-regexp nil t)
+         (when (and (org-at-heading-p)
+                    (not (string-prefix-p "TAGS" (org-get-heading t t t t)))
+                    (not (org-in-commented-heading-p))
+                    (>= (org-current-level) org-supertag-sync-node-creation-level))
+           ;; 只在没有 ID 时创建节点
+           (unless (org-id-get)
+             (condition-case nil
+                 (org-supertag-node-create)
+               (error nil)))
+           (when-let* ((id (org-id-get))
+                      (props (org-supertag-extract-node-props)))
+             (puthash id props nodes))))))
+=======
   (let ((nodes (make-hash-table :test 'equal))
         (org-element-use-cache nil)  ; Disable element cache
         (org-startup-folded nil)     ; Ensure all content is visible
@@ -294,100 +254,240 @@ Returns a hash table mapping node IDs to their properties."
                     (condition-case err
                         (org-supertag-node-create)
                       (error 
-                       (message "Failed to create node ID at line %d: %s"
+                       (message "Failed to create node ID at line %d: %s" 
                                (line-number-at-pos) 
                                (error-message-string err)))))
                   (when-let* ((id (org-id-get))  ; Get ID again after potential creation
                              (props (condition-case err
                                        (save-excursion
                                          (org-back-to-heading t)
-                                         (org-supertag-db-parse-node-properties))
+                                         (let ((element (org-element-at-point)))
+                                           (org-supertag-extract-node-props)))
                                      (error
                                       (message "Error extracting properties at point %d: %s"
                                                (point)
                                                (error-message-string err))
                                       nil))))
                     (puthash id props nodes)))))))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
     nodes))
 
+(defun org-supertag-extract-node-props ()
+  "Extract properties of node at point.
+Returns a plist of node properties or nil if not at a valid node."
+  (when (org-at-heading-p)
+<<<<<<< HEAD
+    (let* ((element (org-element-at-point))
+           (raw-value (org-element-property :raw-value element))
+           (props (list :type :node
+                       :id (org-id-get)
+                       :title raw-value
+                       :raw-value raw-value
+                       :tags (org-element-property :tags element)
+                       :todo-type (org-element-property :todo-type element)
+                       :priority (org-element-property :priority element)
+                       :properties (org-entry-properties nil 'all)
+                       :file-path (buffer-file-name)
+                       :level (org-element-property :level element)
+                       :pos (org-element-property :begin element)
+                       :begin (org-element-property :begin element)
+                       :contents-begin (org-element-property :contents-begin element)
+                       :contents-end (org-element-property :contents-end element)
+                       :olp (org-get-outline-path t))))
+      props)))
+=======
+    (condition-case err
+        (let* ((element
+                ;; Ensure a stable parsing environment to prevent hash mismatches
+                ;; by temporarily overriding potentially interfering variables.
+                (let ((org-element-use-cache nil)
+                      (org-M-RET-may-split-line nil)
+                      (org-startup-folded nil)
+                      (org-hide-emphasis-markers nil))
+                  (save-excursion
+                    (org-back-to-heading t)
+                    (org-element-at-point))))
+               (raw-value (org-element-property :raw-value element)))
+          (unless raw-value
+            (error "Failed to extract raw value from heading"))
+          (let ((props (list :type :node
+                            :id (org-id-get)
+                            :title raw-value
+                            :raw-value raw-value
+                            :tags (org-element-property :tags element)
+                            :todo-type (org-element-property :todo-type element)
+                            :priority (let ((prio (org-element-property :priority element)))
+                                        (cond
+                                         ((numberp prio) (char-to-string prio))
+                                         ((stringp prio) prio)
+                                         ((null prio) nil)
+                                         (t (format "%S" prio))))
+                            :properties (condition-case nil
+                                          (org-entry-properties nil 'all)
+                                        (error nil))
+                            :file-path (buffer-file-name)
+                            :level (or (org-element-property :level element) 1)
+                            :pos (or (org-element-property :begin element) (point))
+                            :begin (or (org-element-property :begin element) (point))
+                            :contents-begin (org-element-property :contents-begin element)
+                            :contents-end (org-element-property :contents-end element)
+                            :olp (condition-case nil
+                                    (org-get-outline-path t)
+                                  (error nil)))))
+            ;; Validate required properties
+            (cl-loop for prop in '(:id :title :file-path :type)
+                     for value = (plist-get props prop)
+                     unless value do
+                     (error "Missing required property %s" prop))
+            props))
+      (error
+       (message "Error extracting node properties: %s" (error-message-string err))
+       nil))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
 (defun org-supertag-db-update-buffer ()
   "Update database with all nodes in current buffer.
-Uses a three-pass approach:
-1. Scan buffer to collect current nodes.
-2. Process updates, creations, and moves, and reconcile references.
-3. Process deletions."
+Uses a two-pass approach:
+1. Scan buffer to collect current nodes
+2. Process updates, moves, and deletions"
   (save-excursion
     (let* ((file (buffer-file-name))
+<<<<<<< HEAD
+           (current-nodes (org-supertag-scan-buffer-nodes))
+=======
            (current-nodes (make-hash-table :test 'equal))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
            (updated 0)
            (deleted 0)
            (moved 0))
       
+<<<<<<< HEAD
+      ;; First pass: check for updates and moves
+=======
       ;; First pass: scan buffer and collect nodes
       (save-restriction
         (widen)
         (goto-char (point-min))
-        (let ((org-element-use-cache nil)
-              (org-startup-folded nil)
-              (org-startup-with-inline-images nil)
-              (org-startup-with-latex-preview nil)
-              (org-hide-emphasis-markers nil))
+        (let ((org-element-use-cache nil)  ; Disable element cache
+              (org-startup-folded nil)      ; Ensure all content is visible
+              (org-startup-with-inline-images nil)  ; Disable image loading
+              (org-startup-with-latex-preview nil)  ; Disable latex preview
+              (org-hide-emphasis-markers nil))      ; Show all markers
           (condition-case err
               (org-map-entries
                (lambda ()
                  (let ((id (org-id-get)))
-                   (when (and id (>= (org-current-level) org-supertag-sync-node-creation-level))
+                   (when (and id
+                            (>= (org-current-level) org-supertag-sync-node-creation-level))
                      (condition-case err2
-                         (when-let* ((props (org-supertag-db-parse-node-properties)))
+                         (when-let* ((props (org-supertag-extract-node-props)))
                            (puthash id props current-nodes))
-                       (error (message "Error extracting properties for node %s: %s" id (error-message-string err2)))))))
+                       (error
+                        (message "Error extracting properties for node %s: %s"
+                                id (error-message-string err2)))))))
                t nil)
-            (error (message "Error scanning buffer: %s" (error-message-string err))))))
+            (error
+             (message "Error scanning buffer: %s" (error-message-string err))))))
       
-      ;; Second pass: process updates, creations, and moves
+      ;; Second pass: process updates and moves
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
       (maphash
        (lambda (id props)
          (let* ((old-node (org-supertag-db-get id))
-                (is-new (null old-node))
-                (is-moved (and old-node (not (string= (plist-get old-node :file-path) file))))
-                (is-updated (and old-node (not is-moved) (org-supertag-node-changed-p old-node props))))
-           (when (or is-new is-moved is-updated)
-             (condition-case err
-                 (let ((old-refs (if is-new nil (plist-get old-node :ref-to)))
-                       (new-refs (plist-get props :ref-to)))
-                   ;; First, save the node itself with all its new data.
-                   (org-supertag-db-add-with-hash id props)
-                   ;; Second, update back-references on other nodes.
-                   (when (fboundp 'org-supertag-db-reconcile-references)
-                     (org-supertag-db-reconcile-references id new-refs old-refs))
-                   ;; Increment counters
-                   (cond (is-moved (cl-incf moved))
-                         (t (cl-incf updated))))
-               (error (message "Error processing node %s: %s" id (error-message-string err)))))))
+                (old-file (and old-node (plist-get old-node :file-path))))
+<<<<<<< HEAD
+           (cond
+            ;; Node moved from another file
+            ((and old-node
+                  (not (string= old-file file)))
+             (let ((preserved-props '(:ref-to :ref-from :ref-count)))
+               (dolist (prop preserved-props)
+                 (when-let* ((value (plist-get old-node prop)))
+                   (setq props (plist-put props prop value)))))
+             (org-supertag-db-add-with-hash id props)
+             (cl-incf moved))
+            ;; Node updated in same file
+            ((and old-node
+                  (org-supertag-node-changed-p old-node props))
+             (let ((preserved-props '(:ref-to :ref-from :ref-count)))
+               (dolist (prop preserved-props)
+                 (when-let* ((value (plist-get old-node prop)))
+                   (setq props (plist-put props prop value)))))
+             (org-supertag-db-add-with-hash id props)
+             (cl-incf updated))
+            ;; New node
+            ((null old-node)
+             (org-supertag-db-add-with-hash id props)
+             (cl-incf updated)))))
+       current-nodes)
+      
+      ;; Second pass: check for deletions
+=======
+           (condition-case err
+               (cond
+                ;; Node moved from another file
+                ((and old-node
+                      (not (string= old-file file)))
+                 (let ((preserved-props '(:ref-to :ref-from :ref-count)))
+                   (dolist (prop preserved-props)
+                     (when-let* ((value (plist-get old-node prop)))
+                       (setq props (plist-put props prop value)))))
+                 (org-supertag-db-add-with-hash id props)
+                 (cl-incf moved))
+                ;; Node updated in same file
+                ((and old-node
+                      (org-supertag-node-changed-p old-node props))
+                 (let ((preserved-props '(:ref-to :ref-from :ref-count)))
+                   (dolist (prop preserved-props)
+                     (when-let* ((value (plist-get old-node prop)))
+                       (setq props (plist-put props prop value)))))
+                 (org-supertag-db-add-with-hash id props)
+                 (cl-incf updated))
+                ;; New node
+                ((null old-node)
+                 (org-supertag-db-add-with-hash id props)
+                 (cl-incf updated)))
+             (error
+              (message "Error processing node %s: %s"
+                       id (error-message-string err))))))
        current-nodes)
       
       ;; Third pass: check for deletions
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
       (maphash
        (lambda (id node)
          (when (and (string= (plist-get node :file-path) file)
                    (null (gethash id current-nodes)))
+<<<<<<< HEAD
+           (org-supertag-db-remove-object id)
+           (cl-incf deleted)))
+       org-supertag-db--object)
+      
+      ;; Report changes
+      (when (or (> updated 0) (> deleted 0) (> moved 0))
+        (message "Buffer sync: %d updated, %d deleted, %d moved"
+                 updated deleted moved)))))
+=======
            (condition-case err
                (progn
                  (org-supertag-db-remove-object id)
                  (cl-incf deleted))
-             (error (message "Error removing node %s: %s" id (error-message-string err))))))
+             (error
+              (message "Error removing node %s: %s"
+                       id (error-message-string err))))))
        org-supertag-db--object)
       
       ;; Report changes
       ;; (when (or (> updated 0) (> deleted 0) (> moved 0))
       ;;   (message "Buffer sync: %d updated, %d deleted, %d moved"
-      ;;            updated deleted moved))
-      )))
+      ;;            updated deleted moved)
+                 )))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag--sync-at-point ()
   "Synchronize node at point with database."
   (when-let* ((id (org-id-get))
-              (props (org-supertag-db-parse-node-properties)))
+              (props (org-supertag-extract-node-props)))
     (let ((old-node (org-supertag-db-get id)))
       (when (or (null old-node)
                 (org-supertag-node-changed-p old-node props))
@@ -472,6 +572,15 @@ Ensures node has ID and is properly registered in database."
 ;; State Management
 ;;-------------------------------------------------------------------
 
+<<<<<<< HEAD
+(defun org-supertag-sync-save-state ()
+  "Save sync state to file."
+  ;; Clean up non-existent files before saving
+  (maphash (lambda (file _state)
+             (unless (file-exists-p file)
+               (remhash file org-supertag-sync--state)))
+           org-supertag-sync--state)
+=======
 (defun org-supertag-sync--cleanup-database ()
   "Clean up database by removing nodes from files not in sync state."
   (let ((nodes-to-remove nil)
@@ -543,6 +652,7 @@ Ensures node has ID and is properly registered in database."
     
     (message "Debug - After cleanup, sync state has %d files" 
              (hash-table-count org-supertag-sync--state)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   
   (with-temp-file org-supertag-sync-state-file
     (let ((print-length nil)
@@ -563,6 +673,21 @@ If file doesn't exist, initialize empty state."
     (org-supertag-sync-save-state)))
 
 ;; Auto initialization
+<<<<<<< HEAD
+(defun org-supertag-sync--maybe-auto-init ()
+  "Maybe initialize sync system automatically.
+Only initialize if auto-sync is enabled and not already initialized."
+  (when (and org-supertag-sync-directories  ; Only if directories are configured
+             (not org-supertag-sync--timer)) ; Not already running
+    (org-supertag-sync-init)))
+
+;; Add to after-init-hook to ensure all variables are loaded
+(add-hook 'after-init-hook #'org-supertag-sync--maybe-auto-init)
+
+;; Initialize when package is loaded
+(eval-after-load 'org-supertag
+  '(org-supertag-sync--maybe-auto-init))
+=======
 (defun org-supertag-sync--ensure-directories ()
   "Ensure sync directories are properly configured."
   (unless org-supertag-sync-directories
@@ -592,146 +717,111 @@ If file doesn't exist, initialize empty state."
   
   org-supertag-sync-directories)
 
-;;-------------------------------------------------------------------
-;; Zombie Node Validation and Cleanup
-;;-------------------------------------------------------------------
-
-(defun org-supertag-sync-validate-and-cleanup-zombie-nodes ()
-  "Validate and clean up all zombie nodes (manually executed).
-Check all nodes in the database, verify if the node ID exists in the corresponding file.
-If the node ID does not exist in the corresponding file, automatically delete the zombie node."
-  (interactive)
-  (let ((all-files (make-hash-table :test 'equal))
-        (zombie-nodes '())
-        (cleaned-count 0)
-        (total-nodes 0)
-        (checked-files 0))
-    
-    ;; First collect all files and their nodes
-    (maphash (lambda (id node)
-               (when (eq (plist-get node :type) :node)
-                 (cl-incf total-nodes)
-                 (let ((file-path (plist-get node :file-path)))
-                   (when file-path
-                     (unless (gethash file-path all-files)
-                       (puthash file-path '() all-files))
-                     (puthash file-path 
-                             (cons (cons id node) (gethash file-path all-files))
-                             all-files)))))
-             org-supertag-db--object)
-    
-    (message "Starting zombie node validation for %d nodes in %d files..." 
-             total-nodes (hash-table-count all-files))
-    
-    ;; Check nodes in files 
-    (maphash (lambda (file-path nodes)
-               (cl-incf checked-files)
-               (message "Checking file (%d/%d): %s" 
-                       checked-files (hash-table-count all-files) file-path)
-               
-               (if (file-exists-p file-path)
-                   ;; File exists, check if the node is in the file
-                   (dolist (node-entry nodes)
-                     (let ((node-id (car node-entry)))
-                       (unless (org-supertag-sync--check-node-exists-in-file node-id file-path)
-                         (message "Found zombie node: %s in file %s" node-id file-path)
-                         (push (cons node-id file-path) zombie-nodes))))
-                 ;; File does not exist, all nodes are zombies
-                 (message "File not found: %s, marking all %d nodes as zombies" 
-                         file-path (length nodes))
-                 (dolist (node-entry nodes)
-                   (let ((node-id (car node-entry)))
-                     (push (cons node-id file-path) zombie-nodes)))))
-             all-files)
-    
-    ;; Directly delete zombie nodes
-    (when zombie-nodes
-      (message "Cleaning up %d zombie nodes..." (length zombie-nodes))
-      (dolist (zombie zombie-nodes)
-        (let ((node-id (car zombie))
-              (file-path (cdr zombie)))
-          (message "Removing zombie node %s from %s" node-id file-path)
-          (org-supertag-db-remove-object node-id)
-          (cl-incf cleaned-count))))
-    
-    (message "Zombie node cleanup completed: checked %d files, %d total nodes, cleaned %d zombie nodes" 
-             checked-files total-nodes cleaned-count)
-    cleaned-count))
+(defun org-supertag-sync--maybe-auto-init ()
+  "Maybe initialize sync system automatically.
+Only initialize if auto-sync is enabled and not already initialized."
+  (when (and (not org-supertag-sync--initialized)  ; Not already initialized
+             (org-supertag-sync--ensure-directories)  ; Ensure directories are loaded
+             (not org-supertag-sync--timer)) ; Not already running
+    (org-supertag-sync-init)))
 
-(defun org-supertag-sync--check-node-exists-in-file (node-id file-path)
-  "Check if the node ID exists in the specified file.
-NODE-ID: The ID of the node to check
-FILE-PATH: The path to the file
-Returns t if the node exists, nil if it does not exist."
-  (and node-id
-       file-path
-       (file-exists-p file-path)
-         (with-current-buffer (find-file-noselect file-path)
-           (save-excursion
-             (save-restriction
-               (widen)
-               (goto-char (point-min))
-               (or (re-search-forward
-                    (format "^[ \t]*:ID:[ \t]+%s[ \t]*$" (regexp-quote node-id))
-                    nil t)
-                   (progn
-                     (goto-char (point-min))
-                     (re-search-forward
-                      (format "^[ \t]*#\\+ID:[ \t]+%s[ \t]*$" (regexp-quote node-id))
-                      nil t))))))))
+(cl-defun org-supertag-sync-init ()
+  "Initialize sync system."
+  (when org-supertag-sync--initialized
+    (cl-return-from org-supertag-sync-init))
+  
+  ;; Ensure directories are loaded
+  (org-supertag-sync--ensure-directories)
+  
+  ;; Ensure data directory exists
+  (unless (file-exists-p org-supertag-data-directory)
+    (make-directory org-supertag-data-directory t))
+  
+  ;; Load sync state
+  (org-supertag-sync-load-state)
+  
+  ;; Clean up database if loaded and has content
+  (when (and (featurep 'org-supertag-db) 
+             (hash-table-p org-supertag-db--object)
+             (> (hash-table-count org-supertag-db--object) 0))
+    (org-supertag-sync--cleanup-database))
+  
+  ;; Setup buffer hooks
+  (org-supertag-sync-setup-buffer-watch)
+  
+  ;; Initial state for all files
+  (let ((all-files (org-supertag-get-all-files))
+        (new-files (org-supertag-scan-sync-directories)))
+    ;; Check for parsing errors in all files
+    (dolist (file (append all-files new-files))
+      (when (and (file-exists-p file)
+                 (> (org-supertag-sync--diagnose-parse-error file) 0))
+        (message "Warning: Found parsing issues in %s" file)))
+    ;; Update state for valid files
+    (dolist (file (append all-files new-files))
+      (when (file-exists-p file)
+        (org-supertag-sync-update-state file))))
+  
+  ;; Save initial state
+  (org-supertag-sync-save-state)
+  
+  ;; Start auto-sync
+  (org-supertag-sync-start-auto-sync)
+  
+  ;; Mark as initialized
+  (setq org-supertag-sync--initialized t))
 
-(defun org-supertag-sync-cleanup-zombie-nodes-for-file (file-path)
-  "Clean up zombie nodes for a specific file.
-FILE-PATH: Path to the file to check
-Returns number of cleaned nodes."
-  (let ((zombie-nodes '())
-        (count 0))
-    
-    (when (file-exists-p file-path)
-      (message "Checking zombie nodes for file: %s" file-path)
-      
-      ;; Get all nodes in the database for the file
-      (maphash (lambda (id node)
-                 (when (and (eq (plist-get node :type) :node)
-                           (string= (plist-get node :file-path) file-path))
-                   ;; Check if the node ID exists in the file
-                   (unless (org-supertag-sync--check-node-exists-in-file id file-path)
-                     (push id zombie-nodes))))
-               org-supertag-db--object)
-      
-      ;; Delete zombie nodes
-      (dolist (node-id zombie-nodes)
-        (message "Removing zombie node: %s" node-id)
-        (org-supertag-db-remove-object node-id)
-        (cl-incf count))
-      
-      (when (> count 0)
-        (message "Cleaned up %d zombie nodes from file: %s" count file-path))))
-    
-    count)
+;; Initialize when appropriate
+(add-hook 'after-init-hook #'org-supertag-sync--maybe-auto-init)
+(with-eval-after-load 'org-supertag
+  (org-supertag-sync--maybe-auto-init))
+(when (featurep 'org-supertag)
+  (org-supertag-sync--maybe-auto-init))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 ;;-------------------------------------------------------------------
-;; Core Functions - Zombie Node Auto Cleanup
+;; Automatic Synchronization
 ;;-------------------------------------------------------------------
-(defun org-supertag-sync--auto-cleanup-zombie-nodes (modified-files)
-  "Automatically clean up zombie nodes inside MODIFIED-FILES.
-Returns the number of cleaned nodes."
-  (let ((cleaned-count 0))
-    (dolist (file modified-files)
-      (when (file-exists-p file)
-        (maphash (lambda (id node)
-                   (when (and (eq (plist-get node :type) :node)
-                              (string= (plist-get node :file-path) file)
-                              (not (org-supertag-sync--check-node-exists-in-file id file)))
-                     (message "[org-supertag] Removing zombie node %s from %s" id file)
-                     (org-supertag-db-remove-object id)
-                     (cl-incf cleaned-count)))
-                 org-supertag-db--object)))
-    cleaned-count))
+
+(defun org-supertag-get-modified-files ()
+  "Get list of files that need synchronization.
+Returns files that have been modified since last sync."
+  (let ((files nil))
+    (maphash
+     (lambda (file state)
+       (when (and (file-exists-p file)
+                  (org-supertag-sync--in-sync-scope-p file)
+                  (org-supertag-sync-check-state file))
+         (push file files)))
+     org-supertag-sync--state)
+    files))
+
+(defun org-supertag-scan-sync-directories ()
+  "Scan sync directories for new org files.
+Returns a list of new files that are not yet in sync state."
+  (let ((new-files nil))
+    (dolist (dir org-supertag-sync-directories)
+      (when (file-exists-p dir)
+        (let ((dir-files (directory-files-recursively 
+                         dir org-supertag-sync-file-pattern t)))
+          (dolist (file dir-files)
+            (when (and (file-regular-p file)
+                      (org-supertag-sync--in-sync-scope-p file)
+                      (not (gethash file org-supertag-sync--state)))
+              (push file new-files))))))
+    new-files))
 
 (defun org-supertag-sync--check-and-sync ()
   "Check and synchronize modified files.
 This is the main sync function called periodically."
+<<<<<<< HEAD
+  ;; Clean up non-existent files from sync state
+  (maphash (lambda (file _state)
+             (unless (file-exists-p file)
+               (message "[org-supertag] Removing non-existent file from sync state: %s" file)
+               (remhash file org-supertag-sync--state)))
+           org-supertag-sync--state)
+=======
   ;; Clean up non-existent files and files out of sync scope from sync state
   (let ((files-to-remove nil))
     (maphash (lambda (file _state)
@@ -752,6 +842,7 @@ This is the main sync function called periodically."
                            (string= (plist-get node :file-path) file))
                    (org-supertag-db-remove-object id)))
                org-supertag-db--object)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
   ;; Check for new files first
   (let ((new-files (org-supertag-scan-sync-directories)))
@@ -765,6 +856,7 @@ This is the main sync function called periodically."
         (nodes-moved 0)
         (nodes-created 0)
         (old-nodes (make-hash-table :test 'equal))
+        (errors nil)
         (updated 0))
     
     ;; First collect existing nodes
@@ -786,34 +878,39 @@ This is the main sync function called periodically."
                      (file-attribute-modification-time
                       (file-attributes b))))))
       
-      ;; Process files without error catching
+      ;; Process files
       (dolist (file modified-files)
-        (message "Syncing file: %s" file)
-        (with-current-buffer (find-file-noselect file)
-          (let ((before-nodes (hash-table-count old-nodes)))
-            ;; Update database
-            (org-supertag-db-update-buffer)
-            ;; Count changes
-            (maphash
-             (lambda (id node)
-               (let ((old-node (gethash id old-nodes)))
-                 (cond
-                  ;; Node moved
-                  ((and old-node
-                        (not (string= (plist-get old-node :file-path)
-                                    (plist-get node :file-path))))
-                   (cl-incf nodes-moved))
-                  ;; New node
-                  ((null old-node)
-                   (cl-incf nodes-created)))))
-             org-supertag-db--object)
-            ;; Count deleted nodes
-            (let ((deleted-in-file (- before-nodes
-                                    (hash-table-count old-nodes))))
-              (setq nodes-deleted (+ nodes-deleted deleted-in-file)))
-            (org-supertag-sync-update-state file)
-            (cl-incf updated)))))
+        (condition-case err
+            (with-current-buffer (find-file-noselect file)
+              (let ((before-nodes (hash-table-count old-nodes)))
+                ;; Update database
+                (org-supertag-db-update-buffer)
+                ;; Count changes
+                (maphash
+                 (lambda (id node)
+                   (let ((old-node (gethash id old-nodes)))
+                     (cond
+                      ;; Node moved
+                      ((and old-node
+                            (not (string= (plist-get old-node :file-path)
+                                        (plist-get node :file-path))))
+                       (cl-incf nodes-moved))
+                      ;; New node
+                      ((null old-node)
+                       (cl-incf nodes-created)))))
+                 org-supertag-db--object)
+                ;; Count deleted nodes
+                (let ((deleted-in-file (- before-nodes
+                                        (hash-table-count old-nodes))))
+                  (setq nodes-deleted (+ nodes-deleted deleted-in-file)))
+                (org-supertag-sync-update-state file)
+                (cl-incf updated)))
+          (error
+           (push (cons file (error-message-string err))
+                 errors)))))
     
+<<<<<<< HEAD
+=======
     ;; Cleanup zombie nodes after processing modified files
     (when modified-files
      ;; (message "[org-supertag] Checking for zombie nodes in modified files...")
@@ -821,8 +918,20 @@ This is the main sync function called periodically."
         (when (> cleaned-nodes 0)
           (message "[org-supertag] Cleaned up %d zombie nodes" cleaned-nodes))))
     
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
     ;; Report results
-    (message "[org-supertag] Node Sync Completed")))
+    (if errors
+        (progn
+          (message "Sync completed with errors: %d files updated, %d errors"
+                   updated (length errors))
+          (with-current-buffer (get-buffer-create "*Org Supertag Sync Errors*")
+            (erase-buffer)
+            (insert "Force Synchronization Errors:\n\n")
+            (dolist (err errors)
+              (insert (format "File: %s\nError: %s\n\n"
+                             (car err) (cdr err))))
+            (display-buffer (current-buffer))))
+      (message "[org-supertag] Node Sync Completed"))))
 
 (defun org-supertag-sync-start-auto-sync (&optional interval)
   "Start automatic synchronization with INTERVAL seconds.
@@ -849,35 +958,334 @@ If INTERVAL is nil, use `org-supertag-sync-auto-interval'."
     (setq org-supertag-sync--timer nil)
     (message "Auto-sync stopped")))
 
-(cl-defun org-supertag-sync-init ()
-  "Initialize sync system."
-  (when org-supertag-sync--initialized
-    (cl-return-from org-supertag-sync-init))
+(defun org-supertag-sync-force-all ()
+  "Force synchronization of all files in scope."
+  (interactive)
+  (let* ((files (org-supertag-get-all-files))
+         (total (length files))
+         (batch-size 10)  ; Process 10 files at a time
+         (current 0)
+         (updated 0)
+         (errors nil)
+         ;; save original values 
+         (org-startup-with-latex-preview nil)
+         (org-startup-folded nil)
+         (org-startup-with-inline-images nil)
+         (org-startup-indented nil)
+         (org-hide-block-startup nil)
+         (org-hide-drawer-startup nil)
+         (org-startup-align-all-tables nil)
+         ;; disable org-element parsing
+         (org-element-use-cache nil)
+         ;; disable auto collect keywords
+         (org--collect-keywords-cache (make-hash-table :test 'equal))
+         ;; 保存节点关系数据
+         (preserved-data (make-hash-table :test 'equal))
+         ;; 保存所有非节点实体
+         (preserved-entities (make-hash-table :test 'equal))
+         ;; 保存所有链接数据
+         (preserved-links (make-hash-table :test 'equal)))
+    
+    ;; 保存所有节点的关系数据
+    (maphash
+     (lambda (id node)
+       (when (eq (plist-get node :type) :node)
+         (let ((rel-data (list :ref-to (plist-get node :ref-to)
+                              :ref-from (plist-get node :ref-from)
+                              :ref-count (plist-get node :ref-count))))
+           (puthash id rel-data preserved-data))))
+     org-supertag-db--object)
+    
+    (maphash
+     (lambda (id entity)
+       (let ((entity-type (plist-get entity :type)))
+         (when (and entity-type (not (eq entity-type :node)))
+           (puthash id (copy-sequence entity) preserved-entities))))
+     org-supertag-db--object)
+    
+    ;; Confirm with user if too many files
+    (when (and (> total 100)
+               (not (yes-or-no-p 
+                     (format "About to process %d files. Continue? " total))))
+      (user-error "Aborted force sync"))
+    
+    ;; Process files in batches
+    (while files
+      ;; Take next batch
+      (let ((batch (seq-take files batch-size)))
+        (setq files (seq-drop files batch-size))
+        
+        ;; Process batch
+        (dolist (file batch)
+          (setq current (1+ current))
+          
+          (when (and (file-exists-p file)
+                    (org-supertag-sync--in-sync-scope-p file))
+            (condition-case err
+                (let ((buf (find-file-noselect file nil nil nil)))
+                  (with-current-buffer buf
+                    ;; disable some features that may cause problems
+                    (setq-local org-startup-with-latex-preview nil
+                              org-startup-folded nil
+                              org-startup-with-inline-images nil
+                              org-startup-indented nil
+                              org-hide-block-startup nil
+                              org-hide-drawer-startup nil
+                              org-startup-align-all-tables nil
+                              org-element-use-cache nil)
+                    ;; ensure buffer is in correct mode
+                    (let ((org-mode-hook nil)
+                          (org-set-regexps-and-options-hook nil)
+                          (org-startup-options-hook nil))
+                      (delay-mode-hooks
+                        (org-mode)))
+                    ;; ensure buffer is fully loaded
+                    (widen)
+                    (goto-char (point-min))
+                    ;; disable some features that may interfere
+                    (let ((org-fold-core-style 'overlays)
+                          (org-element-use-cache nil)
+                          (org-startup-folded nil))
+                      (let ((current-nodes (org-supertag-scan-buffer-nodes)))
+                        (maphash
+                         (lambda (id props)
+                           (when-let* ((rel-data (gethash id preserved-data)))
+                             (dolist (prop '(:ref-to :ref-from :ref-count))
+                               (when-let* ((value (plist-get rel-data prop)))
+                                 (setq props (plist-put props prop value)))))
+                           (org-supertag-db-add-with-hash id props))
+                         current-nodes))
+                      (org-supertag-sync-update-state file))
+                    ;; Save buffer if modified
+                    (when (buffer-modified-p)
+                      (basic-save-buffer)))
+                  ;; Kill buffer
+                  (kill-buffer buf)
+                  (cl-incf updated))
+              (error
+               (push (cons file (error-message-string err))
+                     errors))))
+          
+          ;; Add delay between files
+          (sit-for 0.1))
+        
+        ;; Allow interruption between batches
+        (when (input-pending-p)
+          (when (yes-or-no-p "Interrupt force sync? ")
+            (user-error "Force sync interrupted at file %d/%d" 
+                       current total)))))
+    
+         ;; 恢复所有非节点实体
+     (maphash
+      (lambda (id entity)
+        (org-supertag-db-add id entity))
+      preserved-entities)
+     
+     ;; 恢复所有链接数据
+     (maphash
+      (lambda (link-id link-data)
+        (puthash link-id link-data org-supertag-db--link))
+      preserved-links)
+    
+    ;; Report results
+    (if errors
+        (progn
+          (message "Force sync completed with errors: %d files updated, %d errors"
+                   updated (length errors))
+          (with-current-buffer (get-buffer-create "*Org Supertag Sync Errors*")
+            (erase-buffer)
+            (insert "Force Synchronization Errors:\n\n")
+            (dolist (err errors)
+              (insert (format "File: %s\nError: %s\n\n"
+                             (car err) (cdr err))))
+            (display-buffer (current-buffer))))
+      (message "[org-supertag] Node Sync Completed"))))
+
+;;-------------------------------------------------------------------
+;; Error Recovery
+;;-------------------------------------------------------------------
+
+(defun org-supertag-sync-recover ()
+  "Recover from sync errors.
+1. Stop auto-sync
+2. Reset state
+3. Rebuild from files
+4. Rescan all nodes"
+  (interactive)
+  ;; 1. Stop auto-sync
+  (when org-supertag-sync--timer
+    (cancel-timer org-supertag-sync--timer))
   
-  ;; Ensure directories are loaded
-  (org-supertag-sync--ensure-directories)
+  ;; 2. Reset state
+  (clrhash org-supertag-sync--state)
   
+  ;; 3. Rebuild file state
+  (let ((errors nil))
+    (dolist (file (org-supertag-get-all-files))
+      (when (file-exists-p file)
+        (condition-case err
+            (progn
+              ;; Update file state
+              (org-supertag-sync-update-state file)
+              ;; Scan nodes in file
+              (with-current-buffer (find-file-noselect file)
+                (org-supertag-db-update-buffer)))
+          (error
+           (push (cons file (error-message-string err))
+                 errors)))))
+    
+    ;; Report errors if any
+    (when errors
+      (with-current-buffer (get-buffer-create "*Org Supertag Sync Errors*")
+        (erase-buffer)
+        (insert "Recovery Errors:\n\n")
+        (dolist (err errors)
+          (insert (format "File: %s\nError: %s\n\n"
+                         (car err) (cdr err))))
+        (display-buffer (current-buffer)))))
+  
+  ;; 4. Save recovered state
+  (org-supertag-sync-save-state)
+  
+  ;; 5. Restart auto-sync
+  (org-supertag-sync-start-auto-sync)
+  (message "Recovery completed"))
+
+(defun org-supertag-db-resync-all ()
+  "Resynchronize all nodes in database while preserving relationships.
+Uses ID-based scanning to ensure reliability."
+  (interactive)
+  (when (yes-or-no-p "This will update all nodes. Continue? ")
+    (let ((updated 0)
+          (errors nil)
+          (preserved-data (make-hash-table :test 'equal))
+          (preserved-entities (make-hash-table :test 'equal))
+          (preserved-links (make-hash-table :test 'equal)))
+      
+      ;; 1. First preserve all relationship data
+      (maphash
+       (lambda (id node)
+         (when (eq (plist-get node :type) :node)
+           (let ((rel-data (list :ref-to (plist-get node :ref-to)
+                                :ref-from (plist-get node :ref-from)
+                                :ref-count (plist-get node :ref-count))))
+             (puthash id rel-data preserved-data))))
+       org-supertag-db--object)
+      
+<<<<<<< HEAD
+             ;; 1.5. 保存所有非节点实体
+       (maphash
+        (lambda (id entity)
+          (let ((entity-type (plist-get entity :type)))
+            (when (and entity-type (not (eq entity-type :node)))
+              (puthash id (copy-sequence entity) preserved-entities))))
+        org-supertag-db--object)
+       
+       ;; 1.6. 保存所有链接数据
+       (maphash
+        (lambda (link-id link-data)
+          (puthash link-id (copy-sequence link-data) preserved-links))
+        org-supertag-db--link)
+=======
+      ;; 1.5. 保存所有非节点实体
+      (maphash
+       (lambda (id entity)
+         (let ((entity-type (plist-get entity :type)))
+           (when (and entity-type (not (eq entity-type :node)))
+             (puthash id (copy-sequence entity) preserved-entities))))
+       org-supertag-db--object)
+       
+      ;; 1.6. 保存所有链接数据
+      (maphash
+       (lambda (link-id link-data)
+         (puthash link-id (copy-sequence link-data) preserved-links))
+       org-supertag-db--link)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+      
+      ;; 2. Scan all files and update nodes
+      (dolist (file (org-supertag-get-all-files))
+        (when (file-exists-p file)
+          (condition-case err
+              (with-current-buffer (find-file-noselect file)
+                (let ((current-nodes (org-supertag-scan-buffer-nodes)))
+                  (maphash
+                   (lambda (id props)
+                     ;; Restore relationship data if exists
+                     (when-let* ((rel-data (gethash id preserved-data)))
+                       (dolist (prop '(:ref-to :ref-from :ref-count))
+                         (when-let* ((value (plist-get rel-data prop)))
+                           (setq props (plist-put props prop value)))))
+                     ;; Update node with hash
+                     (org-supertag-db-add-with-hash id props)
+                     (cl-incf updated))
+                   current-nodes)))
+            (error
+             (push (cons file (error-message-string err))
+                   errors)))))
+      
+<<<<<<< HEAD
+             ;; 2.5. 恢复所有非节点实体
+       (maphash
+        (lambda (id entity)
+          (org-supertag-db-add id entity))
+        preserved-entities)
+       
+       ;; 2.6. 恢复所有链接数据
+       (maphash
+        (lambda (link-id link-data)
+          (puthash link-id link-data org-supertag-db--link))
+        preserved-links)
+=======
+      ;; 2.5. 恢复所有非节点实体
+      (maphash
+       (lambda (id entity)
+         (org-supertag-db-add id entity))
+       preserved-entities)
+       
+      ;; 2.6. 恢复所有链接数据
+      (maphash
+       (lambda (link-id link-data)
+         (puthash link-id link-data org-supertag-db--link))
+       preserved-links)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+      
+      ;; 3. Report results
+      (if errors
+          (progn
+            (message "Resync completed with errors: %d nodes updated, %d errors"
+                     updated (length errors))
+            (with-current-buffer (get-buffer-create "*Org Supertag Sync Errors*")
+              (erase-buffer)
+              (insert "Resync Errors:\n\n")
+              (dolist (err errors)
+                (insert (format "File: %s\nError: %s\n\n"
+                               (car err) (cdr err))))
+              (display-buffer (current-buffer))))
+        (message "Successfully resynced %d nodes while preserving relationships"
+                 updated))
+      updated)))
+
+;;-------------------------------------------------------------------
+<<<<<<< HEAD
+;; Initialization
+;;-------------------------------------------------------------------
+
+(defun org-supertag-sync-init ()
+  "Initialize sync system."
   ;; Ensure data directory exists
   (unless (file-exists-p org-supertag-data-directory)
     (make-directory org-supertag-data-directory t))
   
-  ;; Load sync state
+  ;; Load or initialize sync state
   (org-supertag-sync-load-state)
   
-  ;; Clean up database if loaded and has content
-  (when (and (featurep 'org-supertag-db) 
-             (hash-table-p org-supertag-db--object)
-             (> (hash-table-count org-supertag-db--object) 0))
-    (org-supertag-sync--cleanup-database))
-  
   ;; Setup buffer hooks
   (org-supertag-sync-setup-buffer-watch)
   
-  ;; Initial state for all files
+  ;; Initial state for all files, including new ones
   (let ((all-files (org-supertag-get-all-files))
         (new-files (org-supertag-scan-sync-directories)))
-    ;; Update state for valid files
-    (dolist (file (delete-dups (append all-files new-files)))
+    (dolist (file (append all-files new-files))
       (when (file-exists-p file)
         (org-supertag-sync-update-state file))))
   
@@ -885,119 +1293,69 @@ If INTERVAL is nil, use `org-supertag-sync-auto-interval'."
   (org-supertag-sync-save-state)
   
   ;; Start auto-sync
-  (org-supertag-sync-start-auto-sync)
-  
-  ;; Mark as initialized
-  (setq org-supertag-sync--initialized t))
-
-(defun org-supertag-db-resync-all ()
-  "Resynchronize all nodes from files using a safe 'preserve and rebuild' strategy."
-  (interactive)
-  (let ((all-nodes-props (make-hash-table :test 'equal))
-        (errors nil))
-    (when (yes-or-no-p "This will rebuild the node database from source files. Manually created tag relationships will be preserved. Continue? ")
-
-      ;; Phase 1: Clear only node-related data. Tags and their relationships are preserved.
-      (message "Phase 1/4: Removing all node entities and their derived links…")
-      (let ((nodes-to-delete '())
-            (links-to-delete '()))
-        ;; Collect IDs to delete
-        (maphash (lambda (id entity)
-                   (when (eq (plist-get entity :type) :node)
-                     (push id nodes-to-delete)))
-                 org-supertag-db--object)
-        (maphash (lambda (id link)
-                   (when (memq (plist-get link :type) '(:node-tag :node-field))
-                     (push id links-to-delete)))
-                 org-supertag-db--link)
-        
-        ;; Perform deletion
-        (dolist (id nodes-to-delete)
-          (remhash id org-supertag-db--object))
-        (dolist (id links-to-delete)
-          (remhash id org-supertag-db--link))
-        
-        (message "Removed %d nodes and %d node-derived links."
-                 (length nodes-to-delete) (length links-to-delete)))
-
-      ;; Phase 2: Scan all files and build a complete in-memory representation of all nodes.
-      (message "Phase 2/4: Scanning all source files for nodes...")
-      (let* ((files (org-supertag-scan-sync-directories t))
-             (total (length files))
-             (current 0)
-             (processed 0))
-        (dolist (file files)
-          (cl-incf current)
-          (message "Scanning (%d/%d): %s" current total (file-name-nondirectory file))
-          (when (file-exists-p file)
-            (with-demoted-errors "Error scanning %S"
-              (with-temp-buffer
-                (set-buffer-file-coding-system 'utf-8-unix)
-                (let ((coding-system-for-read 'utf-8))
-                  (insert-file-contents file))
-                (org-mode)
-                (save-excursion
-                  (widen)
-                  (goto-char (point-min))
-                  (org-map-entries
-                   (lambda ()
-                     (let ((id (org-id-get)))
-                       (when id
-                         ;; Use the pure parsing function
-                         (when-let* ((parsed-props (org-supertag-db-parse-node-properties))) 
-                           (setq parsed-props 
-                                 (plist-put parsed-props :file-path file))
-                           (puthash id parsed-props all-nodes-props)
-                           (cl-incf processed))))
-                   t nil))))))
-        (message "Found %d nodes in total." (hash-table-count all-nodes-props)))
-
-      ;; Phase 3: Add all nodes, create tags and node-tag links.
-      (message "Phase 3/4: Rebuilding nodes and node-tag links...")
-      (maphash
-       (lambda (id props)
-         ;; 1. Add the node object itself with its hash.
-         (org-supertag-db-add-with-hash id props)
-         ;; 2. Process and register all tags found in the headline.
-         (when-let ((headline-tags (plist-get props :tags)))
-           (dolist (tag-name headline-tags)
-             ;; Defensively skip empty tag names that may result from parsing errors.
-             (unless (or (null tag-name) (string-empty-p tag-name))
-               (let ((sanitized-tag (org-supertag-sanitize-tag-name tag-name)))
-                 ;; Ensure tag object exists (non-destructive)
-                 ;; Only create tag if it does not exist.
-                 (unless (org-supertag-tag-get sanitized-tag)
-                   (org-supertag-tag--create sanitized-tag))
-                 ;; Ensure link exists
-                 (org-supertag-db-link "HAS_TAG" id sanitized-tag))))))
-       all-nodes-props)
-
-      ;; Phase 4: Reconcile all node-to-node references.
-      (message "Phase 4/4: Reconciling node-to-node references...")
-      (maphash (lambda (id props)
-                 (org-supertag-db-reconcile-references id (plist-get props :ref-to) nil))
-               all-nodes-props)
-
-      (if errors
-          (progn
-            (message "Resync completed with errors.")))
-        (message "[org-supertag] Resync completed successfully. %d nodes processed." (hash-table-count all-nodes-props))))))
-
-(defun org-supertag-sync-force-all ()
-  "Force synchronization of all files in scope. This is an alias for `org-supertag-db-resync-all`."
-  (interactive)
-  (org-supertag-db-resync-all))
+  (org-supertag-sync-start-auto-sync))
 
 ;;-------------------------------------------------------------------
+=======
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;; Helper Functions
 ;;-------------------------------------------------------------------
 
+(defun org-supertag-sync--in-sync-scope-p (file)
+  "Check if FILE is within synchronization scope.
+<<<<<<< HEAD
+Returns t if file should be synchronized based on configured directories."
+  (when (and file (file-exists-p file))
+    (let* ((file-dir (file-name-directory (expand-file-name file)))
+           (excluded (cl-some (lambda (dir)
+                               (string-prefix-p 
+                                (expand-file-name dir) file-dir))
+                             org-supertag-sync-exclude-directories))
+           (included (and org-supertag-sync-directories
+                         (cl-some (lambda (dir)
+                                   (string-prefix-p 
+                                    (expand-file-name dir) file-dir))
+                                 org-supertag-sync-directories))))
+=======
+Returns t if file should be synchronized based on configured directories.
+If no directories are configured, returns t for all org files."
+  (when (and file (file-exists-p file))
+    (let* ((expanded-file (expand-file-name file))
+           (file-dir (file-name-directory expanded-file))
+           (excluded (and org-supertag-sync-exclude-directories
+                         (cl-some (lambda (dir)
+                                   (let ((expanded-exclude-dir (expand-file-name dir)))
+                                     (string-prefix-p expanded-exclude-dir file-dir)))
+                                 org-supertag-sync-exclude-directories)))
+           (included (if org-supertag-sync-directories
+                        (cl-some (lambda (dir)
+                                  (let ((expanded-dir (expand-file-name dir)))
+                                    (string-prefix-p expanded-dir file-dir)))
+                                org-supertag-sync-directories)
+                      t)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+      (and included
+           (not excluded)
+           (string-match-p org-supertag-sync-file-pattern file)))))
+
 (defun org-supertag-sync-add-directory (dir)
-  "Add a directory to the sync scope."
-  (interactive "DAdd directory to sync scope: ")
-  (let ((expanded-dir (expand-file-name dir)))
-    (add-to-list 'org-supertag-sync-directories expanded-dir)
-    (message "Added %s to sync scope." expanded-dir)))
+  "Add directory to synchronization scope."
+  (interactive "DAdd directory to sync: ")
+<<<<<<< HEAD
+  (let ((abs-dir (expand-file-name dir)))
+    (unless (member abs-dir org-supertag-sync-directories)
+      (push abs-dir org-supertag-sync-directories)
+      (customize-save-variable 'org-supertag-sync-directories 
+                             org-supertag-sync-directories)
+      (message "Added %s to sync directories" abs-dir))))
+=======
+  (let ((normalized-dir (org-supertag-sync--normalize-directory dir)))
+    (unless (member normalized-dir org-supertag-sync-directories)
+      (push normalized-dir org-supertag-sync-directories)
+      (customize-save-variable 'org-supertag-sync-directories 
+                             org-supertag-sync-directories)
+      (message "Added %s to sync directories" normalized-dir))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-sync-remove-directory (dir)
   "Remove directory from synchronization scope."
@@ -1012,98 +1370,8 @@ If INTERVAL is nil, use `org-supertag-sync-auto-interval'."
                            org-supertag-sync-directories)
     (message "Removed %s from sync directories" abs-dir)))
 
-(defun org-supertag-sync-file (&optional file-path force-p)
-  "Synchronize a single file.
-If FILE-PATH is nil, sync the current buffer's file.
-If FORCE-P is non-nil, sync even if no modifications are detected."
-  (interactive (list (buffer-file-name) current-prefix-arg))
-  (let ((file (or file-path (buffer-file-name))))
-    (when (and file (file-exists-p file))
-      (if (or force-p (org-supertag-sync-check-state file))
-          (with-current-buffer (find-file-noselect file)
-            (let ((org-supertag-db-auto-save nil))
-              (message "Syncing file: %s" file)
-              (org-supertag-sync--file file)
-              ;; Sync filetags after node sync is complete.
-              (org-supertag-sync--sync-filetags file)
-              (org-supertag-sync-update-state file)
-              (org-supertag-db-save)
-              (message "Sync complete for: %s" file)))
-        (message "File is already up to date: %s" file)))))
-
-(defun org-supertag-sync-files (files)
-  "Synchronize a list of files."
-  (dolist (file files)
-    (org-supertag-sync-file file)))
-
-(defun org-supertag-sync-force-rescan ()
-  "Force a full rescan and synchronization of all files."
-  (interactive)
-  (org-supertag-sync-all t))
-
-;; Add a function to sync all nodes in the current buffer
-(defun org-supertag-sync-buffer ()
-  "Force sync of all nodes in the current buffer."
-  (interactive)
-  (org-supertag-sync-file (buffer-file-name) t))
-
-(defun org-supertag-sync--sync-filetags (file)
-  "Synchronize file-level tags for the given FILE.
-This applies #+FILETAGS to all level-1 nodes in the file,
-handling additions, removals, and updating co-occurrence relations."
-  (when (and (featurep 'org-supertag-relation) file)
-    (with-current-buffer (find-file-noselect file)
-      (let* ((parsed-tree (org-element-parse-buffer))
-             (new-file-tags (org-element-map parsed-tree 'keyword
-                              (lambda (k)
-                                (when (string= (org-element-property :key k) "FILETAGS")
-                                  (split-string (org-element-property :value k) " " t)))))
-             (new-file-tags (car (or new-file-tags '(())))) ; Get first match or empty list
-             (file-metadata (org-supertag-db-get-file-metadata file))
-             (old-file-tags (or (plist-get file-metadata :filetags) '()))
-             (tags-to-add (seq-difference new-file-tags old-file-tags))
-             (tags-to-remove (seq-difference old-file-tags new-file-tags))
-             (level-one-nodes (org-element-map parsed-tree 'headline
-                                (lambda (h)
-                                  (when (= (org-element-property :level h) 1)
-                                    (org-element-property :ID h))))))
-        
-        ;; Ensure all new filetags exist in the DB
-        (dolist (tag-name (append tags-to-add tags-to-remove))
-          (org-supertag-tag--create tag-name))
-        
-        ;; Process tags to add
-        (when tags-to-add
-          (dolist (node-id level-one-nodes)
-            (when node-id
-              (dolist (tag-name tags-to-add)
-                ;; Add link with source tracking
-                (org-supertag-db-link "HAS_TAG" node-id tag-name '(:source :filetags))
-                ;; Update co-occurrence
-                (org-supertag-relation-record-cooccurrence node-id tag-name)))))
-        
-        ;; Process tags to remove
-        (when tags-to-remove
-          (dolist (node-id level-one-nodes)
-            (when node-id
-              (dolist (tag-name tags-to-remove)
-                ;; Remove link, only if it was from filetags
-                (let ((link-id (org-supertag-db--get-link-id :node-tag node-id tag-name)))
-                  (when-let ((props (gethash link-id org-supertag-db--link)))
-                    (when (eq (plist-get props :source) :filetags)
-                      (remhash link-id org-supertag-db--link)
-                      ;; Update co-occurrence
-                      (org-supertag-relation-unrecord-cooccurrence node-id tag-name))))))))
-        
-        ;; Update metadata in DB
-        (when (or tags-to-add tags-to-remove)
-          (org-supertag-db-update-file-metadata file :filetags new-file-tags)
-          (message "Synced FILETAGS for %s: +%d added, -%d removed for %d level-1 nodes."
-                   (file-name-nondirectory file)
-                   (length tags-to-add)
-                   (length tags-to-remove)
-                   (length level-one-nodes)))))))
-
+<<<<<<< HEAD
+=======
 ;;;###autoload
 (defun org-supertag-sync-cleanup-database ()
   "Manually clean up database by removing nodes from files not in sync state.
@@ -1119,48 +1387,44 @@ This is useful when files have been removed from sync scope or deleted."
 
 (defun org-supertag-sync--diagnose-parse-error (file)
   "Diagnose and attempt to fix org-element parsing errors in FILE."
-  (condition-case err
-      (with-current-buffer (find-file-noselect file)
-        (save-excursion
-          (save-restriction
-            (widen)
-            (goto-char (point-min))
-            (let ((org-element-use-cache nil)
-                  (case-fold-search t)
-                  (problematic-regions '()))
-              ;; First pass: try to identify problematic regions
-              (while (re-search-forward org-heading-regexp nil t)
-                (when (org-at-heading-p)
-                  (let ((pos (point))
-                        (heading (org-get-heading t t t t)))
-                    (condition-case err
-                        (progn
-                          (org-back-to-heading t)
-                          (let ((element (org-element-at-point)))
-                            (unless (and element
-                                       (org-element-property :raw-value element))
-                              (push (list pos heading "Invalid element structure") problematic-regions))))
-                      (error
-                       (push (list pos heading (error-message-string err)) problematic-regions))))))
-              
-              ;; Report findings
-              (when problematic-regions
-                (with-current-buffer (get-buffer-create "*Org Parse Diagnosis*")
-                  (erase-buffer)
-                  (insert (format "Parse diagnosis for %s\n\n" file))
-                  (dolist (region (nreverse problematic-regions))
-                    (let ((pos (nth 0 region))
-                          (heading (nth 1 region))
-                          (error-msg (nth 2 region)))
-                      (insert (format "Position %d: %s\n  Error: %s\n\n"
-                                    pos heading error-msg))))
-                  (display-buffer (current-buffer))))
-              
-              ;; Return number of problems found
-              (length problematic-regions))))
-    (error
-     (message "Error diagnosing file %s: %s" file (error-message-string err))
-     -1))))
+  (with-current-buffer (find-file-noselect file)
+    (save-excursion
+      (save-restriction
+        (widen)
+        (goto-char (point-min))
+        (let ((org-element-use-cache nil)
+              (case-fold-search t)
+              (problematic-regions '()))
+          ;; First pass: try to identify problematic regions
+          (while (re-search-forward org-heading-regexp nil t)
+            (when (org-at-heading-p)
+              (let ((pos (point))
+                    (heading (org-get-heading t t t t)))
+                (condition-case err
+                    (progn
+                      (org-back-to-heading t)
+                      (let ((element (org-element-at-point)))
+                        (unless (and element
+                                   (org-element-property :raw-value element))
+                          (push (list pos heading "Invalid element structure") problematic-regions))))
+                  (error
+                   (push (list pos heading (error-message-string err)) problematic-regions))))))
+          
+          ;; Report findings
+          (when problematic-regions
+            (with-current-buffer (get-buffer-create "*Org Parse Diagnosis*")
+              (erase-buffer)
+              (insert (format "Parse diagnosis for %s\n\n" file))
+              (dolist (region (nreverse problematic-regions))
+                (let ((pos (nth 0 region))
+                      (heading (nth 1 region))
+                      (error-msg (nth 2 region)))
+                  (insert (format "Position %d: %s\n  Error: %s\n\n"
+                                pos heading error-msg))))
+              (display-buffer (current-buffer))))
+          
+          ;; Return number of problems found
+          (length problematic-regions))))))
 
 ;;;###autoload
 (defun org-supertag-sync-test-auto-id-creation ()
@@ -1211,7 +1475,7 @@ that meet the criteria. Reports results."
                      (cl-incf failed-count)
                      (let ((err-msg (error-message-string err)))
                        (push (list line heading err-msg) errors)
-                       (message "Line %d: %s [Error: %s]" line heading err-msg)))))))))))
+                       (message "Line %d: %s [Error: %s]" line heading err-msg))))))))))))
     
     ;; Report results
     (message "\n=== Auto ID Creation Test Results ===")
@@ -1245,6 +1509,160 @@ that meet the criteria. Reports results."
     
     (list :created created-count :existing existing-count :failed failed-count :errors errors)))
 
+;;-------------------------------------------------------------------
+;; Zombie Node Validation and Cleanup
+;;-------------------------------------------------------------------
+
+(defun org-supertag-sync-validate-and-cleanup-zombie-nodes ()
+  "验证并清理所有僵尸节点（手动执行）。
+检查数据库中所有节点，验证其对应的文件中是否存在该节点 ID。
+如果节点 ID 在对应文件中不存在，则自动删除该僵尸节点。"
+  (interactive)
+  (let ((all-files (make-hash-table :test 'equal))
+        (zombie-nodes '())
+        (cleaned-count 0)
+        (total-nodes 0)
+        (checked-files 0))
+    
+    ;; 首先收集所有文件及其节点
+    (maphash (lambda (id node)
+               (when (eq (plist-get node :type) :node)
+                 (cl-incf total-nodes)
+                 (let ((file-path (plist-get node :file-path)))
+                   (when file-path
+                     (unless (gethash file-path all-files)
+                       (puthash file-path '() all-files))
+                     (puthash file-path 
+                             (cons (cons id node) (gethash file-path all-files))
+                             all-files)))))
+             org-supertag-db--object)
+    
+    (message "Starting zombie node validation for %d nodes in %d files..." 
+             total-nodes (hash-table-count all-files))
+    
+    ;; 检查每个文件中的节点
+    (maphash (lambda (file-path nodes)
+               (cl-incf checked-files)
+               (message "Checking file (%d/%d): %s" 
+                       checked-files (hash-table-count all-files) file-path)
+               
+               (if (file-exists-p file-path)
+                   ;; 文件存在，检查节点是否在文件中
+                   (dolist (node-entry nodes)
+                     (let ((node-id (car node-entry)))
+                       (unless (org-supertag-sync--check-node-exists-in-file node-id file-path)
+                         (message "Found zombie node: %s in file %s" node-id file-path)
+                         (push (cons node-id file-path) zombie-nodes))))
+                 ;; 文件不存在，所有节点都是僵尸节点
+                 (message "File not found: %s, marking all %d nodes as zombies" 
+                         file-path (length nodes))
+                 (dolist (node-entry nodes)
+                   (let ((node-id (car node-entry)))
+                     (push (cons node-id file-path) zombie-nodes)))))
+             all-files)
+    
+    ;; 直接删除僵尸节点
+    (when zombie-nodes
+      (message "Cleaning up %d zombie nodes..." (length zombie-nodes))
+      (dolist (zombie zombie-nodes)
+        (let ((node-id (car zombie))
+              (file-path (cdr zombie)))
+          (message "Removing zombie node %s from %s" node-id file-path)
+          (org-supertag-db-remove-object node-id)
+          (cl-incf cleaned-count))))
+    
+    (message "Zombie node cleanup completed: checked %d files, %d total nodes, cleaned %d zombie nodes" 
+             checked-files total-nodes cleaned-count)
+    cleaned-count))
+
+(defun org-supertag-sync--check-node-exists-in-file (node-id file-path)
+  "检查节点 ID 是否在指定文件中存在。
+NODE-ID: 要检查的节点 ID
+FILE-PATH: 文件路径
+返回 t 如果节点存在，nil 如果不存在。"
+  ;; **FIX: Handle nil file-path properly**
+  (when (and node-id file-path (file-exists-p file-path))
+    (condition-case err
+        (with-current-buffer (find-file-noselect file-path)
+          (save-excursion
+            (save-restriction
+              (widen)
+              (goto-char (point-min))
+              ;; 搜索 :ID: node-id 格式（属性抽屉中的 ID）
+              (or (re-search-forward 
+                   (format "^[ \t]*:ID:[ \t]+%s[ \t]*$" (regexp-quote node-id))
+                   nil t)
+                  ;; 也检查 #+ID: 格式（关键字形式）
+                  (progn
+                    (goto-char (point-min))
+                    (re-search-forward 
+                     (format "^[ \t]*#\\+ID:[ \t]+%s[ \t]*$" (regexp-quote node-id))
+                     nil t))))))
+      (error
+       (message "Error checking node %s in file %s: %s" 
+                node-id file-path (error-message-string err))
+       ;; Return nil on error for nil/invalid file paths to allow cleanup
+       nil))))
+
+(defun org-supertag-sync-cleanup-zombie-nodes-for-file (file-path)
+  "Clean up zombie nodes for a specific file.
+FILE-PATH: Path to the file to check
+Returns number of cleaned nodes."
+  (let ((zombie-nodes '())
+        (cleaned-count 0))
+    
+    (when (file-exists-p file-path)
+      (message "Checking zombie nodes for file: %s" file-path)
+      
+      ;; Get all nodes in the database for the file
+      (maphash (lambda (id node)
+                 (when (and (eq (plist-get node :type) :node)
+                           (string= (plist-get node :file-path) file-path))
+                   ;; Check if the node ID exists in the file
+                   (unless (org-supertag-sync--check-node-exists-in-file id file-path)
+                     (push id zombie-nodes))))
+               org-supertag-db--object)
+      
+      ;; Delete zombie nodes
+      (dolist (node-id zombie-nodes)
+        (message "Removing zombie node: %s" node-id)
+        (org-supertag-db-remove-object node-id)
+        (cl-incf cleaned-count))
+      
+      (when (> cleaned-count 0)
+        (message "Cleaned up %d zombie nodes from file: %s" cleaned-count file-path)))
+    
+    cleaned-count))
+
+(defun org-supertag-sync--auto-cleanup-zombie-nodes (modified-files)
+  "Automatically clean up zombie nodes in modified files (used in auto-sync process).
+MODIFIED-FILES: List of modified files
+Returns number of cleaned nodes."
+  (let ((zombie-nodes '())
+        (cleaned-count 0))
+    
+    ;; Only check nodes in modified files
+    (dolist (file modified-files)
+      (when (file-exists-p file)
+        ;; Get all nodes in the database for the file
+        (maphash (lambda (id node)
+                   (when (and (eq (plist-get node :type) :node)
+                             (string= (plist-get node :file-path) file))
+                     ;; Check if the node ID exists in the file
+                     (unless (org-supertag-sync--check-node-exists-in-file id file)
+                       (push (cons id file) zombie-nodes))))
+                 org-supertag-db--object)))
+    
+    ;; Delete zombie nodes
+    (dolist (zombie zombie-nodes)
+      (let ((node-id (car zombie))
+            (file-path (cdr zombie)))
+        (message "[org-supertag] Removing zombie node %s from %s" node-id file-path)
+        (org-supertag-db-remove-object node-id)
+        (cl-incf cleaned-count)))
+    
+    cleaned-count))
+
 ;;; Debug Functions for Hash Mismatch Issues
 
 (defun org-supertag-debug-hash-differences ()
@@ -1277,7 +1695,7 @@ This helps identify why nodes are being marked as 'updated' when they shouldn't
                        (cl-incf hash-mismatches)
                        ;; Detailed comparison
                        (let ((differences '()))
-                         (dolist (prop '(:raw-value :tags :todo-type :priority))
+                         (dolist (prop '(:raw-value :tags :todo-type :priority :properties))
                            (let ((old-val (plist-get old-node prop))
                                  (new-val (plist-get new-props prop)))
                              (unless (equal old-val new-val)
@@ -1356,7 +1774,7 @@ This helps identify why nodes are being marked as 'updated' when they shouldn't
         (insert (format "ID: %s\n\n" id))
         
         (let ((all-hashes (mapcar (lambda (e) (plist-get e :hash)) extractions))
-              (all-properties (mapcar (lambda (e) (plist-get e :properties)) extractions))))
+              (all-properties (mapcar (lambda (e) (plist-get e :properties)) extractions)))
           
           (if (= (length (delete-dups (copy-sequence all-hashes))) 1)
               (insert "✅ Hash is stable across multiple extractions\n")
@@ -1416,12 +1834,17 @@ This ensures all nodes use the same hash calculation method."
              ;; Always update with new hash to ensure consistency
              (org-supertag-db-add id 
                                   (plist-put node :hash new-hash))
-             (cl-incf updated-count)))))
+             (cl-incf updated-count))))
        org-supertag-db--object)
-    
-    (message "Normalized hashes for %d nodes" updated-count)
-    updated-count))
+      
+      (message "Normalized hashes for %d nodes" updated-count)
+      updated-count)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (provide 'org-supertag-sync)
 
 ;;; org-supertag-sync.el ends here
+<<<<<<< HEAD
+=======
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
diff --git a/org-supertag-tag-company.el b/org-supertag-tag-company.el
new file mode 100644
index 0000000..27c9d57
--- /dev/null
+++ b/org-supertag-tag-company.el
@@ -0,0 +1,125 @@
+;; org-supertag-tag-company.el --- Company completion for org-supertag tags -*- lexical-binding: t -*-
+
+(require 'company)
+
+<<<<<<< HEAD
+=======
+;; Ensure string-trim is available (for Emacs < 24.4 compatibility)
+(unless (fboundp 'string-trim)
+  (defun string-trim (string)
+    "Remove leading and trailing whitespace from STRING."
+    (replace-regexp-in-string "\\`[ \t\n\r]+" ""
+                              (replace-regexp-in-string "[ \t\n\r]+\\'" "" string))))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+(defvar org-supertag-company-prefix-regexp
+  "#\\([[:alnum:]_-]*\\)"
+  "Regexp to match the prefix for company completion.
+Only allows alphanumeric characters, underscore and hyphen after #.")
+
+(defvar org-supertag-company-new-tag-candidate "[New Tag]"
+  "Special candidate text for creating new tag.")
+
+(defun org-supertag-company--make-candidate (tag-name)
+  "Create a company candidate from TAG-NAME."
+  (let* ((tag (org-supertag-tag-get tag-name))
+         (fields (plist-get tag :fields))
+         (field-count (length fields))
+         (base-tag (org-supertag-tag-get-base tag-name))
+         (help-text
+          (if base-tag
+              (format "Extension of #%s with %d field(s)" 
+                      base-tag field-count)
+            (format "Base tag with %d field(s)" field-count))))
+    (propertize tag-name
+                'tag tag
+                'is-new-tag nil
+                'help-echo (format "SuperTag with %d field(s)" field-count))))
+
+(defun org-supertag-company--make-new-tag-candidate ()
+  "Create a special candidate for creating new tag."
+  (propertize org-supertag-company-new-tag-candidate
+              'is-new-tag t
+              'help-echo "Create a new tag"))
+
+(defun org-supertag-company--candidates (prefix)
+  "Get completion candidates for PREFIX."
+  (let* ((prefix-no-hash (if (> (length prefix) 1)
+                            (org-supertag-sanitize-tag-name (substring prefix 1))
+                          ""))
+         (all-tags (org-supertag-get-all-tags))
+         (candidates (cl-loop for tag-name in all-tags
+                            when (string-prefix-p prefix-no-hash tag-name t)
+                            collect (org-supertag-company--make-candidate tag-name))))
+    ;; Always add the new tag option at the end
+    (append candidates (list (org-supertag-company--make-new-tag-candidate)))))
+
+(defun org-supertag-company--post-completion (candidate)
+  "Handle post-completion actions for CANDIDATE.
+If CANDIDATE is a non-existent tag name, create it directly."
+  (let* ((is-new-tag (get-text-property 0 'is-new-tag candidate))
+         (tag-name (cond
+                   (is-new-tag
+<<<<<<< HEAD
+                    (read-string "Enter new tag name: "))
+                   ((get-text-property 0 'tag candidate)
+                    (plist-get (get-text-property 0 'tag candidate) :name))
+                   (t candidate))))
+=======
+                    (let ((input (read-string "Enter new tag name: ")))
+                      ;; Validate input - must be non-empty after trimming
+                      (when (or (null input) (string-empty-p (string-trim input)))
+                        (user-error "Tag name cannot be empty"))
+                      (string-trim input)))
+                   ((get-text-property 0 'tag candidate)
+                    (plist-get (get-text-property 0 'tag candidate) :name))
+                   (t candidate))))
+    ;; Validate final tag-name before proceeding
+    (when (or (null tag-name) (string-empty-p (string-trim tag-name)))
+      (user-error "Invalid tag name: %s" tag-name))
+    
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+    (delete-region (- (point) (length candidate) 1) (point))
+    
+    (org-supertag-inline-insert-tag tag-name)))
+
+;;;###autoload
+(defun org-supertag-company-backend (command &optional arg &rest ignored)
+  "Company backend for org-supertag completion.
+COMMAND, ARG and IGNORED are standard arguments for company backends."
+  (interactive (list 'interactive))
+  (cl-case command
+    (interactive (company-begin-backend 'org-supertag-company-backend))
+    (prefix (and (eq major-mode 'org-mode)
+                 (save-excursion
+                   (when (looking-back org-supertag-company-prefix-regexp
+                                     (line-beginning-position))
+                     (match-string-no-properties 0)))))
+    (candidates (org-supertag-company--candidates arg))
+    (post-completion (org-supertag-company--post-completion arg))
+    (annotation (let* ((tag (get-text-property 0 'tag arg))
+                      (is-new-tag (get-text-property 0 'is-new-tag arg)))
+                 (cond
+                  (is-new-tag " [Create new tag]")
+                  (tag
+                   (let* ((fields (plist-get tag :fields))
+                          (base-tag (org-supertag-tag-get-base (plist-get tag :id))))
+                     (if base-tag
+                         (format " [%d fields, extends %s]"
+                                 (length fields)
+                                 base-tag)
+                       (format " [%d fields]" (length fields))))))))
+    (t nil)))
+
+;;;###autoload
+(defun org-supertag-setup-completion ()
+  "Setup company completion for org-supertag."
+  (when (and (eq major-mode 'org-mode)
+             (featurep 'company))
+    (add-to-list 'company-backends 'org-supertag-company-backend)
+    (make-local-variable 'company-minimum-prefix-length)
+    (setq-local company-minimum-prefix-length 1))) 
+
+(add-hook 'org-mode-hook #'org-supertag-setup-completion)
+
+(provide 'org-supertag-tag-company)
\ No newline at end of file
diff --git a/org-supertag-tag.el b/org-supertag-tag.el
old mode 100644
new mode 100755
index 0049b4b..b126535
--- a/org-supertag-tag.el
+++ b/org-supertag-tag.el
@@ -4,13 +4,8 @@
 ;; Provides tag relationship management functionality
 ;; Core principle: Connect entities through relationships using type, from, and to
 
-;;; Code:
-
-(require 'org-supertag-db)
-(require 'org-supertag-node)
-(require 'org-supertag-field)
-;; Utilities
-(require 'cl-lib) ; For cl-return-from used in early exit
+(require 'org-supertag-tag-company)
+(require 'org-supertag-field-operation)
 
 ;;----------------------------------------------------------------------
 ;; Tag Name Operation
@@ -18,14 +13,16 @@
 
 (defun org-supertag-sanitize-tag-name (name)
   "Convert a name into a valid tag name.
-NAME is the name to convert. It removes leading/trailing whitespace,
-removes all text properties, and converts internal whitespace sequences
-to single underscores."
+NAME is the name to convert
+- Remove leading '#' characters
+- Convert spaces to underscores
+- Validate non-empty"
   (if (or (null name) (string-empty-p name))
       (error "Tag name cannot be empty")
-    (let* ((clean-name (substring-no-properties name))
-           (trimmed (string-trim clean-name))
-           (sanitized (replace-regexp-in-string "\\s-+" "_" trimmed)))
+    (let* ((trimmed (string-trim name))
+           (without-hash (replace-regexp-in-string "^#+" "" trimmed))
+           (sanitized (replace-regexp-in-string "\\s-+" "_" without-hash)))
+      (message "Debug sanitize-tag-name: input=%s output=%s" name sanitized)
       (if (string-empty-p sanitized)
           (error "Invalid tag name: %s" name)
         sanitized))))
@@ -36,6 +33,15 @@ TAG-NAME is the name of the tag to check"
   (let ((sanitized-name (org-supertag-sanitize-tag-name tag-name)))
     (org-supertag-tag-get sanitized-name)))
 
+(defun org-supertag-node-get-parent-tags (node-id)
+  "Get the parent tags of the node.
+NODE-ID: The ID of the current node"
+  (save-excursion
+    (when-let* ((pos (org-id-find node-id t)))
+      (goto-char pos)
+      (when (org-up-heading-safe)  
+        (when-let ((parent-id (org-id-get)))  
+          (org-supertag-node-get-tags parent-id))))))
 ;;----------------------------------------------------------------------
 ;; Tag Base Operation
 ;;----------------------------------------------------------------------
@@ -54,42 +60,103 @@ Examples:
               ((> (length parts) 1)))
     (cons (car parts) (mapconcat #'identity (cdr parts) "_"))))
 
-(defun org-supertag-tag--create (tag-name &rest props)
+<<<<<<< HEAD
+(defun org-supertag-tag-create (tag-name &rest props)
   "Create a new tag.
 TAG-NAME: Name of the tag
 PROPS: Additional properties including:
 - :fields      List of field definitions
 - :behaviors   List of behaviors"
-  (unless (string-to-multibyte tag-name)
-    (error "Tag name '%s' contains invalid characters" tag-name))
-  (let ((sanitized-name (org-supertag-sanitize-tag-name tag-name)))
-    ;; If the tag already exists, do NOT overwrite it. Simply return its ID.
-    (when (org-supertag-tag-get sanitized-name)
-      (cl-return-from org-supertag-tag--create sanitized-name))
-
-    ;; Tag does not exist yet – proceed with normal creation logic.
-    (let* ((parsed-name (org-supertag-tag--parse-name sanitized-name))
-           (base-tag-name (car parsed-name))
-           (new-fields (plist-get props :fields))
-           (base-tag (and base-tag-name
-                          (org-supertag-tag-exists-p base-tag-name)
-                          (org-supertag-tag-get base-tag-name)))
-           (fields (or (if base-tag
-                           (org-supertag-tag--project-fields
-                            base-tag
-                            new-fields
-                            (plist-get props :field-defaults))
-                         new-fields)
-                       '()))
-           (base-props (list :type :tag
-                             :id sanitized-name
-                             :name sanitized-name
-                             :fields fields
-                             :extend-from base-tag-name
-                             :behaviors (plist-get props :behaviors)
-                             :created-at (current-time))))
-      (org-supertag-db-add sanitized-name base-props))
+  (let* ((sanitized-name (org-supertag-sanitize-tag-name tag-name))
+=======
+(defun org-supertag-tag-get-id-by-name (tag-name)
+  "Get the tag ID by TAG-NAME.
+TAG-NAME: Tag name."
+  (when (and tag-name (not (string-empty-p tag-name)))
+    (let (result)
+      (maphash
+       (lambda (id entity)
+         (when (and (eq (plist-get entity :type) :tag)
+                   (string= (plist-get entity :name) tag-name))
+           (setq result id)))
+       org-supertag-db--object)
+      result)))
+
+(defun org-supertag-tag-get-name-by-id (tag-id)
+  "Get the name of the tag with the given ID.
+TAG-ID: The ID of the tag."
+  (when tag-id
+    (let ((entity (gethash tag-id org-supertag-db--object)))
+      (when entity
+        (plist-get entity :name)))))
+
+(defun org-supertag-tag-create (tag-name &rest props)
+  "Create a new tag with a UUID.
+TAG-NAME: Name of the tag
+PROPS: Additional properties including:
+- :fields      List of field definitions
+- :behaviors   List of behaviors
+Returns the new tag's UUID."
+  (let* ((tag-id (org-id-new)) ; Generate a new UUID for the tag
+         (sanitized-name (org-supertag-sanitize-tag-name tag-name))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+         ;; Check if this is an extension tag
+         (parsed-name (org-supertag-tag--parse-name sanitized-name))
+         (base-tag-name (car parsed-name))
+         ;; Get new fields
+         (new-fields (plist-get props :fields))
+         ;; If it's an extension, get base tag fields
+<<<<<<< HEAD
+         (base-tag (and base-tag-name 
+=======
+         (base-tag (and base-tag-name
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+                       (org-supertag-tag-exists-p base-tag-name)
+                       (org-supertag-tag-get base-tag-name)))
+         ;; Combine fields
+         (fields (if base-tag
+                    ;; Project fields from base tag
+<<<<<<< HEAD
+                    (org-supertag-tag--project-fields 
+=======
+                    (org-supertag-tag--project-fields
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+                     base-tag
+                     new-fields
+                     (plist-get props :field-defaults))
+                  ;; Otherwise just use new fields
+                  new-fields))
+         ;; Ensure type is set
+         (base-props (list :type :tag
+<<<<<<< HEAD
+                          :id sanitized-name
+=======
+                          :id tag-id ; Use the new UUID
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+                          :name sanitized-name
+                          :fields fields
+                          :extend-from base-tag-name
+                          :behaviors (plist-get props :behaviors)
+                          :created-at (current-time))))
+<<<<<<< HEAD
+    
+    (message "Creating tag with fields: %S" fields)
+    (org-supertag-db-add sanitized-name base-props)
     sanitized-name))
+=======
+
+    (message "Creating tag '%s' with ID %s" sanitized-name tag-id)
+    (org-supertag-db-add tag-id base-props) ; Use UUID as key
+    tag-id)) ; Return the new UUID
+
+(defun org-supertag-tag-get-or-create (tag-name)
+  "Get a tag's UUID by its name, creating it if it doesn't exist.
+TAG-NAME: The name of the tag to get or create.
+Returns the tag's UUID."
+  (let ((sanitized-name (org-supertag-sanitize-tag-name tag-name)))
+    (or (org-supertag-tag-get-id-by-name sanitized-name)
+        (org-supertag-tag-create sanitized-name))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-tag--project-fields (base-tag new-fields defaults)
   "Project fields from base tag and merge with new fields.
@@ -98,34 +165,38 @@ NEW-FIELDS: Additional fields to add
 DEFAULTS: Plist of field default value overrides"
   (let* ((base-fields (plist-get base-tag :fields))
          result)
+    ;; Process base fields
     (dolist (field base-fields)
       (let* ((field-name (plist-get field :name))
              (default-override (plist-get defaults field-name))
              (projected-field
               (append 
-               (list :projected-from (plist-get base-tag :id))
+               (list :projected-from (plist-get base-tag :id))  ; Mark as projected
                (if default-override
                    (plist-put (copy-sequence field) 
-                              :default default-override)
-                 (copy-sequence field)))))
+                            :default default-override)
+                 (copy-sequence field)))))  ; Always copy field
         (unless (plist-get projected-field :type)
           (error "Field '%s' from base tag '%s' has no type defined"
                  field-name (plist-get base-tag :id)))
         (push projected-field result)))
     
+    ;; Add new fields
     (dolist (field new-fields)
       (let ((field-name (plist-get field :name)))
+        ;; Validate new field
         (unless field-name
           (error "New field must have a name"))
         (unless (plist-get field :type)
           (error "New field '%s' must have a type defined" field-name))
         
+        ;; Check for field name conflicts
         (when (cl-find field-name result
-                       :key (lambda (f) (plist-get f :name))
-                       :test #'equal)
+                      :key (lambda (f) (plist-get f :name))
+                      :test #'equal)
           (error "Field '%s' already exists in base tag '%s'"
                  field-name (plist-get base-tag :id)))
-        (push (copy-sequence field) result)))
+        (push (copy-sequence field) result)))  ; Always copy field
     
     (nreverse result)))
 
@@ -139,12 +210,17 @@ DEFAULTS: Plist of field default value overrides"
     (maphash
      (lambda (id entity)
        (when (and (eq (plist-get entity :type) :tag)
-                  (equal (org-supertag-tag-get-base id) base-tag-name))
+<<<<<<< HEAD
+                 (equal (org-supertag-tag-get-base id) base-tag-name))
+=======
+                 (equal (org-supertag-tag-get-base (plist-get entity :name)) base-tag-name))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
          (push id extensions)))
      org-supertag-db--object)
     extensions))
 
 (defun org-supertag-tag-get (tag-name)
+<<<<<<< HEAD
   "Get tag definition.
 TAG-NAME is the name of the tag to retrieve.
 Returns the tag entity if found and is a valid tag type,
@@ -153,6 +229,21 @@ otherwise returns nil."
     (when (and entity 
                (eq (plist-get entity :type) :tag))
       entity)))
+=======
+  "Get tag definition by its name.
+TAG-NAME is the name of the tag to retrieve.
+Returns the full tag property list if found, otherwise nil."
+  (when (and tag-name (not (string-empty-p tag-name)))
+    (let ((found-tag nil)
+          (sanitized-name (org-supertag-sanitize-tag-name tag-name)))
+      (maphash
+       (lambda (id entity)
+         (when (and (eq (plist-get entity :type) :tag)
+                   (string= (plist-get entity :name) sanitized-name))
+           (setq found-tag entity)))
+       org-supertag-db--object)
+      found-tag)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-get-all-tags ()
   "Get a list of all defined tags."
@@ -164,12 +255,6 @@ otherwise returns nil."
      org-supertag-db--object)
     (delete-dups tags)))
 
-(defun org-supertag-tag-get-id-by-name (name)
-  "Return the ID for a given tag NAME.
-In the current system, the sanitized name is the ID.
-This function is provided for compatibility."
-  (org-supertag-sanitize-tag-name name))
-
 ;;----------------------------------------------------------------------
 ;; Tag-Node Relation Operation
 ;;----------------------------------------------------------------------
@@ -186,36 +271,25 @@ This function is provided for compatibility."
 (defvar org-supertag-skip-text-insertion nil
   "When non-nil, skip text insertion when applying tags, only establish the relationship.")
 
-(defcustom org-supertag-inherit-parent-tags t
-  "When non-nil, automatically inherit all parent node tags to the child node (database only)."
-  :type 'boolean
-  :group 'org-supertag)
-
-(defun org-supertag-tag--inherit-parent-tags (node-id)
-  "Inherit all parent node tags to NODE-ID when `org-supertag-inherit-parent-tags` is non-nil.
-Only creates :node-tag links in database; no inline text insertion."  
-  (when (and org-supertag-inherit-parent-tags node-id)
-    (when-let* ((parent-tags (org-supertag-node-get-parent-tags node-id)))
-      (dolist (ptag parent-tags)
-        (unless (member ptag (org-supertag-node-get-tags node-id))
-          (org-supertag-node-db-add-tag node-id ptag))))))
-
-;; In org-supertag-tag-apply, after establishing the primary tag relationship, call inherit function.
 (defun org-supertag-tag-apply (tag-id)
-  "Apply tag to the node at current position.
-This is now the single entry point for applying a tag. It handles
-database relations, behavior execution, and finally text insertion."
-  (message "DEBUG: org-supertag-tag-apply called with tag-id=%s" tag-id)
+  "Apply tag to the node at current position."
   (let* ((tag (org-supertag-db-get tag-id))
-         (node-id (or org-supertag-force-node-id (org-entry-get nil "ID"))))
+<<<<<<< HEAD
+         ;; Use forced ID if provided
+         (node-id (or org-supertag-force-node-id 
+                      ;; Otherwise check ID in headline properties
+                      (org-entry-get nil "ID"))))
+    
+    ;; Only log debug info when debugging variable is set
+    (when org-supertag-tag-apply-skip-duplicate-id
+      (message "org-supertag-tag-apply: Initial node-id=%s (forced=%s)" 
+               node-id org-supertag-force-node-id))
     
+    ;; Only create new ID if none exists and we're not forcing a specific ID
     (setq node-id (or node-id (org-id-get-create)))
-    (message "DEBUG: applying to node-id=%s" node-id)
-
-    ;; Prevent applying a tag that already exists on the node.
-    (when (member tag-id (org-supertag-node-get-tags node-id))
-      (user-error "Tag '%s' is already applied to this node" tag-id))
     
+    (message "Applying tag: %s to node: %s" tag-id node-id)
+    ;; Validation
     (unless tag
       (error "Tag %s not found" tag-id))
     (unless (eq (plist-get tag :type) :tag)
@@ -223,73 +297,456 @@ database relations, behavior execution, and finally text insertion."
     (unless (org-supertag-db-get node-id)
       (org-supertag-node-sync-at-point))
     
+    ;; Link node and tag
     (org-supertag-node-db-add-tag node-id tag-id)
-    ;; inherit parent tags, database only
-    (org-supertag-tag--inherit-parent-tags node-id)
     
-    ;; Text insertion is now handled here, controlled by the same variable.
+    ;; Apply fields (skip if we're only establishing the relationship)
     (unless org-supertag-skip-text-insertion
-      (org-supertag-inline-insert-tag tag-id))
-
-    (when-let* ((fields (plist-get tag :fields)))
-      (dolist (field fields)
-        (let* ((field-name (plist-get field :name))
-               (field-type (plist-get field :type))
-               (type-def (org-supertag-get-field-type field-type))
-               (initial-value (org-supertag-field-get-initial-value field)))
-          (unless type-def
-            (error "Invalid field type: %s" field-type))
-
-          (when-let* ((formatter (plist-get type-def :formatter))
-                      (formatted-value (if formatter
+      (when-let* ((fields (plist-get tag :fields)))
+        (message "Processing fields for tag %s: %S" tag-id fields)
+=======
+         (tag-name (plist-get tag :name))
+         ;; Use forced ID if provided
+         (node-id (or org-supertag-force-node-id
+                      ;; Otherwise check ID in headline properties
+                      (org-entry-get nil "ID"))))
+
+    ;; Only log debug info when debugging variable is set
+    (when org-supertag-tag-apply-skip-duplicate-id
+      (message "org-supertag-tag-apply: Initial node-id=%s (forced=%s)"
+               node-id org-supertag-force-node-id))
+
+    ;; Only create new ID if none exists and we're not forcing a specific ID
+    (setq node-id (or node-id (org-id-get-create)))
+
+    (message "Applying tag: %s (%s) to node: %s" tag-name tag-id node-id)
+    ;; Validation
+    (unless tag
+      (error "Tag with ID %s not found" tag-id))
+    (unless (eq (plist-get tag :type) :tag)
+      (error "Invalid tag type for ID %s" tag-id))
+    (unless (org-supertag-db-get node-id)
+      (org-supertag-node-sync-at-point))
+
+    ;; Link node and tag
+    (org-supertag-node-db-add-tag node-id tag-id)
+
+    ;; Apply fields (skip if we're only establishing the relationship)
+    (unless org-supertag-skip-text-insertion
+      (when-let* ((fields (plist-get tag :fields)))
+        (message "Processing fields for tag %s: %S" tag-name fields)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+        (dolist (field fields)
+          (let* ((field-name (plist-get field :name))
+                 (field-type (plist-get field :type))
+                 (type-def (org-supertag-get-field-type field-type))
+                 (initial-value (org-supertag-field-get-initial-value field)))
+<<<<<<< HEAD
+            (message "Processing field: name=%s type=%s initial=%S" 
+=======
+            (message "Processing field: name=%s type=%s initial=%S"
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+                     field-name field-type initial-value)
+            (unless type-def
+              (error "Invalid field type: %s" field-type))
+
+            (when-let* ((formatter (plist-get type-def :formatter))
+                        (formatted-value (if formatter
                                            (funcall formatter initial-value field)
                                          (format "%s" initial-value))))
-            (org-supertag-tag--set-field-value 
-             tag-id node-id field-name initial-value)))))
-        
+              (message "Setting field %s = %s" field-name formatted-value)
+              (org-set-property field-name formatted-value)
+<<<<<<< HEAD
+              (org-supertag-tag--set-field-value 
+               tag-id node-id field-name initial-value))))))
+    
+    ;; Add tag to node tags (unless skipped for inline tags)
+    (unless (or org-supertag-tag-apply-skip-headline 
+                org-supertag-skip-text-insertion)
+      (let ((tags (org-get-tags)))
+        (org-set-tags (cons (concat "#" tag-id) tags))))
+    
+=======
+              (org-supertag-tag--set-field-value
+               tag-id node-id field-name initial-value))))))
+
+    ;; Add tag to node tags (unless skipped for inline tags)
+    (unless (or org-supertag-tag-apply-skip-headline
+                org-supertag-skip-text-insertion)
+      (let ((tags (org-get-tags)))
+        (org-set-tags (cons tag-name tags))))
+
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+    ;; Record tag relationships
     (when (featurep 'org-supertag-relation)
       (org-supertag-relation-record-cooccurrence node-id tag-id))
     
+    ;; Behavior Active
     (run-hook-with-args 'org-supertag-after-tag-apply-hook node-id)
     (org-supertag-behavior--on-tag-change node-id tag-id :add)
     (org-supertag-behavior--apply-styles node-id)
     
     node-id))
 
+
 (defun org-supertag-tag--remove (tag-id node-id)
   "Remove a tag from a node.
+<<<<<<< HEAD
 TAG-ID: The tag identifier
-NODE-ID: The node identifier"
+=======
+TAG-ID: The tag identifier (UUID)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+NODE-ID: The node identifier
+
+This function:
+1. Removes the tag-node relationship
+2. Removes all associated field values"
+  ;; 1. Remove tag-node relationship
   (org-supertag-db-remove-link :node-tag node-id tag-id)
+  ;; 2. Remove associated field values
+<<<<<<< HEAD
   (let ((tag (org-supertag-tag-get tag-id)))
+=======
+  (let ((tag (org-supertag-db-get tag-id)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
     (dolist (field-def (plist-get tag :fields))
       (org-supertag-field-remove-value field-def node-id tag-id))))
 
+
+(defun org-supertag-tag-change-tag ()
+  "Change an existing tag to another existing tag on current node.
+This function:
+1. Lists current node's tags for selection
+2. Lists available existing tags (excluding current node's tags) as target
+3. Removes old tag and applies new tag"
+  (interactive)
+  (let* ((node-id (org-id-get))
+<<<<<<< HEAD
+         (current-tags (org-supertag-node-get-tags node-id)))
+    
+    ;; Validation
+    (unless node-id
+      (error "Not on a valid node"))
+    (unless current-tags
+      (error "No tags on current node"))
+    
+    ;; Select source tag to change
+    (let* ((source-tag (completing-read "Select tag to change: " 
+                                      current-tags nil t))
+           ;; Get all existing tags except current node's tags
+           (all-tags (cl-set-difference 
+                     (org-supertag-get-all-tags)
+                     current-tags
+                     :test #'string=))
+=======
+         (current-tag-ids (org-supertag-node-get-tags node-id))
+         (current-tag-names (mapcar #'org-supertag-tag-get-name-by-id current-tag-ids)))
+
+    ;; Validation
+    (unless node-id
+      (error "Not on a valid node"))
+    (unless current-tag-ids
+      (error "No tags on current node"))
+
+    ;; Select source tag to change
+    (let* ((source-tag-name (completing-read "Select tag to change: "
+                                           current-tag-names nil t))
+           (source-tag-id (org-supertag-tag-get-id-by-name source-tag-name))
+           ;; Get all existing tags except current node's tags
+           (all-tags (cl-set-difference
+                      (org-supertag-get-all-tags)
+                      current-tag-names
+                      :test #'string=))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+           ;; Ensure we have available tags to change to
+           (_ (unless all-tags
+                (error "No other existing tags available")))
+           ;; Select target tag
+<<<<<<< HEAD
+           (target-tag (completing-read 
+                       (format "Change '%s' to: " source-tag)
+                       all-tags nil t)))
+      
+      ;; Execute tag change
+      (save-excursion
+        ;; 1. Remove old tag
+        (org-supertag-tag--remove source-tag node-id)
+        ;; Remove from org tags
+        (let* ((current-org-tags (org-get-tags))
+               (new-org-tags (delete (concat "#" source-tag) 
+                                   current-org-tags)))
+          (org-set-tags new-org-tags))
+        
+        ;; 2. Apply new tag
+        (org-supertag-tag-apply target-tag))
+      
+      (message "Changed tag '%s' to '%s'" source-tag target-tag))))
+=======
+           (target-tag-name (completing-read
+                             (format "Change '%s' to: " source-tag-name)
+                             all-tags nil t))
+           (target-tag-id (org-supertag-tag-get-or-create target-tag-name)))
+
+      ;; Execute tag change
+      (save-excursion
+        ;; 1. Remove old tag
+        (org-supertag-tag--remove source-tag-id node-id)
+        ;; Remove from org tags
+        (let* ((current-org-tags (org-get-tags))
+               (new-org-tags (delete source-tag-name
+                                     current-org-tags)))
+          (org-set-tags new-org-tags))
+
+        ;; 2. Apply new tag
+        (org-supertag-tag-apply target-tag-id))
+
+      (message "Changed tag '%s' to '%s'" source-tag-name target-tag-name))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
+
+
 ;;----------------------------------------------------------------------
-;; Tag User Command (Now data-layer functions)
+;; Tag User Command
 ;;----------------------------------------------------------------------
 
-(defun org-supertag-tag--delete-at-all (tag-id)
-  "Delete a tag completely from the database.
-This is the data-layer function. For interactive use, see `org-supertag-inline-delete-all`.
-TAG-ID: The tag identifier to delete."
-  (when (yes-or-no-p (format "Delete tag '%s' from DB? This is a data-only operation. " tag-id))
-    (let ((nodes (org-supertag-db-get-tag-nodes tag-id))
+(defun org-supertag-tag-add-tag (tag-name)
+  "Add a tag to the current headline.
+TAG-NAME can be an existing tag or a new tag name.
+Will prevent duplicate tag application.
+
+Special input format:
+- Normal input: show completion with existing and preset tags
+- Ending with #: directly create new tag without completion"
+  (interactive
+   (let* ((all-tags (org-supertag-get-all-tags))
+          (preset-names (mapcar #'car org-supertag-preset-tags))
+          ;; Remove any existing preset tags from all-tags to avoid duplicates
+          (user-tags (cl-remove-if (lambda (tag) (member tag preset-names)) all-tags))
+          (candidates (delete-dups
+                      (append 
+                       ;; Format preset tags with [P] prefix
+                       (mapcar (lambda (name) 
+                               (format "[P] %s" name))
+                             preset-names)
+                       ;; Regular tags as is
+                       user-tags)))
+          (input (completing-read
+                 "Enter tag name (TAB: complete, end with #: direct create): "
+                 candidates nil nil)))
+     (list
+      (cond
+       ((string-prefix-p "[P] " input)
+        (substring input 4))  ; Remove [P] prefix
+       (t input)))))
+  
+  (when tag-name  
+    (let* ((node-id (org-id-get))
+           ;; Check if input ends with # and remove it
+           (direct-create (string-suffix-p "#" tag-name))
+           (tag-name-clean (if direct-create
+                              (substring tag-name 0 -1)
+                            tag-name))
+           (sanitized-name (org-supertag-sanitize-tag-name tag-name-clean))
+<<<<<<< HEAD
+           (current-tags (org-supertag-node-get-tags node-id)))
+      (message "Adding tag: %s" sanitized-name)
+      ;; Check for duplicate tag
+      (when (member sanitized-name current-tags)
+        (user-error "Tag '%s' is already applied to this node" sanitized-name))
+      
+      (let* ((existing-tag (org-supertag-tag-get sanitized-name))
+             ;; Get or create the tag
+             (tag-id
+              (cond
+               ;; If tag exists, use it
+               (existing-tag
+                sanitized-name)
+               ;; If it's a preset tag, create with preset fields
+               ;; Otherwise create a new empty tag
+               (t
+                ;; If direct creation or confirmed, create tag
+                (if (or direct-create
+                        (y-or-n-p (format "Create new tag '%s'? " sanitized-name)))
+                    (org-supertag-tag-create sanitized-name)
+                  (user-error "Tag creation cancelled"))))))
+        ;; Apply the tag
+        (org-supertag-tag-apply tag-id)
+        ;; Skip immediate field value input for preset tags
+        (message "Tag '%s' applied. Use `org-supertag-tag-edit-fields' to edit field values." tag-id)))))
+=======
+           (current-tag-ids (org-supertag-node-get-tags node-id))
+           (current-tag-names (mapcar #'org-supertag-tag-get-name-by-id current-tag-ids)))
+      (message "Adding tag: %s" sanitized-name)
+      ;; Check for duplicate tag by name
+      (when (member sanitized-name current-tag-names)
+        (user-error "Tag '%s' is already applied to this node" sanitized-name))
+
+      (let* ((tag-id
+              (or (org-supertag-tag-get-id-by-name sanitized-name)
+                  (if (or direct-create
+                          (y-or-n-p (format "Create new tag '%s'? " sanitized-name)))
+                      (org-supertag-tag-create sanitized-name)
+                    (user-error "Tag creation cancelled")))))
+        ;; Apply the tag
+        (org-supertag-tag-apply tag-id)
+        ;; Skip immediate field value input for preset tags
+        (message "Tag '%s' applied. Use `org-supertag-tag-edit-fields' to edit field values." sanitized-name)))))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+
+(defun org-supertag-tag-batch-add-tag (tag-name)
+  "Batch add tags to selected headlines.
+TAG-NAME is the name of the tag to add."
+  (interactive
+   (list (completing-read "Select tag: "
+                         (append 
+                          (org-supertag-get-all-tags)
+                          (mapcar #'car org-supertag-preset-tags)))))
+  (let* ((ast (org-element-parse-buffer))
+         (headlines '())
+         (selected-headlines '())
+         (tag (org-supertag-tag-get tag-name)))
+    
+    ;; Collect all headlines without ID
+    (org-element-map ast 'headline
+      (lambda (headline)
+        (unless (org-element-property :ID headline)
+          (push headline headlines))))
+    
+    ;; Build choices list
+    (let ((choices (append 
+                   '(("Finish" . :finish))  ; Add finish option
+                   (mapcar (lambda (hl)
+                            (cons (org-element-property :raw-value hl)
+                                 hl))
+                          headlines))))
+      
+      ;; Loop until user selects Finish
+      (while (when-let* ((title (completing-read 
+                                (format "Select headline (%d selected): "
+                                       (length selected-headlines))
+                                (mapcar #'car choices)
+                                nil t))
+                         (choice (cdr (assoc title choices))))
+                (unless (eq choice :finish)
+                  ;; Add to selected list
+                  (push choice selected-headlines)
+                  ;; Remove selected option
+                  (setq choices (assoc-delete-all title choices))
+                  t)))  ; Continue loop unless Finish is selected
+      
+      ;; Apply tags to all selected headlines
+      (when selected-headlines
+        (dolist (hl selected-headlines)
+          (save-excursion
+            (goto-char (org-element-property :begin hl))
+            (org-supertag-tag-add-tag tag-name)))
+        (message "Added tag %s to %d headlines" 
+                 tag-name
+                 (length selected-headlines))))))
+
+(defun org-supertag-tag-remove ()
+  "Remove tag from current node.
+This function:
+1. Removes tag from org tags
+2. Calls internal function to remove tag data"
+  (interactive)
+  (let* ((node-id (org-id-get))
+<<<<<<< HEAD
+         (tags (org-supertag-node-get-tags node-id))
+         (tag-id (completing-read "Select tag to remove: " tags nil t)))
+    (unless node-id
+      (error "Not on a valid node"))
+    (unless tags
+      (error "No tags on current node"))
+    ;; 1. Remove from org tags
+    (let* ((current-tags (org-get-tags))
+           (tag-name (concat "#" tag-id))
+           (new-tags (delete tag-name current-tags)))
+      (org-set-tags new-tags))
+    ;; 2. Remove tag data using internal function
+    (org-supertag-tag--remove tag-id node-id)
+    (message "Removed tag '%s' and its fields" tag-id)))
+
+(defun org-supertag-tag-delete-at-all (tag-id)
+  "Delete a tag completely, including removing it from all nodes.
+TAG-ID: The tag identifier to delete.
+=======
+         (tag-ids (org-supertag-node-get-tags node-id))
+         (tag-names (mapcar #'org-supertag-tag-get-name-by-id tag-ids))
+         (tag-name-to-remove (completing-read "Select tag to remove: " tag-names nil t))
+         (tag-id-to-remove (org-supertag-tag-get-id-by-name tag-name-to-remove)))
+    (unless node-id
+      (error "Not on a valid node"))
+    (unless tag-ids
+      (error "No tags on current node"))
+    ;; 1. Remove from org tags
+    (let* ((current-tags (org-get-tags))
+           (new-tags (delete tag-name-to-remove current-tags)))
+      (org-set-tags new-tags))
+    ;; 2. Remove tag data using internal function
+    (org-supertag-tag--remove tag-id-to-remove node-id)
+    (message "Removed tag '%s' and its fields" tag-name-to-remove)))
+
+(defun org-supertag-tag-delete-at-all (tag-name)
+  "Delete a tag completely, including removing it from all nodes.
+TAG-NAME: The name of the tag to delete.
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+This will:
+1. Remove the tag from all nodes that have it
+2. Delete all field values associated with this tag
+3. Delete all database entries related to this tag
+4. Delete the tag definition from the database"
+  (interactive (list (completing-read "Tag to delete: " (org-supertag-get-all-tags))))
+<<<<<<< HEAD
+  (when (yes-or-no-p (format "Delete tag '%s'? This will remove it from all nodes. " tag-id))
+    (let ((nodes (org-supertag-tag-get-nodes tag-id))
           (tag (org-supertag-tag-get tag-id)))
       
+      ;; Validation
       (unless tag
         (error "Tag %s not found" tag-id))
       (unless (eq (plist-get tag :type) :tag)
         (error "Invalid tag type for %s" tag-id))
+=======
+  (when (yes-or-no-p (format "Delete tag '%s'? This will remove it from all nodes. " tag-name))
+    (let* ((tag-id (org-supertag-tag-get-id-by-name tag-name))
+           (nodes (org-supertag-tag-get-nodes tag-id))
+           (tag (org-supertag-db-get tag-id)))
+
+      ;; Validation
+      (unless tag-id
+        (error "Tag '%s' not found" tag-name))
+      (unless tag
+        (error "Tag with ID %s not found in DB" tag-id))
+      (unless (eq (plist-get tag :type) :tag)
+        (error "Invalid tag type for %s" tag-name))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
+      ;; 1. First remove tag from all nodes in org buffers
+      (message "Removing tag from %d nodes in buffers..." (length nodes))
       (dolist (node-id nodes)
-        (when-let* ((pos (condition-case nil (org-id-find node-id t) (error nil))))
+        ;; Remove from org buffer first
+        (when-let* ((pos (condition-case nil
+                           (org-id-find node-id t)
+                         (error nil))))
           (org-with-point-at pos
+            ;; Remove all field properties
             (dolist (field (plist-get tag :fields))
               (let ((field-name (plist-get field :name)))
-                (org-delete-property field-name))))))
+                (org-delete-property field-name)))
+            ;; Remove the tag from org tags
+            (let* ((current-tags (org-get-tags))
+<<<<<<< HEAD
+                   (new-tags (delete (concat "#" tag-id) current-tags)))
+=======
+                   (new-tags (delete tag-name current-tags)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
+              (org-set-tags new-tags)))))
       
+      ;; 2. Find and collect all relationships to be removed
+      (message "Collecting tag relationships...")
       (let ((links-to-remove '()))
+        ;; Find all tag links (node-tag, field values, etc.)
         (maphash (lambda (link-id props)
                    (when (or (equal (plist-get props :from) tag-id)
                              (equal (plist-get props :to) tag-id)
@@ -297,6 +754,8 @@ TAG-ID: The tag identifier to delete."
                      (push link-id links-to-remove)))
                  org-supertag-db--link)
         
+        ;; Remove all collected links using proper API
+        (message "Removing %d tag relationships..." (length links-to-remove))
         (dolist (link-id links-to-remove)
           (when-let* ((link-props (gethash link-id org-supertag-db--link)))
             (let ((link-type (plist-get link-props :type))
@@ -304,13 +763,27 @@ TAG-ID: The tag identifier to delete."
                   (to (plist-get link-props :to)))
               (org-supertag-db-remove-link link-type from to)))))
       
+      ;; 3. Remove the tag using proper database API
+      (message "Removing tag entity from database...")
+      (message "DEBUG: Before DB remove, tag exists: %s" (org-supertag-db-exists-p tag-id))
       (org-supertag-db-remove-object tag-id)
+      (message "DEBUG: After DB remove, tag exists: %s" (org-supertag-db-exists-p tag-id))
+      (message "DEBUG: Is DB dirty? %s" org-supertag-db--dirty)
+      
+      ;; 4. Force database save to persist changes
+      (message "Saving database...")
       (org-supertag-db--mark-dirty)
       (org-supertag-db-save)
       
-      (run-hook-with-args 'org-supertag-after-tag-delete-hook tag-id)
+      ;; 5. Run hooks if defined
+      (when (boundp 'org-supertag-after-tag-delete-hook)
+        (run-hook-with-args 'org-supertag-after-tag-delete-hook tag-id))
       
-      (message "Deleted tag '%s' and removed it from %d nodes" tag-id (length nodes)))))
+<<<<<<< HEAD
+      (message "Deleted tag '%s' and removed it from %d nodes" tag-id (length nodes))))) 
+=======
+      (message "Deleted tag '%s' and removed it from %d nodes" tag-name (length nodes))))) 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defcustom org-supertag-after-tag-delete-hook nil
   "Hook run after a tag is deleted.
@@ -319,122 +792,165 @@ The hook functions are called with one argument:
   :type 'hook
   :group 'org-supertag)
 
+
 ;;----------------------------------------------------------------------
 ;; Preset Tag
 ;;----------------------------------------------------------------------
 
 (defcustom org-supertag-preset-tags
-  '()
-  "Default preset tags with their field definitions."
+  '(("project" . ((:name "status"
+                    :type options 
+                    :options ("planning" "active" "on-hold" "completed" "cancelled")
+                    :description "Project status")
+                 (:name "priority"
+                    :type options
+                    :options ("high" "medium" "low") 
+                    :description "Priority level")
+                 (:name "deadline"
+                  :type date
+                  :description "Due date")
+                 (:name "owner"
+                    :type string
+                    :description "Project owner")))
+
+    ("task" . ((:name "status"
+                :type options
+                :options ("todo" "in-progress" "done" "cancelled")
+                :description "Task status")
+               (:name "priority" 
+                :type options
+                :options ("A" "B" "C")
+                :description "Priority level")
+               (:name "due"
+                :type date
+                :description "Due date")
+               (:name "assignee"
+                :type string
+                :description "Assigned to")))
+
+    ("person" . ((:name "role"
+                 :type string
+                 :description "Role")
+                (:name "email"
+                 :type string
+                 :description "Email address")
+                (:name "phone"
+                 :type string
+                 :description "Phone number")))
+
+    ("meeting" . ((:name "date"
+                  :type date
+                  :description "Meeting date")
+                 (:name "attendees"
+                  :type string
+                  :description "Attendees")
+                 (:name "location"
+                  :type string
+                  :description "Location")))
+
+    ("place" . ((:name "address"
+                :type string
+                :description "Address")
+               (:name "category"
+                :type options
+                :options ("office" "home" "public" "other")
+                :description "Place type")))
+
+    ("company" . ((:name "industry"
+                  :type string
+                  :description "Industry")
+                 (:name "website"
+                  :type string
+                  :description "Website")
+                 (:name "contact"
+                  :type string
+                  :description "Contact person")))
+
+    ("note" . ((:name "category"
+                :type options
+                :options ("idea" "reference" "summary" "other")
+                :description "Note type")
+               (:name "source"
+                :type string
+                :description "Source"))))
+  "Default preset tags with their field definitions.
+Users can override this by setting it in their config files.
+
+Format:
+((tag-name . field-definitions) ...)
+
+Each field definition is a plist with properties:
+- :name        Field name
+- :type        Field type (string, options, date, etc)
+- :options     List of options (for options type)
+- :description Field description
+
+Example user configuration:
+(setq org-supertag-preset-tags
+      (append org-supertag-preset-tags
+              '((\"book\" . ((:name \"status\"
+                             :type options
+                             :options (\"reading\" \"completed\" \"want-to-read\")
+                             :description \"Reading status\")
+                            (:name \"author\"
+                             :type string
+                             :description \"Author\"))))))"
   :type '(repeat (cons (string :tag "Tag name")
                       (repeat (plist :options ((:name string)
                                               (:type (choice (const string)
-                                                             (const options)
-                                                             (const date)))
+                                                            (const options)
+                                                            (const date)))
                                               (:options (repeat string))
                                               (:description string))))))
   :group 'org-supertag
-  :set (lambda (sym val) (set-default sym val)))
+  :set (lambda (sym val)
+         (set-default sym val)
+         ;; Optionally add hook to refresh tag cache if needed
+         ))
 
 (defun org-supertag-get-preset-fields (tag-name)
   "Get predefined fields for a tag.
-TAG-NAME is the name of the tag."
+TAG-NAME is the name of the tag.
+
+Returns a list of field definitions with the following properties:
+- :name        Field name (required)
+- :type        Field type (required)
+- :options     List of options (for options type)
+- :description Field description (optional)
+
+Example return value:
+((:name \"status\"
+  :type options
+  :options (\"todo\" \"in-progress\" \"done\")
+  :description \"Task status\")
+ (:name \"priority\"
+  :type options
+  :options (\"high\" \"medium\" \"low\")))"
   (when-let* ((preset (assoc tag-name org-supertag-preset-tags)))
     (let ((fields (cdr preset)))
-      (cl-loop for field in fields
-               collect (let* ((name (plist-get field :name))
-                              (type (plist-get field :type))
-                              (options (plist-get field :options))
-                              (description (plist-get field :description)))
-                         (unless (and name type)
-                           (error "Invalid field definition in preset tag %s: missing name or type" tag-name))
-                         (append
-                          (list :name name
-                                :type type)
-                          (when description
-                            (list :description description))
-                          (when (and options (eq type 'options))
-                            (list :options options))))))))
-
-(defun org-supertag-tag--set-field-value (tag-id node-id field-name value)
-  "Helper function to set a field value in the database."
-  (let* ((tag (org-supertag-tag-get tag-id))
-         (field-def (cl-find field-name (plist-get tag :fields)
-                             :key (lambda (f) (plist-get f :name))
-                             :test #'string=)))
-    (when field-def
-      (org-supertag-field-set-value node-id field-name value tag-id))))
-
-(defun org-supertag-tag-add-field (tag-id field-def)
-  "Add a new field definition to a tag.
-TAG-ID is the ID of the tag.
-FIELD-DEF is the new field definition plist."
-  (when-let ((tag (org-supertag-db-get tag-id)))
-    (let* ((fields (plist-get tag :fields))
-           (new-fields (append fields (list field-def)))
-           (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
-      (org-supertag-db-add tag-id new-tag)
-      (message "Field added to tag '%s'." tag-id))))
-
-(defun org-supertag-tag-rename-field (tag-id old-name new-name)
-  "Rename a field in a tag's definition.
-TAG-ID is the ID of the tag.
-OLD-NAME is the current name of the field.
-NEW-NAME is the new name for the field."
-  (when-let ((tag (org-supertag-db-get tag-id)))
-    (let* ((fields (plist-get tag :fields))
-           (new-fields (mapcar (lambda (f)
-                                 (if (string= (plist-get f :name) old-name)
-                                     (plist-put (copy-sequence f) :name new-name)
-                                   f))
-                               fields))
-           (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
-      (org-supertag-db-add tag-id new-tag)
-      (message "Field '%s' renamed to '%s' in tag '%s'." old-name new-name tag-id))))
-
-(defun org-supertag-tag--rename (old-name new-name)
-  "Rename a tag in the database.
-This is a data-layer function. Use `org-supertag-inline-rename` for interactive use.
-OLD-NAME: The current name of the tag.
-NEW-NAME: The new name for the tag."
-  (unless (string-to-multibyte new-name)
-    (error "New tag name '%s' contains invalid characters" new-name))
-  (let* ((old-tag (org-supertag-tag-get old-name))
-         (new-tag-name (org-supertag-sanitize-tag-name new-name)))
-    (when (org-supertag-tag-get new-tag-name)
-      (error "Tag %s already exists" new-tag-name))
-    (unless old-tag
-      (error "Tag %s not found" old-name))
-    (org-supertag-db-rename old-name new-tag-name)))
-
-(defun org-supertag-get-all-tags-with-prefix (prefix)
-  "Get all tags starting with a specific prefix."
-  (let ((tags (org-supertag-get-all-tags))
-        (result '()))
-    (dolist (tag tags result)
-      (when (string-prefix-p prefix tag)
-        (push tag result)))))
-
-(defun org-supertag-tag-select-by-prefix (prefix)
-  "Select a tag from a list of tags starting with a specific prefix.
-PREFIX: The prefix to filter tags by."
-  (interactive
-   (list (read-string "Prefix: ")))
-  (let* ((tags (org-supertag-get-all-tags-with-prefix prefix))
-         (selected-tag (completing-read "Select tag: " tags nil t)))
-    (message "Selected tag: %s" selected-tag)))
-
-(defun org-supertag-set-field-and-value (tag field value)
-  "Set field and value for a tag.
-This is a convenience function for interactive use."
-  (interactive
-   (let* ((all-tags (org-supertag-get-all-tags))
-          (tag (completing-read "Select tag: " all-tags nil t))
-          (fields (mapcar (lambda (f) (plist-get f :name))
-                          (plist-get (org-supertag-tag-get tag) :fields)))
-          (field (completing-read "Select field: " fields nil t))
-          (value (read-string (format "Enter value for %s: " field))))
-     (list tag field value)))
-  (org-supertag-tag--set-field-value tag (org-id-get) field value))
+          ;; Validate and normalize field definitions
+          (cl-loop for field in fields
+                   collect (let* ((name (plist-get field :name))
+                                 (type (plist-get field :type))
+                                (options (plist-get field :options))
+                                (description (plist-get field :description)))
+                                
+                                ;; Ensure required fields are present
+                                (unless (and name type)
+                                  (error "Invalid field definition in preset tag %s: missing name or type" tag-name))
+                                
+                                ;; Build normalized field definition
+                                (append
+                                 (list :name name
+                                       :type type)
+                                 ;; Add optional properties
+                                 (when description
+                                   (list :description description))
+                                 (when (and options (eq type 'options))
+                                   (list :options options))))))))
+
+
+(defalias 'org-supertag-set-field-and-value 'org-supertag-tag-edit-fields)
+
 
 (provide 'org-supertag-tag)
diff --git a/org-supertag-ui-archaeology.el b/org-supertag-ui-archaeology.el
new file mode 100755
index 0000000..d1e2250
--- /dev/null
+++ b/org-supertag-ui-archaeology.el
@@ -0,0 +1,54 @@
+;;; org-supertag-ui-archaeology.el --- UI for knowledge archaeology features -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file provides the user interface for the Knowledge Archaeology
+;; feature, allowing users to \"dig\" for the history of a concept.
+
+;;; Code:
+
+(require 'org-supertag-api)
+(require 'org-supertag-ui-sidebar)
+(require 'seq)
+
+;; --- UI Formatting ---
+
+(defun org-supertag-ui-archaeology--format-results (response)
+  "Format the archaeology dig RESPONSE into a string for display."
+  (let ((results (if (and (listp response) (eq (car response) 'success))
+                     (plist-get (cdr response) :result)
+                   nil)))
+    (if (or (null results) (seq-empty-p results))
+        (insert (propertize "No relevant historical notes found for this topic." 'face 'italic))
+      (dolist (node-info results)
+        (let ((title (plist-get node-info :title))
+              (date (plist-get node-info :document_date)))
+          (insert (propertize (format "▪ %s (%s)\n" (or title "Untitled") (or date "No Date"))
+                              'face 'default)))))))
+
+;;;###autoload
+(defun org-supertag-ui-archaeology-dig ()
+  "Interactively perform a knowledge archaeology dig and display results."
+  (interactive)
+  (let ((query (read-string "Knowledge Archaeology - Dig for: ")))
+    (unless (string-empty-p query)
+      (org-supertag-ui-sidebar-show) ; Ensure sidebar is visible
+      (org-supertag-ui-sidebar-render
+       :archaeology
+       (lambda ()
+         (insert (propertize (format "Digging for '%s'..." query) 'face 'italic)))
+       t) ; Clear section and show placeholder
+
+      (org-supertag-api-knowledge-archaeology-dig
+       query
+       (lambda (response)
+         (org-supertag-ui-sidebar-render
+          :archaeology
+          (lambda ()
+            (if (and (listp response) (eq (car response) :error))
+                (insert (propertize (format "Error during dig: %s" (plist-get response :message)) 'face 'error))
+              (org-supertag-ui-archaeology--format-results response)))
+          t))))))
+
+
+(provide 'org-supertag-ui-archaeology)
+;;; org-supertag-ui-archaeology.el ends here 
\ No newline at end of file
diff --git a/org-supertag-ui-conversation.el b/org-supertag-ui-conversation.el
new file mode 100755
index 0000000..83ba596
--- /dev/null
+++ b/org-supertag-ui-conversation.el
@@ -0,0 +1,130 @@
+;;; org-supertag-ui-conversation.el --- UI for conversational features -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file provides the user interface for the conversational AI,
+;; including asking questions, changing modes, and viewing history.
+
+;;; Code:
+
+(require 'org-supertag-api)
+(require 'org-supertag-ui-sidebar)
+(require 'org-supertag-ui-archaeology) ;; For the /d slash command
+
+(defvar org-supertag-ui-conversation-available-modes '("Normal" "Socratic")
+  "A list of available conversational AI modes.")
+
+(defvar org-supertag-ui-conversation--keymap
+  (let ((map (make-sparse-keymap)))
+    (define-key map (kbd "/") 'org-supertag-ui-conversation-handle-slash-command)
+    map)
+  "Keymap for the RAG dialogue input area.")
+
+
+;; --- Conversational AI Modes ---
+
+(defun org-supertag-ui-conversation-set-mode ()
+  "Interactively set the conversational AI mode for the current session."
+  (interactive)
+  (let* ((session-id (with-current-buffer (org-supertag-ui-sidebar--get-buffer)
+                       (org-supertag-ui-sidebar-get-session-id)))
+         (mode (completing-read "Set Dialogue Mode: " org-supertag-ui-conversation-available-modes nil t nil nil "Normal")))
+    (when (and mode (member mode org-supertag-ui-conversation-available-modes))
+      (org-supertag-api-set-dialogue-mode
+       session-id (downcase mode)
+       (lambda (response)
+         (let ((message
+                (if (and (listp response) (eq (car response) :error))
+                    (format "[SYSTEM_ERROR]: Could not set mode - %s" (plist-get response :message))
+                  (format "[SYSTEM]: Dialogue mode set to %s." mode))))
+           (org-supertag-ui-sidebar-render :dialogue (propertize (concat message "\n") 'face 'italic))))))))
+
+(defun org-supertag-ui-conversation-handle-slash-command ()
+  "Handle slash commands entered in the dialogue window."
+  (interactive)
+  (let* ((command-char (read-char "Slash command: "))
+         (command-str (downcase (string command-char))))
+    (cond
+     ((string= command-str "m") ; /m for mode
+      (call-interactively 'org-supertag-ui-conversation-set-mode))
+     ((string= command-str "d") ; /d for dig
+      (call-interactively 'org-supertag-ui-archaeology-dig))
+     (t (message "Unknown slash command: /%c" command-char)))))
+
+
+;; --- RAG Query UI ---
+
+(defun org-supertag-ui-conversation--format-query-response (response query-string)
+  "Format the RESPONSE from an API query and append it to the sidebar."
+  (let* ((is-error (and (listp response) (eq (car response) :error)))
+         (ai-text (if is-error
+                      (plist-get response :message)
+                    (if (and (listp response) (plist-get response :answer))
+                        (plist-get response :answer)
+                      (format "%S" response))))
+         (metadata (unless is-error (plist-get response :metadata)))
+         (hint (and metadata (plist-get metadata :hint)))
+         (mode (and metadata (plist-get metadata :mode)))
+         (formatted-message
+          (propertize (format "[AI - %s]: %s\n" (or mode "response") ai-text) 'face 'font-lock-function-name-face)))
+
+    (org-supertag-ui-sidebar-render :dialogue formatted-message)
+
+    (when (and hint (not (string-empty-p hint)))
+      (let* ((separator (propertize (format "\n%s\n" (make-string 35 ?─)) 'face 'font-lock-comment-face))
+             (hint-header (propertize "💡 Hint from your notes:\n" 'face '(:inherit font-lock-comment-face :weight bold)))
+             (hint-body (propertize (format "%s\n" hint) 'face 'font-lock-string-face)))
+        (org-supertag-ui-sidebar-render :dialogue (concat separator hint-header hint-body))))))
+
+(defun org-supertag-ui-conversation-ask-question (query-string)
+  "Send QUERY-STRING to the RAG backend and display in the sidebar."
+  (interactive (list (read-string "RAG Query: ")))
+  (org-supertag-ui-sidebar-show)
+  (let* ((session-id (with-current-buffer (org-supertag-ui-sidebar--get-buffer)
+                       (org-supertag-ui-sidebar-get-session-id)))
+         (user-message (propertize (format "[USER]: %s\n" query-string) 'face 'bold)))
+    (org-supertag-ui-sidebar-render :dialogue user-message)
+
+    (org-supertag-api-ask-question
+     query-string session-id
+     (lambda (response)
+       (org-supertag-ui-conversation--format-query-response response query-string)))))
+
+
+;; --- Dialogue History UI ---
+
+(defun org-supertag-ui-conversation--format-history (history-response)
+  "Format the HISTORY-RESPONSE for display."
+  (if (null history-response)
+      (insert (propertize "[AI]: No dialogue history data received.\n" 'face 'italic))
+    (let ((turns (if (listp history-response) (plist-get history-response :turns) nil)))
+      (if (or (null turns) (seq-empty-p turns))
+          (insert (propertize "[AI]: No dialogue turns in history record.\n" 'face 'italic))
+        (dolist (turn turns)
+          (let* ((speaker (plist-get turn :speaker))
+                 (text (plist-get turn :text))
+                 (face (if (string= speaker "user") 'bold 'font-lock-function-name-face)))
+            (insert (propertize (format "[%s]: %s\n" (upcase speaker) text) 'face face))))))))
+
+(defun org-supertag-ui-conversation-show-history (&optional count)
+  "Retrieve and display dialogue history in the sidebar's dialogue window."
+  (interactive "P")
+  (org-supertag-ui-sidebar-show)
+  (let ((session-id (with-current-buffer (org-supertag-ui-sidebar--get-buffer)
+                      (org-supertag-ui-sidebar-get-session-id))))
+    (org-supertag-ui-sidebar-render
+     :dialogue (lambda () (insert (propertize "[AI]: Fetching dialogue history...\n" 'face 'italic))) t)
+
+    (org-supertag-api-get-dialogue-history
+     session-id (if (integerp count) count nil)
+     (lambda (response)
+       (org-supertag-ui-sidebar-render
+        :dialogue
+        (lambda ()
+          (if (and (listp response) (eq (car response) :error))
+              (insert (propertize (format "[AI_ERROR]: Could not retrieve history - %s\n" (plist-get response :message)) 'face 'error))
+            (org-supertag-ui-conversation--format-history response)))
+        t)))))
+
+
+(provide 'org-supertag-ui-conversation)
+;;; org-supertag-ui-conversation.el ends here 
\ No newline at end of file
diff --git a/org-supertag-ui-memory.el b/org-supertag-ui-memory.el
new file mode 100755
index 0000000..8afd6de
--- /dev/null
+++ b/org-supertag-ui-memory.el
@@ -0,0 +1,68 @@
+;;; org-supertag-ui-memory.el --- UI for memory synthesis features -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file provides the user interface for reviewing and managing
+;; candidate memories synthesized by the MemorySynthesizer.
+
+;;; Code:
+
+(require 'org-supertag-api)
+(require 'org-supertag-ui-sidebar)
+
+;; --- UI Formatting ---
+
+(defun org-supertag-ui-memory--format-candidates (candidates)
+  "Format memory CANDIDATES for display in the sidebar.
+This function is designed to be called by `org-supertag-ui-sidebar-render`."
+  (if (seq-empty-p candidates)
+      (insert "  (No pending memory candidates to review.)\n")
+    (dolist (candidate candidates)
+      (let* ((candidate-id (plist-get candidate :candidate_id))
+             (proposed-item (plist-get candidate :proposed_item))
+             (item-content (format "%S" (plist-get proposed-item :content))) ; Pretty-print content
+             (justification (plist-get candidate :justification))
+             (confidence (plist-get candidate :confidence_score)))
+
+        (insert (propertize (format "  - Proposed Memory (Confidence: %.2f):\n" confidence)
+                            'face '(:weight bold :foreground "orange")))
+        (insert (format "    Content: %s\n" item-content))
+        (insert (format "    Justification: %s\n" justification))
+        (insert "    ")
+        ;; Accept button
+        (insert (propertize "[Accept]" 'face '(:foreground "green" :underline t)
+                            'action (lambda (_)
+                                      (org-supertag-api-process-candidate-memory candidate-id "accept")
+                                      (message "Accepted memory candidate: %s" candidate-id)
+                                      (org-supertag-ui-memory-show-candidates))
+                            'follow-link t
+                            'help-echo "Click to accept and store this memory."))
+        (insert " ")
+        ;; Reject button
+        (insert (propertize "[Reject]" 'face '(:foreground "red" :underline t)
+                            'action (lambda (_)
+                                      (org-supertag-api-process-candidate-memory candidate-id "reject")
+                                      (message "Rejected memory candidate: %s" candidate-id)
+                                      (org-supertag-ui-memory-show-candidates))
+                            'follow-link t
+                            'help-echo "Click to reject this memory candidate."))
+        (insert "\n\n")))))
+
+;;;###autoload
+(defun org-supertag-ui-memory-show-candidates ()
+  "Fetch and display pending memory candidates for user review in the sidebar."
+  (interactive)
+  (let ((response (org-supertag-api-get-candidate-memories)))
+    (org-supertag-ui-sidebar-show) ; Ensure sidebar is visible
+    (org-supertag-ui-sidebar-render
+     :memory
+     (lambda ()
+       (if (and (eq (car response) 'success)
+                (plist-get (cdr response) :result))
+           (let ((candidates (plist-get (cdr response) :result)))
+             (org-supertag-ui-memory--format-candidates candidates))
+         (insert (propertize "Error fetching memory candidates.\n" 'face '(:foreground "red")))
+         (insert (format "%S" response))))
+     t))) ; t means clear the section
+
+(provide 'org-supertag-ui-memory)
+;;; org-supertag-ui-memory.el ends here 
\ No newline at end of file
diff --git a/org-supertag-ui-ner.el b/org-supertag-ui-ner.el
new file mode 100755
index 0000000..46466ed
--- /dev/null
+++ b/org-supertag-ui-ner.el
@@ -0,0 +1,126 @@
+;;; org-supertag-ui-ner.el --- UI for NER-based features -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file provides the UI for NER-based features, including
+;; suggesting tags and discovering relationships from note content.
+
+;;; Code:
+
+(require 'org-supertag-api)
+(require 'org-supertag-db)
+(require 'org-supertag-ui-sidebar)
+(require 'org-supertag-util)
+
+;; --- Keymap and Dispatcher ---
+
+(defvar org-supertag-ui-ner--suggestion-keymap
+  (let ((map (make-sparse-keymap)))
+    (define-key map [mouse-1] 'org-supertag-ui-ner--dispatch-action)
+    (define-key map (kbd "RET") 'org-supertag-ui-ner--dispatch-action)
+    map)
+  "Keymap for interactive suggestions in the sidebar.")
+
+(defun org-supertag-ui-ner--dispatch-action ()
+  "Dispatch the action stored in text properties at point."
+  (interactive)
+  (let ((action (get-text-property (point) 'action)))
+    (when (functionp action)
+      (funcall action))))
+
+
+;; --- Tag Suggestion UI ---
+
+(defun org-supertag-ui-ner--format-tag-suggestions (suggestions current-note-tags)
+  "Format tag SUGGESTIONS for display."
+  (insert (propertize "NER Tag Suggestions:\n" 'face '(:weight bold)))
+  (if (seq-empty-p suggestions)
+      (insert "  (No suggestions found.)\n")
+    (dolist (suggestion suggestions)
+      (let* ((tag-name (plist-get suggestion :tag))
+             (confidence (plist-get suggestion :confidence))
+             (reason (plist-get suggestion :reason))
+             (already-exists (member tag-name current-note-tags)))
+
+        (insert (format "  %s %s (Confidence: %.2f)\n" (if already-exists "✓" "＋") tag-name confidence))
+        (insert (format "    Reason: %s\n" reason))
+        (unless already-exists
+          (insert "    ")
+          (insert (propertize "[Accept]"
+                              'face 'link
+                              'action (lambda () (org-supertag-ui-ner--accept-tag-suggestion tag-name))
+                              'help-echo (format "Click to add tag '%s'" tag-name)))
+          (insert "\n")))
+      (insert "\n"))))
+
+(defun org-supertag-ui-ner--accept-tag-suggestion (tag-name)
+  "Handle accepting a tag suggestion."
+  (interactive)
+  (let ((node-id (org-id-get-create)))
+    (org-supertag-add tag-name (list node-id))
+    (message "Tag '%s' added to node %s." tag-name node-id)
+    (org-supertag-ui-ner-suggest-tags))) ; Refresh view
+
+(defun org-supertag-ui-ner-suggest-tags ()
+  "Fetch and display NER-based tag suggestions for the current note."
+  (interactive)
+  (let* ((note-content (org-supertag-util-get-node-content-for-llm))
+         (current-tags (org-get-tags-at (point)))
+         (payload `((note_content . ,note-content) (existing_tags . ,current-tags))))
+    (org-supertag-ui-sidebar-show)
+    (org-supertag-ui-sidebar-render
+     :ner-tags
+     (lambda () (insert "  (Fetching NER suggestions...)\n"))
+     t)
+    (let* ((response (org-supertag-api-get-tag-relationship-suggestions payload))
+           (suggestions (and (eq (car response) 'success) (plist-get (cdr response) :result))))
+      (org-supertag-ui-sidebar-render
+       :ner-tags
+       (lambda () (org-supertag-ui-ner--format-tag-suggestions suggestions current-tags))
+       t))))
+
+
+;; --- Relationship Suggestion UI ---
+
+(defun org-supertag-ui-ner--format-relationship-suggestions (relations)
+  "Format relationship SUGGESTIONS for display."
+  (insert (propertize "Inferred Relationships:\n" 'face '(:weight bold)))
+  (if (seq-empty-p relations)
+      (insert "  (No relationships inferred.)\n")
+    (dolist (relation relations)
+      (let ((from (plist-get relation :from_tag))
+            (to (plist-get relation :to_tag))
+            (type (plist-get relation :relation_type)))
+        (insert (format "  %s → %s (%s)\n" from to type))
+        (insert "    ")
+        (insert (propertize "[Create Relation]"
+                            'face 'link
+                            'action (lambda () (org-supertag-ui-ner--accept-relationship-suggestion from to type))
+                            'help-echo (format "Create '%s -> %s' relationship" from to)))
+        (insert "\n\n")))))
+
+(defun org-supertag-ui-ner--accept-relationship-suggestion (from to type)
+  "Handle accepting a relationship suggestion."
+  (interactive)
+  (org-supertag-db-add-relation from to type)
+  (message "Relation '%s -> %s (%s)' created." from to type)
+  (org-supertag-ui-ner-discover-relationships)) ; Refresh view
+
+(defun org-supertag-ui-ner-discover-relationships ()
+  "Fetch and display inferred relationships for the current note."
+  (interactive)
+  (let ((note-content (org-supertag-util-get-node-content-for-llm)))
+    (org-supertag-ui-sidebar-show)
+    (org-supertag-ui-sidebar-render
+     :ner-rels
+     (lambda () (insert "  (Discovering relationships...)\n"))
+     t)
+    (let* ((response (org-supertag-api-discover-inferred-relationships `((note_content . ,note-content))))
+           (relations (and (eq (car response) 'success) (plist-get (cdr response) :result))))
+      (org-supertag-ui-sidebar-render
+       :ner-rels
+       (lambda () (org-supertag-ui-ner--format-relationship-suggestions relations))
+       t))))
+
+
+(provide 'org-supertag-ui-ner)
+;;; org-supertag-ui-ner.el ends here 
\ No newline at end of file
diff --git a/org-supertag-ui-sidebar.el b/org-supertag-ui-sidebar.el
new file mode 100755
index 0000000..b495fa2
--- /dev/null
+++ b/org-supertag-ui-sidebar.el
@@ -0,0 +1,157 @@
+;;; org-supertag-ui-sidebar.el --- Core management for the UI sidebar -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file provides the core, content-agnostic functionality for managing
+;; the org-supertag \"Intelligent Sidebar\". It handles window creation,
+;; buffer management, and the structure of sections, but does not handle
+;; the content of those sections.
+
+;;; Code:
+
+(require 'org-supertag-node)
+(require 'org-supertag-db)
+(require 'window)
+
+;; --- Variables ---
+
+(defcustom org-supertag-ui-sidebar-buffer-name "*org-supertag-intelligent-sidebar*"
+  "Name of the buffer used for the RAG intelligent sidebar."
+  :type 'string
+  :group 'org-supertag)
+
+(defcustom org-supertag-ui-sidebar-window-width 45
+  "Width of the sidebar window in characters."
+  :type 'integer
+  :group 'org-supertag)
+
+(defvar org-supertag-ui-sidebar-sections
+  '((:dialogue . "💬 对话窗口")
+    (:nodes . "🔗 关联节点")
+    (:ner-tags . "📝 建议标签")
+    (:ner-rels . "🔗 建议关系")
+    (:archaeology . "🏺 知识考古")
+    (:memory . "🧠 候选记忆"))
+  "Alist defining the sections of the sidebar. (id . \"Display Title\")")
+
+(defvar-local org-supertag-ui-sidebar--session-id nil
+  "Buffer-local variable to hold the unique session ID for the RAG dialogue.")
+
+;; --- Session Management ---
+
+(defun org-supertag-ui-sidebar-get-session-id ()
+  "Get or create a unique session ID for the current RAG buffer.
+The ID is stored in a buffer-local variable `org-supertag-ui-sidebar--session-id`."
+  (unless org-supertag-ui-sidebar--session-id
+    (setq-local org-supertag-ui-sidebar--session-id
+                (format "emacs-session-%s-%s"
+                        (user-login-name)
+                        (replace-regexp-in-string
+                         "[^a-zA-Z0-9-]" ""
+                         (format "%s" (current-time))))))
+  org-supertag-ui-sidebar--session-id)
+
+
+;; --- Sidebar Management ---
+
+(defun org-supertag-ui-sidebar--get-buffer (&optional force-create)
+  "Get or create the RAG sidebar buffer.
+If FORCE-CREATE is non-nil, create a new buffer even if one exists."
+  (let ((buf (get-buffer org-supertag-ui-sidebar-buffer-name)))
+    (when (or force-create (not buf) (not (buffer-live-p buf)))
+      (setq buf (get-buffer-create org-supertag-ui-sidebar-buffer-name))
+      (with-current-buffer buf
+        (setq-local truncate-lines t)
+        (erase-buffer)
+        (org-supertag-ui-sidebar-get-session-id) ; Initialize session ID
+        (dolist (section org-supertag-ui-sidebar-sections)
+          (let ((title (cdr section)))
+            (insert (propertize (format "╭─ %s " title) 'face 'font-lock-comment-face))
+            (insert (propertize (make-string (max 0 (- org-supertag-ui-sidebar-window-width 4 (length title))) ?─) 'face 'font-lock-comment-face))
+            (insert (propertize "╮\n" 'face 'font-lock-comment-face))
+            (insert "│\n") ; Placeholder for content
+            (insert (propertize (format "╰%s╯\n\n" (make-string (max 0 (- org-supertag-ui-sidebar-window-width 2)) ?─)) 'face 'font-lock-comment-face)))))))
+    buf)
+
+(defun org-supertag-ui-sidebar-show ()
+  "Display the intelligent sidebar in a side window."
+  (interactive)
+  (let ((sidebar-buffer (org-supertag-ui-sidebar--get-buffer)))
+    (display-buffer-in-side-window sidebar-buffer
+                                   `((side . left)
+                                     (window-width . ,org-supertag-ui-sidebar-window-width)
+                                     (slot . 0)
+                                     (window-parameters . ((no-other-window . t)
+                                                           (mode-line-format . none)
+                                                           (header-line-format . "🧠 Living Document Intelligence")))))))
+
+(defun org-supertag-ui-sidebar--find-section-start (buffer section-title)
+  "Find the starting point (after title) of a SECTION-TITLE in BUFFER."
+  (with-current-buffer buffer
+    (goto-char (point-min))
+    (when (search-forward (format "╭─ %s " section-title) nil t)
+      (line-end-position))))
+
+(defun org-supertag-ui-sidebar--find-section-end (buffer section-start-pos)
+  "Find the end point (before next section or buffer end) of a section in BUFFER."
+  (with-current-buffer buffer
+    (goto-char section-start-pos)
+    (save-excursion
+      (if (search-forward "\n╭─ " nil t) ; Start of next section
+          (match-beginning 0)
+        (point-max))))) ; End of buffer if no next section
+
+(defun org-supertag-ui-sidebar-render (section-id content &optional clear-section-p)
+  "Render CONTENT into a specific SECTION-ID in the sidebar.
+SECTION-ID is a keyword like :dialogue, :tags, :nodes.
+If CLEAR-SECTION-P is non-nil, existing content in that section is cleared first.
+CONTENT can be a string, or a function that inserts content."
+  (let* ((sidebar-buffer (org-supertag-ui-sidebar--get-buffer))
+         (section-info (assoc section-id org-supertag-ui-sidebar-sections))
+         (section-title (cdr section-info)))
+    (unless section-info
+      (error "Unknown sidebar section ID: %S" section-id))
+    (with-current-buffer sidebar-buffer
+      (let ((inhibit-read-only t)
+            (start-point (org-supertag-ui-sidebar--find-section-start sidebar-buffer section-title))
+            (end-point))
+        (unless start-point
+          (error "Could not find section '%s' in sidebar" section-title))
+
+        (goto-char start-point)
+        (forward-line 1)
+        (setq start-point (point))
+
+        (setq end-point (save-excursion
+                          (search-forward-regexp (format "^╰%s╯" (make-string (max 0 (- org-supertag-ui-sidebar-window-width 2)) ?─)) nil t)
+                          (match-beginning 0)))
+
+        (when clear-section-p
+          (delete-region start-point end-point))
+
+        (goto-char end-point)
+        (unless (eq (char-before end-point) ?\n) (insert "\n"))
+        
+        (if (functionp content)
+            (funcall content)
+          (insert content))
+        
+        (unless (eq (char-before) ?\n) (insert "\n"))
+
+        (when (get-buffer-window sidebar-buffer)
+          (with-selected-window (get-buffer-window sidebar-buffer)
+            (set-window-point (selected-window) (point-max))))))))
+
+(defun org-supertag-ui-sidebar--insert-section (title content)
+  "Insert a foldable section with TITLE and CONTENT into the current buffer."
+  (let ((inhibit-read-only t))
+    (insert (propertize (format "▼ %s\n" title)
+                        'face 'font-lock-comment-face))
+    (insert (propertize (make-string (max 0 (- org-supertag-ui-sidebar-window-width 4 (length title))) ?─) 'face 'font-lock-comment-face))
+    (insert (propertize "╮\n" 'face 'font-lock-comment-face))
+    (insert "│\n") ; Placeholder for content
+    (insert (propertize (format "╰%s╯\n\n" (make-string (max 0 (- org-supertag-ui-sidebar-window-width 2)) ?─)) 'face 'font-lock-comment-face))
+    (insert content)
+    (insert "\n")))
+
+(provide 'org-supertag-ui-sidebar)
+;;; org-supertag-ui-sidebar.el ends here 
\ No newline at end of file
diff --git a/org-supertag-ui.el b/org-supertag-ui.el
new file mode 100755
index 0000000..17ff4ac
--- /dev/null
+++ b/org-supertag-ui.el
@@ -0,0 +1,55 @@
+;;; org-supertag-ui.el --- Main entry point for the org-supertag UI -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file is the main entry point for all org-supertag user interfaces.
+;; It loads all the individual UI modules and provides a single, unified
+;; command to refresh the entire intelligent sidebar.
+
+;;; Code:
+
+(require 'org-supertag-ui-sidebar)
+(require 'org-supertag-ui-conversation)
+(require 'org-supertag-ui-ner)
+(require 'org-supertag-ui-archaeology)
+(require 'org-supertag-ui-memory)
+(require 'org-supertag-proactive-engine)
+(require 'org-supertag-util)
+
+;;;###autoload
+(defun org-supertag-ui-refresh-sidebar ()
+  "Refresh the entire intelligent sidebar with context from the current node.
+This is the main, unified entry point for the UI."
+  (interactive)
+  (org-supertag-ui-sidebar-show) ; Ensure the sidebar is visible first
+
+  ;; Clear all sections by rendering a loading message
+  (dolist (section-cons org-supertag-ui-sidebar-sections)
+      (org-supertag-ui-sidebar-render
+       (car section-cons)
+       (lambda () (insert (propertize "  (Loading...)\n" 'face 'italic)))
+       t)) ; t means clear the section
+
+  ;; Asynchronously call the refresh/show functions from each module
+  ;; Note: These functions are asynchronous (they take callbacks),
+  ;; so they will not block each other.
+  
+  ;; 1. Conversation & Associated Nodes (part of conversation now)
+  ;; This will ask a default question based on context
+  (let ((context-query (org-supertag-util-get-node-content-for-llm)))
+    (org-supertag-ui-conversation-ask-question (format "Summarize this note and find related content: %s" context-query)))
+
+  ;; 2. NER Features
+  (org-supertag-ui-ner-suggest-tags)
+  (org-supertag-ui-ner-discover-relationships)
+
+  ;; 3. Memory Candidates
+  (org-supertag-ui-memory-show-candidates)
+
+  ;; 4. Proactive Engine
+  (org-supertag-proactive-engine-activate)
+
+  (message "Intelligent Sidebar refresh initiated."))
+
+
+(provide 'org-supertag-ui)
+;;; org-supertag-ui.el ends here
\ No newline at end of file
diff --git a/org-supertag-util.el b/org-supertag-util.el
index 9c2fcde..47453e3 100755
--- a/org-supertag-util.el
+++ b/org-supertag-util.el
@@ -105,16 +105,6 @@ Return a string containing node title and content."
          ;; Combine title and content
         (concat "# " title "\n\n" cleaned-content)))))
 
-(defun org-plist-delete (plist prop)
-  "Delete property PROP from PLIST.
-Returns a new plist with the property removed."
-  (let (result)
-    (while plist
-      (unless (equal (car plist) prop)
-        (setq result (cons (cadr plist) (cons (car plist) result))))
-      (setq plist (cddr plist)))
-    (nreverse result)))
-
 (provide 'org-supertag-util)
 
 ;;; org-supertag-util.el ends here 
\ No newline at end of file
diff --git a/org-supertag-view-chat.el b/org-supertag-view-chat.el
deleted file mode 100644
index b816277..0000000
--- a/org-supertag-view-chat.el
+++ /dev/null
@@ -1,658 +0,0 @@
-;;; org-supertag-view-chat.el --- Chat view for org-supertag -*- lexical-binding: t; -*-
-
-(require 'org-supertag-api)
-(require 'org-supertag-node)
-(require 'org-supertag-db)
-(require 'org-supertag-query)
-(require 'org-supertag-bridge)
-
-(defcustom org-supertag-view-chat-buffer-name "*Org SuperTag Chat View*"
-  "The name of the buffer for the chat view."
-  :type 'string
-  :group 'org-supertag)
-
-(defcustom org-supertag-view-chat-lang "English"
-  "The language used in chat view"
-  :type 'string
-  :group 'org-supertag)
-
-;; --- Faces ---
-(defface org-supertag-chat-label-face
-  '((t :inherit font-lock-keyword-face :weight bold))
-  "Face for labels like 'Analysis:' or 'User:'."
-  :group 'org-supertag)
-
-(defface org-supertag-chat-prompt-face
-  '((t :inherit font-lock-constant-face :weight bold))
-  "Face for the input prompt '>'."
-  :group 'org-supertag)
-
-;; --- Buffer-Local Variables ---
-(defvar-local org-supertag-view-chat--conversation-history nil)
-;; Marker for where the next response will be inserted / prompt begins
-(defvar-local org-supertag-view-chat--response-start-marker nil)
-;; Marker for the beginning of current prompt line (char just after '> ')
-(defvar-local org-supertag-view-chat--prompt-start nil)
-(defvar-local org-supertag-view-chat--current-command nil
-  "Current active chat command (e.g. 'tags', 'expand', etc., nil for default mode).")
-
-;; --- Core Functions ---
-(defun org-supertag-view-chat--md-to-org (md-string)
-  "Convert a markdown string to an org-mode formatted string."
-  (if (and md-string (not (string-empty-p md-string)))
-      (with-temp-buffer
-        (insert md-string)
-        (goto-char (point-min))
-        
-        ;; Basic markdown to org conversions
-        ;; Headers: # title -> * title
-        (while (re-search-forward "^#+ \\(.*\\)" nil t) 
-          (replace-match (concat (make-string (length (match-string 1)) ?*) " " (match-string 1))))
-        
-        ;; Bold: **bold** -> *bold*
-        (while (re-search-forward "\\*\\*\\(.*?\\)\\*\\*" nil t) 
-          (replace-match "*\\1*"))
-        
-        ;; Italics: *italic* -> /italic/
-        (while (re-search-forward "\\*\\(.*?\\)\\*" nil t) 
-          (replace-match "/\\1/"))
-        
-        ;; Inline code: `code` -> =code= (handle before other transformations)
-        (goto-char (point-min))
-        (while (re-search-forward "`\\([^`]*?\\)`" nil t)
-          (replace-match "=\\1="))
-        
-        ;; Lists: - item -> - item (ensure proper spacing)
-        (goto-char (point-min))
-        (while (re-search-forward "^\\([ \t]*\\)[*-+] \\(.*\\)$" nil t)
-          (replace-match (concat (match-string 1) "- \\2")))
-        
-        ;; Links: [text](url) -> [[url][text]]
-        (goto-char (point-min))
-        (while (re-search-forward "\\[\\(.*?\\)\\](\\(.*?\\))" nil t)
-          (replace-match "[[\\2][\\1]]"))
-        
-        ;; Horizontal rules: --- -> -----
-        (goto-char (point-min))
-        (while (re-search-forward "^\\(-{3,}\\|\\*{3,}\\)$" nil t)
-          (replace-match "-----"))
-        
-        ;; Normalize multiple newlines to prevent excessive spacing
-        (goto-char (point-min))
-        (while (re-search-forward "\n\n\n+" nil t)
-          (replace-match "\n\n"))
-        
-        (buffer-string))
-    ""))
-
-;; --- Prompt with command ---
-(defun org-supertag-view-chat--insert-prompt ()
-  "Insert an org headline as the input prompt at the end of the buffer, showing current command if any."
-  (with-current-buffer (get-buffer-create org-supertag-view-chat-buffer-name)
-    (let ((inhibit-read-only t))
-      (goto-char (point-max))
-      (unless (bolp) (insert "\n"))
-      (let ((headline
-             (if org-supertag-view-chat--current-command
-                 (format "* User [%s]:" org-supertag-view-chat--current-command)
-               "* User:")))
-        (insert (propertize headline 'face 'org-supertag-chat-prompt-face))
-        (insert "\n"))))
-
-;; --- Async Callback ---
-(defun org-supertag-view-chat--handle-response (result)
-  "Handle asynchronous response from the RAG backend.
-RESULT is the plist returned from the Python backend."
-  (with-current-buffer (get-buffer-create org-supertag-view-chat-buffer-name)
-    (let ((inhibit-read-only t)
-          (response-content nil)
-          (history nil)
-          (status "error")
-          (error-message "No valid response from backend."))
-
-      ;; Check for bridge/EPC error first
-      (if (and (listp result) (eq (car result) :error))
-          (setq error-message (format "Backend Error: %s" (cadr result)))
-        ;; Otherwise, parse the plist from the handler
-        ;; Handle both direct results and wrapped results from _run_async
-        (let ((actual-result (if (and (listp result) (plist-get result :result))
-                                 (plist-get result :result)
-                               result)))
-          (setq response-content (or (plist-get actual-result :answer) 
-                                     (plist-get actual-result :response)
-                                     (plist-get actual-result :analysis)))
-          (setq history (plist-get actual-result :conversation-history))
-          (setq status (or (plist-get actual-result :status) "success"))
-          (when (plist-get actual-result :error)
-            (setq status "error")
-            (setq error-message (plist-get actual-result :error))))
-
-      ;; Go to the response start marker to clear the "Fetching..." message
-      (goto-char org-supertag-view-chat--response-start-marker)
-      (delete-region (point) (point-max))
-
-      ;; Debug: log raw result
-      ;; (message "[Chat Debug] Raw result: %S" result)
-      ;; (message "[Chat Debug] Result keys: %S" (when (and (listp result) (evenp (length result))) 
-      ;;                                          (cl-loop for (key val) on result by #'cddr collect key)))
-      ;; (let ((actual-result (if (and (listp result) (plist-get result :result))
-      ;;                          (plist-get result :result)
-      ;;                        result)))
-      ;;   (message "[Chat Debug] Actual result: %S" actual-result)
-      ;;   (message "[Chat Debug] Actual result keys: %S" (when (and (listp actual-result) (evenp (length actual-result)))
-      ;;                                                    (cl-loop for (key val) on actual-result by #'cddr collect key)))
-      ;;   (message "[Chat Debug] Answer: %S" (plist-get actual-result :answer))
-      ;;   (message "[Chat Debug] Response: %S" (plist-get actual-result :response))
-      ;;   (message "[Chat Debug] Analysis: %S" (plist-get actual-result :analysis)))
-      ;; (message "[Chat Debug] Response content: %S" response-content)
-      ;; (message "[Chat Debug] Status: %S" status)
-
-      ;; Insert the content. This is the primary conditional logic.2
-      (if (and (string= status "success") response-content)
-          ;; --- THEN: SUCCESS CASE ---
-          (progn
-            (setq org-supertag-view-chat--conversation-history history)
-            ;; push assistant message to history
-            (push (list :role "assistant" :content response-content)
-                  org-supertag-view-chat--conversation-history)
-            (insert "** Assistant\n")
-            (insert (format "%s\n" (org-supertag-view-chat--md-to-org response-content)))
-            (insert "\n")
-
-            ;; Context block
-            (when-let ((sources (plist-get result :source_nodes)))
-              (when (and (listp sources) (> (length sources) 0))
-                ;; Deduplicate by id
-                (let* ((unique-sources
-                        (cl-remove-duplicates sources :key (lambda (s) (plist-get s :id)) :test #'equal)))
-                  ;; Insert org-mode context block
-                  (insert "*** Context\n")
-                  (let ((content-start (point)))
-                    (dolist (src unique-sources)
-                      (let* ((id (plist-get src :id))
-                             (title (or (plist-get src :title) "Untitled"))
-                             (snippet (or (plist-get src :snippet) "No snippet")))
-                        (insert (format "- "))
-                        (insert-text-button title
-                                            'action (lambda (_btn) (org-supertag-view-chat--open-node id))
-                                            'follow-link t
-                                            'help-echo (format "Open node %s" id)
-                                            'face 'org-link)
-                        (insert (format ": %s\n" snippet))))
-                    ;; Create overlay for context lines only
-                    (let ((ov (make-overlay content-start (point))))
-                      (overlay-put ov 'invisible 'org-st-chat-context)
-                      (overlay-put ov 'intangible nil)))
-                  ;; 自动收起 Context 区块
-                  (require 'org)
-                  (run-at-time
-                   0.1 nil
-                   (lambda ()
-                     (with-current-buffer (current-buffer)
-                       (save-excursion
-                         (goto-char (point-min))
-                         (when (re-search-forward "^\\*\\*\\* Context" nil t)
-                           (when (fboundp 'org-fold-hide-subtree)
-                             (org-fold-hide-subtree))))))))
-                  ))
-
-            ;; Finalize buffer state for success
-            (insert "\n\n")
-            (put-text-property org-supertag-view-chat--response-start-marker (point) 'read-only t)
-            (org-supertag-view-chat--insert-prompt))
-
-        ;; --- ELSE: ERROR CASE ---
-        (progn
-          (insert (propertize "--- ERROR ---" 'face 'font-lock-warning-face))
-          (insert (format "\n%s\n\nRaw Response:\n%S" error-message result))
-          ;; Finalize buffer state for error
-          (insert "\n\n")
-          (put-text-property org-supertag-view-chat--response-start-marker (point) 'read-only t)
-          (org-supertag-view-chat--insert-prompt)))))))
-
-(defun org-supertag-view-chat--open-node (id)
-  "Corresponding to the node ID, jump to the org buffer and locate the node."
-  (interactive "sNode ID: ")
-  (let ((marker (org-id-find id t)))
-    (if marker
-        (progn
-          (switch-to-buffer (marker-buffer marker))
-          (goto-char marker)
-          (org-show-entry)
-          (message "Jumped to node %s" id))
-      (message "Node %s not found" id))))
-
-;; --- Sending Logic ---
-;; Prompt begins with ">" followed by optional spaces.
-(defconst org-supertag-view-chat--prompt-regexp "^> *")
-
-(defun org-supertag-view-chat--extract-latest-input ()
-  "Return text after the last prompt line (> )."
-  (save-excursion
-    (goto-char (point-max))
-    (when (re-search-backward org-supertag-view-chat--prompt-regexp nil t)
-      (let ((start (match-end 0)))
-        (string-trim (buffer-substring-no-properties start (line-end-position)))))))
-
-(defun org-supertag-view-chat--current-input ()
-  "Return current prompt line user text (string without surrounding spaces).
-Falls back to regex search when marker is missing or the captured
-text is empty. This prevents the first input cannot be sent issue when
-the prompt marker is unexpectedly nil or misplaced."
-  (let* ((txt (when (and org-supertag-view-chat--prompt-start
-                          (marker-position org-supertag-view-chat--prompt-start))
-                (save-excursion
-                  (goto-char org-supertag-view-chat--prompt-start)
-                  (string-trim (buffer-substring-no-properties (point) (line-end-position)))))))
-    (when (or (not txt) (string-empty-p txt))
-      (save-excursion
-        (beginning-of-line)
-        (when (looking-at "^> *\\(.*\\)$")
-          (setq txt (string-trim (match-string 1))))))
-    (if (and txt (not (string-empty-p txt)))
-        txt
-      (org-supertag-view-chat--extract-latest-input))))
-
-;; -----------------------------------------------------------------------------
-;; Main send input function
-;; -----------------------------------------------------------------------------
-;; --- User commands ---
-(defvar org-supertag-view-chat--user-commands (make-hash-table :test 'equal)
-  "Store all user-defined chat commands, command name -> prompt content.")
-
-(defconst org-supertag-view-chat--command-dir
-  (expand-file-name "command/" (or (bound-and-true-p org-supertag-data-directory)
-                                    (expand-file-name "org-supertag/" user-emacs-directory)))
-  "Path to the custom command prompt files. One .prompt file per command.")
-
-(defun org-supertag-view-chat--load-user-commands ()
-  "Load all custom command prompt files into hash-table."
-  (clrhash org-supertag-view-chat--user-commands)
-  (when (file-directory-p org-supertag-view-chat--command-dir)
-    (dolist (file (directory-files org-supertag-view-chat--command-dir t "\\.prompt$"))
-      (let ((name (file-name-base file)))
-        (with-temp-buffer
-          (insert-file-contents file)
-          (puthash name (buffer-string) org-supertag-view-chat--user-commands))))))
-
-;; Load on startup
-(org-supertag-view-chat--load-user-commands)
-
-;; /define <name> "multi-line prompt, containing $input variable"
-(defun org-supertag-view-chat--define-command (name prompt)
-  "Define a new command, persist the prompt to a file and load it into the hash-table."
-  (unless (file-directory-p org-supertag-view-chat--command-dir)
-    (make-directory org-supertag-view-chat--command-dir t))
-  (let ((file (expand-file-name (concat name ".prompt") org-supertag-view-chat--command-dir)))
-    (with-temp-file file
-      (insert prompt)))
-  (puthash name prompt org-supertag-view-chat--user-commands)
-  (message "Defined command /%s" name))
-
-;; /commands show all commands and their content
-(defun org-supertag-view-chat--list-commands ()
-  "Show all available commands and their prompt content."
-  (interactive)
-  (let ((msg (with-temp-buffer
-               (insert "Commands:\n")
-               (insert "/create-question\n")
-               (maphash (lambda (k v)
-                          (insert (format "/%s\n%s\n---\n" k v)))
-                        org-supertag-view-chat--user-commands)
-               (buffer-string))))
-    (message "%s" msg)))
-
-;; Parse /define command, return (name . prompt) or nil
-(defun org-supertag-view-chat--parse-define (input)
-  (when (string-match "^/define\\s-+\\([a-zA-Z0-9_-]+\\)\\s-+\"\(.*\)\"$" input)
-    (let ((name (match-string 1 input))
-          (prompt (match-string 2 input)))
-      (cons name prompt))))
-
-;; Only a built-in command /create-question
-(defconst org-supertag-view-chat--builtin-commands
-  '(("create-question" . "Please list all important questions related to $input.")))
-
-;; When switch prompt, display content
-(defun org-supertag-view-chat--show-current-command-prompt ()
-  "Display current command prompt in chat buffer"
-  (when org-supertag-view-chat--current-command
-    (let ((prompt (cond
-                    ((and org-supertag-view-chat--current-command
-                          (assoc org-supertag-view-chat--current-command org-supertag-view-chat--builtin-commands))
-                     (cdr (assoc org-supertag-view-chat--current-command org-supertag-view-chat--builtin-commands)))
-                    ((and org-supertag-view-chat--current-command
-                          (gethash org-supertag-view-chat--current-command org-supertag-view-chat--user-commands))
-                     (gethash org-supertag-view-chat--current-command org-supertag-view-chat--user-commands))
-                    (t nil))))
-      (when prompt
-        (let ((inhibit-read-only t))
-          (goto-char (point-max))
-          (unless (bolp) (insert "\n"))
-          (insert (propertize (format "[Prompt for /%s]:\n%s\n" org-supertag-view-chat--current-command prompt)
-                              'face 'font-lock-comment-face)))))))
-
-;; Modify send-input main flow, support /define, /commands, command switch, variable replacement
-(defun org-supertag-view-chat-send-input ()
-  "Send the current prompt line's text to the backend.
-Handles both regular chat queries and special /commands."
-  (interactive)
-  (cl-block org-supertag-view-chat-send-input
-    (let* ((raw (org-supertag-view-chat--current-input))
-           (lang (symbol-value 'org-supertag-view-chat-lang))
-           (input (when raw (string-trim (substring-no-properties raw)))))
-      (message "[Chat Debug] send-input called. Raw input: %S"
-               (substring-no-properties (or raw "")))
-      ;; /define
-      (let ((define-pair (org-supertag-view-chat--parse-define input)))
-        (when define-pair
-          (org-supertag-view-chat--define-command (car define-pair) (cdr define-pair))
-          (let ((inhibit-read-only t))
-            (goto-char (point-max))
-            (unless (bolp) (insert "\n"))
-            (insert (propertize (format "[System] Command /%s defined\n" (car define-pair)) 'face 'font-lock-comment-face)))
-          (org-supertag-view-chat--insert-prompt)
-          (setq org-supertag-view-chat--prompt-start nil)
-          (cl-return-from org-supertag-view-chat-send-input)))
-      ;; /commands
-      (when (string= input "/commands")
-        (let ((inhibit-read-only t))
-          (goto-char (point-max))
-          (unless (bolp) (insert "\n"))
-          (insert (propertize "[System] Available commands:\n" 'face 'font-lock-comment-face))
-          (insert "/create-question\n")
-          (maphash (lambda (k v)
-                     (insert (format "/%s\n%s\n---\n" k v)))
-                   org-supertag-view-chat--user-commands))
-        (org-supertag-view-chat--insert-prompt)
-        (setq org-supertag-view-chat--prompt-start nil)
-        (cl-return-from org-supertag-view-chat-send-input))
-      ;; /command 切换
-      (let ((cmd-pair (org-supertag-view-chat--parse-command input)))
-        (when cmd-pair
-          (let ((cmd (car cmd-pair)))
-            (let ((inhibit-read-only t))
-              (goto-char (point-max))
-              (unless (bolp) (insert "\n"))
-              (insert (propertize "[System] " 'face 'font-lock-comment-face))
-              (cond
-               ((member cmd '("default" "reset"))
-                (setq org-supertag-view-chat--current-command nil)
-                (insert "Switched back to default chat mode\n"))
-               ((or (assoc cmd org-supertag-view-chat--builtin-commands)
-                    (gethash cmd org-supertag-view-chat--user-commands))
-                (setq org-supertag-view-chat--current-command cmd)
-                (insert (format "Switched to command: %s\n" cmd))
-                (org-supertag-view-chat--show-current-command-prompt))
-               (t
-                (insert (format "Unknown command: %s\n" cmd)))))
-          (org-supertag-view-chat--insert-prompt)
-          (setq org-supertag-view-chat--prompt-start nil)
-          (cl-return-from org-supertag-view-chat-send-input)))
-      ;; 正常输入流程
-      (when (and input (not (string-empty-p input)))
-        (let ((inhibit-read-only t))
-          (let ((prompt-line-start
-                 (save-excursion 
-                   (beginning-of-line)
-                   (if (looking-at org-supertag-view-chat--prompt-regexp)
-                       (point)
-                     (when (re-search-backward org-supertag-view-chat--prompt-regexp nil t)
-                       (match-beginning 0))))))
-            (when prompt-line-start
-              (goto-char prompt-line-start)
-              (delete-region prompt-line-start (line-end-position))
-              (insert "* User\n")
-              (insert (format "%s\n" input))
-              (insert "\n")))
-          (setq org-supertag-view-chat--response-start-marker (point-marker))
-          (insert (propertize "Assistant is thinking..." 'face 'italic))
-          ;; Variable replacement when sending
-          (let* ((prompt (cond
-                          ((and org-supertag-view-chat--current-command
-                                (assoc org-supertag-view-chat--current-command org-supertag-view-chat--builtin-commands))
-                           (cdr (assoc org-supertag-view-chat--current-command org-supertag-view-chat--builtin-commands)))
-                          ((and org-supertag-view-chat--current-command
-                                (gethash org-supertag-view-chat--current-command org-supertag-view-chat--user-commands))
-                           (gethash org-supertag-view-chat--current-command org-supertag-view-chat--user-commands))
-                          (t nil)))
-                 (final-input (if (and prompt (string-match-p "\\$input" prompt))
-                                  (replace-regexp-in-string "\\$input" input prompt)
-                                input))
-                 (payload `(("query" . ,final-input)
-                            ("query_text" . ,final-input)
-                            ("lang" . ,lang)
-                            ("command" . ,org-supertag-view-chat--current-command)
-                            ("history" . ,org-supertag-view-chat--conversation-history))))
-            (org-supertag-bridge-call-async
-             "rag/query"
-             (list payload)
-             #'org-supertag-view-chat--handle-response))
-          (push (list :role "user" :content input) org-supertag-view-chat--conversation-history))
-        (setq org-supertag-view-chat--prompt-start nil)
-        (unless (and input (not (string-empty-p input)))
-          (message "[SuperTag Chat] No input detected on current prompt line.")))))))
-
-;; Invisibility spec for context blocks
-(add-to-invisibility-spec 'org-st-chat-context)
-
-
-(defun org-supertag-view-chat--toggle-context (button)
-  "Toggle visibility of the context overlay attached to BUTTON, update arrow."
-  (let ((ov (button-get button 'context-overlay)))
-    (when (overlayp ov)
-      (let* ((currently-hidden (overlay-get ov 'invisible))
-             (new-hidden (if currently-hidden nil 'org-st-chat-context))
-             (label-text (button-get button 'label-text))
-             (new-label (if currently-hidden
-                            (replace-regexp-in-string "^▸" "▾" label-text)
-                          (replace-regexp-in-string "^▾" "▸" label-text))))
-        (overlay-put ov 'invisible new-hidden)
-        ;; Update button label
-        (cond
-         ;; If Emacs version supports button-label-set
-         ((fboundp 'button-label-set)
-          (button-label-set button new-label)
-          (button-put button 'label-text new-label))
-         (t
-          ;; Compatible with old version: delete and rebuild button text
-          (let ((start (button-start button))
-                (end (button-end button)))
-            (let ((inhibit-read-only t))
-              (delete-region start end)
-              (goto-char start)
-              (insert-text-button new-label
-                                  'action #'org-supertag-view-chat--toggle-context
-                                  'follow-link t
-                                  'label-text new-label
-                                  'context-overlay ov)))))))))
-
-;; --- Buffer Setup & UI Commands ---
-
-;;;###autoload
-(defun org-supertag-view-chat-open ()
-  "Open or switch to the Org SuperTag Chat buffer."
-  (interactive)
-  (let* ((buffer (get-buffer-create org-supertag-view-chat-buffer-name)))
-    (with-current-buffer buffer
-      (let ((inhibit-read-only t))
-        (unless (derived-mode-p 'org-supertag-view-chat-mode)
-          (org-supertag-view-chat-mode))
-        (unless (org-supertag-bridge-ready-p)
-          (message "[SuperTag Chat] Starting Python backend…")
-          (org-supertag-bridge-ensure-ready 10))
-        (goto-char (point-max))
-        (when (= (point-min) (point-max))
-          (insert (propertize "#+TITLE: Welcome to SuperTag Chat\n")))
-        (org-supertag-view-chat--insert-prompt))))
-  (display-buffer org-supertag-view-chat-buffer-name)
-  (select-window (get-buffer-window org-supertag-view-chat-buffer-name)))
-
-(defvar org-supertag-view-chat-mode-map
-  (let ((map (make-sparse-keymap)))
-    (define-key map (kbd "C-c C-c") #'org-supertag-view-chat-send-input)
-    (define-key map (kbd "RET") #'org-supertag-view-chat-send-input)
-    map)
-  "Keymap for org-supertag-view-chat-mode.")
-
-;; Major Mode definition with keymap
-(define-derived-mode org-supertag-view-chat-mode org-mode "Org-ST-Chat"
-  "Major mode for the chat view conversation."
-  :group 'org-supertag
-  (setq-local truncate-lines t)
-  (setq-local org-hide-leading-stars t)
-  ;; Make buffer read-only by default, we toggle it when editing prompt
-  (read-only-mode -1)
-  (use-local-map org-supertag-view-chat-mode-map))
-
-;; ;; --- Context Management ---
-;; (defvar-local org-supertag-chat--current-node-context nil
-;;   "Current org node context for Chat View commands.
-;; Structure: (title content tags id filepath)")
-
-;; (defun org-supertag-chat--collect-node-context ()
-;;   "Silently collect current org node context and return it as a plist.
-;; This should be called from the original Org buffer."
-;;   (message "[Chat Debug] collect-node-context called.")
-;;   (when (and (derived-mode-p 'org-mode) (org-at-heading-p))
-;;     (message "[Chat Debug] At heading: t")
-;;     (let* ((title (org-get-heading t t))
-;;            (content (org-get-entry))
-;;            (tags (org-get-tags))
-;;            (id (org-id-get))
-;;            (filepath (buffer-file-name)))
-;;       (message "[Chat Debug] Collected - title: %S, content: %S, tags: %S, id: %S, filepath: %S"
-;;                title content tags id filepath)
-;;       (list :title title
-;;             :content content
-;;             :tags tags
-;;             :id id
-;;             :filepath filepath))))
-
-;; (defun org-supertag-chat--use-current-node-p ()
-;;   "Determine if current node context should be used."
-;;   (let ((result (and org-supertag-chat--current-node-context
-;;                       (equal (plist-get org-supertag-chat--current-node-context :filepath)
-;;                              (buffer-file-name)))))
-;;     (message "[Chat Debug] use-current-node-p returning: %S" result)
-;;     result))
-
-;; --- Command System ---
-(defun org-supertag-view-chat--parse-command (input)
-  "Parse Chat View input for special commands."
-  (message "[Chat Debug] parse-command input: %S" input)
-  (when (string-match "^/\\([a-z-]+\\)\\(?:\\s-+\\(.*\\)\\)?" input)
-    (let ((cmd (match-string 1 input))
-          (args (string-trim (or (match-string 2 input) "")))) ; Ensure args is a string, not nil
-      (message "[Chat Debug] parse-command matched - cmd: %S, args: %S" cmd args)
-      (cons cmd args))))
-
-;; --- Enhanced Send Logic ---
-(defun org-supertag-view-chat--handle-command-response (response)
-  "Handle response from commands and insert into buffer."
-  (with-current-buffer (get-buffer-create org-supertag-view-chat-buffer-name)
-    (let ((inhibit-read-only t))
-      (goto-char org-supertag-view-chat--response-start-marker)
-      (delete-region (point) (point-max))
-      (insert "** Assistant\n")
-      (insert (format "%s\n" (org-supertag-view-chat--md-to-org response)))
-      (insert "\n")
-      (put-text-property org-supertag-view-chat--response-start-marker (point) 'read-only t)
-      (org-supertag-view-chat--insert-prompt))))
-
-;; --- Save conversation ---
-(defgroup org-supertag-chat nil
-  "Chat View configuration for enhanced AI interactions."
-  :group 'org-supertag)
-
-(defcustom org-supertag-chat-default-save-method 'ask
-  "Default save method for conversations."
-  :type '(choice (const :tag "Ask each time" ask)
-                 (const :tag "New file" new-file)
-                 (const :tag "Append to node" append-node)
-                 (const :tag "New subnode" new-node))
-  :group 'org-supertag-chat)
-
-(defcustom org-supertag-chat-save-directory
-  (expand-file-name "chat-notes/" user-emacs-directory)
-  "Directory for saving Chat View conversations."
-  :type 'directory
-  :group 'org-supertag-chat)
-
-(defun org-supertag-view-chat--format-conversation (conversation)
-  "Format conversation for org-mode display."
-  (let ((formatted (replace-regexp-in-string "^" "> " conversation)))
-    (format "#+BEGIN_QUOTE\n%s\n#+END_QUOTE\n" formatted)))
-
-(defun org-supertag-view-chat--save-as-new-file (conversation title)
-  "Save conversation as new org file."
-  (let ((filename (expand-file-name (concat title ".org") org-supertag-chat-save-directory)))
-    (make-directory org-supertag-chat-save-directory t)
-    (with-current-buffer (find-file-noselect filename)
-      (insert (format "#+TITLE: %s\n#+DATE: %s\n#+TAGS: ai-conversation\n\n"
-                      title (format-time-string "%Y-%m-%d")))
-      (insert "* AI Conversation\n\n")
-      (insert (org-supertag-view-chat--format-conversation conversation))
-      (save-buffer)
-      (message "Conversation saved to: %s" filename))))
-
-(defun org-supertag-view-chat--save-append-to-node (conversation)
-  "Append conversation to current org headline."
-  (when (org-at-heading-p)
-    (save-excursion
-      (org-end-of-subtree t)
-      (insert "\n\n* AI Assistant Conversation\n")
-      (insert (format "#+CAPTION: Generated %s\n"
-                      (format-time-string "%Y-%m-%d %H:%M")))
-      (insert (org-supertag-view-chat--format-conversation conversation))
-      (message "Conversation appended to current node"))))
-
-(defun org-supertag-view-chat--save-as-subnode (conversation title)
-  "Create new subnode under current headline."
-  (when (org-at-heading-p)
-    (save-excursion
-      (org-end-of-subtree t)
-      (insert "\n")
-      (org-insert-heading)
-      (insert title)
-      (org-set-property "DATE" (format-time-string "%Y-%m-%d"))
-      (org-set-property "TAGS" "ai-conversation")
-      (insert "\n")
-      (insert (org-supertag-view-chat--format-conversation conversation))
-      (message "Conversation saved as subnode: %s" title))))
-
-(defun org-supertag-view-chat--save-conversation (conversation)
-  "Save Chat View conversation with user choice of method."
-  (interactive "sConversation: ")
-  (let* ((title (read-string "Conversation title: "
-                             (format "Chat-%s" (format-time-string "%Y%m%d"))))
-         (choice (completing-read "Save conversation as: "
-                                  '("new file" "append to current node" "create new subnode")
-                                  nil t)))
-    (pcase choice
-      ("new file" (org-supertag-view-chat--save-as-new-file conversation title))
-      ("append to current node" (org-supertag-view-chat--save-append-to-node conversation))
-      ("create new subnode" (org-supertag-view-chat--save-as-subnode conversation title)))))
-
-(defun org-supertag-view-chat--quick-save (conversation)
-  "Save using default method without prompting."
-  (let ((method (if (eq org-supertag-chat-default-save-method 'ask)
-                    (completing-read "Save as: "
-                                     '("new file" "append to current node" "create new subnode") nil t)
-                  org-supertag-chat-default-save-method)))
-    (pcase method
-      ("new file" (org-supertag-view-chat--save-as-new-file
-                   conversation (format "Chat-%s" (format-time-string "%Y%m%d"))))
-      ("append to current node" (org-supertag-view-chat--save-append-to-node conversation))
-      ("create new subnode" (org-supertag-view-chat--save-as-subnode
-                            conversation (format "AI-Conversation-%s" (format-time-string "%Y%m%d")))))))
-
-(define-key org-supertag-view-chat-mode-map (kbd "C-c C-s")
-  (lambda ()
-    (interactive)
-    (let ((conversation (buffer-substring-no-properties (point-min) (point-max))))
-      (org-supertag-view-chat--quick-save conversation))))
-
-(provide 'org-supertag-view-chat)
-
-;;; org-supertag-view-chat.el ends here
diff --git a/org-supertag-view-column.el b/org-supertag-view-column.el
old mode 100755
new mode 100644
diff --git a/org-supertag-view-discover.el b/org-supertag-view-discover.el
old mode 100755
new mode 100644
diff --git a/org-supertag-view-node.el b/org-supertag-view-node.el
deleted file mode 100644
index eadfa1d..0000000
--- a/org-supertag-view-node.el
+++ /dev/null
@@ -1,317 +0,0 @@
-;;; org-supertag-view-node.el --- Node-centric view for org-supertag -*- lexical-binding: t; -*-
-
-(require 'org-supertag-db)
-(require 'org-supertag-node)
-(require 'org-supertag-tag)
-(require 'org-supertag-field)
-(require 'org-supertag-view-utils)
-
-;;----------------------------------------------------------------------
-;; Variables
-;;----------------------------------------------------------------------
-
-(defvar org-supertag-view-node--current-node-id nil
-  "Current node ID being viewed in the node view buffer.")
-
-;;----------------------------------------------------------------------
-;; Mode Definition
-;;----------------------------------------------------------------------
-
-(defvar org-supertag-view-node-mode-map
-  (let ((map (make-sparse-keymap)))
-    (define-key map (kbd "n") 'next-line)
-    (define-key map (kbd "p") 'previous-line)
-    (define-key map (kbd "RET") 'org-supertag-view-node-edit-field-at-point)
-    (define-key map (kbd "E") 'org-supertag-view-node-edit-field-definition-at-point)
-    (define-key map (kbd "a") 'org-supertag-view-node-add-field)
-    (define-key map (kbd "d") 'org-supertag-view-node-remove-field-at-point)
-    (define-key map (kbd "f") 'org-supertag-feedback-on-relation-at-point)
-
-    (define-key map (kbd "g") 'org-supertag-view-node-refresh)
-    (define-key map (kbd "q") 'quit-window)
-    map)
-  "Keymap for `org-supertag-view-node-mode'.")
-
-(define-derived-mode org-supertag-view-node-mode special-mode "Org-Supertag-Node-View"
-  "Major mode for the unified node view."
-  :group 'org-supertag)
-
-;;----------------------------------------------------------------------
-;; Core Functions
-;;----------------------------------------------------------------------
-
-(defun org-supertag-view-node--get-references (node-id)
-  (when-let* ((node (org-supertag-db-get node-id)))
-    (plist-get node :ref-to)))
-
-(defun org-supertag-view-node--get-referenced-by (node-id)
-  (when-let* ((node (org-supertag-db-get node-id)))
-    (plist-get node :ref-from)))
-
-(defun org-supertag-view-node--format-node-content (node-id)
-  (when-let* ((node (org-supertag-db-get node-id))
-              (title (plist-get node :title))
-              (file-path (plist-get node :file-path))
-              (content (or (plist-get node :content) "")))
-    (let* ((file-name (file-name-nondirectory file-path))
-           (styled-title (propertize title 'face '(:weight bold))))
-      (format "%s (%s)\n%s\n\n" styled-title file-name content))))
-
-(defun org-supertag-view-node--format-display-value (value)
-  "Format VALUE for display.
-If VALUE is a list, join elements with ' / '.
-Otherwise, return VALUE as a string."
-  (if (listp value)
-      (mapconcat #'identity value " / ")
-    (format "%s" (or value ""))))
-
-(defun org-supertag-view-node--insert-metadata-section (node-id)
-  (let ((tags (org-supertag-node-get-tags node-id)))
-    (insert (propertize "🏷️ Tags\n" 'face 'org-level-2))
-    (if (not tags)
-        (insert (propertize "  No metadata found.\n" 'face 'shadow))
-      (dolist (tag-id tags)
-        (insert (propertize (format "  %s\n" tag-id) 'face 'org-tag 'tag-id tag-id 'field-name nil))
-        (when-let* ((tag-def (org-supertag-tag-get tag-id))
-                    (fields (plist-get tag-def :fields)))
-          (if (not fields)
-              (insert (propertize "    No fields defined.\n" 'face 'shadow))
-            (dolist (field-def fields)
-              (let* ((field-name (plist-get field-def :name))
-                     (value (org-supertag-field-get-value node-id field-name tag-id))
-                     (display-value (org-supertag-view-node--format-display-value value))
-                     (start (point)))
-                (insert (propertize (format "    %s: " field-name)
-                                     'face '(:weight bold :underline t)))
-                (insert (propertize (format "%s\n" display-value) 'face 'font-lock-string-face))
-                (add-text-properties start (point)
-                                     `(field-name ,field-name
-                                       tag-id ,tag-id
-                                       field-def ,field-def
-                                       face (:height 1.0))))))))
-      (insert (propertize "    [RET] Edit Value [E] Edit Field Definition [a] Add Field [d] Delete Field\n" 'face 'org-meta-line)))
-    (insert "\n")))
-
-(defun org-supertag-view-node--insert-relations-section (node-id)
-  (insert (propertize "🔗 Relations\n" 'face 'org-level-2))
-  (let ((tags (org-supertag-node-get-tags node-id))
-        (relations '()))
-    (dolist (tag-id tags)
-      (setq relations (append relations (org-supertag-relation-get-all tag-id))))
-    (if relations
-        (dolist (rel relations)
-          (let* ((source-id (plist-get rel :from))
-                 (target-id (plist-get rel :to))
-                 (type (plist-get rel :type))
-                 (source-name (org-supertag-tag-get-name-by-id source-id))
-                 (target-name (org-supertag-tag-get-name-by-id target-id)))
-            (insert (propertize (format "    %s " source-name) 'face 'org-tag))
-            (insert (propertize (format "--[%s]-->" type) 'face 'font-lock-keyword-face))
-            (insert (propertize (format " %s\n" target-name) 'face 'org-tag))
-            ))
-      (insert (propertize "    No relations found.\n" 'face 'shadow))))
-  (insert "\n"))
-
-(defun org-supertag-view-node--insert-similar-entities-section (node-id)
-  (insert (propertize "🧑‍🤝‍🧑 Similar Entities\n" 'face 'org-level-2))
-  (org-supertag-bridge-call-async "query/get_similar_entities" `(("node_id" . ,node-id))
-                                  (lambda (result)
-                                    (with-current-buffer (get-buffer-create "*Org SuperTag Node View*")
-                                      (let ((inhibit-read-only t))
-                                        (goto-char (point-max))
-                                        (if (and result (equal (plist-get result :status) "success"))
-                                            (let ((entities (plist-get result :result)))
-                                              (if entities
-                                                  (dolist (entity entities)
-                                                    (insert (propertize (format "    %s " (plist-get entity :title)) 'face 'org-tag))
-                                                    (insert (propertize (format "(Score: %.2f)\n" (plist-get entity :score)) 'face 'success)))
-                                                (insert (propertize "    No similar entities found.\n" 'face 'shadow))))
-                                          (insert (propertize "    Error getting similar entities.\n" 'face 'error)))))))
-  (insert "\n"))  
-
-(defun org-supertag-view-node--insert-backlinks-section (node-id)
-  (insert (propertize "👉 References\n" 'face 'org-level-2))
-  (let ((refs (org-supertag-view-node--get-references node-id)))
-    (if refs
-        (dolist (ref-id refs)
-          (insert (org-supertag-view-node--format-node-content ref-id)))
-      (insert (propertize "    No references found\n" 'face 'shadow))))
-  (insert "\n")
-  (insert (propertize "👈 Referenced By\n" 'face 'org-level-2))
-  (let ((refd-by (org-supertag-view-node--get-referenced-by node-id)))
-    (if refd-by
-        (dolist (ref-id refd-by)
-          (insert (org-supertag-view-node--format-node-content ref-id)))
-      (insert (propertize "    Not referenced by any nodes\n" 'face 'shadow)))))
-
-(defun org-supertag-view-node--get-field-info-at-point ()
-  "Return plist of field info at point, or nil.
-Tries current position first"
-  (let* ((pos (point))
-         (fallback-pos (max (point-min) (1- pos)))
-         (tag-id     (or (get-text-property pos 'tag-id)
-                         (get-text-property fallback-pos 'tag-id)))
-         (field-name (or (get-text-property pos 'field-name)
-                         (get-text-property fallback-pos 'field-name)))
-         (node-id org-supertag-view-node--current-node-id))
-    (when (and tag-id field-name node-id)
-      (let* ((tag-def   (org-supertag-tag-get tag-id))
-             (field-def (cl-find field-name (plist-get tag-def :fields)
-                                 :key (lambda (f) (plist-get f :name))
-                                 :test #'string=))
-             (value (when field-def
-                      (org-supertag-field-get-value node-id field-name tag-id))))
-        (list :node-id   node-id
-              :tag-id    tag-id
-              :field-name field-name
-              :field-def field-def
-              :value     value)))))
-
-(defun org-supertag-view-node-edit-field-at-point ()
-  "Edit the field value at the current point."
-  (interactive)
-  (when-let* ((field-info (org-supertag-view-node--get-field-info-at-point))
-              (field-def (plist-get field-info :field-def)))
-    (when field-def
-      (let* ((current-value (plist-get field-info :value))
-             (node-id (plist-get field-info :node-id))
-             (tag-id (plist-get field-info :tag-id))
-             (field-name (plist-get field-info :field-name))
-             (new-value (org-supertag-field-read-and-validate-value field-def current-value)))
-        (when new-value
-          (org-supertag-field-set-value node-id field-name new-value tag-id)
-          (org-supertag-view-node-refresh))))))
-
-(defun org-supertag-view-node-edit-field-definition-at-point ()
-  "Edit the field definition (e.g., its name) at the current point."
-  (interactive)
-  (when-let* ((field-info (org-supertag-view-node--get-field-info-at-point))
-              (tag-id (plist-get field-info :tag-id))
-              (field-name (plist-get field-info :field-name))
-              (field-def (plist-get field-info :field-def)))
-    (when (and tag-id field-name field-def)
-      (let* ((current-type (plist-get field-def :type))
-             (current-options (plist-get field-def :options))
-             (action (completing-read "Edit: " '("Name" "Type and Options") nil t)))
-        (cond
-         ((string= action "Name")
-          (let ((new-name (read-string (format "New name for field '%s': " field-name) nil nil field-name)))
-            (when (and new-name (not (string-empty-p new-name)) (not (string= new-name field-name)))
-              (org-supertag-tag-rename-field tag-id field-name new-name)
-              (org-supertag-view-node-refresh))))
-         ((string= action "Type and Options")
-          (let* ((field-type-choices (org-supertag-get-field-types))
-                 (field-type-str (completing-read "Field type: "
-                                                  (mapcar #'car field-type-choices)
-                                                  nil t nil nil (symbol-name current-type)))
-                 (new-type (cdr (assoc field-type-str field-type-choices)))
-                 (new-field-def (list :name field-name :type new-type)))
-            ;; If it's options type, ask for options
-            (when (eq new-type 'options)
-              (let* ((current-options-str (if (and (eq current-type 'options) current-options)
-                                              (mapconcat #'identity current-options ", ")
-                                            ""))
-                     (options-input (read-string "Options (comma separated): " current-options-str))
-                     (new-options (split-string options-input "," t "[ \t\n\r]+")))
-                (setq new-field-def (plist-put new-field-def :options new-options))))
-            (org-supertag-view-node--update-field-definition tag-id field-name new-field-def)
-            (org-supertag-view-node-refresh))))))))
-
-(defun org-supertag-view-node-add-field ()
-  "Add a new field to a tag associated with the current node."
-  (interactive)
-  (let* ((node-id org-supertag-view-node--current-node-id)
-         (tags (org-supertag-node-get-tags node-id))
-         (tag-id (completing-read "Add field to which tag: " tags nil t)))
-    (when tag-id
-      (let* ((field-name (read-string "Field name: "))
-             (field-type-choices (org-supertag-get-field-types))
-             (field-type-str (completing-read "Field type: "
-                                              (mapcar #'car field-type-choices)))
-             (field-type (cdr (assoc field-type-str field-type-choices)))
-             (field-def (list :name field-name :type field-type)))
-        ;; For options type, ask for the options
-        (when (eq field-type 'options)
-          (let* ((options-input (read-string "Options (comma separated): "))
-                 (options-list (split-string options-input "," t "[ \t\n\r]+")))
-            (setq field-def (plist-put field-def :options options-list))))
-        (org-supertag-tag-add-field tag-id field-def)
-        (org-supertag-view-node-refresh)))))
-
-(defun org-supertag-view-node-remove-field-at-point ()
-  "Remove the field definition at point from its tag."
-  (interactive)
-  (when-let* ((field-info (org-supertag-view-node--get-field-info-at-point)))
-    (let ((tag-id (plist-get field-info :tag-id))
-          (field-name (plist-get field-info :field-name)))
-      (when (and field-name (yes-or-no-p (format "Really remove field '%s' from tag '%s'?"
-                                                 field-name tag-id)))
-        (let* ((tag (org-supertag-db-get tag-id))
-               (fields (plist-get tag :fields))
-               (new-fields (cl-remove-if (lambda (f)
-                                           (string= (plist-get f :name) field-name))
-                                         fields))
-               (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
-          (org-supertag-db-add tag-id new-tag)
-          (message "Field '%s' removed from tag '%s'." field-name tag-id)
-          (org-supertag-view-node-refresh))))))
-
-(defun org-supertag-view-node--find-tag-for-field (node-id field-name)
-  (let ((found-tag-id nil))
-    (dolist (tag-id (org-supertag-node-get-tags node-id))
-      (when-let* ((tag-def (org-supertag-tag-get tag-id))
-                  (fields (plist-get tag-def :fields)))
-        (dolist (field-def fields)
-          (when (string= (plist-get field-def :name) field-name)
-            (setq found-tag-id tag-id)))))
-    found-tag-id))
-
-(defun org-supertag-view-node--update-field-definition (tag-id field-name new-field-def)
-  "Update the complete definition of a field in a tag."
-  (when-let* ((tag (org-supertag-db-get tag-id))
-              (fields (plist-get tag :fields)))
-    (let* ((new-fields (mapcar (lambda (f)
-                                 (if (string= (plist-get f :name) field-name)
-                                     new-field-def
-                                   f))
-                               fields))
-           (new-tag (plist-put (copy-sequence tag) :fields new-fields)))
-      (org-supertag-db-add tag-id new-tag)
-      (message "Field '%s' definition updated in tag '%s'." field-name tag-id))))
-
-
-    
-(defun org-supertag-view-node-refresh ()
-  (interactive)
-  (org-supertag-view-node--show-buffer))
-
-(defun org-supertag-view-node--show-buffer ()
-  (let ((buffer (get-buffer-create "*Org SuperTag Node View*")))
-    (with-current-buffer buffer
-      (let ((inhibit-read-only t))
-        (erase-buffer)
-        (org-supertag-view-node-mode)
-        (when-let* ((node (org-supertag-db-get org-supertag-view-node--current-node-id))
-                   (title (plist-get node :title)))
-          (insert (propertize (format "📄 %s\n" title) 'face 'org-level-1))
-          (insert (propertize "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n" 'face 'org-meta-line))
-          (org-supertag-view-node--insert-metadata-section org-supertag-view-node--current-node-id)
-          (org-supertag-view-node--insert-relations-section org-supertag-view-node--current-node-id)
-          (org-supertag-view-node--insert-backlinks-section org-supertag-view-node--current-node-id)
-          (org-supertag-view-node--insert-similar-entities-section org-supertag-view-node--current-node-id)
-          (insert (propertize "\nj/k: move  RET: edit  E: edit field def  a: add field  d: delete field  f: feedback  g: refresh  q: quit\n" 'face 'org-meta-line))
-          ))
-    (org-supertag-view--display-buffer-right buffer)
-    (select-window (get-buffer-window buffer))
-    (goto-char (point-min)))))
-
-;;;###autoload
-(defun org-supertag-view-node-show ()
-  (interactive)
-  (unless (org-at-heading-p)
-    (user-error "Must be on a heading"))
-  (let ((node-id (org-id-get-create)))
-    (setq org-supertag-view-node--current-node-id node-id)
-    (org-supertag-view-node--show-buffer)))
-    
-(provide 'org-supertag-view-node)
diff --git a/org-supertag-view-table.el b/org-supertag-view-table.el
old mode 100755
new mode 100644
index 90b1bc7..cb19d53
--- a/org-supertag-view-table.el
+++ b/org-supertag-view-table.el
@@ -153,7 +153,6 @@ Strategy:
     (with-current-buffer buffer
       (org-mode)
       (org-supertag-view-table-mode)
-      (org-supertag-view-mode 1) ; Enable the minor mode for correct dispatching
       (setq-local org-supertag-view-current-tag tag)
 
       (let ((inhibit-read-only t))
@@ -167,7 +166,7 @@ Strategy:
         (insert " Click the [v] button before a node to directly view its content\n")
         (insert " Editing fields will automatically save changes\n\n")
         ;; Insert table content
-        (org-supertag-view--insert-content-table tag))
+        (org-supertag-view--insert-content-table-with-button tag))
 
       (setq buffer-read-only t)
       (goto-char (point-min)))
@@ -176,7 +175,16 @@ Strategy:
     (switch-to-buffer buffer)
     (delete-other-windows)))
 
-(defun org-supertag-view--insert-content-table (tag)
+(defun org-supertag-view--get-field-value-from-db (node-id field-name)
+  "Get field value from database.
+NODE-ID is the node identifier
+FIELD-NAME is the field name"
+  (let* ((link-id (format ":node-field:%s->%s" node-id field-name))
+         (link (gethash link-id org-supertag-db--link)))
+    (when link
+      (plist-get link :value))))
+
+(defun org-supertag-view--insert-content-table-with-button (tag)
   "Insert content related to TAG in current buffer using org table format with buttons.
 Add a [v] button before each node in each row, clicking it will directly view the node content."
   (insert "Related Nodes:\n\n")
@@ -233,7 +241,7 @@ Add a [v] button before each node in each row, clicking it will directly view th
           ;; Insert field values
           (dolist (field fields)
             (let* ((field-name (plist-get field :name))
-                   (value (org-supertag-field-get-value node-id field-name tag)))
+                   (value (org-supertag-view--get-field-value-from-db node-id field-name)))
               (insert (format "|%s" (or value "")))))
           (insert "|\n")))
       
@@ -242,6 +250,7 @@ Add a [v] button before each node in each row, clicking it will directly view th
         (backward-char)
         (org-table-align)))))
 
+
 (defun org-supertag-view--get-field-index (col)
   "Get field index based on column position COL."
   (let ((pos 0)
@@ -260,6 +269,23 @@ Add a [v] button before each node in each row, clicking it will directly view th
     (when (string-match "\\*Org SuperTag Table View: \\([^*]+\\)\\*" (buffer-name))
       (match-string 1 (buffer-name)))))
 
+(defun org-supertag-view--update-field-value (node-id field-name value)
+  "Update VALUE of FIELD-NAME for NODE-ID.
+Returns t if update was successful, nil otherwise."
+  (when (and node-id field-name)
+    (if (not value)
+        ;; If value is empty, remove field
+        (org-supertag-node-remove-field node-id field-name)
+      ;; Otherwise, update field value
+      (org-supertag-node-set-field node-id field-name value))))
+
+(defun org-supertag-view--get-field-value (node-id field-name)
+  "Get value of FIELD-NAME for NODE-ID."
+  (let* ((tag (org-supertag-view--get-current-tag))
+         (tag-def (org-supertag-tag-get tag)))
+    ;; Use org-supertag-tag-get-field-value to get field value
+    (org-supertag-tag-get-field-value tag-def field-name)))
+
 (defun org-supertag-view-table-get-all-rows ()
   "Get all data rows in the table.
 Returns a list, each element is a list of row data."
@@ -292,7 +318,7 @@ Returns the found node ID, or nil if not found."
       (org-supertag-view--find-node-by-title clean-title))))
 
 (defun org-supertag-view-table-update-field (node-id field-name field-value tag-id)
-  "Update field value in database.
+  "Update field value in database and file.
 NODE-ID is the node identifier
 FIELD-NAME is the field name
 FIELD-VALUE is the new value
@@ -300,15 +326,34 @@ TAG-ID is the tag identifier
 Returns t if update successful, nil if failed."
   (condition-case err
       (progn
-        ;; 1. Update database by calling the centralized field function
-        (org-supertag-field-set-value node-id field-name field-value tag-id)
-
-        ;; The file update part is removed to decouple from org-properties.
-        ;; The database is now the single source of truth.
-
-        ;; 2. Mark database as dirty and schedule save
-        ;; This is now handled by org-supertag-field-set-value,
-        ;; so it can be removed from here.
+        ;; 1. Update database
+        (let ((link-id (format ":node-field:%s->%s" node-id field-name)))
+          (if (and field-value (not (string-empty-p field-value)))
+              ;; If there is a value, update the database
+              (puthash link-id
+                       (list :from node-id
+                             :to field-name
+                             :tag-id tag-id
+                             :value field-value
+                             :created-at (current-time))
+                       org-supertag-db--link)
+            ;; If value is empty, remove field
+            (remhash link-id org-supertag-db--link)))
+        
+        ;; 2. Update file
+        (when-let* ((node-props (gethash node-id org-supertag-db--object))
+                    (file-path (plist-get node-props :file-path))
+                    (pos (plist-get node-props :pos)))
+          (with-current-buffer (find-file-noselect file-path)
+            (save-excursion
+              (goto-char pos)
+              (if (and field-value (not (string-empty-p field-value)))
+                  (org-set-property field-name field-value)
+                (org-delete-property field-name)))))
+        
+        ;; 3. Mark database as dirty and schedule save
+        (org-supertag-db--mark-dirty)
+        (org-supertag-db--schedule-save)
         t)  ;; Return success
     (error
      (message "Error updating field %s: %s" field-name (error-message-string err))
@@ -322,6 +367,12 @@ Returns t if update successful, nil if failed."
       (quit-window)
       (org-supertag-view--show-content-table tag))))
 
+(defun org-supertag-view--get-field-link (node-id field-name)
+  "Get field link for NODE-ID and FIELD-NAME.
+Performs case-insensitive search for field name."
+  (let ((link-id (format ":node-field:%s->%s" node-id field-name)))
+    (gethash link-id org-supertag-db--link)))
+
 (defun org-supertag-view--get-related-nodes (tag)
   "Get nodes related to TAG.
 Returns a list of plists with properties :node, :type, :date and field values."
@@ -340,8 +391,12 @@ Returns a list of plists with properties :node, :type, :date and field values."
                   (mapcar
                    (lambda (field)
                      (let* ((field-name (plist-get field :name))
-                            ;; Use the centralized function to get the value
-                            (value (org-supertag-field-get-value node-id field-name tag)))
+                            ;; Query field value from link hash table
+                            (link-id (format ":node-field:%s->%s" node-id field-name))
+                            (field-link (gethash link-id org-supertag-db--link))
+                            (value (when field-link
+                                    (plist-get field-link :value))))
+                       (message "DEBUG: Node %s Field %s Value %s" node-id field-name value)
                        (cons field-name value)))
                    fields)))
              
@@ -359,46 +414,225 @@ Returns a list of plists with properties :node, :type, :date and field values."
      org-supertag-db--link)
     (nreverse nodes)))
 
+(defun org-supertag-view--edit-field-value (field-info)
+  "Core function to edit field values.
+FIELD-INFO is a property list containing all information about the current field.
+Returns a cons pair of (success . new-value), where success is t if editing is successful."
+  (let* ((field (plist-get field-info :field))
+         (field-type (when field (plist-get field :type)))
+         (field-name (when field (plist-get field :name)))
+         (current-value (plist-get field-info :value))
+         (col (plist-get field-info :col))
+         new-value)
+    
+    (setq new-value
+          (cond
+           ;; Date column or date type
+           ((or (= col 3) (eq field-type 'date))
+            (let ((date (org-read-date nil t nil "Enter date"
+                                      (when (and current-value
+                                                (not (string-empty-p current-value)))
+                                        current-value))))
+              (format-time-string "<%Y-%m-%d %a>" date)))
+           
+           ;; timestamp type
+           ((eq field-type 'timestamp)
+            (let ((date-time (org-read-date t t nil "Enter date and time"
+                                          (when (and current-value
+                                                    (not (string-empty-p current-value)))
+                                            current-value))))
+              (format-time-string "[%Y-%m-%d %a %H:%M]" date-time)))
+           
+           ;; options type
+           ((eq field-type 'options)
+            (let ((options (or (plist-get field :options) '("Option1" "Option2"))))
+              (completing-read
+               (format "Select option for %s: " field-name)
+               options nil t current-value)))
+           
+           ;; tag-reference type
+           ((eq field-type 'tag-reference)
+            (let ((nodes (list)))
+              (maphash
+               (lambda (id props)
+                 (when (eq (plist-get props :type) :node)
+                   (let ((title (or (plist-get props :title)
+                                   (format "Node %s" id))))
+                     (push (cons title id) nodes))))
+               org-supertag-db--object)
+              (let* ((choices (mapcar #'car nodes))
+                     (selected (completing-read
+                              (format "Select node for %s: " field-name)
+                              choices nil t nil)))
+                (when selected
+                  (let ((node-id (cdr (assoc selected nodes))))
+                    (format "[[%s][%s]]" node-id selected))))))
+           
+           ;; list type
+           ((eq field-type 'list)
+            (read-string
+             (format "Enter values for %s (comma-separated): " field-name)
+             current-value))
+           
+           ;; range type
+           ((eq field-type 'range)
+            (read-string
+             (format "Enter range for %s (N-M or N..M): " field-name)
+             current-value))
+           
+           ;; Default to simple string input
+           (t
+            (read-string (format "Edit%s: "
+                                (if field-name
+                                    (format " %s" field-name)
+                                  (format " column %d" col)))
+                        current-value))))
+    
+    (when new-value
+      ;; Validate new value
+      (let ((validated-value (org-supertag-view-validate-field
+                            new-value field-type field)))
+        (when validated-value
+          (cons t validated-value))))))
+
 (defun org-supertag-view-smart-edit ()
-  "Smart edit function - calls the centralized field editor.
+  "Smart edit function - selects the appropriate editing method based on the current field type.
 Immediately saves the field value to the database after editing."
   (interactive)
-  (let* ((field-info (org-supertag-view-table-get-field-info))
-         (field-def (plist-get field-info :field))
-         (current-value (plist-get field-info :value))
-         (tag (plist-get field-info :tag))
-         (node-title (nth 0 (plist-get field-info :row-data)))
-         (node-id (org-supertag-view--find-node-by-title node-title))
-         (field-name (plist-get field-def :name)))
-
-    (unless node-id
-      (error "Could not find node with title: %s" node-title))
-
-    (unless field-def
-      (error "Cannot edit this column. Not a user-defined field."))
-
-    (let ((new-value (org-supertag-field-read-and-validate-value field-def current-value)))
-      (when new-value
-        ;; Save the new value to the database
-        (org-supertag-field-set-value node-id field-name new-value tag)
-
-        ;; Update the value in the table view
-        (org-table-edit-field (format "%s" new-value))
-        (org-table-align)
-
-        (message "Field updated: %s" new-value)
-        (org-supertag-view-table-refresh)))))
+  (if (not (org-at-table-p))
+      ;; Not on a table, switch to overall edit mode
+      (progn
+        (setq buffer-read-only nil)
+        (message "Table unlocked, entering edit mode"))
+    
+    ;; On a table, get field information and edit
+    (let* ((field-info (org-supertag-view-table-get-field-info))
+           (col (plist-get field-info :col))
+           (tag (plist-get field-info :tag))
+           (row-data (plist-get field-info :row-data))
+           (node-title (when row-data
+                        (string-trim
+                         (replace-regexp-in-string "\\[\\(v\\|N/A\\)\\]\\s-*" ""
+                                                 (nth 0 row-data)))))
+           (node-id (when node-title
+                     (org-supertag-view-table-find-node-id node-title)))
+           (field (plist-get field-info :field))
+           (field-name (when field (plist-get field :name))))
+      
+      ;; Ensure read-only status is removed before editing
+      (when buffer-read-only
+        (setq buffer-read-only nil))
+      
+      ;; Edit field value
+      (let* ((edit-result (org-supertag-view--edit-field-value field-info))
+             (success (car edit-result))
+             (new-value (cdr edit-result)))
+        
+        (when (and success new-value)
+          ;; Update database (if it is a custom field)
+          (when (and node-id field-name (> col 3))
+            (org-supertag-view-table-update-field node-id field-name new-value tag))
+          
+          ;; Update table display
+          (org-table-put nil col new-value)
+          (org-table-align)
+          
+          (message "Field updated: %s" new-value))))))
+
+(defun org-supertag-view-validate-field (value field-type field-props)
+  "Validate and format the validity of input values based on field type.
+Validates the input VALUE based on FIELD-TYPE and FIELD-PROPS and returns the formatted value.
+If the value is invalid, returns nil and displays an error message."
+  (cond
+   ;; Date field validation - ensure format is <YYYY-MM-DD XXX>
+   ((eq field-type 'date)
+    (if (string-match-p "^<[0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\}\\( [A-Za-z]\\{3\\}\\)?>$" value)
+        value
+      (message "Invalid date format. Please use <YYYY-MM-DD> format")
+      nil))
+   
+   ;; Timestamp field validation - ensure format is [YYYY-MM-DD XXX HH:MM]
+   ((eq field-type 'timestamp)
+    (if (string-match-p "^\\[[0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\}\\( [A-Za-z]\\{3\\}\\)?\\( [0-9]\\{2\\}:[0-9]\\{2\\}\\)?\\]$" value)
+        value
+      (message "Invalid timestamp format. Please use [YYYY-MM-DD HH:MM] format")
+      nil))
+   
+   ;; Number field validation
+   ((eq field-type 'number)
+    (if (string-match-p "^[+-]?[0-9]*\\.?[0-9]+$" value)
+        value
+      (message "Invalid number format")
+      nil))
+   
+   ;; Range field validation - ensure format is N-M or N..M
+   ((eq field-type 'range)
+    (if (string-match-p "^[0-9]+\\(\\.\\.|[-~]\\)[0-9]+$" value)
+        value
+      (message "Invalid range format. Please use N-M or N..M format")
+      nil))
+   
+   ;; List field validation - ensure format is comma-separated items
+   ((eq field-type 'list)
+    (if (or (string-empty-p value)
+            (string-match-p "^[^,]+\\(,[^,]+\\)*$" value))
+        value
+      (message "Invalid list format. Please use comma-separated items")
+      nil))
+   
+   ;; Options field validation - ensure value is in the allowed options list
+   ((eq field-type 'options)
+    (let ((allowed-options (plist-get field-props :options)))
+      (if (or (string-empty-p value)
+              (member value allowed-options))
+          value
+        (message "Invalid option value. Allowed options: %s"
+                 (mapconcat #'identity allowed-options ", "))
+        nil)))
+   
+   ;; Tag reference field validation - ensure referenced tag exists
+   ((eq field-type 'tag-reference)
+    (if (or (string-empty-p value)
+            (org-supertag-node-exists-p value))
+        value
+      (message "Referenced node does not exist: %s" value)
+      nil))
+   
+   ;; Accept any value by default
+   (t value)))
 
 ;;----------------------------------------------------------------------
 ;; Mode definitions
 ;;----------------------------------------------------------------------
 
-(defun org-supertag-view-table-refresh ()
-  "Refresh the table view by re-generating its content."
+(defun org-supertag-view-refresh ()
+  "Refresh the current view."
   (interactive)
-  (let ((tag (org-supertag-view--get-current-tag)))
-    (when tag
-      (org-supertag-view--show-content-table tag))))
+  (cond
+   ;; in multi-column view mode
+   ((eq major-mode 'org-supertag-column-mode)
+    (org-supertag-view--refresh-column-view))
+   ;; in tag discovery mode
+   ((eq major-mode 'org-supertag-discover-mode)
+    (org-supertag-view--refresh-discover))
+   ;; in tag-only view mode
+   ((and (eq major-mode 'org-mode)
+         (bound-and-true-p org-supertag-view-mode)
+         (string-match-p "\\*Org SuperTag Table View:" (buffer-name)))
+    (let ((tag (progn
+                 (string-match "\\*Org SuperTag Table View: \\(.*\\)\\*" (buffer-name))
+                 (match-string 1 (buffer-name)))))
+      (when tag
+        (org-supertag-view--show-content-table tag))))
+   ;; in traditional view (compatible with old code)
+   (t
+    (let ((tag (car (split-string (buffer-name) ": #"))))
+      (when tag
+        (org-supertag-tag-columns tag))))))
+
+;;----------------------------------------------------------------------
+;; Mode definitions
+;;----------------------------------------------------------------------
 
 (define-derived-mode org-supertag-view-table-mode org-mode "SuperTag-Table"
   "Major mode for displaying tag content in table format.
@@ -417,7 +651,7 @@ This mode is based on org-mode to ensure compatibility with org table functions.
                                (let ((inhibit-read-only t))
                                  (call-interactively 'org-supertag-view-smart-edit))))
     (define-key map (kbd "q") 'quit-window)
-    (define-key map (kbd "g") 'org-supertag-view-table-refresh)
+    (define-key map (kbd "g") 'org-supertag-view-refresh)
     (define-key map (kbd "v") 'org-supertag-view-table-node-at-point)
     (define-key map (kbd "V") 'org-supertag-view-table-view-all-nodes)
     (define-key map (kbd "m") 'org-supertag-view-manage-relations)
@@ -459,7 +693,7 @@ This mode is based on org-mode to ensure compatibility with org table functions.
   (define-key org-supertag-view-table-mode-map (kbd "v")
               'org-supertag-view-table-node-at-point)
   (define-key org-supertag-view-table-mode-map (kbd "g")
-              'org-supertag-view-table-refresh)
+              'org-supertag-view-refresh)
   (define-key org-supertag-view-table-mode-map (kbd "q")
               'quit-window))
 
diff --git a/org-supertag-view-utils.el b/org-supertag-view-utils.el
old mode 100755
new mode 100644
diff --git a/org-supertag-view.el b/org-supertag-view.el
index c7e90cb..612355a 100755
--- a/org-supertag-view.el
+++ b/org-supertag-view.el
@@ -7,19 +7,12 @@
 (require 'org-supertag-view-table)
 (require 'org-supertag-view-discover)
 (require 'org-supertag-view-column)
-(require 'org-supertag-view-node)
-(require 'org-supertag-view-chat)
+
 
 ;;----------------------------------------------------------------------
 ;; Main functions
 ;;---------------------------------------------------------------------- 
 
-;;;###autoload
-(defun org-supertag-view-node ()
-  "Show the unified view for the node at point."
-  (interactive)
-  (org-supertag-view-node-show))
-
 ;;;###autoload
 (defun org-supertag-view-tag ()
   "Show content related to a tag with user's choice of view mode.
@@ -70,17 +63,13 @@ Available views:
   (setq-local org-mode-hook nil)
   (use-local-map org-supertag-view-column-mode-map))
 
-(defvar org-supertag-view-mode-map
-  (let ((map (make-sparse-keymap)))
-    (define-key map (kbd "C-c C-f") 'org-supertag-feedback-submit)
-    map)
-  "Keymap for org-supertag-view-mode.")
+
+
 
 (define-minor-mode org-supertag-view-mode
   "Minor mode for viewing org-supertag tag-related content."
   :lighter " SuperTag-View"
   :group 'org-supertag
-  :keymap org-supertag-view-mode-map
 
   (if org-supertag-view-mode
       ;; When enabling the mode
@@ -92,6 +81,10 @@ Available views:
     ;; When disabling the mode
     (when (boundp 'org-supertag-view--prev-read-only)
       (setq buffer-read-only org-supertag-view--prev-read-only))))
+      
+
 
 (provide 'org-supertag-view)
-;;; org-supertag-view.el ends here
\ No newline at end of file
+
+;;; org-supertag-view.el ends here
+
diff --git a/org-supertag.el b/org-supertag.el
index 02a3342..7259bdd 100755
--- a/org-supertag.el
+++ b/org-supertag.el
@@ -4,7 +4,7 @@
 
 ;; Author: Yibie
 ;; Keywords: org-mode, tags, metadata, workflow, automation
-;; Version: 3.0.5
+;; Version: 3.0.0
 ;; Package-Requires: ((emacs "28.1") (org "9.6"))
 ;; URL: https://github.com/yibie/org-supertag
 
@@ -52,19 +52,29 @@
 (require 'org-supertag-luhmann)
 (require 'org-supertag-view)
 (require 'org-supertag-inline)
+<<<<<<< HEAD
+(require 'org-supertag-sim-epc)
+(require 'org-supertag-sim)
+(require 'org-supertag-backlink)
+(require 'org-supertag-recovery)
+=======
 (require 'org-supertag-bridge)
 (require 'org-supertag-api)
+(require 'org-supertag-backlink)
 (require 'org-supertag-recovery)
+(require 'org-supertag-proactive-engine)
 (require 'org-supertag-background-sync) 
 (require 'org-supertag-auto-tag)
-(require 'org-supertag-completion)
-(require 'org-supertag-smart-companion)
+(require 'org-supertag-ui)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defgroup org-supertag nil
   "Customization options for org-supertag."
   :group 'org
   :prefix "org-supertag-")
 
+<<<<<<< HEAD
+=======
 (defun org-supertag--log (format-string &rest args)
   "Log a message with the '[org-supertag]' prefix."
   (message "[org-supertag] %s" (apply #'format format-string args)))
@@ -83,11 +93,7 @@ Only used when `org-supertag-enable-auto-vectorization' is non-nil."
   :type 'integer
   :group 'org-supertag)
 
-(defvar org-supertag-project-root
-  (file-name-directory (or load-file-name (buffer-file-name)))
-  "The root directory of the org-supertag project.
-This is determined dynamically based on the location of this file.")
-
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 ;;;###autoload
 (define-minor-mode org-supertag-mode
   "Toggle org-supertag mode."
@@ -106,9 +112,12 @@ This is determined dynamically based on the location of this file.")
 (defvar org-supertag--initialized nil
   "Flag to track if org-supertag has been initialized.")
 
+<<<<<<< HEAD
+=======
 (defvar org-supertag--vectorization-init-timer nil
   "Timer for delayed vectorization system initialization.")
 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 (defun org-supertag--initialize-id-system ()
   "Initialize org-id system properly."
   (require 'org-id)
@@ -131,6 +140,10 @@ This is determined dynamically based on the location of this file.")
   (unless (hash-table-p org-id-locations)
     (setq org-id-locations (make-hash-table :test 'equal))))
 
+<<<<<<< HEAD
+(defun org-supertag--enable ()
+  "Enable org-supertag."
+=======
 (defun org-supertag--schedule-vectorization-init ()
   "Schedule vectorization system initialization if auto-vectorization is enabled."
   (when org-supertag-enable-auto-vectorization
@@ -165,6 +178,7 @@ This is determined dynamically based on the location of this file.")
 
 (defun org-supertag--enable ()
   "Enable the core, non-network parts of org-supertag."
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   (unless org-supertag--initialized
     ;; 1. Initialize ID system
     (org-supertag--initialize-id-system)
@@ -172,14 +186,47 @@ This is determined dynamically based on the location of this file.")
     (org-supertag-db-ensure-data-directory)
     ;; 3. Initialize database
     (org-supertag-db-init)
-    ;; 4. Initialize sync system
-    (when (fboundp 'org-supertag-sync-init)
-      (org-supertag-sync-init))
-    ;; 5. Initialize relation module
-    (org-supertag-relation-init)
-    ;; 6. Setup auto-save hook for Emacs exit
+<<<<<<< HEAD
+    ;; 4. Setup auto-save
+    (org-supertag-db--setup-auto-save)
+    ;; 5. Initialize sync system
+    (org-supertag-sync-init)
+    ;; 6. Initialize similarity system and EPC service
+    (when (featurep 'org-supertag-sim)
+      (condition-case err
+          (progn
+            (require 'org-supertag-sim)
+            (org-supertag-sim-init))
+        (error
+         (message "Failed to initialize similarity system: %s"
+                  (error-message-string err)))))
+    ;; 7. Add hooks
     (add-hook 'kill-emacs-hook #'org-supertag-db-save)
+    ;; Mark as initialized
+    (setq org-supertag--initialized t)))
 
+(defun org-supertag--disable ()
+  "Disable org-supertag."
+  ;; 1. Save database
+  (org-supertag-db-save)
+  ;; 2. Clean up auto-save timer
+  (org-supertag-db--cleanup-auto-save)
+  ;; 3. Clean up sync system
+  (when org-supertag-sync--timer
+    (cancel-timer org-supertag-sync--timer)
+    (setq org-supertag-sync--timer nil))
+  ;; 4. Stop EPC server
+  (when (featurep 'org-supertag-sim-epc)
+    (org-supertag-sim-epc-stop-server))
+  ;; 5. Clear cache
+  (org-supertag-db--cache-clear)
+  ;; 6. Remove hooks
+  (remove-hook 'kill-emacs-hook #'org-supertag-db-save)
+  ;; Reset initialization flag
+  (setq org-supertag--initialized nil))
+=======
+    ;; 4. Setup auto-save hook for Emacs exit
+    (add-hook 'kill-emacs-hook #'org-supertag-db-save)
     ;; Mark as initialized
     (setq org-supertag--initialized t)
     (org-supertag--log "Core system initialized.")))
@@ -191,7 +238,6 @@ This is determined dynamically based on the location of this file.")
     ;; Try to save db before shutting down
     (org-supertag-db-save)
     (remove-hook 'kill-emacs-hook #'org-supertag-db-save)
-
     ;; Stop all services that might have been started
     (when (fboundp 'org-supertag-auto-tag-stop-silent-scan)
       (org-supertag-auto-tag-stop-silent-scan))
@@ -200,6 +246,7 @@ This is determined dynamically based on the location of this file.")
     (when (fboundp 'org-supertag-bridge-kill-process)
         (org-supertag-bridge-kill-process))
     (setq org-supertag--initialized nil)))
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (defun org-supertag-cleanup ()
   "Clean up org-supertag resources.
@@ -209,14 +256,49 @@ Used for manual cleanup or system state reset."
   (org-supertag-db-save)
   ;; 2. Clean up auto-save timer
   (org-supertag-db--cleanup-auto-save)
+<<<<<<< HEAD
+  ;; 3. Stop EPC server
+  (when (featurep 'org-supertag-sim-epc)
+    (org-supertag-sim-epc-stop-server))
+=======
   ;; 3. Stop Python bridge process
   (org-supertag-bridge-kill-process)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
   ;; 4. Clear cache
   (org-supertag-db--cache-clear)
   ;; 5. Reset dirty data flag
   (org-supertag-db--clear-dirty))
 
 ;;;###autoload
+<<<<<<< HEAD
+(defun org-supertag-setup ()
+  "Setup org-supertag."
+  (interactive)
+  (unless org-supertag--initialized
+    (org-supertag--enable)
+
+    ;; Load custom behaviors first, so the registry is populated
+    (let ((custom-file (expand-file-name "org-supertag-custom-behavior.el"
+                                       org-supertag-data-directory)))
+      (unless (file-exists-p custom-file)
+        (unless (file-exists-p org-supertag-data-directory)
+          (make-directory org-supertag-data-directory t))
+        (when-let* ((template (locate-library "org-supertag-custom-behavior.el")))
+          (copy-file template custom-file)
+          (message "Created custom behaviors file at %s" custom-file)))
+      (when (file-exists-p custom-file)
+        (load custom-file)))
+
+    ;; Now that custom behaviors are loaded, initialize and enable the behavior system
+    ;; This will trigger org-supertag-behavior--init,
+    ;; which in turn calls org-supertag-behavior--setup-scheduled-behaviors
+    (org-supertag-behavior-mode 1)
+
+    ;; Add org-supertag-mode to org-mode-hook
+    (add-hook 'org-mode-hook #'org-supertag-mode))
+  ;; It's good practice for setup functions to signal completion or success.
+  (message "org-supertag setup complete."))
+=======
 (defun org-supertag-init-vectorization ()
   "Manually initialize the vectorization system.
 This command can be used to start the vectorization system if automatic
@@ -267,9 +349,9 @@ the bridge is confirmed to be ready."
   ;; 3. Add the service start functions to the 'bridge ready' hook.
   ;;    These functions will be called automatically by the bridge
   ;;    once it has successfully connected to the Python backend.
+  (add-hook 'org-supertag-bridge-ready-hook #'org-supertag-auto-tag-start-silent-scan)
+  ;; Background sync will check its auto-start setting internally
   (add-hook 'org-supertag-bridge-ready-hook #'org-supertag-background-sync-start)
-  (add-hook 'org-supertag-bridge-ready-hook #'org-supertag-scheduler-start)
-  (add-hook 'org-supertag-bridge-ready-hook #'org-supertag-smart-companion-setup)
   
   ;; 4. Finally, start the Python bridge process.
   ;;    Once ready, it will trigger the hook above.
@@ -280,8 +362,7 @@ the bridge is confirmed to be ready."
 ;; Auto-setup when loading the package
 ;; This ensures org-supertag-mode is automatically added to org-mode-hook
 (org-supertag--setup)
-
-(add-hook 'org-supertag-db-after-load-hook #'org-supertag-behavior-setup)
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
 
 (provide 'org-supertag)
 
diff --git a/picture/figure10.png b/picture/figure10.png
old mode 100755
new mode 100644
diff --git a/picture/figure11.gif b/picture/figure11.gif
old mode 100755
new mode 100644
diff --git a/picture/figure12.gif b/picture/figure12.gif
old mode 100755
new mode 100644
diff --git a/picture/figure13.png b/picture/figure13.png
old mode 100755
new mode 100644
diff --git a/picture/figure14.gif b/picture/figure14.gif
old mode 100755
new mode 100644
diff --git a/picture/figure15.gif b/picture/figure15.gif
old mode 100755
new mode 100644
diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index a9bc425..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,26 +0,0 @@
-[build-system]
-requires = ["setuptools>=61.0"]
-build-backend = "setuptools.build_meta"
-
-[project]
-name = "simtag"
-version = "0.1.0"
-authors = [
-  { name="Yibie", email="your@email.com" },
-]
-description = "Python backend for org-supertag"
-readme = "README.md"
-requires-python = ">=3.8"
-classifiers = [
-    "Programming Language :: Python :: 3",
-    "License :: OSI Approved :: MIT License",
-    "Operating System :: OS Independent",
-]
-
-[project.urls]
-"Homepage" = "https://github.com/yibie/org-supertag"
-
-[tool.setuptools.packages.find]
-where = ["."]
-include = ["simtag*"]
-exclude = ["test*"] 
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..22dc415
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,10 @@
+setuptools>=65.5.0
+wheel>=0.40.0
+numpy>=1.26.0
+torch>=2.1.0
+sentence-transformers>=2.2.2
+requests>=2.31.0
+epc>=0.0.5
+huggingface-hub>=0.16.4
+transformers>=4.34.0
+urllib3<2.0.0
diff --git a/run_simtag_epc.sh b/run_simtag_epc.sh
new file mode 100755
index 0000000..0735e69
--- /dev/null
+++ b/run_simtag_epc.sh
@@ -0,0 +1,37 @@
+#!/bin/bash
+# Wrapper script to activate venv and run simtag_epc.py
+
+# Get the directory where the script resides
+SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
+VENV_DIR="$SCRIPT_DIR/.venv"
+
+# Enable debug mode
+set -x
+
+# Check if venv exists
+if [ -d "$VENV_DIR" ]; then
+  # Activate the virtual environment
+  if [ -z "$VIRTUAL_ENV" ]; then
+      source "$VENV_DIR/bin/activate"
+      echo "Activated venv: $VENV_DIR" >&2
+  else
+      echo "Venv already active: $VIRTUAL_ENV" >&2
+  fi
+else
+  echo "Warning: Virtual environment not found at $VENV_DIR. Using system Python." >&2
+fi
+
+# Execute the Python script with all passed arguments
+PYTHON_EXEC="python"
+if [ -n "$VIRTUAL_ENV" ] && [ -x "$VENV_DIR/bin/python" ]; then
+    PYTHON_EXEC="$VENV_DIR/bin/python"
+fi
+
+# Add debug output
+echo "Using Python: $PYTHON_EXEC" >&2
+echo "Script arguments: $@" >&2
+echo "Current directory: $(pwd)" >&2
+echo "Python path: $(which $PYTHON_EXEC)" >&2
+
+# Run Python with unbuffered output
+exec "$PYTHON_EXEC" -u "$SCRIPT_DIR/simtag_epc.py" "$@" 
\ No newline at end of file
diff --git a/run_simtag_epc_venv.sh b/run_simtag_epc_venv.sh
new file mode 100755
index 0000000..1673056
--- /dev/null
+++ b/run_simtag_epc_venv.sh
@@ -0,0 +1,151 @@
+#!/bin/bash
+
+# Get the absolute path of the directory where the script is located
+SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
+VENV_DIR="${SCRIPT_DIR}/.venv"
+PYTHON_SCRIPT="${SCRIPT_DIR}/simtag_epc.py"
+PYTHON_EXE="python3"
+
+# Output error message to standard error and exit
+function error_exit {
+    echo "Error: $1" >&2
+    exit 1
+}
+
+echo "========================================"
+echo "SimTag EPC Startup Script"
+echo "========================================"
+echo "Script Directory: ${SCRIPT_DIR}"
+echo "Python Script: ${PYTHON_SCRIPT}"
+echo "Virtual Env Dir: ${VENV_DIR}"
+
+# Check if the Python script exists
+if [ ! -f "$PYTHON_SCRIPT" ]; then
+    error_exit "Python script not found: ${PYTHON_SCRIPT}"
+fi
+
+# First, clean up any possible residual Python processes
+echo "Checking for running SimTag processes..."
+if pgrep -f "python.*simtag_epc.py" > /dev/null; then
+    echo "Found a running SimTag process, attempting to terminate..."
+    pkill -f "python.*simtag_epc.py" || true
+    sleep 1
+fi
+
+# Ensure the virtual environment exists
+if [ ! -d "$VENV_DIR" ]; then
+    echo "Virtual environment (${VENV_DIR}) does not exist, creating it..."
+    if ! command -v $PYTHON_EXE &> /dev/null; then
+        error_exit "'$PYTHON_EXE' command not found. Cannot create virtual environment."
+    fi
+    $PYTHON_EXE -m venv "$VENV_DIR" || error_exit "Failed to create virtual environment."
+    echo "Virtual environment created."
+else
+    echo "Using existing virtual environment: ${VENV_DIR}"
+fi
+
+# Activate the virtual environment
+if [ -f "${VENV_DIR}/bin/activate" ]; then
+    echo "Activating the virtual environment..."
+    source "${VENV_DIR}/bin/activate"
+    PYTHON_IN_VENV="${VENV_DIR}/bin/python"
+    PIP_IN_VENV="${VENV_DIR}/bin/pip"
+else
+    error_exit "Virtual environment activation script not found in ${VENV_DIR}/bin/"
+fi
+
+# Ensure all necessary dependencies are installed *within the virtual environment*
+echo "Checking/Installing dependencies within virtual environment..."
+REQUIRED_PACKAGES=(epc sentence-transformers numpy requests ollama)
+INSTALL_CMD="${PIP_IN_VENV} install --upgrade"
+PACKAGES_TO_INSTALL=""
+
+for pkg in "${REQUIRED_PACKAGES[@]}"; do
+    echo -n "Checking for ${pkg}... "
+    if ! $PYTHON_IN_VENV -c "import importlib.util; exit(0) if importlib.util.find_spec('$pkg') else exit(1)" &> /dev/null; then
+        echo "Not found."
+        PACKAGES_TO_INSTALL+="${pkg} "
+    else
+        echo "Found."
+    fi
+done
+
+if [ -n "$PACKAGES_TO_INSTALL" ]; then
+    echo "Installing/Upgrading missing packages: ${PACKAGES_TO_INSTALL}"
+    $INSTALL_CMD $PACKAGES_TO_INSTALL || error_exit "Failed to install dependencies in venv."
+else
+    echo "All required packages seem to be installed."
+fi
+
+# Set environment variables
+export SIMTAG_EPC_MODE="1"
+export PYTHONPATH="${SCRIPT_DIR}:${PYTHONPATH}"
+
+# Output debug information
+echo ""
+echo "Runtime Environment Information:"
+echo "- Script Directory: ${SCRIPT_DIR}"
+echo "- Python Interpreter: $($PYTHON_IN_VENV --version 2>&1) (from ${PYTHON_IN_VENV})"
+echo "- Command Line Arguments: $@"
+
+# Check if the simtag module directory exists
+if [ -d "${SCRIPT_DIR}/simtag" ]; then
+    echo "- simtag module directory: exists"
+else
+    echo "- simtag module directory: does not exist (warning)"
+    mkdir -p "${SCRIPT_DIR}/simtag"
+    touch "${SCRIPT_DIR}/simtag/__init__.py"
+    echo "  Minimal module directory created"
+fi
+
+# Check the port parameter
+PORT_ARG=""
+PORT_NUM=""
+for i in "${!@}"; do
+    arg="${@:$i:1}"
+    if [[ "$arg" == "--port" ]]; then
+        next_index=$((i + 1))
+        next_arg="${@:$next_index:1}"
+        if [[ "$next_arg" =~ ^[0-9]+$ ]]; then
+            PORT_ARG="--port $next_arg"
+            PORT_NUM="$next_arg"
+            echo "- Using Port: $PORT_NUM"
+            break
+        fi
+    elif [[ "$arg" == --port=* ]]; then
+        PORT_ARG="$arg"
+        PORT_NUM="${arg#*=}"
+        echo "- Using Port: $PORT_NUM"
+        break
+    fi
+done
+
+if [ -z "$PORT_ARG" ]; then
+    echo "- Port not specified, using default port (0)"
+    PORT_NUM=0
+fi
+
+echo ""
+echo "========================================"
+echo "Starting SimTag EPC Server..."
+echo "========================================"
+
+cd "$SCRIPT_DIR" || error_exit "Unable to switch to the script directory"
+
+if [[ ! -z "$PORT_NUM" ]] && [[ "$PORT_NUM" -ne 0 ]]; then
+    if nc -z 127.0.0.1 "$PORT_NUM" &> /dev/null; then
+        echo "Warning: Port $PORT_NUM is already in use"
+        echo "Attempting to terminate the process occupying the port..."
+        PID_ON_PORT=$(lsof -ti :"$PORT_NUM" -sTCP:LISTEN)
+        if [[ ! -z "$PID_ON_PORT" ]]; then
+            echo "Killing process(es) on port $PORT_NUM: $PID_ON_PORT"
+            kill -9 $PID_ON_PORT || echo "Failed to kill process $PID_ON_PORT, maybe already terminated."
+            sleep 1
+        else
+            echo "Warning: Could not find specific PID using lsof on port $PORT_NUM."
+        fi
+    fi
+fi
+
+echo "Executing command: $PYTHON_IN_VENV $PYTHON_SCRIPT $@"
+exec "$PYTHON_IN_VENV" "$PYTHON_SCRIPT" "$@"
diff --git a/simple_async_test.el b/simple_async_test.el
new file mode 100644
index 0000000..4587376
--- /dev/null
+++ b/simple_async_test.el
@@ -0,0 +1,205 @@
+;;; simple_async_test.el --- 简单的异步测试 -*- lexical-binding: t; -*-
+
+(require 'org-supertag-bridge)
+
+(defvar test-async-start-time nil)
+(defvar test-async-sessions-before nil)
+(defvar test-async-sessions-after nil)
+(defvar test-async-deferred nil)
+
+;; 全局调试开关
+(defvar org-supertag-debug-mode nil
+  "全局调试模式开关。设置为 t 时显示详细的调试信息。")
+
+(defun org-supertag-toggle-debug-mode ()
+  "切换调试模式。"
+  (interactive)
+  (setq org-supertag-debug-mode (not org-supertag-debug-mode))
+  (when org-supertag-debug-mode
+    ;; 启用时设置所有调试选项
+    (setq org-supertag-bridge-epc-debug t)
+    (setq org-supertag-bridge-deferred-debug t)
+    (setq org-supertag-bridge-enable-verbose-async-debug t))
+  (message "Org SuperTag 调试模式: %s" 
+           (if org-supertag-debug-mode "已启用" "已禁用")))
+
+(defun test-variable-scope ()
+  "测试异步回调中的变量作用域问题"
+  (interactive)
+  (let ((test-var "我是外层变量")
+        (test-list '(1 2 3)))
+    
+    (message "=== 变量作用域测试 ===")
+    (message "调用前 test-var: %s" test-var)
+    (message "调用前 test-list: %S" test-list)
+    
+    ;; 异步调用，回调中访问外层变量
+    (let ((call-deferred (org-supertag-bridge-epc-call-deferred
+                          org-supertag-bridge--python-epc-manager
+                          'get_status
+                          '())))
+      
+      (org-supertag-bridge-deferred-nextc 
+       call-deferred
+       (lambda (result)
+         (message "🔍 回调中访问变量:")
+         (message "  test-var: %s" test-var)
+         (message "  test-list: %S" test-list)
+         (message "  result status: %s" (plist-get result :status)))))))
+
+(defun simple-async-test ()
+  "测试异步调用"
+  (interactive)
+  ;; 启用调试
+  ;; (setq org-supertag-bridge-epc-debug t)
+  ;; (setq org-supertag-bridge-deferred-debug t)  ; 启用 deferred 调试
+  
+  (let ((test-async-start-time (current-time)))
+    (message "=== 简单异步测试 ===")
+    
+    ;; 清空日志缓冲区
+    (when (get-buffer "*org-supertag-bridge-epc-log*")
+      (with-current-buffer "*org-supertag-bridge-epc-log*"
+        (erase-buffer)))
+    (when (get-buffer "*org-supertag-bridge-deferred-log*")
+      (with-current-buffer "*org-supertag-bridge-deferred-log*"
+        (erase-buffer)))
+    
+    (message "调用前 sessions: %S" (org-supertag-bridge-epc-manager-sessions org-supertag-bridge--python-epc-manager))
+    
+    ;; 创建一个简单的 deferred 对象来测试
+    (let* ((simple-callback (lambda (result)
+                              (message "🎉 简单回调执行！结果: %S" result)
+                              (let ((elapsed (float-time (time-since test-async-start-time))))
+                                (message "回调耗时: %.3f秒" elapsed))))
+           (deferred-obj (org-supertag-bridge-deferred-new simple-callback)))
+      
+      ;; 直接调用 EPC
+      (let ((call-deferred (org-supertag-bridge-epc-call-deferred
+                            org-supertag-bridge--python-epc-manager
+                            'get_status
+                            '())))
+        
+        ;; 链接回调
+        (org-supertag-bridge-deferred-nextc call-deferred simple-callback)
+        
+        (message "调用后 sessions: %S" (org-supertag-bridge-epc-manager-sessions org-supertag-bridge--python-epc-manager))
+        (message "Deferred 对象: %S" call-deferred)
+        
+        ;; 2秒后检查结果
+        (run-with-timer 2 nil
+                        (lambda ()
+                          (message "=== 2秒后检查结果 ===")
+                          (message "当前 sessions: %S" (org-supertag-bridge-epc-manager-sessions org-supertag-bridge--python-epc-manager))
+                          (message "Deferred 状态: %S" (org-supertag-bridge-deferred-object-status call-deferred))
+                          (message "Deferred 值: %S" (org-supertag-bridge-deferred-object-value call-deferred))
+                          
+                          ;; 显示日志内容
+                          (when (get-buffer "*org-supertag-bridge-epc-log*")
+                            (message "=== EPC 日志内容 ===")
+                            (with-current-buffer "*org-supertag-bridge-epc-log*"
+                              (message "%s" (buffer-string))))
+                          
+                          (when (get-buffer "*org-supertag-bridge-deferred-log*")
+                            (message "=== Deferred 日志内容 ===")
+                            (with-current-buffer "*org-supertag-bridge-deferred-log*"
+                              (message "%s" (buffer-string))))))))))
+
+(defun test-deferred-chain-debug ()
+  "详细跟踪 deferred 链的执行过程"
+  (interactive)
+  (message "=== Deferred 链调试测试 ===")
+  
+  ;; 启用所有调试
+  ;; (setq org-supertag-bridge-epc-debug t)
+  ;; (setq org-supertag-bridge-deferred-debug t)
+  ;; (setq org-supertag-bridge-enable-verbose-async-debug t)
+  
+  ;; 清空日志
+  (when (get-buffer "*org-supertag-bridge-deferred-log*")
+    (with-current-buffer "*org-supertag-bridge-deferred-log*"
+      (erase-buffer)))
+  
+  (let ((d (org-supertag-bridge-epc-call-deferred
+            org-supertag-bridge--python-epc-manager
+            'get_status
+            '())))
+    
+    (message "1. 创建了 EPC deferred: %S" d)
+    (message "   初始状态: %S" (org-supertag-bridge-deferred-object-status d))
+    (message "   初始值: %S" (org-supertag-bridge-deferred-object-value d))
+    
+    ;; 添加回调
+    (let ((result-deferred 
+           (org-supertag-bridge-deferred-nextc d
+             (lambda (result)
+               (message "🎯 用户回调被执行！")
+               (message "   result: %S" result)
+               result))))
+      
+      (message "2. 添加回调后的 deferred: %S" result-deferred)
+      (message "   原始 deferred 的 next: %S" (org-supertag-bridge-deferred-object-next d))
+      
+      ;; 5秒后检查状态
+      (run-with-timer 5 nil
+                      (lambda ()
+                        (message "=== 5秒后状态检查 ===")
+                        (message "原始 deferred 状态: %S" (org-supertag-bridge-deferred-object-status d))
+                        (message "原始 deferred 值: %S" (org-supertag-bridge-deferred-object-value d))
+                        (message "结果 deferred 状态: %S" (org-supertag-bridge-deferred-object-status result-deferred))
+                        (message "结果 deferred 值: %S" (org-supertag-bridge-deferred-object-value result-deferred))
+                        
+                        ;; 显示 deferred 日志
+                        (when (get-buffer "*org-supertag-bridge-deferred-log*")
+                          (message "=== Deferred 日志 ===")
+                          (with-current-buffer "*org-supertag-bridge-deferred-log*"
+                            (message "%s" (buffer-string)))))))))
+
+(defun test-bridge-callback ()
+  "直接测试 bridge 层的回调处理"
+  (interactive)
+  (message "=== 测试 Bridge 层回调 ===")
+  
+  ;; 启用详细调试
+  ;; (setq org-supertag-bridge-enable-verbose-async-debug t)
+  
+  (org-supertag-bridge-call-async
+   "get_status"
+   nil
+   (lambda (result)
+     (message "🔥 Bridge 回调执行！")
+     (message "  result: %S" result)
+     (message "  status: %s" (if result (plist-get result :status) "nil"))))
+  
+  (message "Bridge 调用已发送，等待回调..."))
+
+(defun test-api-bulk-process ()
+  "测试 API bulk process 调用"
+  (interactive)
+  (message "=== 测试 API bulk process ===")
+  
+  ;; 创建一个简单的测试数据
+  (let ((test-data `(("nodes" . ())
+                     ("links" . ())
+                     ("ids_to_delete" . ())
+                     ("sync_timestamp" . ,(format-time-string "%Y-%m-%dT%H:%M:%SZ" (current-time) t)))))
+    
+    (message "发送测试数据: %S" test-data)
+    
+    (org-supertag-api-bulk-process-snapshot
+     test-data
+     (lambda (result)
+       (message "🎯 API 回调执行！")
+       (message "  result: %S" result)
+       (message "  status: %s" (if result (plist-get result :status) "nil"))))))
+
+;; 运行测试
+;;(simple-async-test)
+;;(test-variable-scope)
+;;(test-api-bulk-process)
+;;(test-deferred-chain-debug)
+
+;; 使用说明：
+;; 1. 启用调试模式：M-x org-supertag-toggle-debug-mode
+;; 2. 运行测试：(test-deferred-chain-debug) 或其他测试函数
+;; 3. 禁用调试模式：再次运行 M-x org-supertag-toggle-debug-mode
diff --git a/simtag/__init__.py b/simtag/__init__.py
old mode 100755
new mode 100644
index 5b08d55..50360a3
--- a/simtag/__init__.py
+++ b/simtag/__init__.py
@@ -1 +1,16 @@
-# This file makes Python treat the `simtag` directory as a package. 
\ No newline at end of file
+<<<<<<< HEAD
+"""
+SimTag Main Package
+Provides tag vector similarity search, entity extraction, and tag generation functionality
+"""
+
+from .config import Config
+from .entity_extractor import EntityExtractor
+from .ollama_bridge import OllamaBridge
+from .tag_vectors import TagVectorEngine
+
+__version__ = "1.0.0"
+__all__ = ['Config', 'EntityExtractor', 'OllamaBridge', 'TagVectorEngine'] 
+=======
+# This file makes Python treat the `simtag` directory as a package. 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
diff --git a/simtag/__pycache__/__init__.cpython-313.pyc b/simtag/__pycache__/__init__.cpython-313.pyc
new file mode 100644
index 0000000..e1d4f89
Binary files /dev/null and b/simtag/__pycache__/__init__.cpython-313.pyc differ
diff --git a/simtag/__pycache__/config.cpython-313.pyc b/simtag/__pycache__/config.cpython-313.pyc
new file mode 100644
index 0000000..950fea1
Binary files /dev/null and b/simtag/__pycache__/config.cpython-313.pyc differ
diff --git a/simtag/__pycache__/entity_extractor.cpython-313.pyc b/simtag/__pycache__/entity_extractor.cpython-313.pyc
new file mode 100644
index 0000000..4a22238
Binary files /dev/null and b/simtag/__pycache__/entity_extractor.cpython-313.pyc differ
diff --git a/simtag/__pycache__/epc_server.cpython-313.pyc b/simtag/__pycache__/epc_server.cpython-313.pyc
new file mode 100644
index 0000000..7089d14
Binary files /dev/null and b/simtag/__pycache__/epc_server.cpython-313.pyc differ
diff --git a/simtag/__pycache__/ollama_bridge.cpython-313.pyc b/simtag/__pycache__/ollama_bridge.cpython-313.pyc
new file mode 100644
index 0000000..aa8522f
Binary files /dev/null and b/simtag/__pycache__/ollama_bridge.cpython-313.pyc differ
diff --git a/simtag/__pycache__/tag_generator.cpython-313.pyc b/simtag/__pycache__/tag_generator.cpython-313.pyc
new file mode 100644
index 0000000..595cf3b
Binary files /dev/null and b/simtag/__pycache__/tag_generator.cpython-313.pyc differ
diff --git a/simtag/__pycache__/tag_relation_analyzer.cpython-313.pyc b/simtag/__pycache__/tag_relation_analyzer.cpython-313.pyc
new file mode 100644
index 0000000..d3d888d
Binary files /dev/null and b/simtag/__pycache__/tag_relation_analyzer.cpython-313.pyc differ
diff --git a/simtag/__pycache__/tag_vectors.cpython-313.pyc b/simtag/__pycache__/tag_vectors.cpython-313.pyc
new file mode 100644
index 0000000..5692362
Binary files /dev/null and b/simtag/__pycache__/tag_vectors.cpython-313.pyc differ
diff --git a/simtag/bm25.idx b/simtag/bm25.idx
deleted file mode 100644
index 8faf51d..0000000
Binary files a/simtag/bm25.idx and /dev/null differ
diff --git a/simtag/config.py b/simtag/config.py
old mode 100755
new mode 100644
index 28ae65e..7530832
--- a/simtag/config.py
+++ b/simtag/config.py
@@ -1,73 +1,277 @@
 """
-Configuration Management Module
-Provides project configuration and state management
+<<<<<<< HEAD
+SimTag Configuration Management Module
+Handles all SimTag related configuration options and environment settings
 """
+
 import os
+import sys
 import logging
-import os.path as osp
-from typing import Optional, Dict, Any, List
-import toml # <--- NEW: Import toml for dynamic config persistence
-from pydantic import BaseModel, Field
-from pydantic_settings import BaseSettings, SettingsConfigDict
+import json
+import subprocess
+from typing import Dict, Any, Optional
+from .utils.logging import setup_logging
 
-
-class OllamaBackendConfig(BaseModel):
-    base_url: str = "http://localhost:11434"
-    default_model: str = "gemma3:12b"
-    #default_model: str = "qwen3:14b-q4_K_M"
-    timeout: int = 60
-
-class LLMConfig(BaseModel):
-    provider: str = "ollama"
-    primary_backend: str = "ollama"
-    backends: Dict[str, OllamaBackendConfig] = Field(default_factory=lambda: {
-        "ollama": OllamaBackendConfig()
-    })
-
-class EmbeddingConfig(BaseModel):
-    provider: str = "ollama"  # Now defaults to "ollama"
-    max_input_tokens: int = 512
-    use_cache: bool = True
-    cache_size: int = 1000
+def check_dependencies() -> bool:
+    """Check if necessary dependencies are installed
     
-    llama_cpp: Dict[str, Any] = Field(default_factory=lambda: {
+    Returns:
+        bool: Whether the dependencies are met
+    """
+    try:
+        import torch
+        import sentence_transformers
+        import requests
+        import epc
+        import numpy
+        import urllib3
+        
+        # Log dependency version information
+        logging.info(f"Python version: {sys.version.split()[0]}")
+        logging.info(f"PyTorch version: {torch.__version__}")
+        logging.info(f"Sentence-Transformers version: {sentence_transformers.__version__}")
+        logging.info(f"Urllib3 version: {urllib3.__version__}")
+        
+        return True
+    except ImportError as e:
+        logging.error(f"Missing necessary dependencies: {e}")
+        return False
 
-        "model_path": "/path/to/your/gguf/Qwen3-Embedding-0.6B-GGUF/qwen3-embedding-0.6b.q8_0.gguf"
-    })
+def check_environment() -> bool:
+    """Check if the running environment meets the requirements
     
-    ollama: Dict[str, Any] = Field(default_factory=lambda: {
-        "base_url": "http://localhost:11434",
-        "model_name": "nomic-embed-text",
-        "dimension": None  # Optional: specify dimension if needed
-    })
-
-    # Whether to output debug information
-    debug: bool = False
+    Returns:
+        bool: Whether it is in the correct environment
+    """
+    # Check Python version
+    if sys.version_info < (3, 9):  # Relax version requirement to 3.9
+        logging.warning(f"Current Python version {sys.version_info.major}.{sys.version_info.minor} may be too low")
+        return False
+        
+    # Check dependencies
+    return check_dependencies()
+
+def ensure_environment():
+    """Ensure that the running environment meets the requirements"""
+    if not check_environment():
+        msg = """
+The running environment does not meet the requirements:
+1. Requires Python 3.9 or higher
+2. Requires the installation of the following dependencies:
+   - torch
+   - sentence-transformers
+   - requests
+   - epc
+   - numpy
+   - urllib3
+
+If there is a lack of dependencies, they can be installed using the following command:
+uv pip install torch sentence-transformers requests epc numpy urllib3
+"""
+        raise RuntimeError(msg)
 
-class Config(BaseSettings):
-    """Main configuration class, loaded from simtag.yaml."""
-    model_config = SettingsConfigDict(
-        env_file=os.getenv("SIMTAG_CONFIG_PATH", ".env"),
-        env_file_encoding='utf-8'
-    )
+class Config:
+    """SimTag Configuration Management Class"""
     
-    db_path: str = "/tmp/test_db.sqlite"
-    rag_graph_depth: int = 2
-    
-    llm: LLMConfig = Field(default_factory=LLMConfig)
-    embedding: EmbeddingConfig = Field(default_factory=EmbeddingConfig)
+    DEFAULT_MODEL_NAME = "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"
     
-    config_data: Dict[str, Any] = Field(default_factory=dict, repr=False)
+    def __init__(self, 
+                 vector_file: str = None,
+                 db_file: str = None,
+                 model_name: str = DEFAULT_MODEL_NAME,
+                 debug: bool = False,
+                 log_file: str = None,
+                 host: str = '127.0.0.1',
+                 port: int = 0):
+        """Initialize the configuration
+        
+        Args:
+            vector_file: Vector file path (specified by org-supertag-sim-epc-vector-file)
+            db_file: Database file path (specified by org-supertag-db-file)
+            model_name: Ollama model name
+            debug: Whether to enable debug mode
+            log_file: Log file path
+            host: Server address
+            port: Server port
+        """
+        # Check the environment
+        ensure_environment()
+        
+        # Use the file paths passed directly
+        self.vector_file = vector_file
+        self.db_file = db_file
+        
+        # Validate the file paths
+        if not self.vector_file:
+            raise ValueError("Vector file path not specified")
+        if not self.db_file:
+            raise ValueError("Database file path not specified")
+            
+        # Log file path - use the directory where the vector file is located
+        log_dir = os.path.dirname(self.vector_file)
+        self.log_file = log_file or os.path.join(log_dir, "simtag_epc.log")
+        
+        # Other settings
+        self.model_name = model_name
+        self.debug = debug
+        self.log_level = logging.DEBUG if debug else logging.INFO
+        self.host = host
+        self.port = port
+        
+        # Create the log directory
+        if self.log_file:
+            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+            
+        # Log the path information
+        logging.info(f"Configuration initialized:")
+        logging.info(f"Vector file: {self.vector_file}")
+        logging.info(f"Database file: {self.db_file}")
+        logging.info(f"Log file: {self.log_file}")
+
+    def ensure_ollama(self) -> bool:
+        """Ensure Ollama is available"""
+        try:
+            # Simple check if the ollama command is available
+            subprocess.run(["ollama", "--version"], capture_output=True, check=True)
+            return True
+        except (subprocess.CalledProcessError, FileNotFoundError):
+            logging.error("Ollama is not installed or not available")
+            return False
+
+    def initialize_server(self) -> bool:
+        """Initialize the server environment"""
+        try:
+            # Ensure the environment meets the requirements
+            ensure_environment()
+            
+            # Ensure Ollama is available
+            if not self.ensure_ollama():
+                raise RuntimeError("Ollama is not installed or not available")
+            
+            # Create the necessary directories
+            os.makedirs(os.path.dirname(self.vector_file), exist_ok=True)
+            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+            
+            # Set up the logging, passing specific parameters instead of self
+            setup_logging(
+                log_file=self.log_file,
+                log_level=self.log_level,
+                debug=self.debug
+            )
+            
+            return True
+            
+        except Exception as e:
+            logging.error(f"Failed to initialize the server environment: {e}")
+            return False
+
+    def setup(self):
+        """Set up the running environment"""
+        # Apply environment variables
+        for key, value in self.env_vars.items():
+            if key == "PYTHONPATH":
+                # For PYTHONPATH, we need to append instead of overwrite
+                current_path = os.environ.get("PYTHONPATH", "")
+                if current_path:
+                    os.environ["PYTHONPATH"] = f"{value}:{current_path}"
+                else:
+                    os.environ["PYTHONPATH"] = value
+            else:
+                os.environ[key] = value
+        
+        # Create the necessary directories
+        os.makedirs(os.path.dirname(self.vector_file), exist_ok=True)
+        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
+        
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert the configuration to a dictionary"""
+        return {
+            "vector_file": self.vector_file,
+            "db_file": self.db_file,
+            "model_name": self.model_name,
+            "debug": self.debug,
+            "log_file": self.log_file,
+            "is_initialized": self.is_initialized,
+            "env_vars": self.env_vars,
+            "host": self.host,
+            "port": self.port
+        }
+        
+    def save(self, filepath: str) -> None:
+        """Save the configuration to a file"""
+        with open(filepath, 'w') as f:
+            json.dump(self.to_dict(), f, indent=2)
+            
+    @classmethod
+    def load(cls, filepath: str) -> 'Config':
+        """Load the configuration from a file"""
+        if not os.path.exists(filepath):
+            return cls()
+            
+        with open(filepath, 'r') as f:
+            config_dict = json.load(f)
+            
+        config = cls(
+            vector_file=config_dict.get("vector_file"),
+            db_file=config_dict.get("db_file"),
+            model_name=config_dict.get("model_name"),
+            debug=config_dict.get("debug", False),
+            log_file=config_dict.get("log_file"),
+            host=config_dict.get("host", '127.0.0.1'),
+            port=config_dict.get("port", 0)
+        )
+        
+        config.is_initialized = config_dict.get("is_initialized", False)
+        if "env_vars" in config_dict:
+            config.env_vars.update(config_dict["env_vars"])
+            
+        return config
+=======
+Configuration Management Module
+Provides project configuration and state management
+"""
+import os
+import logging
+import os.path as osp
+from dataclasses import dataclass, field
+from typing import Optional, Dict, Any, List
+import toml # <--- NEW: Import toml for dynamic config persistence
+from pydantic import BaseModel
+
+# Define the default analysis config dictionary at the module level
+analysis_config = {
+    "enable_inferred_relations": True,
+    "inference_model": "phi4-mini:3.8b",
+    "processing_workers": 4,
+}
+
+# Define a default for processing_config as well
+processing_config = {
+    "processing_workers": 4, # You can add other defaults here
+}
+
+@dataclass
+class Config(BaseModel):
+    """Main configuration class, loaded from simtag.yaml."""
+    config_data: Dict[str, Any] = field(default_factory=dict, repr=False)
     # --- NEW: Central Data Directory ---
-    data_directory: str = Field(default_factory=lambda: os.environ.get("ORG_SUPERTAG_DATA_DIRECTORY", os.path.expanduser("~/.emacs.d/org-supertag")))
+    data_directory: str = field(default_factory=lambda: os.environ.get("ORG_SUPERTAG_DATA_DIRECTORY", os.path.expanduser("~/.emacs.d/org-supertag")))
 
     # Vector database path
-    vector_db_path: str = Field(default_factory=lambda: os.path.expanduser("~/.emacs.d/org-supertag/supertag_vector.db"))
+    vector_db_path: str = field(default_factory=lambda: os.path.expanduser("~/.emacs.d/org-supertag/supertag_vector.db"))
     # --- NEW: Path for dynamic config TOML file ---
-    dynamic_config_file_path: str = Field(default_factory=lambda: os.path.expanduser("~/.emacs.d/org-supertag/runtime_config.toml"))
-    user_roles_file_path: str = Field(default_factory=lambda: os.path.expanduser("~/.emacs.d/org-supertag/roles.toml"))
-    
+    dynamic_config_file_path: str = field(default_factory=lambda: os.path.expanduser("~/.emacs.d/org-supertag/runtime_config.toml"))
     
+    # --- LLM Client Configuration ---
+    llm_client_config: Dict[str, Any] = field(default_factory=lambda: {
+        'provider': 'ollama', # Default provider
+        'base_url': 'http://localhost:11434', # Default for Ollama
+        'default_model': 'hf.co/unsloth/gemma-3-4b-it-GGUF', # Default generation model
+        #'default_embedding_model': 'nomic-embed-text', # Default embedding model
+        'default_embedding_model': 'qwen3:0.6b', # 使用Qwen3 0.6B作为默认嵌入模型
+        'api_key': None, # For providers like OpenAI
+        'timeout': 120 # Default request timeout
+    })
 
     # Deprecated Ollama-specific fields (values moved to llm_client_config)
     # ollama_model: str = "gemma"
@@ -77,15 +281,16 @@ class Config(BaseSettings):
     # embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2" # Removed, use llm_client_config['default_embedding_model']
     
     # Whether to use cache
-    # use_cache: bool = True # MOVED to EmbeddingConfig
+    use_cache: bool = True
     
     # Cache size
-    # cache_size: int = 1000 # MOVED to EmbeddingConfig
+    cache_size: int = 1000
     
     # Whether to output debug information
-    # debug: bool = False # MOVED to EmbeddingConfig
+    debug: bool = False
 
     # --- RAG Retrieval Configuration ---
+    rag_graph_depth: int = 2
     rag_vector_results: int = 10
     rag_time_range_days: Optional[int] = None  # Nil for no limit, e.g., 365 for last year
     rag_semantic_threshold: float = 0.7
@@ -93,23 +298,24 @@ class Config(BaseSettings):
     rag_time_decay: bool = True
     rag_retrieval_mode: str = "balanced"  # 'precise', 'balanced', 'exploratory'
 
-    # --- New Hybrid Retrieval Parameters ---
-    rag_neighbor_depth: int = 2             # BFS depth when expanding neighbors
-    rag_vector_graph_ratio: float = 0.5     # Ratio of vector-based vs graph-based nodes
-    rag_snippet_len: int = 200              # Max chars per snippet when building context
-
+    # --- RAG Context Window Management ---
+    rag_context_window_size: int = 4000  # Total tokens
+    rag_context_core_content_ratio: float = 0.4
+    rag_context_supplementary_ratio: float = 0.35
+    rag_context_background_ratio: float = 0.25
+    rag_context_priorities: List[str] = field(default_factory=lambda: [
+        "current_node",
+        "recent_nodes",
+        "high_frequency_concepts",
+        "historical_dialogue", # Optional
+        "cross_domain_relations" # Optional
+    ])
     rag_context_auto_truncate: bool = True
     rag_context_keep_summary: bool = True
 
-    rag_bm25_top_k: int = 10  # Number of BM25 results to retrieve
-    rag_bm25_weight: float = 0.5  # Weight multiplier for BM25 score when computing relevance
-
-    # Path to BM25 index file (single-file pickle). Default inside simtag/ directory
-    bm25_index_path: str = Field(default_factory=lambda: os.path.join(os.path.dirname(__file__), "bm25.idx"))
-
     # --- Dialogue Mode RAG Presets ---
     # Stored as a dictionary mapping mode name to a dict of RAG config overrides
-    rag_mode_presets: Dict[str, Dict[str, Any]] = Field(default_factory=lambda: {
+    rag_mode_presets: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
         "normal": {
             "rag_retrieval_mode": "precise",
             "rag_graph_depth": 1,
@@ -132,9 +338,17 @@ class Config(BaseSettings):
         }
     })
 
+    # --- Memory Mechanism Configuration ---
+    memory_preferences_limit: int = 50
+    memory_patterns_limit: int = 30
+    memory_dialogues_limit: int = 200
+    memory_cleanup_threshold: float = 0.3 # Confidence threshold for cleanup
+    memory_token_budget_preferences: float = 0.15 # Percentage of total context
+    memory_token_budget_patterns: float = 0.15
+    memory_token_budget_dialogue_history: float = 0.10
 
     # --- Entity Extractor Configuration ---
-    entity_extractor_config: Dict[str, Any] = Field(default_factory=lambda: {
+    entity_extractor_config: Dict[str, Any] = field(default_factory=lambda: {
         "entity_types": None, # Example: [{'name': 'PERSON', 'description': '...'}]
         "extraction_prompt_template": None, # Optional: Path to a custom prompt template or the template string itself
         "gleaning_prompt_template": None, # Optional
@@ -145,47 +359,73 @@ class Config(BaseSettings):
         "max_retries": 2     # 最大重试次数
     })
     
-    # --- Sync Handler Configuration ---
-    sync_max_concurrent_llm_tasks: int = 2 # Max concurrent LLM tasks during sync (increased for better performance)
-    use_semantic_embedding_ids: bool = True # Use semantic IDs for embeddings instead of UUIDs
-
+    # --- SmartNER Service Configuration ---
+    use_smart_ner_service: bool = True  # 启用 SmartNERServiceV2 优化实现
+    
+    # --- Multicore Processing Configuration ---
+    multicore_config: Dict[str, Any] = field(default_factory=lambda: {
+        "enabled": True,  # 启用多核心处理
+        "max_workers": None,  # 工作进程数：None为自动优化（推荐2），用户可设置1-16
+        "max_ollama_instances": None,  # Ollama实例数：None为自动优化（推荐2），用户可设置1-8
+        "enable_multi_ollama": True,  # 启用多Ollama实例支持
+        "process_type": "process",  # "process" 或 "thread" - 使用process实现真正并行
+        "embedding_batch_threshold": 3,  # 嵌入任务启用多核心的最小任务数
+        "ner_batch_threshold": 2,  # NER任务启用多核心的最小任务数
+        "timeout": 300,  # 任务超时时间（秒）
+        "memory_threshold_percent": 85,  # 内存使用率阈值，超过则禁用多核心
+        # 🎯 基于性能测试的推荐配置（用户可覆盖）
+        "recommended_workers": 2,  # 推荐的工作进程数（基于测试发现的最优值）
+        "recommended_instances": 2,  # 推荐的Ollama实例数（基于测试发现的最优值）
+    })
+    
+    # --- Embedding Service Configuration ---
+    embedding_config: Dict[str, Any] = field(default_factory=lambda: {
+        "primary_backend": "llama_cpp",  # 使用 llama.cpp 作为主要后端
+        "fallback_backends": ["ollama"],  # 只使用ollama作为备用后端，避免本地模型下载问题
+        "cache_enabled": True,
+        "batch_size": 16,  # 减小批处理大小以避免llama.cpp序列ID问题
+        "max_retries": 3,  # 增加重试次数
+        "local_model": "sentence-transformers/all-MiniLM-L6-v2",  # 保留配置但不使用
+        # "ollama_model": "nomic-embed-text",
+        # "ollama_timeout": 300,
+        "llama_cpp_model_path": "~/.models/Qwen3-Embedding-0.6B-GGUF/Qwen3-Embedding-0.6B-Q8_0.gguf",  # Qwen3模型路径
+        "llama_cpp_binary": "llama-embedding",  # 二进制文件名
+        "llama_cpp_pooling": "cls",  # 改为cls池化策略以避免mean pooling的序列ID问题
+        "llama_cpp_threads": None,  # 线程数，None表示自动检测
+        "llama_cpp_batch_size": 8,  # 添加llama.cpp专用的批处理大小
+        "llama_cpp_max_context": 512,  # 最大上下文长度
+        
+        # 长文本处理配置
+        "long_text_strategy": "chunk",  # 长文本处理策略: "truncate" | "chunk"
+        "chunk_size": 400,  # 分块大小（字符数）
+        "chunk_overlap": 50,  # 分块重叠（字符数）
+        "chunk_aggregation": "mean",  # 聚合策略: "mean" | "weighted_mean" | "max_pool"
+        "max_chunks": 10,  # 最大分块数量（防止过长文本）
+        "force_update": True,  # 强制更新所有内容
+        "incremental_update": False,  # 禁用增量更新
+        "embedding_enabled": True,  # 强制启用嵌入
+    })
     
     # ==============================================================================
     # Analysis and Post-Processing Configuration
     # ==============================================================================
-    processing_config: dict = Field(default_factory=dict)
-    analysis_config: dict = Field(default_factory=dict)
-    retrieval_config: dict = Field(default_factory=dict)
-    
-    # --- Chat Roles Configuration ---
-    chat_roles: Dict[str, str] = Field(default_factory=dict)
-    
+    processing_config: dict = field(default_factory=dict)
+    analysis_config: dict = field(default_factory=dict)
+    retrieval_config: dict = field(default_factory=dict)
     log_file: Optional[str] = None
 
-    def __init__(self, config_dict: Optional[Dict] = None, **values: Any):
-        """
-        Custom initializer to bridge dictionary-based configuration with Pydantic models.
-        """
-        if config_dict is None:
-            config_dict = {}
-
-        # Allow individual values to override dictionary values
-        init_data = {**config_dict, **values}
-        
-        # Pydantic V2 requires explicit initialization of sub-models from dicts
-        # if the __init__ is overridden.
-        if 'llm' in init_data and isinstance(init_data['llm'], dict):
-            init_data['llm'] = LLMConfig(**init_data['llm'])
-            
-        if 'embedding' in init_data and isinstance(init_data['embedding'], dict):
-            init_data['embedding'] = EmbeddingConfig(**init_data['embedding'])
-            
-        super().__init__(**init_data)
-        
-        # Keep the rest of the logic for dynamic/env var loading
-        self._dynamic_config_values: Dict[str, Any] = {}
-        self._load_dynamic_config()
-        self._load_user_roles()
+    def __init__(self, **data):
+        super().__init__(**data)
+        self.llm_client_config = self.config_data.get('llm_client_config', self.llm_client_config)
+        self.processing_config = self.config_data.get('processing_config', processing_config)
+        self.analysis_config = self.config_data.get('analysis_config', analysis_config)
+        self.retrieval_config = self.config_data.get('retrieval_config', {
+            "enable_inferred_relations": True,
+            "inference_model": "phi4-mini:3.8b",
+            "processing_workers": 4,
+        })
+        self._dynamic_config_values: Dict[str, Any] = {} 
+        self._load_dynamic_config() # <--- NEW: Load dynamic config on init
 
         # Check environment variable configuration
         env_vector_db = os.environ.get("ORG_SUPERTAG_VECTOR_DB")
@@ -207,9 +447,8 @@ class Config(BaseSettings):
 
         env_llm_default_model = os.environ.get("ORG_SUPERTAG_LLM_DEFAULT_MODEL")
         if env_llm_default_model:
-            if 'ollama' in self.llm.backends:
-                self.llm.backends['ollama'].default_model = env_llm_default_model
-                logging.getLogger("config").info(f"LLM Default Model set from environment: {env_llm_default_model}")
+            self.llm_client_config['default_model'] = env_llm_default_model
+            logging.getLogger("config").info(f"LLM Default Model set from environment: {env_llm_default_model}")
 
         env_llm_embedding_model = os.environ.get("ORG_SUPERTAG_LLM_EMBEDDING_MODEL")
         if env_llm_embedding_model:
@@ -220,7 +459,7 @@ class Config(BaseSettings):
         if env_llm_api_key:
             self.llm_client_config['api_key'] = env_llm_api_key
             # Be cautious logging API keys, even parts of them.
-            logging.getLogger("config").info("LLM API Key loaded from environment (not displaying value).")
+            logging.getLogger("config").info(f"LLM API Key loaded from environment (not displaying value).")
 
         # Environment variable overrides for Entity Extractor Config
         env_ee_entity_types_json = os.environ.get("ORG_SUPERTAG_EE_ENTITY_TYPES_JSON")
@@ -228,7 +467,7 @@ class Config(BaseSettings):
             try:
                 import json
                 self.entity_extractor_config['entity_types'] = json.loads(env_ee_entity_types_json)
-                logging.getLogger("config").info("Loaded Entity Extractor entity types from environment.")
+                logging.getLogger("config").info(f"Loaded Entity Extractor entity types from environment.")
             except json.JSONDecodeError:
                 logging.getLogger("config").warning("Failed to parse ORG_SUPERTAG_EE_ENTITY_TYPES_JSON from environment.")
 
@@ -313,6 +552,8 @@ class Config(BaseSettings):
             # "ollama_model": self.ollama_model, # Deprecated
             # "embedding_model": self.embedding_model, # Removed
             "llm_client_config": self.llm_client_config,
+            "use_cache": self.use_cache,
+            "cache_size": self.cache_size,
             "entity_extractor_config": self.entity_extractor_config,
             "multicore_config": self.multicore_config,
             "embedding_config": self.embedding_config,
@@ -354,7 +595,7 @@ class Config(BaseSettings):
                 logger.warning(f"Failed to load dynamic configuration from {self.dynamic_config_file_path}: {e}")
                 self._dynamic_config_values = {} # Ensure it's a dict on failure
         else:
-            logger.info("Dynamic configuration file not found or path not set. Initializing with empty dynamic config.")
+            logger.info(f"Dynamic configuration file not found or path not set. Initializing with empty dynamic config.")
             self._dynamic_config_values = {}
 
     def _save_dynamic_config(self) -> None:
@@ -375,18 +616,6 @@ class Config(BaseSettings):
                 logger.error(f"Failed to save dynamic configuration to {self.dynamic_config_file_path}: {e}")
     # --- END NEW ---
 
-    def _load_user_roles(self):
-        """Loads user-defined roles from the TOML file."""
-        self.chat_roles = {}  # Start with an empty dictionary
-        if os.path.exists(self.user_roles_file_path):
-            try:
-                with open(self.user_roles_file_path, 'r', encoding='utf-8') as f:
-                    user_roles = toml.load(f)
-                self.chat_roles.update(user_roles)
-                logging.getLogger("config").info(f"Loaded and merged user-defined roles from {self.user_roles_file_path}")
-            except Exception as e:
-                logging.getLogger("config").warning(f"Failed to load user-defined roles from {self.user_roles_file_path}: {e}")
-
     @classmethod
     def get_fast_ner_models(cls) -> Dict[str, str]:
         """获取推荐的快速实体识别模型列表
@@ -409,7 +638,7 @@ class Config(BaseSettings):
             int: 向量维度
         """
         # 获取当前配置的嵌入模型
-        embedding_model = self.embedding.llama_cpp.get("model_path", "")
+        embedding_model = self.llm_client_config.get('default_embedding_model', '')
         
         # 预定义的模型维度映射
         model_dimensions = {
@@ -456,11 +685,4 @@ class Config(BaseSettings):
         default_dim = model_dimensions['default']
         logging.getLogger("config").warning(f"No dimension mapping found for {embedding_model}, using default: {default_dim}")
         return default_dim
-
-class RAGConfig(BaseModel):
-    enabled: bool = True
-    search_top_k: int = 5
-    traversal_max_hops: int = 1
-
-class BehaviorConfig(BaseModel):
-    proactive_tag_enabled: bool = True
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
diff --git a/simtag/config_manager.py b/simtag/config_manager.py
new file mode 100755
index 0000000..2fbd924
--- /dev/null
+++ b/simtag/config_manager.py
@@ -0,0 +1,142 @@
+#!/usr/bin/env python3
+"""
+配置管理工具
+用于动态调整 org-supertag 的配置选项
+"""
+
+import sys
+from typing import Dict, Any
+from simtag.config import Config
+
+def show_current_config():
+    """显示当前配置"""
+    config = Config()
+    embedding_config = config.embedding_config
+    
+    print("=== 当前嵌入配置 ===")
+    print(f"后端: {embedding_config.get('backend', 'llama_cpp')}")
+    print(f"长文本策略: {embedding_config.get('long_text_strategy', 'truncate')}")
+    print(f"分块大小: {embedding_config.get('chunk_size', 400)} 字符")
+    print(f"分块重叠: {embedding_config.get('chunk_overlap', 50)} 字符")
+    print(f"聚合方法: {embedding_config.get('chunk_aggregation', 'mean')}")
+    print(f"最大分块数: {embedding_config.get('max_chunks', 10)}")
+    print(f"上下文长度: {embedding_config.get('llama_cpp_max_context', 512)}")
+    print(f"池化策略: {embedding_config.get('llama_cpp_pooling', 'cls')}")
+
+def set_long_text_strategy(strategy: str):
+    """设置长文本处理策略"""
+    if strategy not in ['truncate', 'chunk']:
+        print(f"错误: 无效的策略 '{strategy}'. 可选: 'truncate', 'chunk'")
+        return False
+    
+    config = Config()
+    config.embedding_config['long_text_strategy'] = strategy
+    
+    print(f"✅ 长文本策略已设置为: {strategy}")
+    
+    if strategy == 'chunk':
+        print("📝 分块策略说明:")
+        print("  - 长文本会被分割成多个块")
+        print("  - 每个块单独生成嵌入")
+        print("  - 最终结果通过聚合得到")
+        print("  - 保留完整信息但处理时间较长")
+    else:
+        print("📝 截断策略说明:")
+        print("  - 长文本会被截断到上下文长度")
+        print("  - 处理速度快但可能丢失信息")
+    
+    return True
+
+def set_chunk_config(size: int = None, overlap: int = None, aggregation: str = None, max_chunks: int = None):
+    """设置分块配置"""
+    config = Config()
+    
+    if size is not None:
+        if size < 100 or size > 1000:
+            print("错误: 分块大小应在 100-1000 之间")
+            return False
+        config.embedding_config['chunk_size'] = size
+        print(f"✅ 分块大小设置为: {size}")
+    
+    if overlap is not None:
+        if overlap < 0 or overlap > 200:
+            print("错误: 分块重叠应在 0-200 之间")
+            return False
+        config.embedding_config['chunk_overlap'] = overlap
+        print(f"✅ 分块重叠设置为: {overlap}")
+    
+    if aggregation is not None:
+        if aggregation not in ['mean', 'weighted_mean', 'max_pool']:
+            print("错误: 聚合方法应为 'mean', 'weighted_mean', 'max_pool' 之一")
+            return False
+        config.embedding_config['chunk_aggregation'] = aggregation
+        print(f"✅ 聚合方法设置为: {aggregation}")
+    
+    if max_chunks is not None:
+        if max_chunks < 1 or max_chunks > 50:
+            print("错误: 最大分块数应在 1-50 之间")
+            return False
+        config.embedding_config['max_chunks'] = max_chunks
+        print(f"✅ 最大分块数设置为: {max_chunks}")
+    
+    return True
+
+def main():
+    """主函数"""
+    if len(sys.argv) < 2:
+        print("使用方法:")
+        print("  python config_manager.py show                    # 显示当前配置")
+        print("  python config_manager.py strategy <truncate|chunk>  # 设置长文本策略")
+        print("  python config_manager.py chunk --size 400 --overlap 50 --aggregation mean --max-chunks 10")
+        print("")
+        print("示例:")
+        print("  python config_manager.py strategy chunk         # 启用分块策略")
+        print("  python config_manager.py strategy truncate      # 启用截断策略")
+        print("  python config_manager.py chunk --size 300       # 设置分块大小为300字符")
+        return
+    
+    command = sys.argv[1]
+    
+    if command == "show":
+        show_current_config()
+    
+    elif command == "strategy":
+        if len(sys.argv) < 3:
+            print("错误: 请指定策略 (truncate 或 chunk)")
+            return
+        strategy = sys.argv[2]
+        set_long_text_strategy(strategy)
+    
+    elif command == "chunk":
+        # 解析参数
+        args = sys.argv[2:]
+        size = None
+        overlap = None
+        aggregation = None
+        max_chunks = None
+        
+        i = 0
+        while i < len(args):
+            if args[i] == "--size" and i + 1 < len(args):
+                size = int(args[i + 1])
+                i += 2
+            elif args[i] == "--overlap" and i + 1 < len(args):
+                overlap = int(args[i + 1])
+                i += 2
+            elif args[i] == "--aggregation" and i + 1 < len(args):
+                aggregation = args[i + 1]
+                i += 2
+            elif args[i] == "--max-chunks" and i + 1 < len(args):
+                max_chunks = int(args[i + 1])
+                i += 2
+            else:
+                print(f"未知参数: {args[i]}")
+                return
+        
+        set_chunk_config(size, overlap, aggregation, max_chunks)
+    
+    else:
+        print(f"未知命令: {command}")
+
+if __name__ == "__main__":
+    main() 
\ No newline at end of file
diff --git a/simtag/container.py b/simtag/container.py
new file mode 100755
index 0000000..577d6cb
--- /dev/null
+++ b/simtag/container.py
@@ -0,0 +1,91 @@
+from dependency_injector import containers, providers
+from simtag.config import Config
+from simtag.core.graph_service import GraphService
+from simtag.services.llm_client import LLMClient
+from simtag.core.tagging import TaggingEngine
+from simtag.services.ner_service import NERService
+from simtag.services.embedding_service import EmbeddingService
+from simtag.core.memory_engine import MemoryEngine
+from simtag.services.memory_synthesizer import MemorySynthesizer
+from simtag.core.entity_extractor import OrgSupertagEntityExtractor, LLMEntityExtractor
+from simtag.services.content_processor import ContentProcessor, ProcessingConfig, ProcessingMode
+from simtag.core.rag_engine import OrgSupertagRAGEngine
+from simtag.services.user_interface import UserInterfaceService
+from simtag.module.node_processor import NodeProcessor
+from simtag.module.sync_handler import SyncHandler
+from simtag.module.query_handler import QueryHandler
+from simtag.module.diagnostics_handler import DiagnosticsHandler
+from simtag.module.autotag_handler import AutotagHandler
+from simtag.services.smart_ner_service import SmartNERService
+from simtag.module.resonance_handler import ResonanceHandler
+import asyncio
+import dataclasses
+
+class AppContainer(containers.DeclarativeContainer):
+    config = providers.Configuration()
+    
+    config_obj = providers.Factory(
+        lambda config_dict: Config(**{
+            key: config_dict[key]
+            for key in [f.name for f in dataclasses.fields(Config) if f.init]
+            if key in config_dict
+        }),
+        config_dict=config
+    )
+
+    # --- Unified Data Service ---
+    graph_service = providers.Singleton(
+        GraphService,
+        db_path=config.vector_db_path,
+        config=config_obj
+    )
+
+    # Core Services
+    llm_client = providers.Factory(
+        lambda config_dict: LLMClient(
+            provider=config_dict.get('llm_client_config', {}).get('provider', 'ollama'),
+            config=config_dict.get('llm_client_config', {})
+        ),
+        config_dict=config
+    )
+    embedding_service = providers.Singleton(
+        EmbeddingService,
+        config_dict=config
+    )
+    smart_ner_service = providers.Singleton(
+        SmartNERService,
+        llm_async_callable=llm_client.provided.generate,
+        config=config
+    )
+    ner_service = providers.Singleton(
+        NERService,
+        config=config.ner,
+        llm_client=llm_client,
+        smart_ner_service=smart_ner_service
+    )
+    
+    entity_extractor = providers.Singleton(
+        LLMEntityExtractor,
+        llm_client=llm_client,
+        config=config.analysis_config
+    )
+    
+    # Processing Services
+    content_processor = providers.Singleton(ContentProcessor, config=config_obj)
+    
+    # Engines
+    rag_engine = providers.Singleton(OrgSupertagRAGEngine, graph_service=graph_service, llm_client=llm_client, config=config_obj)
+    memory_engine = providers.Singleton(MemoryEngine, config=config_obj, llm_client=llm_client)
+    memory_synthesizer = providers.Singleton(MemorySynthesizer, config=config_obj, memory_engine=memory_engine, llm_client=llm_client)
+    engine = providers.Singleton(TaggingEngine, config=config_obj, graph_service=graph_service, llm_client=llm_client)
+    
+    # UI Service
+    user_interface = providers.Singleton(UserInterfaceService, graph_service=graph_service)
+    
+    # Handlers
+    node_processor = providers.Factory(NodeProcessor, config=config_obj, llm_client=llm_client, ner_service=ner_service, graph_service=graph_service, content_processor=content_processor, emacs_client=config.emacs_client, embedding_service=embedding_service, entity_extractor=entity_extractor)
+    sync_handler = providers.Factory(SyncHandler, node_processor=node_processor, engine=engine, emacs_client=config.emacs_client)
+    query_handler = providers.Factory(QueryHandler, engine=engine, user_interface=user_interface, graph_service=graph_service, emacs_client=config.emacs_client)
+    diagnostics_handler = providers.Factory(DiagnosticsHandler, config=config_obj, llm_client=llm_client, graph_service=graph_service, memory_engine=memory_engine, entity_extractor=entity_extractor, rag_engine=rag_engine, emacs_client=config.emacs_client, data_dir=config.data_dir, content_processor=content_processor)
+    autotag_handler = providers.Factory(AutotagHandler, llm_client=llm_client, ner_service=ner_service)
+    resonance_handler = providers.Factory(ResonanceHandler, graph_service=graph_service, llm_client=llm_client, config=config_obj) 
\ No newline at end of file
diff --git a/simtag/context.py b/simtag/context.py
deleted file mode 100644
index d2b47f3..0000000
--- a/simtag/context.py
+++ /dev/null
@@ -1,128 +0,0 @@
-import logging
-from typing import Optional
-
-from .config import Config
-from .core.graph_service import GraphService
-from .services.embedding_service import EmbeddingService
-from .services.llm_client import LLMClient
-from .services.rag_service import RAGService
-from simtag.module.rag_handler import RAGHandler
-from simtag.module.sync_handler import SyncHandler
-from simtag.module.autotag_handler import AutotagHandler
-from simtag.module.knowledge_handler import KnowledgeHandler
-from simtag.module.diagnostics_handler import DiagnosticsHandler
-from simtag.module.feedback_handler import FeedbackHandler
-from simtag.module.query_handler import QueryHandler
-from simtag.module.smart_companion_handler import SmartCompanionHandler
-from simtag.module.ai_handler import AIHandler
-
-logger = logging.getLogger(__name__)
-
-class AppContext:
-    """
-    A simple singleton container for holding and providing access to all
-    application-level services. This replaces the dependency-injector container
-    for a more explicit and straightforward approach to service management.
-    """
-    def __init__(self):
-        self.config: Optional[Config] = None
-        self.embedding_service: Optional[EmbeddingService] = None
-        self.llm_client: Optional[LLMClient] = None
-        self.graph_service: Optional[GraphService] = None
-        self.rag_service: Optional[RAGService] = None
-        self.sync_handler: Optional[SyncHandler] = None
-        self.autotag_handler: Optional[AutotagHandler] = None
-        self.diagnostics_handler: Optional[DiagnosticsHandler] = None
-        self.feedback_handler: Optional[FeedbackHandler] = None
-        self.query_handler: Optional[QueryHandler] = None
-        self.rag_handler: Optional[RAGHandler] = None
-        self.knowledge_handler: Optional[KnowledgeHandler] = None
-        self.smart_companion_handler: Optional[SmartCompanionHandler] = None
-        self.ai_handler: Optional[AIHandler] = None
-        self.emacs_client: Optional[any] = None # Will be set from SimTagBridge
-        self.port: Optional[int] = None # Will be set from SimTagBridge
-
-    def initialize(self, port: int, emacs_client: any):
-        """
-        Initializes all services in the correct dependency order.
-        """
-        logger.info("Initializing application context and services...")
-        
-        self.port = port
-        self.emacs_client = emacs_client
-
-        # Level 0: No dependencies
-        self.config = Config()
-        logger.info("Config loaded.")
-
-        # Level 1: Depends on Config
-        self.llm_client = LLMClient(config=self.config.llm)
-        logger.info("Core services (LLM) initialized.")
-
-        # Level 2: Depends on Level 1 services
-        self.graph_service = GraphService(
-            db_path=self.config.vector_db_path,
-            config=self.config
-        )
-        # EmbeddingService depends on both config and graph_service
-        self.embedding_service = EmbeddingService(
-            config=self.config,
-            graph_service=self.graph_service
-        )
-        logger.info("GraphService and EmbeddingService initialized.")
-        
-        # Level 3: Depends on Level 2 services (RAG services)
-        self.rag_service = RAGService(
-            llm_client=self.llm_client,
-            graph_service=self.graph_service,
-            embedding_service=self.embedding_service,
-            config=self.config
-        )
-        logger.info("RAG and Orchestration services initialized.")
-        
-        # Level 4: Handlers (depend on various services)
-        self.sync_handler = SyncHandler(
-            graph_service=self.graph_service,
-            embedding_service=self.embedding_service,
-            rag_service=self.rag_service,
-            config=self.config,
-        )
-        self.autotag_handler = AutotagHandler(
-            llm_client=self.llm_client,
-        )
-        self.diagnostics_handler = DiagnosticsHandler(
-            config=self.config,
-            graph_service=self.graph_service,
-            llm_client=self.llm_client,
-            embedding_service=self.embedding_service
-        )
-        self.feedback_handler = FeedbackHandler(
-            config=self.config
-        )
-        self.query_handler = QueryHandler(
-            graph_service=self.graph_service
-        )
-        self.rag_handler = RAGHandler(
-            rag_service=self.rag_service
-        )
-        self.knowledge_handler = KnowledgeHandler(
-            config=self.config,
-            graph_service=self.graph_service,
-            embedding_service=self.embedding_service,
-            rag_service=self.rag_service,
-            llm_client=self.llm_client
-        )
-        self.smart_companion_handler = SmartCompanionHandler(
-            autotag_handler=self.autotag_handler
-        )
-        self.ai_handler = AIHandler(
-            rag_service=self.rag_service,
-            graph_service=self.graph_service
-        )
-        # 向后兼容：旧代码可能仍访问 reasoning_handler
-        self.reasoning_handler = self.knowledge_handler
-        logger.info("All handlers initialized.")
-        logger.info("Application context fully initialized.")
-
-# Global context instance
-context = AppContext() 
\ No newline at end of file
diff --git a/simtag/core/__init__.py b/simtag/core/__init__.py
index 28c4b75..a6c6ecf 100755
--- a/simtag/core/__init__.py
+++ b/simtag/core/__init__.py
@@ -1,9 +1,22 @@
 """
-Core services for simtag.
+SimTag core package
+提供核心的标签处理和存储功能
 """
 
 from .graph_service import GraphService
+from .tagging import TaggingEngine
+from .memory_engine import MemoryEngine
+from .rag_engine import OrgSupertagRAGEngine
+from .sync import SyncOrchestrator
+from .entity_extractor import OrgSupertagEntityExtractor
 
 __all__ = [
     "GraphService",
+    "TaggingEngine",
+    "MemoryEngine",
+    "OrgSupertagRAGEngine",
+    "SyncOrchestrator",
+    "OrgSupertagEntityExtractor"
 ]
+
+# This file makes Python treat the `core` directory as a sub-package of `simtag`.
\ No newline at end of file
diff --git a/simtag/core/entity_extractor.py b/simtag/core/entity_extractor.py
new file mode 100755
index 0000000..48432d5
--- /dev/null
+++ b/simtag/core/entity_extractor.py
@@ -0,0 +1,301 @@
+"""
+Org SuperTag Entity Extractor Module
+
+This module is responsible for extracting entities from org-mode content
+by leveraging a unified, prompt-based approach with an LLM.
+"""
+import asyncio
+import logging
+import json
+import re
+from typing import Dict, List, Any, Optional, Callable, Awaitable, Tuple
+from dataclasses import dataclass, field
+from abc import ABC, abstractmethod
+
+# Import unified prompt management and tag processor
+try:
+    from ..prompts import create_prompt, DEFAULT_ENTITY_TYPES, INFER_RELATIONS_PROMPT
+    from ..utils.unified_tag_processor import UnifiedTagProcessor, TagResult
+except ImportError:
+    # Fallback for direct execution
+    import sys
+    import os
+    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+    from prompts import create_prompt, DEFAULT_ENTITY_TYPES, INFER_RELATIONS_PROMPT
+    from utils.unified_tag_processor import UnifiedTagProcessor, TagResult
+
+from simtag.services.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+@dataclass
+class ExtractedEntity:
+    """Represents a single entity (tag) extracted from the text."""
+    name: str
+    confidence: Optional[float] = None
+    reasoning: Optional[str] = None
+    source_node_id: Optional[str] = None # ID of the org-node from which it was extracted
+    # The 'type' field is retained for future use but is not populated by the current prompt.
+    type: str = "concept"
+    attributes: Dict[str, Any] = field(default_factory=dict)
+
+class ExtractedRelation:
+    def __init__(self, source: str, target: str, relationship: str, description: str):
+        self.source = source
+        self.target = target
+        self.relationship = relationship
+        self.description = description
+
+class BaseExtractor(ABC):
+    @abstractmethod
+    def extract(self, text: str) -> Tuple[List[ExtractedEntity], List[ExtractedRelation]]:
+        pass
+
+class LLMEntityExtractor(BaseExtractor):
+    """
+    Uses an LLM to extract entities and their relationships from unstructured text.
+    """
+    def __init__(self, llm_client: LLMClient, config: Dict[str, Any]):
+        self.llm_client = llm_client
+        self.config = config
+        self.prompt_template = INFER_RELATIONS_PROMPT
+
+    async def extract(self, text: str, existing_entities: List[str] = None) -> Dict[str, List[Dict[str, Any]]]:
+        """
+        Extracts entities and relations from the given text.
+
+        Args:
+            text: The text content to analyze.
+            existing_entities: A list of known entity names to provide context.
+
+        Returns:
+            A dictionary containing 'entities' and 'relations' found in the text.
+            Returns empty lists if extraction fails or text is empty.
+        """
+        if not text or not text.strip():
+            return {"entities": [], "relations": []}
+
+        try:
+            prompt = self.prompt_template.format(
+                text_content=text,
+                existing_entities=", ".join(existing_entities) if existing_entities else "N/A"
+            )
+            
+            # Use the dedicated model for inference if specified in the config
+            inference_model = self.config.get("inference_model")
+
+            # Await the asynchronous call to the LLM client, passing the specific model
+            response_text = await self.llm_client.generate(prompt, model=inference_model)
+
+            # The response is expected to be a JSON string, possibly with markdown code fences.
+            # We need to robustly parse it.
+            return self._parse_llm_response(response_text)
+
+        except Exception as e:
+            logger.error(f"Failed to extract entities/relations from text: {e}", exc_info=True)
+            return {"entities": [], "relations": []}
+
+    def _parse_llm_response(self, response_text: str) -> Dict[str, List[Dict[str, Any]]]:
+        """
+        Parses the JSON response from the LLM, handling potential markdown fences.
+        """
+        try:
+            # Clean up markdown code block fences if they exist
+            if response_text.strip().startswith("```json"):
+                response_text = response_text.strip()[7:-3].strip()
+            elif response_text.strip().startswith("```"):
+                 response_text = response_text.strip()[3:-3].strip()
+
+            data = json.loads(response_text)
+            
+            # Validate basic structure
+            entities = data.get("entities", [])
+            relations = data.get("relations", [])
+            
+            if not isinstance(entities, list) or not isinstance(relations, list):
+                logger.warning(f"LLM response has invalid structure: {data}")
+                return {"entities": [], "relations": []}
+
+            return {"entities": entities, "relations": relations}
+
+        except json.JSONDecodeError:
+            logger.error(f"Failed to decode LLM JSON response: {response_text}")
+            return {"entities": [], "relations": []}
+        except Exception as e:
+            logger.error(f"An unexpected error occurred while parsing LLM response: {e}")
+            return {"entities": [], "relations": []}
+
+# You can add other extractor implementations here if needed,
+# e.g., a SpaCy-based one for faster, less-detailed extraction.
+
+class OrgSupertagEntityExtractor:
+    """
+    Extracts entities from org-mode node content using a single-pass LLM call.
+    This class has been simplified to align with a unified prompt that returns
+    a standard JSON format, removing the need for complex parsing and multi-stage
+    "gleaning" rounds.
+    """
+
+    def __init__(self,
+                 llm_async_callable: Callable[[str], Awaitable[str]],
+                 entity_extractor_config: Optional[Dict[str, Any]] = None):
+        """
+        Initializes the OrgSupertagEntityExtractor.
+
+        Args:
+            llm_async_callable: An asynchronous function that takes a prompt string
+                                and returns the LLM completion string.
+            entity_extractor_config: Configuration dictionary for the extractor.
+                                     Supported keys:
+                                     - 'entity_types': Optional[List[str]]
+        """
+        self.llm_async_callable = llm_async_callable
+        config = entity_extractor_config if entity_extractor_config else {}
+
+        self.entity_types = config.get('entity_types', DEFAULT_ENTITY_TYPES)
+        
+        # 使用统一标签处理器
+        self.unified_processor = UnifiedTagProcessor()
+
+        logger.info(f"OrgSupertagEntityExtractor initialized with entity types: {self.entity_types}")
+
+    def _convert_tag_result_to_extracted_entity(self, tag_result: TagResult, node_id: Optional[str]) -> ExtractedEntity:
+        """将 TagResult 转换为 ExtractedEntity 以保持 API 兼容性"""
+        return ExtractedEntity(
+            name=tag_result.tag_name,
+            confidence=tag_result.confidence,
+            reasoning=tag_result.reasoning,
+            source_node_id=node_id,
+            type="concept",  # 默认类型
+            attributes={"source": tag_result.source}
+        )
+
+    async def _parse_llm_result(self, llm_output: str, node_id: Optional[str]) -> List[ExtractedEntity]:
+        """
+        使用统一标签处理器解析 LLM 输出为 ExtractedEntity 对象列表。
+        这个版本使用 UnifiedTagProcessor 来处理所有 JSON 清理和解析逻辑。
+        """
+        entities: List[ExtractedEntity] = []
+        
+        try:
+            # 使用统一处理器处理 LLM 响应
+            # 为单个节点创建临时 note_id 列表
+            temp_note_id = node_id or "temp_node"
+            note_results = self.unified_processor.process_llm_response(
+                response_str=llm_output,
+                note_ids=[temp_note_id]
+            )
+            
+            # 检查是否有处理错误
+            if note_results and note_results[0].error:
+                logger.error(f"UnifiedTagProcessor failed for node {node_id}: {note_results[0].error}")
+                return entities
+            
+            # 转换 TagResult 为 ExtractedEntity
+            if note_results and note_results[0].tags:
+                for tag_result in note_results[0].tags:
+                    entity = self._convert_tag_result_to_extracted_entity(tag_result, node_id)
+                    entities.append(entity)
+            
+            logger.debug(f"Successfully parsed {len(entities)} entities using UnifiedTagProcessor")
+            
+        except Exception as e:
+            logger.error(f"Unexpected error in _parse_llm_result for node {node_id}: {e}")
+            
+        return entities
+
+    async def extract_from_org_node(
+        self,
+        node_content: str,
+        node_id: str,
+        existing_tags: Optional[List[str]] = None
+    ) -> List[ExtractedEntity]:
+        """
+        Performs the full entity extraction process for a single org-mode node.
+
+        This process involves:
+        1. Creating a prompt using the unified `create_prompt` function.
+        2. Calling the LLM with the generated prompt.
+        3. Parsing the structured JSON result from the LLM.
+        4. Returning the list of extracted entities.
+
+        Args:
+            node_content: The text content of the org-mode node.
+            node_id: The unique identifier for the org-mode node.
+            existing_tags: A list of tags already associated with the node to avoid duplicates.
+
+        Returns:
+            A list of ExtractedEntity objects.
+        """
+        logger.debug(f"Starting entity extraction for node_id: {node_id}")
+
+        if not node_content.strip():
+            logger.debug("Node content is empty, skipping extraction.")
+            return []
+        
+        # 1. Create the prompt
+        prompt = create_prompt(
+            contents=[node_content],
+            entity_types=self.entity_types,
+            existing_tags_lists=[existing_tags or []]
+        )
+
+        # 2. Call the LLM
+        try:
+            llm_response = await self.llm_async_callable(prompt)
+        except Exception as e:
+            logger.error(f"LLM call failed during entity extraction for node {node_id}: {e}")
+            return []
+
+        if not llm_response:
+            logger.warning(f"LLM returned an empty response for node {node_id}.")
+            return []
+
+        # 3. Parse the result
+        extracted_entities = await self._parse_llm_result(llm_response, node_id)
+
+        logger.info(f"Extracted {len(extracted_entities)} entities from node {node_id}.")
+        return extracted_entities
+
+# Example usage for testing
+async def main_test_entity_extractor():
+    """A simple main function to test the entity extractor."""
+    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+    # A mock LLM callable for testing purposes
+    async def mock_llm_callable(prompt: str) -> str:
+        print("\n--- LLM Prompt ---\n")
+        print(prompt)
+        print("\n--- Mock LLM Response ---\n")
+        # Simulate a valid JSON response based on the prompt
+        mock_response = json.dumps([
+            {"tag_name": "Python", "confidence": 0.9, "reasoning": "The text explicitly mentions Python programming."},
+            {"tag_name": "Asyncio", "confidence": 0.85, "reasoning": "The example uses asyncio for concurrency."}
+        ])
+        print(mock_response)
+        return mock_response
+
+    extractor = OrgSupertagEntityExtractor(llm_async_callable=mock_llm_callable)
+
+    test_content = """
+    This is a test document about Python programming.
+    We are using the asyncio library to handle concurrent tasks.
+    It's important for modern application development.
+    Existing tags should be respected.
+    """
+    test_node_id = "test-node-123"
+    existing_tags = ["Programming"]
+
+    entities = await extractor.extract_from_org_node(
+        node_content=test_content,
+        node_id=test_node_id,
+        existing_tags=existing_tags
+    )
+
+    print(f"\n--- Final Extracted Entities ({len(entities)}) ---")
+    for entity in entities:
+        print(entity)
+
+if __name__ == '__main__':
+    # To run this test, execute `python -m simtag.core.entity_extractor` from the project root
+    asyncio.run(main_test_entity_extractor()) 
\ No newline at end of file
diff --git a/simtag/core/graph_service.py b/simtag/core/graph_service.py
index 7ea6aab..1c80d0d 100755
--- a/simtag/core/graph_service.py
+++ b/simtag/core/graph_service.py
@@ -1,55 +1,54 @@
-
 """
 Unified Knowledge Graph Service
 """
 import logging
 import sqlite3
+import numpy as np
 import json
 import threading
-import time
-from typing import Dict, List, Any, Optional, Tuple, TYPE_CHECKING
+from typing import Dict, List, Any, Optional, Set, Tuple
 from dataclasses import dataclass, field
 from enum import Enum
 from datetime import datetime
 
 # Ensure numpy is available
+NP_AVAILABLE = False
 try:
     import numpy as np
-    if TYPE_CHECKING:
-        from numpy import ndarray
+    NP_AVAILABLE = True
 except ImportError:
-    np = None
-    if TYPE_CHECKING:
-        from typing import Any as ndarray
     logging.getLogger("GraphService").error("NumPy library not found. Vector operations will be disabled.")
 
 # Configure logger for this module
 logger = logging.getLogger(__name__)
 
-# --- Enums for Node and Relation Types ---
-class NodeType(Enum):
-    TEXT = "TEXT"
-    ENTITY = "ENTITY"
-    PERSON = "PERSON"
-    ORGANIZATION = "ORGANIZATION"
-    PROJECT = "PROJECT"
-    CONCEPT = "CONCEPT"
-    EVENT = "EVENT"
-    TECHNOLOGY = "TECHNOLOGY"
-    ALIAS = "ALIAS"
-    TAG = "TAG"
-
-class RelationType(Enum):
-    HAS_ENTITY = "HAS_ENTITY"
-    REF_TO = "REF_TO"
-    IS_ALIAS_OF = "IS_ALIAS_OF"
-    MENTIONS = "MENTIONS"
-    AUTHORED_BY = "AUTHORED_BY"
-    IMPLEMENTS = "IMPLEMENTS"
-    CAUSES = "CAUSES"
-    PART_OF = "PART_OF"
-    IS_A = "IS_A"
-    RELATED_TO = "RELATED_TO"
+# --- Data Classes (can be moved to a separate file later) ---
+
+class TagStatus(Enum):
+    """Tag governance status types."""
+    DRAFT = "draft"
+    REVIEW = "review"
+    ACTIVE = "active"
+    DEPRECATED = "deprecated"
+    MERGED = "merged"
+    SPLIT = "split"
+    ARCHIVED = "archived"
+
+@dataclass
+class Rule:
+    """Represents a governance rule."""
+    type: str # RuleType enum can be used here
+    rule: Dict[str, Any]
+    description: Optional[str] = None
+    created_at: datetime = field(default_factory=datetime.now)
+
+@dataclass
+class HistoryEntry:
+    """Represents a history entry for status or type changes."""
+    timestamp: datetime
+    old_value: Any
+    new_value: Any
+    reason: Optional[str] = None
 
 class GraphService:
     """
@@ -60,6 +59,10 @@ class GraphService:
     def __init__(self, db_path: str, config=None):
         """
         Initializes the GraphService.
+
+        Args:
+            db_path: The path to the SQLite database file.
+            config: Optional configuration object.
         """
         self.db_path = db_path
         self.config = config
@@ -69,6 +72,7 @@ class GraphService:
         
         self._init_connection()
         
+        # Try to initialize database, with automatic unlock if needed
         try:
             self._init_db()
         except sqlite3.OperationalError as e:
@@ -76,8 +80,8 @@ class GraphService:
                 self.logger.warning("Database locked during initialization, attempting to force unlock...")
                 if self.force_unlock_database():
                     self.logger.info("Database unlocked, retrying initialization...")
-                    self._init_connection()
-                    self._init_db()
+                    self._init_connection()  # Reconnect
+                    self._init_db()  # Retry initialization
                 else:
                     raise Exception("Failed to unlock database during initialization") from e
             else:
@@ -96,8 +100,12 @@ class GraphService:
         """Creates and configures a new SQLite connection."""
         conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=30.0)
         
+        # Configure SQLite for better concurrency
         conn.execute("PRAGMA journal_mode=WAL")
         conn.execute("PRAGMA synchronous=NORMAL")
+        conn.execute("PRAGMA cache_size=10000")
+        conn.execute("PRAGMA temp_store=memory")
+        conn.execute("PRAGMA mmap_size=268435456")  # 256MB
         
         conn.enable_load_extension(True)
         try:
@@ -117,598 +125,821 @@ class GraphService:
         return self._init_connection()
 
     def _init_db(self):
-        """Initializes the database schema, being idempotent."""
+        """
+        Initializes the database schema, creating tables if they don't exist.
+        This method is designed to be idempotent.
+        """
+        import time
         max_retries = 5
         retry_delay = 1.0
         
         for attempt in range(max_retries):
             try:
                 conn = self._get_connection()
-                with conn:
-                    cursor = conn.cursor()
-                    cursor.execute("PRAGMA journal_mode=WAL")
+                cursor = conn.cursor()
+                
+                # Enable WAL mode if not already enabled
+                cursor.execute("PRAGMA journal_mode=WAL")
+                
+                # 1. Drop old tables for a fresh start (as agreed)
+                cursor.execute("DROP TABLE IF EXISTS relations")
+                cursor.execute("DROP TABLE IF EXISTS tags")
+                cursor.execute("DROP TABLE IF EXISTS nodes")
+                cursor.execute("DROP TABLE IF EXISTS node_embeddings_vss")
+                cursor.execute("DROP TABLE IF EXISTS tag_embeddings_vss")
+
+                # 2. Recreate Nodes Table with Type and Name
+                # 'name' is for ENTITY nodes (e.g., 'PYTHON'), for TEXT nodes it can be NULL.
+                # 'type' distinguishes between 'TEXT' and 'ENTITY'.
+                cursor.execute("""
+                CREATE TABLE IF NOT EXISTS nodes (
+                    node_id TEXT PRIMARY KEY,
+                    type TEXT NOT NULL,
+                    name TEXT,
+                    content TEXT,
+                    title TEXT,
+                    file_path TEXT,
+                    pos INTEGER,
+                    olp TEXT,
+                    level INTEGER,
+                    scheduled TEXT,
+                    deadline TEXT,
+                    todo TEXT,
+                    priority TEXT,
+                    modified_at TEXT,
+                    properties TEXT,
+                    raw_value TEXT,
+                    hash TEXT,
+                    content_hash TEXT,
+                    document_date TEXT,
+                    UNIQUE(name, type)
+                )
+                """)
+                cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_type ON nodes (type)")
+                cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_name ON nodes (name)")
+
+                # 3. Recreate Relations Table (Schema is good, just recreating)
+                # This table connects nodes from the 'nodes' table, regardless of their type.
+                cursor.execute("""
+                CREATE TABLE IF NOT EXISTS relations (
+                    relation_id TEXT PRIMARY KEY,
+                    source_id TEXT NOT NULL,
+                    target_id TEXT NOT NULL,
+                    type TEXT NOT NULL,
+                    weight REAL DEFAULT 1.0,
+                    properties TEXT,
+                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    FOREIGN KEY (source_id) REFERENCES nodes (node_id) ON DELETE CASCADE,
+                    FOREIGN KEY (target_id) REFERENCES nodes (node_id) ON DELETE CASCADE
+                )
+                """)
+                cursor.execute("CREATE INDEX IF NOT EXISTS idx_relations_source_id ON relations (source_id)")
+                cursor.execute("CREATE INDEX IF NOT EXISTS idx_relations_target_id ON relations (target_id)")
+                cursor.execute("CREATE INDEX IF NOT EXISTS idx_relations_type ON relations (type)")
+
+                # 4. Recreate a single VSS Table for all node types
+                if self.has_vector_ext:
+                    vector_dim = self.config.get_vector_dimension_for_model()
                     
-                    # Nodes Table
-                    cursor.execute("""
-                    CREATE TABLE IF NOT EXISTS nodes (
-                        node_id TEXT PRIMARY KEY,
-                        type TEXT NOT NULL,
-                        name TEXT,
-                        content TEXT,
-                        title TEXT,
-                        file_path TEXT,
-                        pos INTEGER,
-                        olp TEXT,
-                        level INTEGER,
-                        scheduled TEXT,
-                        deadline TEXT,
-                        todo TEXT,
-                        priority TEXT,
-                        modified_at TEXT,
-                        knowledge_status TEXT,
-                        properties TEXT,
-                        raw_value TEXT,
-                        hash TEXT,
-                        content_hash TEXT,
-                        document_date TEXT,
-                        relations_inferred_at TEXT,
-                        aliases TEXT,
-                        priority_score REAL
-                    )
-                    """)
-                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_type ON nodes (type)")
-                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_name ON nodes (name)")
-                    # 如果数据库已存在于旧版本，确保 knowledge_status 列存在
-                    cursor.execute("PRAGMA table_info(nodes)")
-                    existing_cols = [row[1] for row in cursor.fetchall()]
-                    if 'knowledge_status' not in existing_cols:
-                        self.logger.info("Adding missing column 'knowledge_status' to existing nodes table…")
-                        cursor.execute("ALTER TABLE nodes ADD COLUMN knowledge_status TEXT")
-
-                    # 添加索引（在确保列存在之后）
-                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_knowledge_status ON nodes (knowledge_status)")
-
-                    # Relations Table
-                    cursor.execute("""
-                    CREATE TABLE IF NOT EXISTS relations (
-                        relation_id TEXT PRIMARY KEY,
-                        source_id TEXT NOT NULL,
-                        target_id TEXT NOT NULL,
-                        type TEXT NOT NULL,
-                        weight REAL DEFAULT 1.0,
-                        properties TEXT,
-                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                        FOREIGN KEY (source_id) REFERENCES nodes (node_id) ON DELETE CASCADE,
-                        FOREIGN KEY (target_id) REFERENCES nodes (node_id) ON DELETE CASCADE
+                    cursor.execute(f"""
+                    CREATE VIRTUAL TABLE IF NOT EXISTS node_embeddings_vss USING vec0(
+                        embedding FLOAT[{vector_dim}]
                     )
                     """)
-                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_relations_source_id ON relations (source_id)")
-                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_relations_target_id ON relations (target_id)")
-                    
-                    # VSS Table
-                    if self.has_vector_ext:
-                        vector_dim = self.config.get_vector_dimension_for_model()
-                        cursor.execute(f"""
-                        CREATE VIRTUAL TABLE IF NOT EXISTS node_embeddings_vss USING vec0(
-                            embedding FLOAT[{vector_dim}],
-                            node_id TEXT
-                        )
-                        """
-                        )
-                        cursor.execute(f"""
-                        CREATE VIRTUAL TABLE IF NOT EXISTS entity_embeddings_vss USING vec0(
-                            embedding FLOAT[{vector_dim}],
-                            entity_id TEXT
-                        )
-                        """
-                        )
-                    
-                self.logger.info("Database schema initialized.")
-                break
+
+                conn.commit()
+                self.logger.info("Database schema re-initialized for heterogenous graph structure.")
+                break  # Success, exit retry loop
                 
             except sqlite3.OperationalError as e:
                 if "database is locked" in str(e).lower() and attempt < max_retries - 1:
-                    self.logger.warning(f"Database locked, retrying... ({attempt + 1})")
+                    self.logger.warning(f"Database locked, retrying in {retry_delay}s (attempt {attempt + 1}/{max_retries})")
                     time.sleep(retry_delay)
-                    retry_delay *= 2
+                    retry_delay *= 2  # Exponential backoff
                     continue
                 else:
-                    self.logger.error(f"Failed to initialize database: {e}", exc_info=True)
+                    self.logger.error(f"Failed to initialize database schema after {max_retries} attempts: {e}")
                     raise
+            except Exception as e:
+                self.logger.error(f"Unexpected error during database initialization: {e}")
+                raise
 
-    # --- Schema Helpers ---
-
-    def get_table_columns(self, table_name: str) -> List[str]:
-        """Gets a list of column names for a given table."""
-        conn = self._get_connection()
-        cursor = conn.cursor()
-        cursor.execute(f"PRAGMA table_info({table_name})")
-        return [row[1] for row in cursor.fetchall()]
-
-    # --- Knowledge-status helpers ---
+    # --- Public API ---
 
-    def update_node_status(self, node_id: str, status: str):
-        """Updates the knowledge_status of a node."""
-        conn = self._get_connection()
-        with conn:
-            conn.execute(
-                "UPDATE nodes SET knowledge_status = ?, modified_at = ? WHERE node_id = ?",
-                (status, datetime.now().isoformat(), node_id),
-            )
+    # --- Node Operations ---
 
-    def get_nodes_by_status(self, status: str, limit: int = 10) -> List[str]:
-        """Returns up to <limit> node IDs with the given knowledge_status."""
+    def upsert_text_node(self, node_data: Dict[str, Any]):
+        """
+        Upserts a single text node, its associated entities (tags), and its references.
+        This is the primary method for adding content to the graph.
+        """
         conn = self._get_connection()
-        cursor = conn.cursor()
-        cursor.execute(
-            "SELECT node_id FROM nodes WHERE knowledge_status = ? ORDER BY priority_score DESC, modified_at DESC LIMIT ?",
-            (status, limit),
-        )
-        return [row[0] for row in cursor.fetchall()]
+        try:
+            # 1. Prepare and separate data
+            text_node_id = node_data['node_id']
+            tags = node_data.pop('tags', []) # Extract tags
+            references_to_raw = node_data.pop('ref_to', []) # Extract references
+
+            # The 'ref_to' data might be a JSON string, so we need to parse it.
+            references_to = []
+            if isinstance(references_to_raw, str):
+                try:
+                    parsed_refs = json.loads(references_to_raw)
+                    if isinstance(parsed_refs, list):
+                        references_to = parsed_refs
+                except json.JSONDecodeError:
+                    self.logger.warning(f"Could not parse 'ref_to' JSON string for node {text_node_id}: {references_to_raw}")
+            elif isinstance(references_to_raw, list):
+                references_to = references_to_raw
+            
+            node_to_upsert = {**node_data, 'type': 'TEXT'}
+            self._upsert_nodes_internal([node_to_upsert])
+
+            # 2. Process entities (tags)
+            if tags:
+                entity_nodes = []
+                for tag_name in tags:
+                    entity_id, entity_node = self._prepare_entity_node(tag_name)
+                    entity_nodes.append(entity_node)
+                
+                # Upsert entity nodes
+                self._upsert_nodes_internal(entity_nodes)
+
+                # Create tag relations
+                tag_relations = []
+                for tag_name in tags:
+                    entity_id = self._get_entity_id_by_name(tag_name)
+                    if entity_id:
+                        tag_relations.append({
+                            'source_id': text_node_id,
+                            'target_id': entity_id,
+                            'type': 'HAS_ENTITY'
+                        })
+                
+                if tag_relations:
+                    self.bulk_upsert_relations(tag_relations)
+            
+            # 3. Process direct references (links)
+            if references_to:
+                ref_relations = []
+                for target_id in references_to:
+                    if isinstance(target_id, str) and target_id:
+                        ref_relations.append({
+                            'source_id': text_node_id,
+                            'target_id': target_id,
+                            'type': 'REF_TO'
+                        })
+                if ref_relations:
+                    self.bulk_upsert_relations(ref_relations)
+
+            conn.commit()
+            self.logger.info(f"Successfully upserted node {text_node_id} with {len(tags)} entities and {len(references_to)} references.")
 
-    # --- Node and Relation Operations ---
-    
-    def upsert_entity(self, entity_data: Dict[str, Any]):
+        except Exception as e:
+            conn.rollback()
+            self.logger.error(f"Failed to upsert text node {node_data.get('node_id')}: {e}", exc_info=True)
+            raise
+        finally:
+            # Committing should happen here after all operations for the node are successful.
+            # However, since this function is now part of a larger transaction in NodeProcessor,
+            # we let the caller manage the commit/rollback.
+            # For standalone use, a commit would be here.
+            pass
+
+    def bulk_upsert_entity_nodes(self, entities_data: List[Dict[str, str]]):
         """
-        Inserts or updates an entity (node or tag), handling both dict and list-of-pairs input,
-        including nested structures like the 'properties' field.
+        Bulk upserts entity nodes from a list of dictionaries containing name and description.
         """
-        # --- Robust Data Normalization (Level 1: Top-level entity object) ---
-        if isinstance(entity_data, list):
-            try:
-                entity_data = {str(item[0]): item[1] for item in entity_data if isinstance(item, (list, tuple)) and len(item) == 2}
-            except (TypeError, IndexError) as e:
-                self.logger.error(f"Failed to convert top-level entity alist to dict: {entity_data}. Error: {e}")
-                return
-
-        if not isinstance(entity_data, dict):
-            self.logger.error(f"upsert_entity expects a dictionary, but received type {type(entity_data)}.")
-            return
-
-        # --- Robust Data Normalization (Level 2: Nested 'properties' field) ---
-        props = entity_data.get('properties')
-        if isinstance(props, list):
-            try:
-                entity_data['properties'] = {str(item[0]): item[1] for item in props if isinstance(item, (list, tuple)) and len(item) == 2}
-            except (TypeError, IndexError) as e:
-                self.logger.error(f"Failed to convert nested properties alist to dict: {props}. Error: {e}")
-                # Continue with empty properties instead of failing the whole entity
-                entity_data['properties'] = {}
-
-        # Map 'id' to 'node_id' for consistency
-        if 'id' in entity_data and 'node_id' not in entity_data:
-            entity_data['node_id'] = entity_data['id']
-            
-        node_id = entity_data.get('node_id')
-        if not node_id:
-            self.logger.warning(f"Skipping entity upsert due to missing 'node_id'. Data: {entity_data}")
+        if not entities_data:
             return
 
-        # --- Default knowledge_status handling ---
-        # If the incoming entity represents a node (not a tag, etc.) and the caller
-        # 没有显式设置 knowledge_status，则默认将其标记为 "PENDING"，
-        # 这样它会被后台知识提取流程自动拾取。
-        if entity_data.get('type') == 'node' and 'knowledge_status' not in entity_data:
-            entity_data['knowledge_status'] = 'PENDING'
-
         conn = self._get_connection()
-        with conn:
-            cursor = conn.cursor()
-            cursor.execute("SELECT properties FROM nodes WHERE node_id = ?", (node_id,))
-            existing = cursor.fetchone()
-
-            if existing:
-                # UPDATE
-                existing_props = json.loads(existing[0] or '{}')
-                new_props = entity_data.get('properties', {})
-
-                # --- Final Data Integrity Check ---
-                # Ensure both existing and new props are dictionaries before merging.
-                # This handles historical data that might have been stored incorrectly.
-                if isinstance(existing_props, list):
-                    try:
-                        existing_props = {str(item[0]): item[1] for item in existing_props if isinstance(item, (list, tuple)) and len(item) == 2}
-                    except (TypeError, IndexError):
-                        existing_props = {} # Default to empty dict on failure
-                
-                if isinstance(new_props, list):
-                    try:
-                        new_props = {str(item[0]): item[1] for item in new_props if isinstance(item, (list, tuple)) and len(item) == 2}
-                    except (TypeError, IndexError):
-                        new_props = {} # Default to empty dict on failure
-
-                # Merge descriptions if both exist
-                new_desc = new_props.get('description')
-                if new_desc and existing_props.get('description'):
-                    new_props['description'] = f"{existing_props.get('description')}\n---\n{new_desc}"
-                
-                merged_props = {**existing_props, **new_props}
-                entity_data['properties'] = json.dumps(merged_props)
-
-                valid_columns = self.get_table_columns('nodes')
-                update_clauses = [f"`{k}` = ?" for k in entity_data if k != 'node_id' and k in valid_columns]
-                
-                update_values = []
-                for k, v in entity_data.items():
-                    if k != 'node_id' and k in valid_columns:
-                        if isinstance(v, (dict, list)) and k != 'properties':
-                             update_values.append(json.dumps(v))
-                        else:
-                            update_values.append(v)
-                
-                if not update_clauses: return
-                update_values.append(node_id)
-                sql = f"UPDATE nodes SET {', '.join(update_clauses)} WHERE node_id = ?"
-                cursor.execute(sql, tuple(update_values))
+        try:
+            entity_nodes_to_upsert = []
+            for entity_info in entities_data:
+                name = entity_info.get('name')
+                description = entity_info.get('description')
+                if not name:
+                    continue
 
-            else:
-                # INSERT
-                props = entity_data.get('properties', {})
-                if isinstance(props, list):
-                    try:
-                        entity_data['properties'] = {str(item[0]): item[1] for item in props if isinstance(item, (list, tuple)) and len(item) == 2}
-                    except (TypeError, IndexError):
-                        entity_data['properties'] = {}
-                
-                valid_columns = self.get_table_columns('nodes')
-                cols = [c for c in valid_columns if c in entity_data]
-                vals = []
-                for c in cols:
-                    v = entity_data.get(c)
-                    if isinstance(v, (dict, list)):
-                        vals.append(json.dumps(v))
-                    else:
-                        vals.append(v)
+                _, entity_node = self._prepare_entity_node(name)
+                entity_node['properties'] = json.dumps({'description': description})
+                entity_nodes_to_upsert.append(entity_node)
+            
+            if entity_nodes_to_upsert:
+                self._upsert_nodes_internal(entity_nodes_to_upsert)
+                # The commit is handled by the calling context (NodeProcessor)
                 
-                placeholders = ', '.join(['?' for _ in cols])
-                sql = f"INSERT INTO nodes ({', '.join(f'`{c}`' for c in cols)}) VALUES ({placeholders})"
-                cursor.execute(sql, tuple(vals))
+        except Exception as e:
+            self.logger.error(f"Failed during bulk entity node upsert: {e}", exc_info=True)
+            # No rollback here, let the higher-level transaction manager handle it.
+            raise
+
+    def _prepare_entity_node(self, name: str) -> Tuple[str, Dict[str, Any]]:
+        """Prepares a dictionary representing an ENTITY node."""
+        if not isinstance(name, str) or not name.strip():
+            # Return a placeholder for invalid input to avoid downstream errors
+            invalid_name = f"INVALID_NAME_{datetime.now().isoformat()}"
+            self.logger.warning(f"Invalid entity name provided (type: {type(name)}). Using placeholder: {invalid_name}")
+            name = invalid_name
+
+        # Normalize name to create a consistent ID
+        normalized_name = name.strip().upper()
+        entity_id = f"ENTITY_{normalized_name}"
+        return entity_id, {
+            "node_id": entity_id,
+            "type": "ENTITY",
+            "name": normalized_name,
+            "title": name, # Store original casing for display
+            "modified_at": datetime.now().isoformat()
+        }
 
-    def upsert_relationship(self, source_id: str, target_id: str, type: str, properties: Optional[Dict[str, Any]] = None):
-        """Inserts or updates a relationship, merging properties."""
-        properties = properties or {}
+    def _get_entity_id_by_name(self, name: str) -> Optional[str]:
+        """Gets an entity's node_id by its normalized name."""
+        if not isinstance(name, str):
+            self.logger.warning(f"_get_entity_id_by_name called with non-string argument: {name} (type: {type(name)}). This may indicate an issue with the LLM output. Returning None.")
+            return None
+        normalized_name = name.strip().upper()
         conn = self._get_connection()
-        relation_id = f"{source_id}-{type}-{target_id}"
-        properties.setdefault('weight', 1.0)
-        
-        with conn:
-            cursor = conn.cursor()
-            cursor.execute("SELECT properties, weight FROM relations WHERE relation_id = ?", (relation_id,))
-            existing = cursor.fetchone()
-
-            if existing:
-                # UPDATE
-                existing_props = json.loads(existing[0] or '{}')
-                existing_weight = existing[1]
-                
-                final_weight = existing_weight + properties.get('weight', 1.0)
-                merged_props = {**existing_props, **properties}
+        cursor = conn.cursor()
+        cursor.execute("SELECT node_id FROM nodes WHERE type='ENTITY' AND name=?", (normalized_name,))
+        result = cursor.fetchone()
+        return result[0] if result else None
 
-                sql = "UPDATE relations SET weight = ?, properties = ? WHERE relation_id = ?"
-                cursor.execute(sql, (final_weight, json.dumps(merged_props, indent=2), relation_id))
+    def _format_node_output(self, node_row: sqlite3.Row) -> Optional[Dict[str, Any]]:
+        """Formats a raw database row into a structured node dictionary."""
+        if not node_row:
+            return None
+        
+        node_dict = dict(node_row)
+        
+        # Deserialize JSON fields
+        node_dict['properties'] = json.loads(node_dict.get('properties', '{}') or '{}')
+        
+        # For TEXT nodes, dynamically fetch associated tags from relations
+        if node_dict.get('type') == 'TEXT':
+            tags = self._get_tags_for_node(node_dict['node_id'])
+            node_dict['tags'] = tags
+        
+        return node_dict
 
-            else:
-                # INSERT
-                sql = "INSERT INTO relations (relation_id, source_id, target_id, type, weight, properties) VALUES (?, ?, ?, ?, ?, ?)"
-                props_json = json.dumps(properties, indent=2)
-                cursor.execute(sql, (relation_id, source_id, target_id, type, properties['weight'], props_json))
-
-            # --- Tag Incremental Update ---
-            if type == "HAS_TAG":
-                cursor.execute("SELECT type FROM nodes WHERE node_id = ?", (target_id,))
-                row = cursor.fetchone()
-                if row and row[0] == "TAG":
-                    self.update_node_status(target_id, "STALE")
-
-    def mark_node_relations_inferred(self, node_id: str):
-        """Marks a node to indicate that its relations have been inferred."""
+    def _get_tags_for_node(self, node_id: str) -> List[str]:
+        """Retrieves all tag names associated with a given text node."""
         conn = self._get_connection()
-        with conn:
-            cursor = conn.cursor()
-            cursor.execute("UPDATE nodes SET relations_inferred_at = ? WHERE node_id = ?", (datetime.now().isoformat(), node_id))
+        cursor = conn.cursor()
+        sql = """
+            SELECT n.title FROM relations r
+            JOIN nodes n ON r.target_id = n.node_id
+            WHERE r.source_id = ? AND r.type = 'HAS_ENTITY' AND n.type = 'ENTITY'
+        """
+        cursor.execute(sql, (node_id,))
+        return [row[0] for row in cursor.fetchall()]
+
+    def _upsert_nodes_internal(self, nodes_data: List[Dict[str, Any]]):
+        """Helper to bulk upsert nodes, ensuring all required fields are present."""
+        if not nodes_data:
+            return
 
-    def get_nodes_needing_relation_inference(self, limit: int = 5) -> List[str]:
-        """Fetches node IDs that haven't had their relations inferred yet."""
         conn = self._get_connection()
         cursor = conn.cursor()
-        query = "SELECT node_id FROM nodes WHERE relations_inferred_at IS NULL ORDER BY priority_score DESC, modified_at DESC LIMIT ?"
-        cursor.execute(query, (limit,))
-        return [row[0] for row in cursor.fetchall()]
 
-    def _format_node_output(self, node_row: sqlite3.Row) -> Optional[Dict[str, Any]]:
-        """Formats a database row into a node dictionary."""
-        if not node_row: return None
-        node_dict = dict(node_row)
-        if node_dict.get('properties'):
-            try:
-                node_dict['properties'] = json.loads(node_dict['properties'])
-            except (json.JSONDecodeError, TypeError):
-                node_dict['properties'] = {}
-        return node_dict
+        # Define all columns to ensure order
+        columns = [
+            'node_id', 'type', 'name', 'content', 'title', 'file_path', 'pos',
+            'olp', 'level', 'scheduled', 'deadline', 'todo', 'priority',
+            'modified_at', 'properties', 'raw_value', 'hash', 'content_hash',
+            'document_date'
+        ]
+
+        nodes_to_upsert = []
+        for node in nodes_data:
+            # Ensure all columns have a value (default to None if missing)
+            values = []
+            for col in columns:
+                val = node.get(col)
+                # Convert list types to string for datetime fields
+                if col in ['scheduled', 'deadline', 'modified_at', 'created_at', 'document_date'] and isinstance(val, list):
+                    # Elisp time lists are typically [high low microsecs picosecs]
+                    # For now, just convert to string representation
+                    val = str(val) if val else None
+                # Ensure olp is serialized as string if it's a list
+                elif col == 'olp' and isinstance(val, list):
+                    val = json.dumps(val)
+                # Ensure properties is serialized as string
+                elif col == 'properties' and isinstance(val, dict):
+                    val = json.dumps(val)
+                # Handle any other list types by converting to string
+                elif isinstance(val, list):
+                    val = json.dumps(val) if val else None
+                values.append(val)
+            nodes_to_upsert.append(tuple(values))
+
+        # SQL for upserting
+        placeholders = ', '.join(['?'] * len(columns))
+        sql = f"""
+            INSERT INTO nodes ({', '.join(columns)})
+            VALUES ({placeholders})
+            ON CONFLICT(node_id) DO UPDATE SET
+                type=excluded.type,
+                name=excluded.name,
+                content=excluded.content,
+                title=excluded.title,
+                file_path=excluded.file_path,
+                pos=excluded.pos,
+                olp=excluded.olp,
+                level=excluded.level,
+                scheduled=excluded.scheduled,
+                deadline=excluded.deadline,
+                todo=excluded.todo,
+                priority=excluded.priority,
+                modified_at=excluded.modified_at,
+                properties=excluded.properties,
+                raw_value=excluded.raw_value,
+                hash=excluded.hash,
+                content_hash=excluded.content_hash,
+                document_date=excluded.document_date
+        """
+        cursor.executemany(sql, nodes_to_upsert)
 
     def get_node_by_id(self, node_id: str) -> Optional[Dict[str, Any]]:
-        """Retrieves a single node by its ID."""
+        """Retrieves a single node by its ID, including its tags."""
         conn = self._get_connection()
         conn.row_factory = sqlite3.Row
         cursor = conn.cursor()
         cursor.execute("SELECT * FROM nodes WHERE node_id = ?", (node_id,))
-        node_row = cursor.fetchone()
-        conn.row_factory = None
-        return self._format_node_output(node_row)
+        row = cursor.fetchone()
+        conn.row_factory = None # Reset row factory
+        return self._format_node_output(row)
 
     def get_nodes_by_ids(self, node_ids: List[str]) -> List[Dict[str, Any]]:
-        """Retrieves a list of nodes by their IDs."""
-        if not node_ids: return []
-        conn = self._get_connection()
-        conn.row_factory = sqlite3.Row
+        """Retrieves multiple nodes' metadata by their IDs."""
+        if not node_ids:
+            return []
+        
         placeholders = ','.join('?' for _ in node_ids)
+        query = f"SELECT * FROM nodes WHERE node_id IN ({placeholders})"
+        
+        conn = self._get_connection()
         cursor = conn.cursor()
-        cursor.execute(f"SELECT * FROM nodes WHERE node_id IN ({placeholders})", node_ids)
-        results = [self._format_node_output(row) for row in cursor.fetchall()]
-        conn.row_factory = None
-        return results
+        cursor.execute(query, node_ids)
+        
+        return [self._format_node_output(row) for row in cursor.fetchall()]
 
     def delete_nodes_by_ids(self, node_ids: List[str]):
-        """Deletes one or more nodes and their associated data."""
-        if not node_ids: return
-        conn = self._get_connection()
-        with conn:
-            placeholders = ','.join('?' for _ in node_ids)
-            conn.execute(f"DELETE FROM nodes WHERE node_id IN ({placeholders})", node_ids)
-            if self.has_vector_ext:
-                # 清理两个嵌入表以确保完整性
-                conn.execute(f"DELETE FROM node_embeddings_vss WHERE node_id IN ({placeholders})", node_ids)
-                conn.execute(f"DELETE FROM entity_embeddings_vss WHERE entity_id IN ({placeholders})", node_ids)
-
-    # --- Vector Operations ---
-
-    def get_node_embedding_by_id(self, node_id: str) -> Optional["ndarray"]:
-        """Retrieves the vector embedding for a single node."""
-        if not self.has_vector_ext or not np: return None
+        """
+        Deletes a list of nodes and their associated VSS embeddings by their IDs.
+        This operation is performed within a single transaction.
+        Relationships are handled by the 'ON DELETE CASCADE' foreign key constraint.
+        """
+        if not node_ids:
+            self.logger.info("delete_nodes_by_ids called with an empty list.")
+            return
+
         conn = self._get_connection()
         cursor = conn.cursor()
-        cursor.execute("SELECT embedding FROM node_embeddings_vss WHERE node_id = ?", (node_id,))
-        result = cursor.fetchone()
-        return np.frombuffer(result[0], dtype=np.float32) if result and result[0] and isinstance(result[0], bytes) else None
 
-    def find_similar_nodes(self, query_embedding: List[float], top_k=10) -> List[Dict[str, Any]]:
-        """Finds nodes with embeddings similar to the query embedding."""
-        if not self.has_vector_ext or not np: return []
-        query_embedding_np = np.array(query_embedding, dtype=np.float32)
+        try:
+            # 1. Get rowids of nodes to be deleted BEFORE deleting them.
+            # This is crucial for cleaning up the VSS table.
+            id_placeholders = ','.join('?' for _ in node_ids)
+            rowid_query = f"SELECT rowid FROM nodes WHERE node_id IN ({id_placeholders})"
+            cursor.execute(rowid_query, node_ids)
+            rowids_to_delete = [row[0] for row in cursor.fetchall()]
+            
+            # 2. Delete from the main 'nodes' table.
+            # The 'ON DELETE CASCADE' on the 'relations' table will automatically
+            # clean up any relationships pointing to or from these nodes.
+            delete_nodes_query = f"DELETE FROM nodes WHERE node_id IN ({id_placeholders})"
+            cursor.execute(delete_nodes_query, node_ids)
+            deleted_node_count = cursor.rowcount
+            self.logger.info(f"Deleted {deleted_node_count} records from the 'nodes' table.")
+
+            # 3. Delete from the VSS table using the collected rowids.
+            if self.has_vector_ext and rowids_to_delete:
+                vss_placeholders = ','.join('?' for _ in rowids_to_delete)
+                delete_vss_query = f"DELETE FROM node_embeddings_vss WHERE rowid IN ({vss_placeholders})"
+                cursor.execute(delete_vss_query, rowids_to_delete)
+                self.logger.info(f"Deleted {cursor.rowcount} embeddings from the VSS table.")
+
+            # 4. Commit the transaction.
+            conn.commit()
+            self.logger.info(f"Successfully deleted {deleted_node_count} nodes and their related data.")
+
+        except Exception as e:
+            conn.rollback()
+            self.logger.error(f"Failed to delete nodes: {e}", exc_info=True)
+            raise
+
+    # --- Tag Operations ---
+    def bulk_upsert_tags(self, tags_data: List[Dict[str, Any]]):
+        """Bulk inserts or updates tag metadata."""
+        if not tags_data:
+            return
+
         conn = self._get_connection()
         cursor = conn.cursor()
-        # 使用 entity_embeddings_vss 表进行统一搜索
-        try:
-            # 使用正确的 sqlite-vec KNN 查询语法
-            cursor.execute(
-                "SELECT entity_id, distance FROM entity_embeddings_vss WHERE embedding MATCH ? AND k = ?",
-                (query_embedding_np.tobytes(), top_k)
+
+        sql = """
+            INSERT INTO tags (tag_id, name, description, modified_at)
+            VALUES (?, ?, ?, ?)
+            ON CONFLICT(tag_id) DO UPDATE SET
+                name=excluded.name,
+                description=excluded.description,
+                modified_at=excluded.modified_at;
+        """
+
+        tags_to_upsert = [
+            (
+                tag.get('tag_id'), tag.get('name'), tag.get('description'),
+                tag.get('modified_at', datetime.now().isoformat())
             )
-        except sqlite3.OperationalError as e:
-            self.logger.error(f"Vector search failed: {e}")
-            return []
-        
-        similar_entities_with_distance = cursor.fetchall()
-        similar_entity_ids = [row[0] for row in similar_entities_with_distance]
-        # 修复：使用 entity_id 作为键，因为在统一嵌入系统中，entity_id 就是 node_id
-        full_entity_data_map = {entity['node_id']: entity for entity in self.get_nodes_by_ids(similar_entity_ids)}
-        results = []
-        for entity_id, distance in similar_entities_with_distance:
-            # 修复：在统一嵌入系统中，entity_id 就是 node_id
-            if entity_id in full_entity_data_map:
-                entity_data = full_entity_data_map[entity_id]
-                entity_data['distance'] = distance
-                results.append(entity_data)
-        return results
-
-    def upsert_node_embedding(self, node_id: str, embedding: "ndarray"):
-        """Inserts or updates a node's vector embedding."""
-        if not self.has_vector_ext or not np: return
-        conn = self._get_connection()
-        with conn:
-            cursor = conn.cursor()
-            cursor.execute("REPLACE INTO node_embeddings_vss (node_id, embedding) VALUES (?, ?)", (node_id, embedding.tobytes()))
+            for tag in tags_data
+        ]
 
-    def get_entity_embedding_by_id(self, entity_id: str) -> Optional["ndarray"]:
-        """Retrieves the vector embedding for a single entity."""
-        if not self.has_vector_ext or not np: return None
+        try:
+            cursor.executemany(sql, tags_to_upsert)
+            conn.commit()
+            self.logger.info(f"Successfully upserted {len(tags_to_upsert)} tags.")
+        except Exception as e:
+            conn.rollback()
+            self.logger.error(f"Failed to bulk upsert tags: {e}", exc_info=True)
+            raise
+
+    def get_tag_by_name(self, name: str) -> Optional[Dict[str, Any]]:
+        """Retrieves a single tag by its name (case-insensitive)."""
         conn = self._get_connection()
+        conn.row_factory = sqlite3.Row
         cursor = conn.cursor()
-        cursor.execute("SELECT embedding FROM entity_embeddings_vss WHERE entity_id = ?", (entity_id,))
-        result = cursor.fetchone()
-        return np.frombuffer(result[0], dtype=np.float32) if result and result[0] and isinstance(result[0], bytes) else None
+        cursor.execute("SELECT * FROM tags WHERE name = ?", (name,))
+        row = cursor.fetchone()
+        conn.row_factory = None
+        return dict(row) if row else None
+
+    # --- Relation Operations ---
+    def bulk_upsert_relations(self, relations_data: List[Dict[str, Any]]):
+        """Bulk inserts or updates relations."""
+        if not relations_data:
+            return
 
-    def upsert_entity_embedding(self, entity_id: str, embedding: "ndarray"):
-        """Inserts or updates an entity's description vector embedding."""
-        if not self.has_vector_ext or not np: return
         conn = self._get_connection()
-        with conn:
-            cursor = conn.cursor()
-            cursor.execute("REPLACE INTO entity_embeddings_vss (entity_id, embedding) VALUES (?, ?)", (entity_id, embedding.tobytes()))
-
-    def find_similar_entities(self, query_embedding: List[float], top_k=5) -> List[Dict[str, Any]]:
-        """Finds entities with descriptions similar to the query embedding."""
-        if not self.has_vector_ext or not np: return []
-        query_embedding_np = np.array(query_embedding, dtype=np.float32)
+        cursor = conn.cursor()
+
+        sql = """
+            INSERT INTO relations (relation_id, source_id, target_id, type, weight, properties)
+            VALUES (?, ?, ?, ?, ?, ?)
+            ON CONFLICT(relation_id) DO UPDATE SET
+                source_id=excluded.source_id,
+                target_id=excluded.target_id,
+                type=excluded.type,
+                weight=excluded.weight,
+                properties=excluded.properties;
+        """
+
+        relations_to_upsert = [
+            (
+                rel.get('relation_id'), rel.get('source_id'), rel.get('target_id'),
+                rel.get('type'), rel.get('weight', 1.0),
+                json.dumps(rel.get('properties', {}))
+            )
+            for rel in relations_data
+        ]
+
+        try:
+            cursor.executemany(sql, relations_to_upsert)
+            conn.commit()
+            self.logger.info(f"Successfully upserted {len(relations_to_upsert)} relations.")
+        except Exception as e:
+            conn.rollback()
+            self.logger.error(f"Failed to bulk upsert relations: {e}", exc_info=True)
+            raise
+
+    # --- Embedding and Vector Search Operations ---
+
+    def get_node_embedding_by_id(self, node_id: str) -> Optional[np.ndarray]:
+        """Retrieves a node's embedding from the VSS table by its ID (rowid)."""
+        if not self.has_vector_ext:
+            self.logger.warning("Vector extension not available.")
+            return None
+        
         conn = self._get_connection()
         cursor = conn.cursor()
+        
         try:
-            # 使用正确的 sqlite-vec KNN 查询语法
-            cursor.execute(
-                "SELECT entity_id, distance FROM entity_embeddings_vss WHERE embedding MATCH ? AND k = ?",
-                (query_embedding_np.tobytes(), top_k)
-            )
-        except sqlite3.OperationalError as e:
-            self.logger.error(f"Vector search failed: {e}")
+            # sqlite-vec uses rowid to reference vectors. We assume node_id can be mapped to it.
+            # A common pattern is to use a text ID that is also the rowid IF it's an INTEGER PRIMARY KEY.
+            # Since our node_id is TEXT, we must have a mapping.
+            # Let's assume for now that the node_id is stored as the rowid for simplicity.
+            # This is a key design point to get right.
+            # A lookup table node_id -> rowid would be needed.
+            # We will perform this lookup now.
+            node_rowid = cursor.execute("SELECT rowid FROM nodes WHERE node_id = ?", (node_id,)).fetchone()
+            if not node_rowid:
+                self.logger.debug(f"Node with id {node_id} not found, cannot get embedding.")
+                return None
+            
+            rowid = node_rowid[0]
+            cursor.execute("SELECT embedding FROM node_embeddings_vss WHERE rowid = ?", (rowid,))
+            row = cursor.fetchone()
+            if row and row[0]:
+                if isinstance(row[0], bytes):
+                    return np.frombuffer(row[0], dtype=np.float32)
+                elif isinstance(row[0], str):
+                    return np.array(json.loads(row[0]), dtype=np.float32)
+            return None
+        except Exception as e:
+            self.logger.error(f"Could not retrieve node embedding for id {node_id}: {e}", exc_info=True)
+            return None
+
+    def find_similar_nodes(self, query_vector: np.ndarray, top_k: int = 10) -> List[Tuple[str, float]]:
+        """
+        Finds similar nodes using vector search.
+        It now searches only text nodes.
+        """
+        if not self.has_vector_ext or not NP_AVAILABLE:
+            self.logger.warning("Vector search is disabled.")
             return []
+
+        conn = self._get_connection()
+        cursor = conn.cursor()
         
-        similar_entities_with_distance = cursor.fetchall()
-        similar_entity_ids = [row[0] for row in similar_entities_with_distance]
-        # 获取完整的实体数据并添加距离信息
-        full_entity_data_map = {entity['node_id']: entity for entity in self.get_nodes_by_ids(similar_entity_ids)}
-        results = []
-        for entity_id, distance in similar_entities_with_distance:
-            if entity_id in full_entity_data_map:
-                entity_data = full_entity_data_map[entity_id]
-                entity_data['distance'] = distance
-                results.append(entity_data)
-        return results
+        query_vector_list = query_vector.tolist()
+        
+        # Find similar vectors in the VSS table and join with nodes table to get node_id
+        # and filter for only 'TEXT' type nodes.
+        sql = f"""
+            SELECT n.node_id, v.distance
+            FROM node_embeddings_vss v
+            JOIN nodes n ON v.rowid = n.rowid
+            WHERE n.type = 'TEXT'
+            ORDER BY vec_distance(v.embedding, json(?))
+            LIMIT ?
+        """
+        
+        try:
+            results = cursor.execute(sql, (json.dumps(query_vector_list), top_k)).fetchall()
+            return [(row[0], 1 - row[1]) for row in results]  # Convert distance to similarity
+        except Exception as e:
+            self.logger.error(f"Failed to find similar nodes: {e}", exc_info=True)
+            return []
 
-    # --- Graph Traversal and Querying ---
+    def upsert_node_embedding(self, node_id: str, embedding: np.ndarray):
+        """
+        Upserts an embedding for a node into the VSS table.
+        """
+        if not self.has_vector_ext or not NP_AVAILABLE:
+            self.logger.warning(f"Cannot upsert embedding for node {node_id}, vector support is disabled.")
+            return
 
+        conn = self._get_connection()
+        cursor = conn.cursor()
+        
+        embedding_bytes = embedding.tobytes()
 
+        # We need the rowid of the node in the main table to upsert into the VSS table.
+        cursor.execute("SELECT rowid FROM nodes WHERE node_id = ?", (node_id,))
+        result = cursor.fetchone()
+        if not result:
+            self.logger.error(f"Cannot upsert embedding, node_id {node_id} not found in nodes table.")
+            return
+        
+        row_id = result[0]
+        
+        try:
+            # Upsert into VSS table using rowid
+            cursor.execute(
+                "INSERT OR REPLACE INTO node_embeddings_vss (rowid, embedding) VALUES (?, ?)",
+                (row_id, embedding_bytes)
+            )
+            conn.commit()
+            self.logger.info(f"Successfully upserted embedding for node {node_id}.")
+        except Exception as e:
+            conn.rollback()
+            self.logger.error(f"Failed to upsert embedding for node {node_id}: {e}", exc_info=True)
+            raise
+
+    # --- Graph Traversal ---
 
     def get_neighbors(self, node_id: str, relation_type: Optional[str] = None) -> List[Dict[str, Any]]:
-        """Retrieves neighboring nodes connected by a specific relation type."""
+        """Retrieves neighbor nodes of a given node, based on stored relations."""
         conn = self._get_connection()
         cursor = conn.cursor()
+        
+        placeholders = '?'
+        params = [node_id]
         if relation_type:
-            cursor.execute("SELECT target_id FROM relations WHERE source_id = ? AND type = ?", (node_id, relation_type))
-        else:
-            cursor.execute("SELECT target_id FROM relations WHERE source_id = ?", (node_id,))
+            sql += " AND type = ?"
+            params.append(relation_type)
+        
+        cursor.execute(sql, params)
         neighbor_ids = [row[0] for row in cursor.fetchall()]
+
+        if not neighbor_ids:
+            return []
+        
+        # Get full node details for the neighbors
         return self.get_nodes_by_ids(neighbor_ids)
 
-    def get_nodes_linked_to_tag(self, tag_id: str) -> List[Dict[str, Any]]:
+    def search_nodes_by_title_content(self, search_query: str, limit: int = 10) -> List[Dict[str, Any]]:
         """
-        Retrieves all nodes that are linked from a specific tag.
-        This is primarily used to gather context for embedding a tag concept.
+        Performs a LIKE search on node titles and content.
         """
+        if not search_query.strip():
+            return []
+
         conn = self._get_connection()
-        # This query finds all nodes that are the 'source' of a link where the tag is the 'target'.
-        # This corresponds to a node having a tag.
         cursor = conn.cursor()
-        cursor.execute("""
-            SELECT n.*
-            FROM nodes n
-            JOIN relations r ON n.node_id = r.source_id
-            WHERE r.target_id = ? AND r.type = 'HAS_TAG'
-        """, (tag_id,))
         
-        node_rows = cursor.fetchall()
+        search_pattern = f"%{search_query.strip()}%"
+        query = """
+            SELECT *
+            FROM nodes 
+            WHERE title LIKE ? OR content LIKE ?
+            ORDER BY 
+                CASE 
+                    WHEN title LIKE ? THEN 1 
+                    ELSE 2 
+                END,
+                LENGTH(title) ASC
+            LIMIT ?
+        """
         
-        # Since the schema is unified, we need to re-establish the row factory to parse rows into dicts.
-        conn.row_factory = sqlite3.Row
-        cursor = conn.cursor()
-        cursor.execute("""
-            SELECT n.*
-            FROM nodes n
-            JOIN relations r ON n.node_id = r.source_id
-            WHERE r.target_id = ? AND r.type = 'HAS_TAG'
-        """, (tag_id,))
-        
-        results = [self._format_node_output(row) for row in cursor.fetchall()]
-        conn.row_factory = None # Reset row factory
-        return results
+        try:
+            conn.row_factory = sqlite3.Row
+            cursor.execute(query, (search_pattern, search_pattern, search_pattern, limit))
+            rows = cursor.fetchall()
+            conn.row_factory = None
+
+            results = []
+            for row in rows:
+                node = dict(row)
+                node['tags'] = json.loads(node.get('tags', '[]'))
+                node['properties'] = json.loads(node.get('properties', '{}'))
+                results.append(node)
+            return results
+        except Exception as e:
+            self.logger.error(f"Error searching nodes by content: {e}", exc_info=True)
+            return []
 
-    def search_nodes_by_title_content(self, search_query: str, limit: int = 10) -> List[Dict[str, Any]]:
-        """Performs a full-text search on node titles and content. Also finds nodes linked to matching tags."""
-        conn = self._get_connection()
-        conn.row_factory = sqlite3.Row  # Enable row -> dict conversion
-        like_query = f"%{search_query}%"
-        cursor = conn.cursor()
-        # 1) direct match on title / content
-        cursor.execute(
-            "SELECT * FROM nodes WHERE title LIKE ? OR content LIKE ? ORDER BY modified_at DESC LIMIT ?",
-            (like_query, like_query, limit)
-        )
-        rows = cursor.fetchall()
-        results: List[Dict[str, Any]] = [self._format_node_output(row) for row in rows if row]
-
-        # 2) if not enough, search by tag linkage
-        if len(results) < limit:
-            collected_ids = {n['node_id'] for n in results}
-            tag_limit = limit * 2  # fetch extra for deduplication buffer
-            cursor.execute(
-                """
-                SELECT n.*
-                FROM nodes n
-                JOIN relations r ON n.node_id = r.source_id
-                JOIN nodes t ON t.node_id = r.target_id
-                WHERE r.type = 'HAS_TAG'
-                  AND (t.title LIKE ? OR t.name LIKE ? OR t.node_id LIKE ?)
-                LIMIT ?
-                """,
-                (like_query, like_query, like_query, tag_limit)
-            )
-            tag_rows = cursor.fetchall()
-            for row in tag_rows:
-                node = self._format_node_output(row)
-                if node and node['node_id'] not in collected_ids:
-                    results.append(node)
-                    collected_ids.add(node['node_id'])
-                    if len(results) >= limit:
-                        break
-        conn.row_factory = None  # Reset row factory
-        return results
-
-    def get_node_with_neighbors(self, node_id: str) -> Optional[Dict[str, Any]]:
-        """Retrieves a node along with its neighbors."""
-        node = self.get_node_by_id(node_id)
-        if not node: return None
+    def search_expand(self, query_vector: np.ndarray, top_k: int = 5, expansion_hops: int = 1) -> Dict[str, List[Dict[str, Any]]]:
+        """
+        Performs topology-enhanced search.
+        1. Find initial seed nodes via vector search.
+        2. Expand from seed nodes to find related entities.
+        3. Expand from entities to find other related text nodes.
+        """
+        self.logger.info(f"Starting search_expand with top_k={top_k}, expansion_hops={expansion_hops}")
+        
+        # 1. Initial vector search for seed nodes
+        initial_nodes_tuples = self.find_similar_nodes(query_vector, top_k=top_k)
+        if not initial_nodes_tuples:
+            self.logger.info("No initial nodes found in vector search.")
+            return {"initial": [], "expanded": []}
+
+        initial_node_ids = [item[0] for item in initial_nodes_tuples]
+        initial_nodes = self.get_nodes_by_ids(initial_node_ids)
+        self.logger.debug(f"Found {len(initial_nodes)} initial seed nodes.")
+
+        # Use a set to keep track of all node IDs found to avoid duplicates
+        all_found_node_ids = set(initial_node_ids)
+        
+        # 2. Graph Expansion
+        current_expansion_front = set(initial_node_ids)
+        
+        for hop in range(expansion_hops):
+            self.logger.debug(f"Expansion hop {hop + 1}/{expansion_hops}")
+            
+            # Find related entities from the current front of text nodes
+            related_entity_ids = set()
+            for node_id in current_expansion_front:
+                neighbors = self.get_neighbors(node_id, relation_type='HAS_ENTITY')
+                for neighbor in neighbors:
+                    related_entity_ids.add(neighbor['node_id'])
+            
+            if not related_entity_ids:
+                self.logger.debug("No related entities found to expand from.")
+                break # Stop if no further expansion is possible
+            
+            self.logger.debug(f"Found {len(related_entity_ids)} related entities.")
+            
+            # Find new text nodes related to these entities
+            newly_found_text_ids = set()
+            for entity_id in related_entity_ids:
+                # We need a method to get neighbors that are sources of a relation
+                source_neighbors = self._get_source_neighbors(entity_id, relation_type='HAS_ENTITY')
+                for neighbor in source_neighbors:
+                    if neighbor['node_id'] not in all_found_node_ids:
+                        newly_found_text_ids.add(neighbor['node_id'])
+            
+            if not newly_found_text_ids:
+                self.logger.debug("Entities did not lead to any new text nodes.")
+                break
+
+            self.logger.debug(f"Found {len(newly_found_text_ids)} new text nodes in hop {hop + 1}.")
+            all_found_node_ids.update(newly_found_text_ids)
+            current_expansion_front = newly_found_text_ids
+
+        # 3. Aggregate results
+        expanded_node_ids = list(all_found_node_ids - set(initial_node_ids))
+        expanded_nodes = self.get_nodes_by_ids(expanded_node_ids) if expanded_node_ids else []
+        
+        self.logger.info(f"Search expanded from {len(initial_nodes)} to {len(all_found_node_ids)} total nodes.")
+
+        return {
+            "initial": initial_nodes,
+            "expanded": expanded_nodes
+        }
+
+    def _get_source_neighbors(self, node_id: str, relation_type: Optional[str] = None) -> List[Dict[str, Any]]:
+        """Helper to get neighbors where the given node_id is the target."""
         conn = self._get_connection()
         cursor = conn.cursor()
-        cursor.execute("SELECT source_id, type FROM relations WHERE target_id = ?", (node_id,))
-        parents = [{"node_id": row[0], "relation_type": row[1]} for row in cursor.fetchall()]
-        cursor.execute("SELECT target_id, type FROM relations WHERE source_id = ?", (node_id,))
-        children = [{"node_id": row[0], "relation_type": row[1]} for row in cursor.fetchall()]
-        node['relations'] = {
-            'parents': self.get_nodes_by_ids([p['node_id'] for p in parents]),
-            'children': self.get_nodes_by_ids([c['node_id'] for c in children]),
-        }
-        return node
         
-    # --- Database Maintenance and Stats ---
+        sql = "SELECT source_id FROM relations WHERE target_id = ?"
+        params = [node_id]
+        if relation_type:
+            sql += " AND type = ?"
+            params.append(relation_type)
+            
+        cursor.execute(sql, params)
+        source_ids = [row[0] for row in cursor.fetchall()]
+        
+        if not source_ids:
+            return []
+            
+        return self.get_nodes_by_ids(source_ids)
 
     def get_stats(self) -> Dict[str, Any]:
-        """Retrieves statistics about the graph."""
+        """Returns statistics about the database."""
         conn = self._get_connection()
         cursor = conn.cursor()
-        stats = {}
-        cursor.execute("SELECT COUNT(*) FROM nodes")
-        stats['node_count'] = cursor.fetchone()[0]
-        cursor.execute("SELECT COUNT(*) FROM relations")
-        stats['relation_count'] = cursor.fetchone()[0]
-        cursor.execute("SELECT type, COUNT(*) FROM nodes GROUP BY type")
-        stats['nodes_by_type'] = dict(cursor.fetchall())
-        cursor.execute("SELECT type, COUNT(*) FROM relations GROUP BY type")
-        stats['relations_by_type'] = dict(cursor.fetchall())
+        
+        total_nodes = cursor.execute("SELECT COUNT(*) FROM nodes").fetchone()[0]
+        text_nodes = cursor.execute("SELECT COUNT(*) FROM nodes WHERE type = 'TEXT'").fetchone()[0]
+        entity_nodes = cursor.execute("SELECT COUNT(*) FROM nodes WHERE type = 'ENTITY'").fetchone()[0]
+        total_relations = cursor.execute("SELECT COUNT(*) FROM relations").fetchone()[0]
+        
+        stats = {
+            'total_nodes': total_nodes,
+            'text_nodes': text_nodes,
+            'entity_nodes': entity_nodes,
+            'total_relations': total_relations,
+            'db_path': self.db_path,
+            'vector_extension': 'sqlite-vec' if self.has_vector_ext else 'Not loaded'
+        }
         return stats
 
-
-
     def close(self):
-        """Safely closes the database connection for the current thread."""
-        if hasattr(self, 'local') and hasattr(self.local, 'conn') and self.local.conn:
-            self.logger.debug(f"Closing database connection for thread {threading.get_ident()}")
+        """Closes the database connection for the current thread."""
+        if hasattr(self.local, 'conn') and self.local.conn is not None:
             self.local.conn.close()
             self.local.conn = None
-
+            self.logger.debug(f"Closed database connection for thread {threading.get_ident()}") 
+    
     def force_unlock_database(self):
-        """
-        Forces the database to unlock by clearing the WAL journal file.
-        WARNING: This is a last resort and can lead to data corruption if the
-        database is actively being written to by another process.
-        """
+        """Force unlock the database by closing all connections and clearing WAL files if needed."""
         try:
-            wal_path = f"{self.db_path}-wal"
-            shm_path = f"{self.db_path}-shm"
-
+            # Close current connection
             self.close()
-
-            import os
-            if os.path.exists(wal_path):
-                os.remove(wal_path)
-                self.logger.info(f"Removed WAL file: {wal_path}")
-            if os.path.exists(shm_path):
-                os.remove(shm_path)
-                self.logger.info(f"Removed SHM file: {shm_path}")
-                
+            
+            # Try to connect and run a simple query to check if database is accessible
+            test_conn = sqlite3.connect(self.db_path, timeout=5.0)
+            test_conn.execute("SELECT 1").fetchone()
+            test_conn.close()
+            
+            self.logger.info("Database is accessible, no force unlock needed.")
             return True
+            
+        except sqlite3.OperationalError as e:
+            if "database is locked" in str(e).lower():
+                self.logger.warning("Database is locked, attempting to force unlock...")
+                
+                # Remove WAL and SHM files if they exist
+                import os
+                wal_file = self.db_path + "-wal"
+                shm_file = self.db_path + "-shm"
+                
+                try:
+                    if os.path.exists(wal_file):
+                        os.remove(wal_file)
+                        self.logger.info(f"Removed WAL file: {wal_file}")
+                    if os.path.exists(shm_file):
+                        os.remove(shm_file)
+                        self.logger.info(f"Removed SHM file: {shm_file}")
+                        
+                    # Try to connect again
+                    test_conn = sqlite3.connect(self.db_path, timeout=5.0)
+                    test_conn.execute("SELECT 1").fetchone()
+                    test_conn.close()
+                    
+                    self.logger.info("Database unlocked successfully.")
+                    return True
+                    
+                except Exception as cleanup_error:
+                    self.logger.error(f"Failed to force unlock database: {cleanup_error}")
+                    return False
+            else:
+                self.logger.error(f"Database error (not lock-related): {e}")
+                return False
         except Exception as e:
-            self.logger.error(f"Failed to force unlock database: {e}", exc_info=True)
-            return False
-
-    def __del__(self):
-        """Ensures the connection is closed when the object is destroyed."""
-        self.close()
+            self.logger.error(f"Unexpected error during database unlock: {e}")
+            return False
\ No newline at end of file
diff --git a/simtag/core/memory_engine.py b/simtag/core/memory_engine.py
new file mode 100755
index 0000000..4d1878c
--- /dev/null
+++ b/simtag/core/memory_engine.py
@@ -0,0 +1,1424 @@
+"""
+Memory Engine for Org SuperTag
+
+This module implements the MemoryEngine responsible for managing user preferences, 
+behavioral patterns, dialogue history, and constructing context snapshots for the LLM.
+Based on living-doc-features.org sections 10.7.A.4 and 10.8.F.
+"""
+import asyncio
+import logging
+import time
+from typing import Dict, List, Any, Optional, Union, Callable, Awaitable
+from dataclasses import dataclass, field
+from enum import Enum
+import sqlite3 # Added for SQLite persistence
+import json # Added for JSON serialization
+
+# Assuming Config and other services will be imported when needed
+# from simtag.config import Config
+# from ..services.llm_client import LLMClient 
+# from .storage import StorageService # If we use a unified storage service
+
+logger = logging.getLogger(__name__)
+
+class MemoryItemType(Enum):
+    USER_PREFERENCE = "user_preference"
+    BEHAVIORAL_PATTERN = "behavioral_pattern"
+    DIALOGUE_TURN = "dialogue_turn"
+    CONTEXT_SNAPSHOT = "context_snapshot" # A snapshot of what was sent to LLM
+    USER_FEEDBACK = "user_feedback" # Explicit feedback on AI responses
+    SYSTEM_STATE = "system_state" # e.g., current dialogue mode
+
+@dataclass
+class MemoryItem:
+    """Generic container for a piece of memory."""
+    id: str 
+    type: MemoryItemType
+    content: Any
+    timestamp: Optional[float] = None
+    metadata: Optional[Dict[str, Any]] = None
+
+    def __post_init__(self):
+        if self.timestamp is None:
+            self.timestamp = time.time()
+        if self.metadata is None:
+            self.metadata = {}
+
+@dataclass
+class UserPreference(MemoryItem):
+    """Stores explicit user preferences."""
+    key: str = "" # Default value to avoid field order issues
+    # content field from MemoryItem will store the value of the preference
+    # type will be MemoryItemType.USER_PREFERENCE
+    def __post_init__(self):
+        super().__post_init__()  # Call parent's __post_init__ first
+        if not self.key:  # Validate that key was actually provided
+            raise ValueError("UserPreference requires a non-empty 'key' field")
+        self.type = MemoryItemType.USER_PREFERENCE
+        if not self.id: # Auto-generate ID if not provided
+            self.id = f"pref_{self.key}_{self.timestamp}"
+
+@dataclass
+class BehavioralPattern(MemoryItem):
+    """Stores inferred user behavioral patterns."""
+    pattern_type: str = "" # Default value to avoid field order issues
+    confidence: float = 0.0 # Confidence in the inferred pattern
+    def __post_init__(self):
+        super().__post_init__()  # Call parent's __post_init__ first
+        if not self.pattern_type:  # Validate that pattern_type was actually provided
+            raise ValueError("BehavioralPattern requires a non-empty 'pattern_type' field")
+        self.type = MemoryItemType.BEHAVIORAL_PATTERN
+        if not self.id:
+            self.id = f"pattern_{self.pattern_type}_{self.timestamp}"
+        self.metadata['confidence'] = self.confidence
+
+
+@dataclass
+class DialogueTurn:
+    """Represents a single turn in a dialogue."""
+    speaker: str = "" # Default value to avoid field order issues
+    text: str = "" # Default value to avoid field order issues
+    timestamp: float = field(default_factory=time.time)
+    metadata: Dict[str, Any] = field(default_factory=dict) # e.g., associated node_id, embeddings
+    
+    def __post_init__(self):
+        if not self.speaker or not self.text:  # Validate required fields
+            raise ValueError("DialogueTurn requires non-empty 'speaker' and 'text' fields")
+
+@dataclass
+class DialogueHistory(MemoryItem):
+    """Stores a sequence of dialogue turns for a session or topic."""
+    session_id: str = "" # Default value to avoid field order issues
+    turns: List[DialogueTurn] = field(default_factory=list)
+    summary: Optional[str] = None # Optional summary of the dialogue
+    def __post_init__(self):
+        super().__post_init__()  # Call parent's __post_init__ first
+        if not self.session_id:  # Validate that session_id was actually provided
+            raise ValueError("DialogueHistory requires a non-empty 'session_id' field")
+        self.type = MemoryItemType.DIALOGUE_TURN # Or a new type like DIALOGUE_SESSION
+        if not self.id:
+            self.id = f"dialogue_{self.session_id}_{self.timestamp}"
+        self.content = self.turns # Store turns in content for generic access
+
+@dataclass
+class ContextSnapshot(MemoryItem):
+    """
+    Represents the context provided to the LLM at a specific point in time.
+    This helps in understanding LLM behavior and for debugging.
+    """
+    triggering_query: str = "" # Default value to avoid field order issues
+    # content will be a dict of context elements: 
+    # e.g. {'prompt': str, 'retrieved_docs': List[str], 'user_prefs': Dict}
+    # type will be MemoryItemType.CONTEXT_SNAPSHOT
+    llm_response: Optional[str] = None # The response generated from this context
+    def __post_init__(self):
+        super().__post_init__()  # Call parent's __post_init__ first
+        if not self.triggering_query:  # Validate that triggering_query was actually provided
+            raise ValueError("ContextSnapshot requires a non-empty 'triggering_query' field")
+        self.type = MemoryItemType.CONTEXT_SNAPSHOT
+        if not self.id:
+            self.id = f"snapshot_{self.timestamp}"
+
+
+class MemoryEngine:
+    """
+    Manages various forms of memory for the org-supertag system.
+    Handles storage, retrieval, and processing of memory items.
+    """
+
+    def __init__(self, 
+                 config: Any, # simtag.config.Config
+                 llm_client: Optional[Any] = None, # LLMClient for summarization etc.
+                 # storage_service: Optional[Any] = None # For persistent storage - replacing with direct SQLite
+                 db_path: Optional[str] = None # New parameter for DB path
+                ):
+        """
+        Initializes the MemoryEngine.
+
+        Args:
+            config: Configuration object.
+            llm_client: Optional LLM client for tasks like summarization.
+            db_path: Optional path to the SQLite database file. If None, uses in-memory.
+        """
+        self.config = config
+        self.llm_client = llm_client
+        self.db_path = db_path if db_path else getattr(config, 'memory_db_path', None)
+        self.db_conn: Optional[sqlite3.Connection] = None
+
+        # In-memory storage for now; will be augmented/replaced by storage_service
+        self._memory_items: Dict[str, MemoryItem] = {}
+        
+        if self.db_path:
+            logger.info(f"MemoryEngine initializing with SQLite persistence at: {self.db_path}")
+            self._init_db()
+        else:
+            logger.info("MemoryEngine initializing with in-memory storage only.")
+
+        self.retention_period = getattr(config, 'memory_retention_period_days', 30) * 86400 # seconds
+        self.max_elements = getattr(config, 'memory_max_elements', 10000)
+        self.auto_summary_interval = getattr(config, 'memory_auto_summary_interval_hours', 24) * 3600 # seconds
+        
+        logger.info(f"MemoryEngine initialized. Retention: {self.retention_period/86400} days, Max elements: {self.max_elements}.")
+        # TODO: Load existing memory from storage_service if provided and implemented.
+        # If using DB, load from DB here.
+        if self.db_conn:
+            self._load_memory_from_db() # Placeholder for method to load data on startup
+
+    def _init_db(self):
+        """Initializes the SQLite database connection and creates tables if they don't exist."""
+        if not self.db_path:
+            return
+        try:
+            self.db_conn = sqlite3.connect(self.db_path)
+            self.db_conn.row_factory = sqlite3.Row # Access columns by name
+            cursor = self.db_conn.cursor()
+            self._create_db_tables(cursor)
+            self.db_conn.commit()
+            logger.info(f"SQLite database initialized successfully at {self.db_path}")
+        except sqlite3.Error as e:
+            logger.error(f"Error initializing SQLite database at {self.db_path}: {e}", exc_info=True)
+            self.db_conn = None # Ensure connection is None if setup failed
+
+    def _create_db_tables(self, cursor: sqlite3.Cursor):
+        """Creates the necessary SQLite tables for memory persistence."""
+        # User Preferences Table
+        cursor.execute("""
+        CREATE TABLE IF NOT EXISTS user_preferences (
+            id TEXT PRIMARY KEY,
+            key TEXT UNIQUE NOT NULL,
+            content_json TEXT,
+            timestamp REAL,
+            metadata_json TEXT
+        )
+        """)
+        logger.debug("Table 'user_preferences' checked/created.")
+
+        # Behavioral Patterns Table
+        cursor.execute("""
+        CREATE TABLE IF NOT EXISTS behavioral_patterns (
+            id TEXT PRIMARY KEY,
+            pattern_type TEXT NOT NULL,
+            content_json TEXT,
+            confidence REAL,
+            timestamp REAL,
+            metadata_json TEXT 
+        )
+        """)
+        logger.debug("Table 'behavioral_patterns' checked/created.")
+
+        # Dialogue Sessions Table
+        cursor.execute("""
+        CREATE TABLE IF NOT EXISTS dialogue_sessions (
+            id TEXT PRIMARY KEY, 
+            session_id TEXT UNIQUE NOT NULL,
+            summary TEXT,
+            timestamp REAL,
+            metadata_json TEXT
+        )
+        """)
+        logger.debug("Table 'dialogue_sessions' checked/created.")
+
+        # Dialogue Turns Table
+        cursor.execute("""
+        CREATE TABLE IF NOT EXISTS dialogue_turns (
+            turn_id TEXT PRIMARY KEY,
+            session_db_id TEXT NOT NULL, 
+            speaker TEXT NOT NULL,
+            text TEXT NOT NULL,
+            timestamp REAL,
+            metadata_json TEXT,
+            turn_order INTEGER, 
+            FOREIGN KEY (session_db_id) REFERENCES dialogue_sessions(id) ON DELETE CASCADE
+        )
+        """)
+        logger.debug("Table 'dialogue_turns' checked/created.")
+        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dialogue_turns_session_db_id ON dialogue_turns (session_db_id);")
+
+        # Context Snapshots Table
+        cursor.execute("""
+        CREATE TABLE IF NOT EXISTS context_snapshots (
+            id TEXT PRIMARY KEY,
+            triggering_query TEXT,
+            content_json TEXT, 
+            llm_response TEXT,
+            timestamp REAL,
+            metadata_json TEXT
+        )
+        """)
+        logger.debug("Table 'context_snapshots' checked/created.")
+
+        # Optional: A generic memory_items table if you want to store all item types generically
+        # For now, specific tables are clearer based on distinct dataclasses.
+
+    def _load_memory_from_db(self):
+        """Loads memory items from DB into in-memory cache on startup."""
+        if not self.db_conn:
+            return
+
+        logger.info("Loading initial memory items from database...")
+        loaded_count = 0
+        # Load User Preferences
+        try:
+            cursor = self.db_conn.cursor()
+            cursor.execute("SELECT id, key, content_json, timestamp, metadata_json FROM user_preferences")
+            for row in cursor.fetchall():
+                try:
+                    pref = UserPreference(
+                        id=row['id'], 
+                        key=row['key'], 
+                        content=json.loads(row['content_json']) if row['content_json'] else None,
+                        timestamp=row['timestamp'],
+                        metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                    )
+                    # Add to in-memory cache, typically self._memory_items
+                    # This ensures that items loaded from DB are also in the working cache.
+                    self._memory_items[pref.id] = pref
+                    loaded_count +=1
+                except (json.JSONDecodeError, ValueError, TypeError) as e: # Catch errors during object creation
+                    logger.error(f"Error reconstructing UserPreference from DB row ID {row['id']} (key: {row['key']}): {e}")
+            if loaded_count > 0:
+                logger.info(f"Loaded {loaded_count} user preferences from DB into cache.")
+        except sqlite3.Error as e:
+            logger.error(f"Error loading user preferences from DB: {e}", exc_info=True)
+        
+        # Load Behavioral Patterns
+        loaded_bp_count = 0
+        try:
+            cursor = self.db_conn.cursor()
+            cursor.execute("SELECT id, pattern_type, content_json, confidence, timestamp, metadata_json FROM behavioral_patterns")
+            for row in cursor.fetchall():
+                try:
+                    pattern = BehavioralPattern(
+                        id=row['id'],
+                        pattern_type=row['pattern_type'],
+                        content=json.loads(row['content_json']) if row['content_json'] else None,
+                        confidence=row['confidence'],
+                        timestamp=row['timestamp'],
+                        metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                    )
+                    self._memory_items[pattern.id] = pattern # Add to cache
+                    loaded_bp_count += 1
+                except (json.JSONDecodeError, ValueError, TypeError) as e:
+                    logger.error(f"Error reconstructing BehavioralPattern from DB row ID {row['id']}: {e}")
+            if loaded_bp_count > 0:
+                logger.info(f"Loaded {loaded_bp_count} behavioral patterns from DB into cache.")
+        except sqlite3.Error as e:
+            logger.error(f"Error loading behavioral patterns from DB: {e}", exc_info=True)
+
+        # Load Dialogue History (Sessions and Turns)
+        loaded_dh_count = 0
+        if self.db_conn: # Ensure db_conn is checked before use
+            try:
+                cursor = self.db_conn.cursor()
+                # First, load all dialogue sessions
+                cursor.execute("SELECT id, session_id, summary, timestamp, metadata_json FROM dialogue_sessions")
+                sessions_data = cursor.fetchall()
+                for session_row in sessions_data:
+                    try:
+                        # Fetch turns for this session
+                        turns_cursor = self.db_conn.cursor()
+                        turns_cursor.execute("SELECT turn_id, speaker, text, timestamp, metadata_json, turn_order FROM dialogue_turns WHERE session_db_id = ? ORDER BY turn_order ASC", (session_row['id'],))
+                        dialogue_turns: List[DialogueTurn] = []
+                        for turn_row in turns_cursor.fetchall():
+                            turn_metadata = json.loads(turn_row['metadata_json']) if turn_row['metadata_json'] else {}
+                            # The DialogueTurn dataclass does not have turn_id or turn_order as direct fields.
+                            # We can store them in metadata if needed, or adjust DialogueTurn if it's only for DB.
+                            # For now, let's assume they are not part of the reconstructed DialogueTurn object directly, unless put in metadata.
+                            dt = DialogueTurn(
+                                speaker=turn_row['speaker'],
+                                text=turn_row['text'],
+                                timestamp=turn_row['timestamp'],
+                                metadata=turn_metadata
+                            )
+                            dialogue_turns.append(dt)
+                        
+                        session_metadata = json.loads(session_row['metadata_json']) if session_row['metadata_json'] else {}
+                        dh = DialogueHistory(
+                            id=session_row['id'],
+                            session_id=session_row['session_id'],
+                            turns=dialogue_turns,
+                            summary=session_row['summary'],
+                            timestamp=session_row['timestamp'],
+                            metadata=session_metadata
+                        )
+                        self._memory_items[dh.id] = dh # Cache the reconstructed DialogueHistory
+                        loaded_dh_count += 1
+                    except (json.JSONDecodeError, ValueError, TypeError) as e:
+                        logger.error(f"Error reconstructing DialogueHistory from DB for session_id {session_row['session_id']}: {e}")
+                if loaded_dh_count > 0:
+                    logger.info(f"Loaded {loaded_dh_count} dialogue histories (sessions with turns) from DB into cache.")
+            except sqlite3.Error as e:
+                logger.error(f"Error loading dialogue histories from DB: {e}", exc_info=True)
+
+        # Load Context Snapshots
+        loaded_cs_count = 0
+        if self.db_conn:
+            try:
+                cursor = self.db_conn.cursor()
+                cursor.execute("SELECT id, triggering_query, content_json, llm_response, timestamp, metadata_json FROM context_snapshots")
+                for row in cursor.fetchall():
+                    try:
+                        snapshot = ContextSnapshot(
+                            id=row['id'],
+                            triggering_query=row['triggering_query'],
+                            content=json.loads(row['content_json']) if row['content_json'] else None,
+                            llm_response=row['llm_response'],
+                            timestamp=row['timestamp'],
+                            metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                        )
+                        self._memory_items[snapshot.id] = snapshot # Add to cache
+                        loaded_cs_count += 1
+                    except (json.JSONDecodeError, ValueError, TypeError) as e:
+                        logger.error(f"Error reconstructing ContextSnapshot from DB row ID {row['id']}: {e}")
+                if loaded_cs_count > 0:
+                    logger.info(f"Loaded {loaded_cs_count} context snapshots from DB into cache.")
+            except sqlite3.Error as e:
+                logger.error(f"Error loading context snapshots from DB: {e}", exc_info=True)
+
+    async def add_memory_item(self, item: MemoryItem) -> bool:
+        """
+        Adds a new memory item to the store.
+        Manages max elements constraint and persists to DB if applicable.
+        """
+        # In-memory cache management (optional, can be removed if DB is primary source)
+        if len(self._memory_items) >= self.max_elements:
+            # Pruning logic might need to be DB-aware if _memory_items is just a cache
+            # For now, assume _memory_items is a cache that can be pruned independently.
+            # await self.prune_memory(target_count=self.max_elements - 1) # Make space
+            # Simplification: If DB is source of truth, cache eviction is different.
+            # Let's assume for now the cache has its own pruning not directly tied to DB table limits.
+            logger.warning(f"In-memory cache limit ({self.max_elements}) reached. Pruning not fully implemented with DB.")
+
+        self._memory_items[item.id] = item # Keep in-memory cache for now
+        logger.debug(f"Added/Updated memory item in cache: {item.id} (Type: {item.type.value}). Total cached items: {len(self._memory_items)}")
+        
+        # Persistence logic
+        if self.db_conn:
+            if item.type == MemoryItemType.USER_PREFERENCE and isinstance(item, UserPreference):
+                try:
+                    cursor = self.db_conn.cursor()
+                    cursor.execute("""
+                        INSERT INTO user_preferences (id, key, content_json, timestamp, metadata_json)
+                        VALUES (?, ?, ?, ?, ?)
+                        ON CONFLICT(key) DO UPDATE SET
+                            id = excluded.id, 
+                            content_json = excluded.content_json,
+                            timestamp = excluded.timestamp,
+                            metadata_json = excluded.metadata_json
+                    """, (item.id, item.key, json.dumps(item.content), item.timestamp, json.dumps(item.metadata)))
+                    self.db_conn.commit()
+                    logger.debug(f"UserPreference '{item.key}' (ID: {item.id}) upserted into DB.")
+                except sqlite3.Error as e:
+                    logger.error(f"Error upserting UserPreference '{item.key}' to DB: {e}", exc_info=True)
+                    return False
+            elif item.type == MemoryItemType.BEHAVIORAL_PATTERN and isinstance(item, BehavioralPattern):
+                try:
+                    cursor = self.db_conn.cursor()
+                    # Assuming BehavioralPattern.id is unique and primary key for its table.
+                    # ON CONFLICT(id) DO UPDATE for general upsert behavior if items can be re-added/updated by id.
+                    cursor.execute("""
+                        INSERT INTO behavioral_patterns (id, pattern_type, content_json, confidence, timestamp, metadata_json)
+                        VALUES (?, ?, ?, ?, ?, ?)
+                        ON CONFLICT(id) DO UPDATE SET
+                            pattern_type = excluded.pattern_type,
+                            content_json = excluded.content_json,
+                            confidence = excluded.confidence,
+                            timestamp = excluded.timestamp,
+                            metadata_json = excluded.metadata_json
+                    """, (
+                        item.id, 
+                        item.pattern_type, 
+                        json.dumps(item.content), 
+                        item.confidence, 
+                        item.timestamp, 
+                        json.dumps(item.metadata)
+                    ))
+                    self.db_conn.commit()
+                    logger.debug(f"BehavioralPattern '{item.pattern_type}' (ID: {item.id}) upserted into DB.")
+                except sqlite3.Error as e:
+                    logger.error(f"Error upserting BehavioralPattern '{item.pattern_type}' to DB: {e}", exc_info=True)
+                    return False # Indicate failure
+            elif item.type == MemoryItemType.CONTEXT_SNAPSHOT and isinstance(item, ContextSnapshot):
+                try:
+                    cursor = self.db_conn.cursor()
+                    cursor.execute("""
+                        INSERT INTO context_snapshots (id, triggering_query, content_json, llm_response, timestamp, metadata_json)
+                        VALUES (?, ?, ?, ?, ?, ?)
+                        ON CONFLICT(id) DO UPDATE SET
+                            triggering_query = excluded.triggering_query,
+                            content_json = excluded.content_json,
+                            llm_response = excluded.llm_response,
+                            timestamp = excluded.timestamp,
+                            metadata_json = excluded.metadata_json
+                    """, (
+                        item.id, 
+                        item.triggering_query,
+                        json.dumps(item.content),
+                        item.llm_response,
+                        item.timestamp, 
+                        json.dumps(item.metadata)
+                    ))
+                    self.db_conn.commit()
+                    logger.debug(f"ContextSnapshot (ID: {item.id}) for query '{item.triggering_query[:50]}...' upserted into DB.")
+                except sqlite3.Error as e:
+                    logger.error(f"Error upserting ContextSnapshot (ID: {item.id}) to DB: {e}", exc_info=True)
+                    return False # Indicate failure
+            elif item.type == MemoryItemType.DIALOGUE_TURN and isinstance(item, DialogueHistory):
+                # This branch is for updating an existing DialogueHistory session, e.g., after summarization.
+                # add_dialogue_turn handles new turns and initial session creation/update.
+                try:
+                    cursor = self.db_conn.cursor()
+                    cursor.execute("""
+                        UPDATE dialogue_sessions 
+                        SET summary = ?, timestamp = ?, metadata_json = ?
+                        WHERE id = ?
+                    """, (
+                        item.summary,
+                        item.timestamp,
+                        json.dumps(item.metadata),
+                        item.id
+                    ))
+                    if cursor.rowcount == 0:
+                        logger.warning(f"Attempted to update DialogueHistory (ID: {item.id}) summary/metadata in DB, but no matching session found. It might be a new session not yet fully persisted by add_dialogue_turn, or an ID mismatch.")
+                        # If it was meant to be a new session, it should have gone via add_dialogue_turn.
+                        # For safety, we could try an INSERT here if rowcount is 0, but it might indicate a logic flaw elsewhere.
+                        # For now, just log if no update happened.
+                    else:
+                        self.db_conn.commit()
+                        logger.debug(f"DialogueHistory (ID: {item.id}) summary/metadata updated in DB.")
+                except sqlite3.Error as e:
+                    logger.error(f"Error updating DialogueHistory (ID: {item.id}) summary/metadata in DB: {e}", exc_info=True)
+                    return False
+            # TODO: Add similar persistence for other future MemoryItemTypes if any.
+        return True
+
+    async def get_memory_item(self, item_id: str) -> Optional[MemoryItem]:
+        """Retrieves a specific memory item by ID. (Primarily from cache for now)"""
+        cached_item = self._memory_items.get(item_id)
+        if cached_item:
+            return cached_item
+        
+        # Example: Fetch UserPreference from DB if not in cache
+        if self.db_conn: # Check if db_conn is not None
+            # Determine table and type based on item_id prefix or other conventions if possible
+            # This is a simplified lookup. A more robust system might require type hint or separate getters.
+            if item_id.startswith("pref_"):
+                table_name = "user_preferences"
+                ItemClass = UserPreference
+                logger.debug(f"Attempting to fetch UserPreference ID '{item_id}' from DB.")
+                try:
+                    cursor = self.db_conn.cursor()
+                    cursor.execute(f"SELECT * FROM {table_name} WHERE id = ?", (item_id,))
+                    row = cursor.fetchone()
+                    if row:
+                        item = ItemClass(
+                            id=row['id'], 
+                            key=row['key'], 
+                            content=json.loads(row['content_json']) if row['content_json'] else None,
+                            timestamp=row['timestamp'],
+                            metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                        )
+                        self._memory_items[item.id] = item # Cache it
+                        return item
+                except sqlite3.Error as e:
+                    logger.error(f"Error fetching UserPreference by id '{item_id}' from DB: {e}")
+            elif item_id.startswith("pattern_"):
+                # Similar logic for BehavioralPattern if needed by ID
+                logger.debug(f"Attempting to fetch BehavioralPattern ID '{item_id}' from DB.")
+                # ... implementation to fetch BehavioralPattern by ID ...
+                pass # Placeholder
+            elif item_id.startswith("dialogue_"):
+                logger.debug(f"Attempting to fetch DialogueHistory ID '{item_id}' from DB.")
+                try:
+                    cursor = self.db_conn.cursor()
+                    cursor.execute("SELECT id, session_id, summary, timestamp, metadata_json FROM dialogue_sessions WHERE id = ?", (item_id,))
+                    session_row = cursor.fetchone()
+                    if session_row:
+                        # Found the session, now fetch its turns
+                        turns_cursor = self.db_conn.cursor()
+                        turns_query = "SELECT speaker, text, timestamp, metadata_json FROM dialogue_turns WHERE session_db_id = ? ORDER BY turn_order ASC"
+                        turns_cursor.execute(turns_query, (session_row['id'],))
+                        
+                        dialogue_turns_from_db: List[DialogueTurn] = []
+                        for turn_row in turns_cursor.fetchall():
+                            dialogue_turns_from_db.append(DialogueTurn(
+                                speaker=turn_row['speaker'], 
+                                text=turn_row['text'], 
+                                timestamp=turn_row['timestamp'], 
+                                metadata=json.loads(turn_row['metadata_json']) if turn_row['metadata_json'] else {}
+                            ))
+                        
+                        history_obj = DialogueHistory(
+                            id=session_row['id'], 
+                            session_id=session_row['session_id'], 
+                            turns=dialogue_turns_from_db,
+                            summary=session_row['summary'], 
+                            timestamp=session_row['timestamp'],
+                            metadata=json.loads(session_row['metadata_json']) if session_row['metadata_json'] else {}
+                        )
+                        self._memory_items[history_obj.id] = history_obj # Cache it
+                        return history_obj
+                except sqlite3.Error as e:
+                    logger.error(f"Error fetching DialogueHistory by id '{item_id}' from DB: {e}")
+            # Add logic for other types (ContextSnapshot) if get_memory_item by ID is needed for them
+
+        return None # Not found in cache or DB for the types handled above
+
+    async def get_memory_items(self, 
+                               item_type: Optional[MemoryItemType] = None,
+                               max_count: Optional[int] = None,
+                               time_window_seconds: Optional[float] = None,
+                               sort_by_time: bool = True # True for descending (most recent first)
+                              ) -> List[MemoryItem]:
+        """
+        Retrieves memory items, with optional filtering and sorting.
+        Prioritizes DB for UserPreference if available.
+        """
+        # Special handling for UserPreference to query DB directly if available
+        if item_type == MemoryItemType.USER_PREFERENCE and self.db_conn:
+            logger.debug(f"Fetching UserPreferences from DB with filters: window={time_window_seconds}s, max={max_count}, sort_desc={sort_by_time}")
+            results: List[MemoryItem] = []
+            try:
+                query = "SELECT id, key, content_json, timestamp, metadata_json FROM user_preferences"
+                params = []
+                conditions = []
+
+                if time_window_seconds:
+                    min_timestamp = time.time() - time_window_seconds
+                    conditions.append("timestamp >= ?")
+                    params.append(min_timestamp)
+                
+                if conditions:
+                    query += " WHERE " + " AND ".join(conditions)
+                
+                query += " ORDER BY timestamp " + ("DESC" if sort_by_time else "ASC")
+                
+                if max_count is not None:
+                    query += " LIMIT ?"
+                    params.append(max_count)
+
+                cursor = self.db_conn.cursor()
+                cursor.execute(query, tuple(params))
+                
+                for row in cursor.fetchall():
+                    try:
+                        pref = UserPreference(
+                            id=row['id'], 
+                            key=row['key'], 
+                            content=json.loads(row['content_json']) if row['content_json'] else None,
+                            timestamp=row['timestamp'],
+                            metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                        )
+                        results.append(pref)
+                        # Update in-memory cache with items retrieved from DB
+                        self._memory_items[pref.id] = pref 
+                    except (json.JSONDecodeError, ValueError, TypeError) as e:
+                        logger.error(f"Error reconstructing UserPreference from DB row during get_memory_items (ID: {row['id']}): {e}")
+                logger.debug(f"Retrieved {len(results)} UserPreferences from DB.")
+                return results
+            except sqlite3.Error as e:
+                logger.error(f"DB error fetching UserPreferences in get_memory_items: {e}", exc_info=True)
+                # Fall through to in-memory cache if DB query fails
+        
+        elif item_type == MemoryItemType.BEHAVIORAL_PATTERN and self.db_conn:
+            logger.debug(f"Fetching BehavioralPatterns from DB with filters: window={time_window_seconds}s, max={max_count}, sort_desc={sort_by_time}")
+            results: List[MemoryItem] = []
+            try:
+                query = "SELECT id, pattern_type, content_json, confidence, timestamp, metadata_json FROM behavioral_patterns"
+                params = []
+                conditions = []
+
+                if time_window_seconds:
+                    min_timestamp = time.time() - time_window_seconds
+                    conditions.append("timestamp >= ?")
+                    params.append(min_timestamp)
+                
+                if conditions:
+                    query += " WHERE " + " AND ".join(conditions)
+                
+                query += " ORDER BY timestamp " + ("DESC" if sort_by_time else "ASC")
+                
+                if max_count is not None:
+                    query += " LIMIT ?"
+                    params.append(max_count)
+
+                cursor = self.db_conn.cursor()
+                cursor.execute(query, tuple(params))
+                
+                for row in cursor.fetchall():
+                    try:
+                        pattern = BehavioralPattern(
+                            id=row['id'],
+                            pattern_type=row['pattern_type'],
+                            content=json.loads(row['content_json']) if row['content_json'] else None,
+                            confidence=row['confidence'],
+                            timestamp=row['timestamp'],
+                            metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                        )
+                        results.append(pattern)
+                        self._memory_items[pattern.id] = pattern # Update cache
+                    except (json.JSONDecodeError, ValueError, TypeError) as e:
+                        logger.error(f"Error reconstructing BehavioralPattern from DB row during get_memory_items (ID: {row['id']}): {e}")
+                logger.debug(f"Retrieved {len(results)} BehavioralPatterns from DB.")
+                return results
+            except sqlite3.Error as e:
+                logger.error(f"DB error fetching BehavioralPatterns in get_memory_items: {e}", exc_info=True)
+                # Fall through to in-memory cache if DB query fails
+
+        elif item_type == MemoryItemType.DIALOGUE_TURN and self.db_conn: # Assuming this means get DialogueHistory objects
+            logger.debug(f"Fetching DialogueHistory sessions from DB with filters: window={time_window_seconds}s, max={max_count}, sort_desc={sort_by_time}")
+            results: List[MemoryItem] = [] # Will store DialogueHistory objects
+            try:
+                query = "SELECT id, session_id FROM dialogue_sessions" # Select session_id to pass to get_dialogue_history
+                params = []
+                conditions = []
+
+                if time_window_seconds:
+                    min_timestamp = time.time() - time_window_seconds
+                    conditions.append("timestamp >= ?")
+                    params.append(min_timestamp)
+                
+                if conditions:
+                    query += " WHERE " + " AND ".join(conditions)
+                
+                query += " ORDER BY timestamp " + ("DESC" if sort_by_time else "ASC")
+                
+                if max_count is not None:
+                    query += " LIMIT ?"
+                    params.append(max_count)
+
+                cursor = self.db_conn.cursor()
+                cursor.execute(query, tuple(params))
+                
+                session_rows = cursor.fetchall()
+                for session_row in session_rows:
+                    session_id_from_db = session_row['session_id']
+                    # Fetch the full DialogueHistory object using the existing method
+                    # This reuses its logic for fetching turns and caching.
+                    # max_turns for get_dialogue_history itself is not specified here, so it fetches all turns for the session.
+                    # The max_count in this method (get_memory_items) applies to the number of sessions.
+                    history_obj = await self.get_dialogue_history(session_id_from_db, max_turns=None) 
+                    if history_obj:
+                        results.append(history_obj)
+                        # self._memory_items[history_obj.id] = history_obj # get_dialogue_history already caches it
+                
+                logger.debug(f"Retrieved {len(results)} DialogueHistory sessions from DB query.")
+                return results # Returns List[DialogueHistory] which are MemoryItem compatible
+            except sqlite3.Error as e:
+                logger.error(f"DB error fetching DialogueHistory session IDs in get_memory_items: {e}", exc_info=True)
+                # Fall through to in-memory cache if DB query fails
+
+        elif item_type == MemoryItemType.CONTEXT_SNAPSHOT and self.db_conn:
+            logger.debug(f"Fetching ContextSnapshots from DB with filters: window={time_window_seconds}s, max={max_count}, sort_desc={sort_by_time}")
+            results: List[MemoryItem] = []
+            try:
+                query = "SELECT id, triggering_query, content_json, llm_response, timestamp, metadata_json FROM context_snapshots"
+                params = []
+                conditions = []
+
+                if time_window_seconds:
+                    min_timestamp = time.time() - time_window_seconds
+                    conditions.append("timestamp >= ?")
+                    params.append(min_timestamp)
+                
+                if conditions:
+                    query += " WHERE " + " AND ".join(conditions)
+                
+                query += " ORDER BY timestamp " + ("DESC" if sort_by_time else "ASC")
+                
+                if max_count is not None:
+                    query += " LIMIT ?"
+                    params.append(max_count)
+
+                cursor = self.db_conn.cursor()
+                cursor.execute(query, tuple(params))
+                
+                for row in cursor.fetchall():
+                    try:
+                        snapshot = ContextSnapshot(
+                            id=row['id'],
+                            triggering_query=row['triggering_query'],
+                            content=json.loads(row['content_json']) if row['content_json'] else None,
+                            llm_response=row['llm_response'],
+                            timestamp=row['timestamp'],
+                            metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                        )
+                        results.append(snapshot)
+                        self._memory_items[snapshot.id] = snapshot # Update cache
+                    except (json.JSONDecodeError, ValueError, TypeError) as e:
+                        logger.error(f"Error reconstructing ContextSnapshot from DB row during get_memory_items (ID: {row['id']}): {e}")
+                logger.debug(f"Retrieved {len(results)} ContextSnapshots from DB.")
+                return results
+            except sqlite3.Error as e:
+                logger.error(f"DB error fetching ContextSnapshots in get_memory_items: {e}", exc_info=True)
+                # Fall through to in-memory cache if DB query fails
+
+        # Fallback to in-memory cache for other types or if DB fails
+        logger.debug(f"Fetching memory items from in-memory cache (type: {item_type}, window: {time_window_seconds}s, max: {max_count}).")
+        candidate_items = list(self._memory_items.values())
+        
+        if item_type:
+            candidate_items = [item for item in candidate_items if item.type == item_type]
+            
+        if time_window_seconds:
+            current_time = time.time()
+            candidate_items = [item for item in candidate_items if (current_time - item.timestamp) <= time_window_seconds]
+            
+        if sort_by_time:
+            candidate_items.sort(key=lambda item: item.timestamp, reverse=True)
+            
+        if max_count is not None:
+            candidate_items = candidate_items[:max_count]
+            
+        logger.debug(f"Retrieved {len(candidate_items)} memory items (type: {item_type}, window: {time_window_seconds}s, max: {max_count}).")
+        return candidate_items
+
+    async def update_user_preference(self, key: str, value: Any) -> UserPreference:
+        """Creates or updates a user preference and persists it to the database."""
+        timestamp = time.time()
+        pref_id = f"pref_{key}_{timestamp}" # Generate a potentially new ID
+        
+        # Try to fetch existing from DB to see if it's an update or new
+        # This also serves to get the most current ID if key exists
+        if self.db_conn:
+            try:
+                cursor = self.db_conn.cursor()
+                cursor.execute("SELECT id, content_json, metadata_json, timestamp FROM user_preferences WHERE key = ?", (key,))
+                row = cursor.fetchone()
+                if row:
+                    logger.info(f"Updating existing preference '{key}' in DB.")
+                    # Use existing ID if found, update content and timestamp
+                    pref_id = row['id'] 
+                    # Create a UserPreference object for consistency, then it will be added via add_memory_item
+                    updated_pref = UserPreference(
+                        id=pref_id, 
+                        key=key, 
+                        content=value, 
+                        timestamp=timestamp, # New timestamp for update
+                        metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {} # Preserve old metadata or update?
+                                                                                             # For now, let's assume metadata is not changed by this simple update value func
+                    )
+                else:
+                    logger.info(f"Creating new preference '{key}' in DB.")
+                    updated_pref = UserPreference(id=pref_id, key=key, content=value, timestamp=timestamp)
+                
+                # Let add_memory_item handle the actual upsert and caching
+                await self.add_memory_item(updated_pref)
+                return updated_pref # Return the object that was (or would be) upserted
+
+            except sqlite3.Error as e:
+                logger.error(f"DB error during update_user_preference for key '{key}': {e}", exc_info=True)
+                # Fallback to in-memory only if DB fails, or re-raise?
+                # For now, create object and it won't be persisted if add_memory_item fails on DB part.
+                # This behavior might need refinement.
+                # Create an in-memory version, it won't be persisted if db_conn is None or add_memory_item fails db part
+                temp_pref_for_failure = UserPreference(id=pref_id, key=key, content=value, timestamp=timestamp)
+                # Manually add to cache if DB op failed before add_memory_item call
+                self._memory_items[temp_pref_for_failure.id] = temp_pref_for_failure
+                return temp_pref_for_failure
+        else:
+            # No DB connection, purely in-memory behavior (similar to original but using add_memory_item for cache)
+            logger.info(f"No DB connection. Handling preference '{key}' in-memory.")
+            # Check in-memory cache for existing preference by key
+            existing_cached_pref: Optional[UserPreference] = None
+            for item_id, item_obj in self._memory_items.items():
+                if isinstance(item_obj, UserPreference) and item_obj.key == key:
+                    existing_cached_pref = item_obj
+                    break
+            
+            if existing_cached_pref:
+                existing_cached_pref.content = value
+                existing_cached_pref.timestamp = timestamp
+                # add_memory_item will update the cache again, which is fine.
+                await self.add_memory_item(existing_cached_pref) # Ensures it's in cache via a single path
+                return existing_cached_pref
+            else:
+                new_pref = UserPreference(id=pref_id, key=key, content=value, timestamp=timestamp)
+                await self.add_memory_item(new_pref)
+                return new_pref
+
+    async def get_user_preference(self, key: str, default: Optional[Any] = None) -> Optional[Any]:
+        """Retrieves the latest value for a user preference key from DB or cache."""
+        # Try cache first for speed
+        for item_id, item_obj in reversed(list(self._memory_items.items())): # Check recent cache items first
+            if isinstance(item_obj, UserPreference) and item_obj.key == key:
+                logger.debug(f"Retrieved preference '{key}' from cache.")
+                return item_obj.content
+
+        if self.db_conn:
+            try:
+                cursor = self.db_conn.cursor()
+                cursor.execute("SELECT id, content_json, metadata_json, timestamp FROM user_preferences WHERE key = ? ORDER BY timestamp DESC LIMIT 1", (key,))
+                row = cursor.fetchone()
+                if row:
+                    logger.debug(f"Retrieved preference '{key}' from DB.")
+                    # Create a UserPreference object to cache it and return content
+                    pref = UserPreference(
+                        id=row['id'], 
+                        key=key, 
+                        content=json.loads(row['content_json']) if row['content_json'] else None,
+                        timestamp=row['timestamp'],
+                        metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                    )
+                    self._memory_items[pref.id] = pref # Update cache
+                    return pref.content
+            except sqlite3.Error as e:
+                logger.error(f"DB error during get_user_preference for key '{key}': {e}", exc_info=True)
+        
+        logger.debug(f"Preference '{key}' not found in cache or DB. Returning default.")
+        return default
+
+    async def infer_behavioral_pattern(self, pattern_type: str, data: Any, confidence: float = 0.5) -> BehavioralPattern:
+        """
+        Infers and stores a behavioral pattern.
+        This is a placeholder; real inference would be more complex.
+        """
+        pattern = BehavioralPattern(
+            pattern_type=pattern_type, 
+            content=data, 
+            confidence=confidence,
+            id=f"pattern_{pattern_type}_{time.time()}" # Ensure ID is unique
+        )
+        await self.add_memory_item(pattern)
+        logger.info(f"Inferred behavioral pattern: {pattern_type} with data {data} (Confidence: {confidence})")
+        return pattern
+
+    async def add_dialogue_turn(self, session_id: str, speaker: str, text: str, metadata: Optional[Dict]=None) -> DialogueHistory:
+        """Adds a dialogue turn to a session, creating or updating it in memory and DB."""
+        if metadata is None:
+            metadata = {}
+        
+        current_time = time.time()
+        new_turn = DialogueTurn(speaker=speaker, text=text, timestamp=current_time, metadata=metadata)
+
+        session_history: Optional[DialogueHistory] = None
+
+        # 1. Try to find existing DialogueHistory in cache by session_id
+        cached_histories = [item for item in self._memory_items.values() if isinstance(item, DialogueHistory) and item.session_id == session_id]
+        if cached_histories:
+            cached_histories.sort(key=lambda h: h.timestamp, reverse=True)
+            session_history = cached_histories[0]
+            logger.debug(f"Found DialogueHistory for session '{session_id}' in cache (ID: {session_history.id}).")
+        
+        # 2. If not in cache, try to find in DB by session_id
+        if not session_history and self.db_conn:
+            try:
+                cursor = self.db_conn.cursor()
+                cursor.execute("SELECT id, summary, timestamp, metadata_json FROM dialogue_sessions WHERE session_id = ? ORDER BY timestamp DESC LIMIT 1", (session_id,))
+                row = cursor.fetchone()
+                if row:
+                    logger.debug(f"Found DialogueHistory for session '{session_id}' in DB (DB ID: {row['id']}). Reconstructing.")
+                    # Reconstruct session. For turns, we might load them all or defer to get_dialogue_history.
+                    # For add_dialogue_turn, we primarily need the session object to append to.
+                    # Let's load recent turns to make the cached object somewhat useful.
+                    turns_cursor = self.db_conn.cursor()
+                    turns_cursor.execute("SELECT speaker, text, timestamp, metadata_json FROM dialogue_turns WHERE session_db_id = ? ORDER BY turn_order DESC LIMIT 20", (row['id'],)) # Load recent 20 for cache
+                    loaded_turns: List[DialogueTurn] = []
+                    for turn_row in reversed(turns_cursor.fetchall()): # Reverse to maintain chronological order
+                        loaded_turns.append(DialogueTurn(
+                            speaker=turn_row['speaker'], text=turn_row['text'], timestamp=turn_row['timestamp'], 
+                            metadata=json.loads(turn_row['metadata_json']) if turn_row['metadata_json'] else {}
+                        ))
+                    session_history = DialogueHistory(
+                        id=row['id'], session_id=session_id, turns=loaded_turns, 
+                        summary=row['summary'], timestamp=row['timestamp'], 
+                        metadata=json.loads(row['metadata_json']) if row['metadata_json'] else {}
+                    )
+                    self._memory_items[session_history.id] = session_history # Update cache
+            except sqlite3.Error as e:
+                logger.error(f"DB error looking up session '{session_id}': {e}", exc_info=True)
+
+        # 3. Append turn and update/create session object
+        if session_history:
+            session_history.turns.append(new_turn)
+            session_history.timestamp = current_time # Update session timestamp for recent activity
+            self._memory_items[session_history.id] = session_history # Re-cache if updated
+            logger.info(f"Appended turn to DialogueHistory for session '{session_id}'. Cache ID: {session_history.id}. New turn count: {len(session_history.turns)}")
+        else:
+            # Truly new session (not in cache, not in DB)
+            session_history_id = f"dialogue_{session_id}_{current_time}"
+            session_history = DialogueHistory(id=session_history_id, session_id=session_id, turns=[new_turn], timestamp=current_time)
+            self._memory_items[session_history.id] = session_history # Add to cache
+            logger.info(f"Created new DialogueHistory for session '{session_id}'. Cache ID: {session_history.id}")
+
+        # 4. Persist session and turn to DB
+        if self.db_conn and session_history: 
+            try:
+                cursor = self.db_conn.cursor()
+                # Upsert DialogueSession. Use session_history.id as the definitive ID.
+                # ON CONFLICT on session_id handles cases where another instance might have created it with a different primary ID.
+                cursor.execute("""
+                    INSERT INTO dialogue_sessions (id, session_id, summary, timestamp, metadata_json)
+                    VALUES (?, ?, ?, ?, ?)
+                    ON CONFLICT(session_id) DO UPDATE SET
+                        id = excluded.id, -- Prefer the ID from the current session_history object if conflict on session_id
+                        summary = excluded.summary, 
+                        timestamp = excluded.timestamp, 
+                        metadata_json = excluded.metadata_json
+                    WHERE excluded.timestamp > timestamp; -- Only update if new data is fresher for the same session_id
+                    -- If the primary key (id) conflicts, it implies we are updating an existing known session by its definitive ID.
+                    -- No, the above ON CONFLICT(session_id) needs to be primary. If ID matches, it's an update. If session_id matches but ID is different, we update the existing row.
+                    -- Let's simplify: Upsert on ID. If new session ID, it will insert. If existing ID, it will update.
+                    -- The lookup logic above should ensure we get the correct session_history.id if session_id exists.
+                """)
+                # Revised UPSERT for dialogue_sessions to be safer:
+                # First, try to insert with the specific ID. If that session_id already exists with a *different* ID, this could fail
+                # or create issues if session_id is unique. The previous lookup should handle this.
+                # The most robust way is to ensure session_id is the key for finding existing, then use its known DB ID.
+                cursor.execute("""
+                    INSERT INTO dialogue_sessions (id, session_id, summary, timestamp, metadata_json)
+                    VALUES (?, ?, ?, ?, ?)
+                    ON CONFLICT(id) DO UPDATE SET
+                        summary = excluded.summary, 
+                        timestamp = excluded.timestamp, 
+                        metadata_json = excluded.metadata_json
+                """, (
+                    session_history.id, 
+                    session_history.session_id, 
+                    session_history.summary, 
+                    session_history.timestamp, 
+                    json.dumps(session_history.metadata)
+                ))
+                
+                turn_db_id = f"turn_{session_history.id}_{new_turn.timestamp}_{speaker[:10].replace(" ", "_")}" 
+                turn_order = len(session_history.turns) 
+
+                cursor.execute("""
+                    INSERT INTO dialogue_turns (turn_id, session_db_id, speaker, text, timestamp, metadata_json, turn_order)
+                    VALUES (?, ?, ?, ?, ?, ?, ?)
+                    ON CONFLICT(turn_id) DO UPDATE SET -- In case a turn with exact same generated ID exists (unlikely but good practice)
+                        speaker=excluded.speaker, text=excluded.text, timestamp=excluded.timestamp, metadata_json=excluded.metadata_json, turn_order=excluded.turn_order
+                """, (
+                    turn_db_id, session_history.id, new_turn.speaker, new_turn.text, 
+                    new_turn.timestamp, json.dumps(new_turn.metadata), turn_order
+                ))
+                self.db_conn.commit()
+                logger.debug(f"Persisted dialogue session '{session_history.session_id}' (ID: {session_history.id}) and turn (DB ID: {turn_db_id}) to DB.")
+            except sqlite3.Error as e:
+                logger.error(f"Error persisting dialogue turn for session '{session_id}' to DB: {e}", exc_info=True)
+        
+        return session_history
+
+    async def get_dialogue_history(self, session_id: str, max_turns: Optional[int] = None) -> Optional[DialogueHistory]:
+        """Retrieves a specific dialogue history by session_id, prioritizing DB."""
+        logger.debug(f"Attempting to retrieve dialogue history for session_id: {session_id}")
+
+        # Try in-memory cache first, but be mindful it might be partially loaded by add_dialogue_turn
+        # If a full load is needed and DB exists, DB should be the source of truth.
+        cached_history: Optional[DialogueHistory] = None
+        for item_id, item_obj in self._memory_items.items():
+            if isinstance(item_obj, DialogueHistory) and item_obj.session_id == session_id:
+                # If max_turns is not specified, and cached item seems complete enough, consider returning it.
+                # However, for definitive full history, DB is better.
+                # Let's assume if we have it in cache, it was placed there by a previous full load or add_dialogue_turn.
+                # If max_turns is specified, we might need to re-fetch or filter turns from DB anyway.
+                cached_history = item_obj
+                if max_turns is None or len(cached_history.turns) >= max_turns: # Simple check
+                    logger.debug(f"Found potentially suitable DialogueHistory for session '{session_id}' in cache.")
+                    # Return a copy with potentially sliced turns if max_turns is set
+                    if max_turns is not None:
+                        # Ensure turns are sorted by their timestamp or inherent order if not already
+                        # DialogueTurn objects from cache are already ordered as they were appended.
+                        final_turns = cached_history.turns[-max_turns:] # Get the last N turns
+                        # Create a new DialogueHistory instance to avoid modifying the cached one directly if turns are sliced
+                        return DialogueHistory(
+                            id=cached_history.id, session_id=cached_history.session_id, turns=final_turns,
+                            summary=cached_history.summary, timestamp=cached_history.timestamp, metadata=cached_history.metadata
+                        )
+                    return cached_history # Return the cached one as is
+                break # Found a candidate, but may need DB for specific max_turns
+
+        if self.db_conn:
+            try:
+                cursor = self.db_conn.cursor()
+                cursor.execute("SELECT id, summary, timestamp, metadata_json FROM dialogue_sessions WHERE session_id = ? ORDER BY timestamp DESC LIMIT 1", (session_id,))
+                session_row = cursor.fetchone()
+
+                if session_row:
+                    session_db_id = session_row['id']
+                    logger.debug(f"Found dialogue session in DB (ID: {session_db_id}) for session_id: {session_id}. Fetching turns.")
+                    
+                    turns_query = "SELECT speaker, text, timestamp, metadata_json, turn_order FROM dialogue_turns WHERE session_db_id = ? ORDER BY turn_order ASC"
+                    turn_params = [session_db_id]
+                    
+                    # If max_turns is specified, we adjust the query to get the latest N turns.
+                    # SQLite doesn't directly support LIMIT on a subquery sorted differently easily.
+                    # So, we fetch all, then slice, or fetch N turns sorted descending then reverse in Python.
+                    # Let's fetch N turns ordered DESC then reverse in Python for simplicity if max_turns is given.
+                    if max_turns is not None:
+                        turns_query = "SELECT speaker, text, timestamp, metadata_json, turn_order FROM dialogue_turns WHERE session_db_id = ? ORDER BY turn_order DESC LIMIT ?"
+                        turn_params.extend([max_turns])
+                    
+                    turns_cursor = self.db_conn.cursor()
+                    turns_cursor.execute(turns_query, tuple(turn_params))
+                    
+                    dialogue_turns_from_db: List[DialogueTurn] = []
+                    db_rows = turns_cursor.fetchall()
+                    if max_turns is not None: # If we fetched DESC, reverse to get chronological for DialogueHistory
+                        db_rows.reverse()
+
+                    for turn_row in db_rows:
+                        dialogue_turns_from_db.append(DialogueTurn(
+                            speaker=turn_row['speaker'], 
+                            text=turn_row['text'], 
+                            timestamp=turn_row['timestamp'], 
+                            metadata=json.loads(turn_row['metadata_json']) if turn_row['metadata_json'] else {}
+                        ))
+                    
+                    reconstructed_history = DialogueHistory(
+                        id=session_db_id, 
+                        session_id=session_id, 
+                        turns=dialogue_turns_from_db,
+                        summary=session_row['summary'], 
+                        timestamp=session_row['timestamp'],
+                        metadata=json.loads(session_row['metadata_json']) if session_row['metadata_json'] else {}
+                    )
+                    self._memory_items[reconstructed_history.id] = reconstructed_history # Update cache with fully loaded history
+                    logger.info(f"Reconstructed DialogueHistory for session '{session_id}' from DB with {len(dialogue_turns_from_db)} turns.")
+                    return reconstructed_history
+                else:
+                    logger.debug(f"Dialogue session_id '{session_id}' not found in DB.")
+                    return None # Not found in DB
+            except sqlite3.Error as e:
+                logger.error(f"DB error fetching dialogue history for session_id '{session_id}': {e}", exc_info=True)
+                # If DB error, and we had a cached_history, maybe return that? Or None?
+                # For now, if DB fails, we don't fall back to a potentially stale/partial cache entry if full load was intended.
+                return cached_history # Fallback to (potentially partial) cache if DB fails after cache check
+        else:
+            # No DB, rely on cache if it was found earlier and met criteria
+            if cached_history: # This means it passed the initial cache check (e.g. max_turns satisfied)
+                logger.debug(f"Returning DialogueHistory for session '{session_id}' from cache (no DB connection).")
+                return cached_history
+
+        logger.debug(f"DialogueHistory for session '{session_id}' not found.")
+        return None
+
+    async def summarize_dialogue_history(self, dialogue_history_id: str) -> bool:
+        """
+        Summarizes a dialogue history using the LLM. (Placeholder)
+        Requires self.llm_client to be set.
+        """
+        if not self.llm_client:
+            logger.warning("LLM client not available for summarizing dialogue history.")
+            return False
+            
+        item = await self.get_memory_item(dialogue_history_id)
+        if not isinstance(item, DialogueHistory):
+            logger.warning(f"Dialogue history {dialogue_history_id} not found or not a DialogueHistory item.")
+            return False
+
+        if not item.turns:
+            logger.info(f"No turns to summarize for dialogue {dialogue_history_id}.")
+            item.summary = "Empty dialogue."
+            return True
+
+        # Construct prompt for summarization
+        dialogue_text = "\n".join([f"{turn.speaker}: {turn.text}" for turn in item.turns])
+        prompt = f"""
+        Summarize the following dialogue concisely. Capture the main topics and outcomes.
+        If the dialogue is very short, a brief phrase is sufficient.
+
+        Dialogue:
+        ---
+        {dialogue_text}
+        ---
+
+        Summary:
+        """
+        try:
+            summary_text = await self.llm_client.generate(prompt=prompt)
+            item.summary = summary_text
+            item.timestamp = time.time() # Update timestamp as it's modified
+            logger.info(f"Summarized dialogue {dialogue_history_id}. Summary: {summary_text[:100]}...")
+            # TODO: Persist updated item if using storage_service
+            return True
+        except Exception as e:
+            logger.error(f"Error summarizing dialogue {dialogue_history_id}: {e}")
+            return False
+
+    async def record_context_snapshot(self, triggering_query: str, context_content: Dict[str, Any], llm_response: Optional[str]=None) -> ContextSnapshot:
+        """Records a snapshot of the context provided to the LLM."""
+        snapshot = ContextSnapshot(
+            triggering_query=triggering_query, 
+            content=context_content,
+            llm_response=llm_response,
+            id=f"snap_{time.time()}" # Ensure unique ID
+        )
+        await self.add_memory_item(snapshot)
+        logger.debug(f"Recorded context snapshot for query: {triggering_query[:50]}")
+        return snapshot
+
+    async def build_context_for_llm(self, 
+                                    session_id: Optional[str] = None, 
+                                    user_query: Optional[str] = None,
+                                    max_dialogue_turns: int = 5,
+                                    include_preferences_keys: Optional[List[str]] = None
+                                   ) -> Dict[str, Any]:
+        """
+        Builds a context dictionary to be used for LLM prompts.
+        Combines relevant dialogue history, user preferences, behavioral patterns.
+
+        This is a simplified version. Token budget allocation as per 10.8.F needs careful implementation.
+        """
+        context = {}
+        
+        # 1. Dialogue History (if session_id provided)
+        if session_id:
+            history = await self.get_dialogue_history(session_id, max_turns=max_dialogue_turns)
+            if history:
+                context['dialogue_history'] = {
+                    'summary': history.summary,
+                    'recent_turns': [{'speaker': t.speaker, 'text': t.text} for t in history.turns]
+                }
+        
+        # 2. User Preferences (selectively, based on keys or general config)
+        prefs_to_include = {}
+        default_pref_keys = getattr(self.config, 'memory_context_default_pref_keys', ['llm_model', 'rag_strategy'])
+        keys_to_fetch = include_preferences_keys if include_preferences_keys is not None else default_pref_keys
+        
+        for key in keys_to_fetch:
+            value = await self.get_user_preference(key)
+            if value is not None:
+                prefs_to_include[key] = value
+        if prefs_to_include:
+            context['user_preferences'] = prefs_to_include
+            
+        # 3. Behavioral Patterns (e.g., most recent relevant ones)
+        # This is highly dependent on what patterns are stored and how they are deemed relevant.
+        # For now, let's fetch a few recent generic ones.
+        # patterns = await self.get_memory_items(item_type=MemoryItemType.BEHAVIORAL_PATTERN, max_count=2, sort_by_time=True)
+        # if patterns:
+        #     context['behavioral_patterns'] = [{'type': p.pattern_type, 'details': p.content, 'confidence': p.metadata.get('confidence')} for p in patterns if isinstance(p, BehavioralPattern)]
+
+        # 4. Current Query (if provided)
+        if user_query:
+            context['current_query'] = user_query
+
+        # TODO: Implement token budget allocation (section 10.8.F)
+        # This would involve:
+        # - Calculating token counts for each context component.
+        # - Prioritizing components based on config (e.g., user_prefs 15%, current_content 30%, graph_info 20%, etc.).
+        # - Truncating or summarizing components to fit the budget.
+        # For now, this method just gathers potential context elements. The RAG engine or LLM caller
+        # would be responsible for final prompt construction and fitting it to token limits.
+
+        logger.debug(f"Built context: { {k: type(v) for k,v in context.items()} }")
+        return context
+
+    async def prune_memory(self, force_prune_all: bool = False, target_count: Optional[int] = None) -> int:
+        """
+        Prunes old memory items from the database based on retention period.
+        Also, ensures the in-memory cache reflects these changes.
+
+        Args:
+            force_prune_all: If True, ignores retention and attempts to clear (not implemented yet for DB).
+            target_count: If specified, tries to prune towards this count (primarily for cache, complex for DB).
+
+        Returns:
+            The number of items effectively pruned from the DB (approximate).
+        """
+        if force_prune_all:
+            logger.warning("force_prune_all=True is not fully implemented for DB backend. Will only prune by retention.")
+            # If we wanted to clear tables: self._memory_items.clear(); cursor.execute("DELETE FROM table") etc.
+
+        if not self.db_conn:
+            logger.info("No DB connection, skipping DB pruning. In-memory cache pruning (if any) would happen here.")
+            # Original in-memory pruning logic would go here if self._memory_items was the sole source of truth
+            # and max_elements was to be enforced on it directly without a DB.
+            # For now, this method primarily focuses on DB pruning when db_conn is available.
+            return 0
+
+        pruned_count_total = 0
+        cutoff_timestamp = time.time() - self.retention_period
+        logger.info(f"Pruning memory items older than timestamp: {cutoff_timestamp} (Retention: {self.retention_period/86400:.2f} days)")
+
+        tables_to_prune_by_timestamp = [
+            "user_preferences",
+            "behavioral_patterns",
+            "context_snapshots",
+            "dialogue_sessions" # Deleting sessions will cascade to dialogue_turns
+        ]
+
+        try:
+            cursor = self.db_conn.cursor()
+            for table_name in tables_to_prune_by_timestamp:
+                # Get count before deleting for logging
+                # Using a direct execute for count first might be slightly less efficient but good for logging.
+                # Alternatively, check cursor.rowcount after DELETE if the DB driver supports it reliably for DELETE.
+                # For SQLite, cursor.rowcount after DELETE should work.
+                
+                # Let's get IDs to be deleted for more accurate cache invalidation later, if we don't full reload
+                # For now, we do full reload, so direct DELETE is fine.
+                
+                initial_rowcount_query = f"SELECT COUNT(*) FROM {table_name} WHERE timestamp < ?"
+                cursor.execute(initial_rowcount_query, (cutoff_timestamp,))
+                num_to_delete = cursor.fetchone()[0]
+
+                if num_to_delete > 0:
+                    delete_query = f"DELETE FROM {table_name} WHERE timestamp < ?"
+                    cursor.execute(delete_query, (cutoff_timestamp,))
+                    deleted_this_table = cursor.rowcount # Number of rows affected by the DELETE
+                    self.db_conn.commit()
+                    logger.info(f"Pruned {deleted_this_table} old records from table '{table_name}'. (Initially expected: {num_to_delete})")
+                    pruned_count_total += deleted_this_table
+                else:
+                    logger.debug(f"No old records to prune from table '{table_name}' based on timestamp < {cutoff_timestamp}.")
+            
+            if pruned_count_total > 0:
+                logger.info(f"DB pruned {pruned_count_total} records. Reloading in-memory cache to reflect changes.")
+                self._memory_items.clear() # Clear the entire cache
+                self._load_memory_from_db() # Reload all types of memory items from DB
+                logger.info(f"In-memory cache reloaded. Current cache size: {len(self._memory_items)}.")
+            else:
+                logger.info("No records were pruned from the database based on retention period.")
+
+        except sqlite3.Error as e:
+            logger.error(f"Error during DB pruning: {e}", exc_info=True)
+            # Potentially rollback if transactions were started explicitly, but commit is per statement here.
+            return -1 # Indicate error
+
+        logger.info(f"Memory pruning (retention-based) complete. Total DB records affected: {pruned_count_total}.")
+        return pruned_count_total
+
+    async def get_memory_dashboard_data(self) -> Dict[str, Any]:
+        """
+        Provides data for a user-facing dashboard to manage memory. (Stub)
+        As per 10.8.F "Memory Management Dashboard".
+        """
+        stats_by_type = {mem_type.value: 0 for mem_type in MemoryItemType}
+        total_items = len(self._memory_items)
+        for item in self._memory_items.values():
+            stats_by_type[item.type.value] += 1
+        
+        recent_items_summary = []
+        # Get a few recent items as examples
+        recent_items = await self.get_memory_items(max_count=5, sort_by_time=True)
+        for item in recent_items:
+            summary = f"ID: {item.id}, Type: {item.type.value}, Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(item.timestamp))}"
+            if isinstance(item, UserPreference):
+                summary += f", Key: {item.key}, Value: {str(item.content)[:50]}"
+            elif isinstance(item, DialogueHistory):
+                summary += f", Session: {item.session_id}, Turns: {len(item.turns)}"
+            elif isinstance(item, ContextSnapshot):
+                summary += f", Query: {item.triggering_query[:50]}"
+            recent_items_summary.append(summary)
+
+        return {
+            "total_items": total_items,
+            "items_by_type": stats_by_type,
+            "retention_period_days": self.retention_period / 86400,
+            "max_elements_limit": self.max_elements,
+            "auto_summary_interval_hours": self.auto_summary_interval / 3600,
+            "recent_items_preview": recent_items_summary
+            # Add more data as needed: e.g., oldest item, newest item, specific preferences list
+        }
+
+# Example Usage / Test function
+async def main_test_memory_engine():
+    import asyncio
+    import time
+    from simtag.config import Config
+    from simtag.services.llm_client import LLMClient # For mocking or real use
+    # MemoryItem, UserPreference, DialogueHistory, DialogueTurn, ContextSnapshot, MemoryItemType are already imported in this file
+
+    # Basic logger for test output
+    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    test_logger = logging.getLogger("memory_engine_test")
+    test_logger.info("Starting Memory Engine Test")
+
+    # 1. Setup Config and Mock LLMClient
+    config = Config()
+    # Ensure a default llm_client_config for the test if not fully set up elsewhere
+    if not config.llm_client_config.get('base_url'): 
+        config.llm_client_config['base_url'] = "http://localhost:11434"
+        config.llm_client_config['default_model'] = "gemma:2b"
+        config.llm_client_config['default_embedding_model'] = "nomic-embed-text"
+    
+    config.memory_dialogues_limit = 5 # Keep test dialogue history small
+    config.memory_preferences_limit = 3
+
+    # Mock LLMClient methods (only summarize_history needs it for now)
+    async def mock_summarize_history(prompt: str, model: Optional[str] = None) -> str:
+        test_logger.info(f"Mock LLM summarize_history called with prompt (first 100): {prompt[:100]}...")
+        return f"Mocked summary of history: {prompt.splitlines()[-1][:50]}..."
+
+    mock_llm_client = LLMClient(llm_config=config.llm_client_config)
+    # For MemoryEngine, only `generate` (used by summarize_history) might be needed.
+    # If summarize_history directly calls generate, then mock that.
+    # Assuming summarize_history might become more complex, we mock a specific method or the generic one.
+    # Let's assume summarize_history in MemoryEngine internally calls self.llm_client.generate
+    mock_llm_client.generate = mock_summarize_history
+
+    # 2. Initialize MemoryEngine
+    memory_engine = MemoryEngine(config=config, llm_client=mock_llm_client)
+
+    # 3. Test adding and retrieving memory items
+    test_logger.info("\n--- Testing User Preferences ---")
+    pref1 = UserPreference(key="theme", value="dark")
+    pref2 = UserPreference(key="language", value="Python")
+    pref3 = UserPreference(key="verbosity", value="high")
+    pref4_overwrite = UserPreference(key="theme", value="light") # Overwrites pref1
+    
+    await memory_engine.update_user_preference(pref1.key, pref1.value)
+    await memory_engine.update_user_preference(pref2.key, pref2.value)
+    await memory_engine.update_user_preference(pref3.key, pref3.value)
+    all_prefs = await memory_engine.get_user_preferences()
+    test_logger.info(f"  Initial preferences ({len(all_prefs)}): {all_prefs}")
+    assert len(all_prefs) == 3
+
+    await memory_engine.update_user_preference(pref4_overwrite.key, pref4_overwrite.value)
+    updated_pref = await memory_engine.get_user_preference("theme")
+    test_logger.info(f"  Updated preference 'theme': {updated_pref.value if updated_pref else None}")
+    assert updated_pref and updated_pref.value == "light"
+    all_prefs_after_update = await memory_engine.get_user_preferences()
+    test_logger.info(f"  All preferences after update ({len(all_prefs_after_update)}): {all_prefs_after_update}")
+    assert len(all_prefs_after_update) == 3 # Should still be 3 due to overwrite and limit
+
+    test_logger.info("\n--- Testing Dialogue History ---")
+    session_id_1 = "test_session_001"
+    await memory_engine.add_dialogue_turn(session_id_1, "user", "Hello AI!", metadata={'timestamp': time.time() - 10})
+    await memory_engine.add_dialogue_turn(session_id_1, "ai", "Hello User!", metadata={'timestamp': time.time() - 9})
+    await memory_engine.add_dialogue_turn(session_id_1, "user", "How are you?", metadata={'timestamp': time.time() - 8})
+    await memory_engine.add_dialogue_turn(session_id_1, "ai", "I am fine.", metadata={'timestamp': time.time() - 7})
+    await memory_engine.add_dialogue_turn(session_id_1, "user", "What is 1+1?", metadata={'timestamp': time.time() - 6})
+    await memory_engine.add_dialogue_turn(session_id_1, "ai", "It is 2.", metadata={'timestamp': time.time() - 5}) # 6th turn, will exceed limit of 5
+
+    history_session_1 = await memory_engine.get_dialogue_history(session_id_1)
+    test_logger.info(f"  Dialogue history for {session_id_1} ({len(history_session_1.turns if history_session_1 else [])} turns):")
+    if history_session_1:
+        for turn in history_session_1.turns:
+            test_logger.info(f"    {turn.role}: {turn.content} (ts: {turn.timestamp})")
+        assert len(history_session_1.turns) == config.memory_dialogues_limit
+    else:
+        test_logger.warning("  No history found for session_id_1")
+
+    test_logger.info("\n--- Testing Context Snapshots ---")
+    snapshot1 = ContextSnapshot(
+        id="snap1", 
+        triggering_query="What is X?", 
+        context_content={"key_entities": ["X"], "retrieved_docs_count": 2},
+        llm_response="X is a variable."
+    )
+    await memory_engine.record_context_snapshot(snapshot1.triggering_query, snapshot1.context_content, snapshot1.llm_response, snapshot1.id)
+    # Test retrieving (though MemoryEngine doesn't have a direct get_snapshot by id yet)
+    # For now, we can see it in the dashboard or general memory items list.
+
+    test_logger.info("\n--- Testing build_context_for_llm ---")
+    context_for_llm = await memory_engine.build_context_for_llm(session_id_1, user_query="Tell me more.")
+    test_logger.info(f"  Built context for LLM: {context_for_llm}")
+    assert "preferences_summary" in context_for_llm
+    assert "dialogue_history_summary" in context_for_llm # As summarize_history is mocked, it should run
+
+    test_logger.info("\n--- Testing Memory Dashboard ---")
+    dashboard_data = await memory_engine.get_memory_dashboard_data()
+    test_logger.info(f"  Memory dashboard data: User Prefs: {len(dashboard_data.get('user_preferences',[]))}, Dialogues: {len(dashboard_data.get('recent_dialogue_sessions',[]))}, Snapshots: {len(dashboard_data.get('context_snapshots',[]))}")
+    test_logger.info(f"  Dashboard detail: {dashboard_data}")
+    assert len(dashboard_data.get('user_preferences', [])) == 3
+    assert len(dashboard_data.get('recent_dialogue_sessions', {}).get(session_id_1, {}).get('turns', [])) == config.memory_dialogues_limit
+    assert len(dashboard_data.get('context_snapshots', [])) == 1
+
+    await mock_llm_client.close()
+    test_logger.info("Memory Engine Test Finished")
+
+if __name__ == '__main__':
+    asyncio.run(main_test_memory_engine()) 
\ No newline at end of file
diff --git a/simtag/core/rag_engine.py b/simtag/core/rag_engine.py
new file mode 100755
index 0000000..7d9fc11
--- /dev/null
+++ b/simtag/core/rag_engine.py
@@ -0,0 +1,500 @@
+"""
+Org SuperTag RAG Engine Module
+
+This module implements the RAG (Retrieval Augmented Generation) engine 
+for org-supertag, providing different strategies for context retrieval.
+"""
+import asyncio
+import logging
+from typing import Dict, List, Any, Optional, Tuple, TypedDict
+import os
+import numpy as np
+import tempfile # For temporary vector DB
+
+# Corrected and new imports
+from simtag.config import Config
+from .graph_service import GraphService
+from ..services.llm_client import LLMClient
+from .entity_extractor import LLMEntityExtractor, ExtractedEntity
+
+logger = logging.getLogger(__name__)
+
+class StructuredContextOutput(TypedDict):
+    """
+    Defines the structured output from retrieval methods.
+    """
+    entities: List[Dict[str, Any]] # Changed from GraphEntity
+    relations: List[Dict[str, Any]] # Changed from GraphRelation
+    documents: List[Dict[str, Any]] # Existing document format
+
+
+class OrgSupertagRAGEngine:
+    """
+    Implements the RAG engine with naive, light, and mini query strategies.
+    Integrates Knowledge Graph and Vector Storage.
+    Based on living-doc-features.org section 10.7.A.3
+    """
+
+    def __init__(self,
+                 graph_service: GraphService,
+                 llm_client: LLMClient,
+                 config: Config
+                 ):
+        """
+        Initializes the OrgSupertagRAGEngine.
+
+        Args:
+            graph_service: Instance of the unified GraphService.
+            llm_client: Client for interacting with an LLM for generation.
+            config: Configuration object.
+        """
+        self.graph_service = graph_service
+        self.llm_client = llm_client
+        self.config = config
+        
+        # Use the new, unified LLMEntityExtractor
+        self.entity_extractor = LLMEntityExtractor(
+            llm_client=llm_client,
+            config=config.analysis_config # Use the analysis config block
+        )
+        
+        logger.info("OrgSupertagRAGEngine initialized.")
+
+    async def _naive_retrieval(
+        self,
+        query_text: str,
+        query_embedding: Optional[np.ndarray],
+        top_k: int
+    ) -> StructuredContextOutput:
+        """Performs naive vector store retrieval."""
+        logger.debug(f"Performing naive retrieval for query: '{query_text[:50]}...'")
+        
+        if not self.graph_service.has_vector_ext:
+            logger.warning("Vector extension is not available for naive retrieval.")
+            return StructuredContextOutput(entities=[], relations=[], documents=[])
+
+        if query_embedding is None:
+            logger.debug("No pre-computed embedding, generating one for the query.")
+            query_embedding = await self.llm_client.get_embedding(query_text)
+            if query_embedding is None:
+                logger.error("Failed to generate embedding for query.")
+                return StructuredContextOutput(entities=[], relations=[], documents=[])
+        
+        # Find similar node IDs
+        similar_node_tuples = self.graph_service.find_similar_nodes(query_embedding, top_k=top_k)
+        if not similar_node_tuples:
+            return StructuredContextOutput(entities=[], relations=[], documents=[])
+
+        node_ids = [node_id for node_id, score in similar_node_tuples]
+        scores_map = {node_id: score for node_id, score in similar_node_tuples}
+
+        # Get node details
+        nodes_data = self.graph_service.get_nodes_by_ids(node_ids)
+
+        standardized_documents: List[Dict[str, Any]] = []
+        for doc_data in nodes_data:
+            node_id = doc_data.get("node_id")
+            standardized_doc = {
+                "id": node_id,
+                "text": doc_data.get("content", "Content unavailable"),
+                "score": scores_map.get(node_id, 0.0), # Use score from similarity search
+                "retrieval_source_type": "vector_search",
+                **{k: v for k, v in doc_data.items() if k not in ["id", "text", "score"]}
+            }
+            standardized_documents.append(standardized_doc)
+        
+        logger.debug(f"Naive retrieval found {len(standardized_documents)} documents.")
+        return StructuredContextOutput(entities=[], relations=[], documents=standardized_documents)
+
+    async def _light_retrieval(
+        self,
+        query_text: str,
+        query_embedding: Optional[np.ndarray],
+        extracted_entities: List[Dict[str, Any]],
+        top_k: int,
+        max_neighbor_depth: int,
+        vector_graph_ratio: Optional[float] = None # New parameter for balancing
+    ) -> StructuredContextOutput: # Changed return type
+        """Performs light retrieval: naive search + K-hop graph neighbors for context."""
+        logger.debug(f"Performing light retrieval for query: '{query_text[:50]}...', ratio: {vector_graph_ratio}")
+
+        final_entities: List[Dict[str, Any]] = []
+        final_relations: List[Dict[str, Any]] = [] # Remains empty for now
+        combined_documents: List[Dict[str, Any]] = []
+
+        # Determine split for naive vs graph results
+        _ratio = vector_graph_ratio if vector_graph_ratio is not None and 0.0 <= vector_graph_ratio <= 1.0 else 0.5 # Default to 0.5
+        
+        num_naive_results = int(top_k * _ratio)
+        num_graph_results_target = top_k - num_naive_results
+        if top_k == 1 and num_naive_results == 0 : # Ensure at least one if top_k is 1
+             num_naive_results = 1
+             num_graph_results_target = 0
+        elif top_k > 1 and num_naive_results == 0: # Ensure naive gets at least 1 if ratio is too low for it but top_k > 1
+            num_naive_results = 1
+            num_graph_results_target = top_k - 1
+
+        # 1. Perform naive vector retrieval first
+        if num_naive_results > 0:
+            naive_output = await self._naive_retrieval(query_text, query_embedding, num_naive_results)
+            combined_documents.extend(naive_output.documents)
+        else:
+            naive_output = StructuredContextOutput(entities=[], relations=[], documents=[]) # Ensure naive_output is defined
+
+        # 2. Enhance with graph context (K-hop neighbors)
+        graph_enhanced_docs_raw: List[Dict[str, Any]] = []
+        start_entities_for_graph: List[Dict[str, Any]] = []
+
+        if extracted_entities:
+            # Convert extracted entities to a simpler dict format for consistency
+            for ext_entity in extracted_entities:
+                if ext_entity.get('id'): 
+                    # We first need to check if this entity exists in our graph to get its full data
+                    graph_entity = await self.graph_service.get_tag_by_name(ext_entity.get('name')) # Assuming tags are the main entities for now
+                    if graph_entity:
+                        start_entities_for_graph.append(graph_entity)
+
+            final_entities.extend(start_entities_for_graph)
+            
+            start_entity_ids = [e['id'] for e in start_entities_for_graph]
+
+            if start_entity_ids and num_graph_results_target > 0:
+                neighbor_nodes = []
+                for start_id in start_entity_ids:
+                    # NOTE: get_neighbors is a simplified replacement for the old get_neighbor_documents
+                    # max_neighbor_depth is not used yet, would require recursive calls.
+                    neighbors = self.graph_service.get_neighbors(start_id)
+                    neighbor_nodes.extend(neighbors)
+
+                # Standardize neighbor_nodes to document format
+                for i, node_data in enumerate(neighbor_nodes):
+                    standardized_neighbor_doc = {
+                        "id": node_data.get("node_id", f"neighbor_{i}"),
+                        "text": node_data.get("content", "Neighbor content unavailable"),
+                        "score": 0.70, # Default score for graph neighbors
+                        "retrieval_source_type": "graph_neighbor",
+                        **{k: v for k, v in node_data.items() if k not in ["id", "text", "score"]}
+                    }
+                    graph_enhanced_docs_raw.append(standardized_neighbor_doc)
+        
+        # Add graph-enhanced documents, avoiding duplicates
+        # combined_documents already contains standardized docs from naive_output
+        seen_doc_ids = {doc.get('id') for doc in combined_documents if doc.get('id')}
+        for doc in graph_enhanced_docs_raw: # Iterate standardized graph docs
+            if doc.get('id') not in seen_doc_ids:
+                combined_documents.append(doc)
+                seen_doc_ids.add(doc.get('id'))
+
+        # Simple sort by score, can be more sophisticated
+        # This sort might need adjustment if scores are not uniformly present or comparable
+        sorted_documents = sorted(combined_documents, key=lambda x: x.get('score', 0.0), reverse=True)
+
+        logger.debug(f"Light retrieval found {len(final_entities)} entities and {len(sorted_documents)} documents.")
+        
+        return StructuredContextOutput(
+            entities=final_entities, 
+            relations=final_relations, 
+            documents=sorted_documents[:top_k] # Return top_k overall documents
+        )
+
+    async def _mini_retrieval(
+        self,
+        query_text: str,
+        extracted_entities: List[Dict[str, Any]],
+        target_entity_types: Optional[List[str]],
+        top_k: int, # Overall top_k if specific ones aren't provided
+        max_reasoning_depth: int,
+        top_k_reasoning_paths: Optional[int] = None, # New specific top_k
+        top_k_vector_supplement: Optional[int] = None  # New specific top_k
+    ) -> StructuredContextOutput:
+        """
+        Perform RAG retrieval using the 'mini' strategy (GraphRAG-focused).
+        Retrieves entities, relations, and supporting documents based on reasoning paths and vector search.
+        """
+        logger.debug(f"_mini_retrieval called with query: '{query_text}', entities: {extracted_entities}, types: {target_entity_types}")
+
+        final_entities: List[Dict[str, Any]] = []
+        final_relations: List[Dict[str, Any]] = []
+        final_documents: List[Dict[str, Any]] = []
+        
+        entity_names_from_query = [entity.get('name') for entity in extracted_entities if entity.get('name')]
+
+        # Determine top_k for paths and vector search
+        _top_k_paths = top_k_reasoning_paths if top_k_reasoning_paths is not None else top_k
+        _top_k_vector = top_k_vector_supplement if top_k_vector_supplement is not None else top_k
+        # If only overall top_k is given, we might split it, e.g., 70% paths, 30% vector, or use it fully for both.
+        # For simplicity now, if specific top_k not given, use overall top_k for both calls if they are > 0.
+        # A more sophisticated split could be: top_k // 2 for each if specific ones are None.
+        # Let's refine this: if specific are None, use the main top_k for paths, and maybe a smaller portion for vector supplement.
+        if top_k_reasoning_paths is None:
+            _top_k_paths = top_k # Default to overall top_k for paths
+        if top_k_vector_supplement is None:
+            _top_k_vector = max(1, top_k // 2) # Default to half for vector, ensuring at least 1 if top_k > 0
+            if _top_k_paths == top_k: # If paths took full budget, maybe vector gets fewer
+                _top_k_vector = max(1, top_k // 3) 
+
+        if _top_k_paths <= 0 and _top_k_vector <= 0:
+            logger.warning("_mini_retrieval: Both path and vector top_k are zero. Returning empty.")
+            return StructuredContextOutput(entities=[], relations=[], documents=[])
+
+        # 1. Find reasoning paths from the knowledge graph
+        if _top_k_paths > 0:
+            logger.warning("Reasoning path retrieval is currently stubbed out in this refactoring phase.")
+            # path_nodes_data, path_relations_data, path_description_docs_raw = \
+            #     await self.graph_service.find_reasoning_paths_with_details(
+            #         entity_names_from_query,
+            #         target_entity_types,
+            #         max_paths=_top_k_paths, 
+            #         max_depth=max_reasoning_depth
+            #     )
+            # Faking the output to avoid breaking the flow
+            path_nodes_data, path_relations_data, path_description_docs_raw = [], [], []
+
+            seen_entity_ids = set()
+            for node_data in path_nodes_data:
+                if node_data and node_data.get('id') not in seen_entity_ids:
+                    final_entities.append(node_data)
+                    seen_entity_ids.add(node_data.get('id'))
+            
+            seen_relation_ids = set()
+            for rel_data in path_relations_data:
+                if rel_data and rel_data.get('id') not in seen_relation_ids:
+                    final_relations.append(rel_data)
+                    seen_relation_ids.add(rel_data.get('id'))
+
+            final_documents.extend(path_description_docs_raw)
+        else:
+            path_relations_str_desc = "" 
+            # path_description_docs_raw = [] # Not needed if not fetched
+
+        # 2. Supplement with a naive vector search
+        if _top_k_vector > 0:
+            # Embedding is passed as None to let _naive_retrieval generate it
+            vector_supplement_output = await self._naive_retrieval(query_text, None, _top_k_vector)
+            
+            # Combine results, avoiding duplicates
+            seen_doc_ids = {doc['id'] for doc in final_documents if doc.get('id')}
+            for doc in vector_supplement_output['documents']:
+                if doc.get('id') not in seen_doc_ids:
+                    final_documents.append(doc)
+                    seen_doc_ids.add(doc.get('id'))
+
+        # Simple sort by score, can be more sophisticated
+        sorted_documents = sorted(final_documents, key=lambda x: x.get('score', 0.0), reverse=True)
+        
+        logger.debug(f"Mini retrieval found {len(final_entities)} entities, {len(final_relations)} relations, and {len(sorted_documents)} documents.")
+
+        return StructuredContextOutput(
+            entities=final_entities,
+            relations=final_relations,
+            documents=sorted_documents[:top_k]
+        )
+
+    async def retrieve_context(
+        self,
+        query_text: str,
+        strategy: str = "naive",
+        top_k: Optional[int] = None,
+        target_entity_types: Optional[List[str]] = None, 
+        max_reasoning_depth: Optional[int] = None, 
+        max_neighbor_depth: Optional[int] = None, 
+        vector_graph_ratio: Optional[float] = None, # For light strategy
+        top_k_reasoning_paths: Optional[int] = None, # For mini strategy
+        top_k_vector_supplement: Optional[int] = None # For mini strategy
+    ) -> StructuredContextOutput:
+        """
+        Main entry point for retrieving context using a specified strategy.
+        """
+        # Set defaults from config if not provided
+        cfg_top_k = top_k if top_k is not None else self.config.retrieval_config.get('default_top_k', 5)
+        cfg_max_reasoning_depth = max_reasoning_depth if max_reasoning_depth is not None else self.config.retrieval_config.get('default_max_reasoning_depth', 3)
+        cfg_max_neighbor_depth = max_neighbor_depth if max_neighbor_depth is not None else self.config.retrieval_config.get('default_max_neighbor_depth', 2)
+
+        query_embedding: Optional[np.ndarray] = None
+        extracted_entities: List[Dict[str, Any]] = []
+
+        # Generate embedding and extract entities if needed by the strategy
+        if strategy in ["light", "mini", "naive"]: # naive needs embedding now
+            # In parallel for efficiency
+            tasks = []
+            if strategy in ["light", "mini"]:
+                # The new extractor returns a dict, not just entities
+                tasks.append(self.entity_extractor.extract(query_text))
+            
+            tasks.append(self.llm_client.get_embedding(query_text))
+            
+            results = await asyncio.gather(*tasks)
+            
+            if strategy in ["light", "mini"]:
+                extraction_result = results.pop(0)
+                # Adapt to the new return format
+                extracted_entities = extraction_result.get("entities", [])
+            
+            query_embedding = results.pop(0)
+
+        logger.info(f"Retrieving context for query '{query_text[:50]}...' with strategy '{strategy}'")
+
+        if strategy == "naive":
+            return await self._naive_retrieval(query_text, query_embedding, cfg_top_k)
+        
+        elif strategy == "light":
+            return await self._light_retrieval(
+                query_text,
+                query_embedding,
+                extracted_entities,
+                cfg_top_k,
+                cfg_max_neighbor_depth,
+                vector_graph_ratio
+            )
+        
+        elif strategy == "mini":
+            _target_entity_types = target_entity_types if target_entity_types is not None else getattr(self.config, 'rag_mini_default_target_types', ['concept', 'project', 'method'])
+            return await self._mini_retrieval(
+                query_text,
+                extracted_entities, 
+                _target_entity_types, 
+                cfg_top_k, 
+                cfg_max_reasoning_depth,
+                top_k_reasoning_paths=top_k_reasoning_paths, # Pass through
+                top_k_vector_supplement=top_k_vector_supplement # Pass through
+            )
+        
+        else:
+            logger.warning(f"Unknown retrieval strategy: {strategy}. Defaulting to naive.")
+            # Ensure query_embedding is available for the default naive call
+            if query_embedding is None: # It would be None if strategy was initially "mini"
+                 query_embedding = await self.llm_client.get_embedding(query_text)
+                 if query_embedding is None:
+                    logger.error("Failed to get query embedding for default naive strategy.")
+                    return StructuredContextOutput(entities=[], relations=[], documents=[])
+            return await self._naive_retrieval(query_text, query_embedding, cfg_top_k)
+
+    async def generate_response(
+        self,
+        query_text: str,
+        structured_context: StructuredContextOutput, # Changed parameter name and type
+        prompt_template: Optional[str] = None,
+        # TODO: Add other LLM params like temperature, max_tokens from config or method args
+    ) -> str:
+        """
+        Generates a response using the LLM based on the query and retrieved structured context.
+        """
+        logger.debug(f"Generating response for query: '{query_text[:50]}...'")
+
+        if not structured_context or (
+            not structured_context.get('entities') and 
+            not structured_context.get('relations') and 
+            not structured_context.get('documents')
+        ):
+            logger.warning("No context provided to generate_response. Returning a default message.")
+            return "I don't have enough information to answer that query."
+
+        # 1. Format the structured context into a string for the LLM
+        context_str = self._format_structured_context_for_llm(structured_context)
+
+        # 2. Prepare the prompt
+        final_prompt_template = prompt_template or self.config.rag_default_prompt_template
+        
+        # Ensure the template can handle 'query' and 'context_str'
+        # Example template: "Context: {context_str}\n\nQuestion: {query}\n\nAnswer:"
+        if "{context_str}" not in final_prompt_template or "{query}" not in final_prompt_template:
+            logger.warning("Prompt template does not contain {{context_str}} or {{query}}. Using a basic fallback.")
+            # Basic fallback prompt that includes the structured context string
+            prompt = f"Based on the following information:\n{context_str}\n\nAnswer the question: {query_text}"
+        else:
+            try:
+                prompt = final_prompt_template.format(context_str=context_str, query=query_text)
+            except KeyError as e:
+                logger.error(f"Error formatting prompt template with key {e}. Using basic fallback.")
+                prompt = f"Based on the following information:\n{context_str}\n\nAnswer the question: {query_text}"
+
+        # 3. Call the LLM
+        try:
+            response_text = await self.llm_client.generate_text(
+                prompt,
+                # TODO: Pass through LLM generation parameters like max_tokens, temperature from config
+                # max_tokens=self.config.llm_max_tokens_response,
+                # temperature=self.config.llm_temperature_response
+            )
+            return response_text
+        except Exception as e:
+            logger.error(f"Error generating response from LLM: {e}")
+            return "Sorry, I encountered an error while trying to generate a response."
+
+    def _format_structured_context_for_llm(self, context: StructuredContextOutput) -> str:
+        """
+        Formats the structured context (entities, relations, documents) into a single string.
+        """
+        parts = []
+
+        if context.get('entities'):
+            parts.append("Relevant Entities:")
+            for entity in context['entities']:
+                # Assuming entity is a dict-like structure (e.g., GraphEntity TypedDict)
+                name = entity.get('name', entity.get('id', 'Unknown Entity'))
+                ent_type = entity.get('type', 'N/A')
+                desc = entity.get('description', 'No description')
+                parts.append(f"  - Entity: {name} (Type: {ent_type})\n    Description: {desc}")
+            parts.append("\n") # Add a newline for separation
+
+        if context.get('relations'):
+            parts.append("Key Relationships:")
+            for relation in context['relations']:
+                # Assuming relation is a dict-like structure (e.g., GraphRelation TypedDict)
+                source_id = relation.get('source_id', 'Unknown Source')
+                target_id = relation.get('target_id', 'Unknown Target')
+                rel_type = relation.get('type', 'related to')
+                desc = relation.get('description', 'No description')
+                # TODO: Future enhancement - resolve source/target IDs to names if feasible here
+                parts.append(f"  - Relation: {source_id} --[{rel_type}]--> {target_id}\n    Description: {desc}")
+            parts.append("\n")
+
+        if context.get('documents'):
+            parts.append("Supporting Content/Documents:")
+            for i, doc in enumerate(context['documents']):
+                doc_id = doc.get('id', f'doc_{i+1}')
+                content = doc.get('text', 'No content') # Simplified default for content
+                # Use the new retrieval_source_type field
+                source_type = doc.get('retrieval_source_type', 'Unknown Source Type') 
+                score_info = f" (Score: {doc['score']:.2f})" if isinstance(doc.get('score'), float) else ""
+                parts.append(f"  - Document ID: {doc_id} (Source Type: {source_type}){score_info}\n    Content: {content[:500]}...") # Display first 500 chars
+            parts.append("\n")
+        
+        return "\n".join(parts).strip()
+
+    async def process_query_and_generate(
+        self,
+        query_text: str,
+        strategy: str = "naive",
+        top_k: Optional[int] = None,
+        target_entity_types: Optional[List[str]] = None,
+        max_reasoning_depth: Optional[int] = None,
+        max_neighbor_depth: Optional[int] = None,
+        vector_graph_ratio: Optional[float] = None,
+        top_k_reasoning_paths: Optional[int] = None, # New
+        top_k_vector_supplement: Optional[int] = None  # New
+    ) -> str:
+        """
+        Retrieves context and generates a response based on the specified strategy.
+        """
+        logger.info(f"Processing query: '{query_text[:50]}...' with strategy '{strategy}'")
+        
+        structured_context_output = await self.retrieve_context(
+            query_text=query_text,
+            strategy=strategy,
+            top_k=top_k,
+            target_entity_types=target_entity_types,
+            max_reasoning_depth=max_reasoning_depth,
+            max_neighbor_depth=max_neighbor_depth,
+            vector_graph_ratio=vector_graph_ratio,
+            top_k_reasoning_paths=top_k_reasoning_paths,         # Pass through
+            top_k_vector_supplement=top_k_vector_supplement  # Pass through
+        )
+        
+        response_text = await self.generate_response(
+            query_text=query_text,
+            structured_context=structured_context_output # Changed from context_docs
+        )
+        
+        return response_text
+
diff --git a/simtag/core/sync.py b/simtag/core/sync.py
new file mode 100755
index 0000000..d22447f
--- /dev/null
+++ b/simtag/core/sync.py
@@ -0,0 +1,473 @@
+"""
+Sync Orchestrator Module
+Handles the synchronization of full database snapshots.
+"""
+import logging
+import numpy as np
+import json
+from typing import List, Optional, Dict, Any
+import time
+
+# Forward declare types for type hinting if VectorStorage and LLMClient are complex imports
+# from .storage import VectorStorage # Assuming VectorStorage is in .storage
+# from ..services.llm_client import LLMClient # Assuming LLMClient is in ..services
+
+logger = logging.getLogger(__name__)
+
+class SyncOrchestrator:
+    def __init__(self, storage, llm_client, logger: logging.Logger):
+        """
+        Initializes the SyncOrchestrator.
+
+        Args:
+            storage: An instance of VectorStorage.
+            llm_client: An instance of LLMClient.
+            logger: A logging.Logger instance.
+        """
+        self.storage = storage
+        self.llm_client = llm_client
+        self.logger = logger
+        self.logger.info("SyncOrchestrator initialized.")
+
+    def sync_full_snapshot(self, db_snapshot: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Processes a full database snapshot containing tags and nodes.
+        This method was moved from TaggingEngine.
+        """
+        self.logger.info(f"SyncOrchestrator: Starting sync with full DB snapshot.")
+        if not isinstance(db_snapshot, dict):
+            self.logger.error(f"SyncOrchestrator: db_snapshot is not a dictionary (type: {type(db_snapshot)}). Aborting sync.")
+            return {"status": "error", "message": "Invalid snapshot format (not a dict)", "processed_tags": 0, "processed_nodes": 0}
+
+        raw_tags = db_snapshot.get('tags', [])
+        raw_nodes = db_snapshot.get('nodes', [])
+        self.logger.info(f"SyncOrchestrator: Snapshot contains {len(raw_tags)} tags and {len(raw_nodes)} nodes.")
+
+        # --- Add detailed logging for raw_tags itself --- 
+        self.logger.info(f"SyncOrchestrator: raw_tags before access: {raw_tags!r} (type: {type(raw_tags)})")
+        # --- End detailed logging ---
+
+        # --- Add detailed logging for the first raw tag and node --- 
+        if raw_tags:
+            example_tag_entry = next(iter(raw_tags.values())) if isinstance(raw_tags, dict) and raw_tags else raw_tags[0] if raw_tags else None
+            if example_tag_entry:
+                self.logger.info(f"SyncOrchestrator: Example raw tag entry from snapshot: {example_tag_entry!r} (type: {type(example_tag_entry)})")
+        if raw_nodes:
+            example_node_entry = next(iter(raw_nodes.values())) if isinstance(raw_nodes, dict) and raw_nodes else raw_nodes[0] if raw_nodes else None
+            if example_node_entry:
+                 self.logger.info(f"SyncOrchestrator: Example raw node entry from snapshot: {example_node_entry!r} (type: {type(example_node_entry)})")
+        # --- End detailed logging ---
+
+        processed_tags_for_storage = [] # For metadata
+        tag_embeddings_for_vss = [] # For VSS embeddings, if active
+        vss_active = self.storage.has_vector_ext
+
+        # --- Tag Processing --- 
+        self.logger.info("SyncOrchestrator: Starting tag processing.")
+        existing_tag_ids_in_db = set(self.storage.list_tag_ids())
+        self.logger.info(f"SyncOrchestrator: Found {len(existing_tag_ids_in_db)} tag IDs currently in database.")
+        incoming_tag_ids_from_snapshot = set()
+
+        # Process tags
+        raw_tags_list = [] 
+        if isinstance(raw_tags, dict):
+            for tag_id_key, props_list in raw_tags.items():
+                if isinstance(props_list, list) and len(props_list) >= 1:
+                    entry = [str(tag_id_key)] + props_list
+                    raw_tags_list.append(entry)
+                else:
+                    self.logger.warning(f"SyncOrchestrator: Skipping malformed tag entry (dict value not list or too short) for key {tag_id_key}: value {props_list!r}")
+        elif isinstance(raw_tags, list):
+            raw_tags_list = raw_tags 
+        else:
+            self.logger.warning(f"SyncOrchestrator: raw_tags is neither a dict nor a list (type: {type(raw_tags)}). Cannot process tags.")
+
+        for tag_entry_list in raw_tags_list:
+            if isinstance(tag_entry_list, list) and len(tag_entry_list) >= 2:
+                tag_id = str(tag_entry_list[0])
+                name = tag_entry_list[1]
+                created_at = tag_entry_list[2] if len(tag_entry_list) > 2 else None
+                modified_at = tag_entry_list[3] if len(tag_entry_list) > 3 else None
+                
+                incoming_tag_ids_from_snapshot.add(tag_id) # Collect incoming tag ID
+
+                # Prepare base metadata dictionary
+                tag_meta_dict = {
+                    'tag_id': tag_id,
+                    'name': name,
+                    'created_at': created_at,
+                    'modified_at': modified_at,
+                    # These are other potential fields for tag_metadata table
+                    'fields': None, 
+                    'tag_type': None,
+                    'relation_type_hint': None,
+                    'description': None,
+                    'icon': None,
+                    'color': None,
+                    'behaviors': None
+                }
+
+                # Generate embedding for the tag name
+                tag_vector_list = None
+                if name and self.llm_client:
+                    try:
+                        self.logger.debug(f"SyncOrchestrator: Generating embedding for tag: {name} (ID: {tag_id})")
+                        tag_vector_list = self.llm_client.get_embedding_sync(name)
+                        if not tag_vector_list:
+                            self.logger.warning(f"SyncOrchestrator: Embedding generation returned None/empty for tag: {name} (ID: {tag_id})")
+                        else:
+                            self.logger.debug(f"SyncOrchestrator: Successfully generated vector for tag: {name} (ID: {tag_id}). Dim: {len(tag_vector_list)}.")
+                    except Exception as e:
+                        self.logger.error(f"SyncOrchestrator: Failed to generate embedding for tag {name} (ID: {tag_id}): {e}")
+                elif not name:
+                     self.logger.warning(f"SyncOrchestrator: Tag name is empty for ID: {tag_id}. Skipping embedding generation.")
+
+                if vss_active:
+                    # For VSS, metadata goes to processed_tags_for_storage (without vector info explicitly)
+                    # and embedding goes to tag_embeddings_for_vss
+                    processed_tags_for_storage.append(tag_meta_dict)
+                    if tag_vector_list:
+                        tag_embeddings_for_vss.append({
+                            'tag_id_ref': tag_id,
+                            'embedding': tag_vector_list
+                        })
+                else:
+                    # For non-VSS, vector info is added to the metadata dict itself
+                    if tag_vector_list:
+                        tag_meta_dict['vector'] = tag_vector_list # Stored as list, storage layer will convert to blob
+                        tag_meta_dict['vector_dim'] = len(tag_vector_list)
+                    else:
+                        tag_meta_dict['vector'] = None
+                        tag_meta_dict['vector_dim'] = 0
+                    processed_tags_for_storage.append(tag_meta_dict)
+            else:
+                self.logger.warning(f"SyncOrchestrator: Skipping malformed tag entry (list item not list or too short): {tag_entry_list!r}")
+        
+        # Determine tags to delete (present in DB but not in current snapshot)
+        tag_ids_to_delete = list(existing_tag_ids_in_db - incoming_tag_ids_from_snapshot)
+        if tag_ids_to_delete:
+            self.logger.info(f"SyncOrchestrator: Found {len(tag_ids_to_delete)} tags to delete from DB (stale entries): {tag_ids_to_delete}")
+            try:
+                self.storage.delete_tags_by_ids(tag_ids_to_delete)
+                self.logger.info(f"SyncOrchestrator: Successfully deleted {len(tag_ids_to_delete)} stale tags.")
+            except Exception as e:
+                self.logger.error(f"SyncOrchestrator: Error during stale tag deletion: {e}")
+        else:
+            self.logger.info("SyncOrchestrator: No stale tags to delete from DB.")
+
+        # Upsert tag metadata
+        if processed_tags_for_storage:
+            self.logger.info(f"SyncOrchestrator: Upserting {len(processed_tags_for_storage)} tag metadata records.")
+            try:
+                self.storage.bulk_insert_tag_vectors(processed_tags_for_storage)
+                self.logger.info(f"SyncOrchestrator: Tag metadata upsert successful for {len(processed_tags_for_storage)} tags.")
+            except Exception as e:
+                self.logger.error(f"SyncOrchestrator: Error during tag metadata upsert: {e}")
+        
+        # Upsert tag embeddings to VSS table if VSS is active
+        if vss_active and tag_embeddings_for_vss:
+            self.logger.info(f"SyncOrchestrator: Upserting {len(tag_embeddings_for_vss)} tag embeddings to VSS table.")
+            try:
+                self.storage.bulk_upsert_tag_embeddings(tag_embeddings_for_vss)
+                self.logger.info(f"SyncOrchestrator: Tag VSS embeddings upsert successful for {len(tag_embeddings_for_vss)} tags.")
+            except Exception as e:
+                self.logger.error(f"SyncOrchestrator: Error during tag VSS embeddings upsert: {e}")
+
+        # --- Node Processing --- 
+        self.logger.info("SyncOrchestrator: Starting node processing.")
+        # Process nodes
+        # MODIFICATION: Get existing node IDs from DB and prepare to collect incoming node IDs
+        self.logger.info("SyncOrchestrator: Fetching existing node IDs from database.")
+        existing_node_ids_in_db = self.storage.get_all_node_ids()
+        self.logger.info(f"SyncOrchestrator: Found {len(existing_node_ids_in_db)} node IDs currently in database.")
+        incoming_node_ids_from_snapshot = set()
+        # END MODIFICATION
+
+        processed_nodes_for_storage = []
+        nodes_processed_count = 0 # Counts nodes attempted for processing
+        failed_node_ids_during_embedding = []
+
+        # NEW: Prepare separate list for VSS embeddings if sqlite-vec is active
+        node_embeddings_for_vss = []
+
+        # Determine if VSS is active via storage property
+        vss_active = self.storage.has_vector_ext
+        if vss_active:
+            self.logger.info("SyncOrchestrator: sqlite-vec is active. Node embeddings will be stored in VSS table.")
+        else:
+            self.logger.info("SyncOrchestrator: sqlite-vec is NOT active. Node embeddings (if any) will be stored in main node_vectors table.")
+
+        if isinstance(raw_nodes, dict):
+            for node_id_key, properties_list in raw_nodes.items():
+                actual_node_id = str(node_id_key)
+                if isinstance(properties_list, list) and len(properties_list) >= 4:
+                    title_val = properties_list[0]
+                    content_val = properties_list[1]
+                    tag_ids_list = properties_list[2] if isinstance(properties_list[2], list) else []
+                    file_path = properties_list[3]
+                    created_at = properties_list[4] if len(properties_list) > 4 else None
+                    modified_at = properties_list[5] if len(properties_list) > 5 else None
+                    # Attempt to get other potential fields from Elisp if provided, falling back to None
+                    pos_val = properties_list[6] if len(properties_list) > 6 else None
+                    olp_val = properties_list[7] if len(properties_list) > 7 else None
+                    level_val = properties_list[8] if len(properties_list) > 8 else None
+                    scheduled_val = properties_list[9] if len(properties_list) > 9 else None
+                    deadline_val = properties_list[10] if len(properties_list) > 10 else None
+                    todo_val = properties_list[11] if len(properties_list) > 11 else None
+                    priority_val = properties_list[12] if len(properties_list) > 12 else None
+                    props_json_val = properties_list[13] if len(properties_list) > 13 else None # Assuming this is already JSON string or dict
+                    raw_value_val = properties_list[14] if len(properties_list) > 14 else None
+                    hash_val = properties_list[15] if len(properties_list) > 15 else None
+                    content_hash_val = properties_list[16] if len(properties_list) > 16 else None
+
+                    log_title = title_val if title_val is not None else "[No Title]"
+                    self.logger.info(f"SyncOrchestrator: Processing node ID: {actual_node_id}, Title: {log_title[:50]}...")
+
+                    # MODIFIED: Combine title and body for embedding
+                    title_for_embedding = title_val if title_val else ""
+                    body_for_embedding = content_val if content_val else ""
+
+                    if title_for_embedding and body_for_embedding:
+                        text_to_embed = title_for_embedding + "\n" + body_for_embedding
+                    elif title_for_embedding:
+                        text_to_embed = title_for_embedding
+                    elif body_for_embedding:
+                        text_to_embed = body_for_embedding
+                    else:
+                        text_to_embed = ""
+                    
+                    content_for_embedding = text_to_embed.strip()
+                    # END MODIFICATION
+                    
+                    node_metadata_item = {
+                        "node_id": actual_node_id, # Renamed from 'id' to match schema
+                        "title": title_val,
+                        "content": content_for_embedding, # Storing full content in metadata table
+                        "tags": json.dumps(tag_ids_list), # Renamed from 'tags_json'
+                        "file_path": file_path,
+                        "created_at": created_at,
+                        "modified_at": modified_at,
+                        "pos": pos_val,
+                        "olp": olp_val,
+                        "level": level_val,
+                        "scheduled": scheduled_val,
+                        "deadline": deadline_val,
+                        "todo": todo_val,
+                        "priority": priority_val,
+                        "properties": props_json_val if isinstance(props_json_val, str) else json.dumps(props_json_val),
+                        "raw_value": raw_value_val,
+                        "hash": hash_val,
+                        "content_hash": content_hash_val
+                    }
+
+                    # MODIFICATION: Collect incoming node ID
+                    incoming_node_ids_from_snapshot.add(actual_node_id)
+                    # END MODIFICATION
+
+                    node_vector_list = None
+                    if content_for_embedding and self.llm_client:
+                        try:
+                            self.logger.debug(f"SyncOrchestrator: Generating embedding for node {actual_node_id} (content length: {len(content_for_embedding)} chars)")
+                            node_vector_list = self.llm_client.get_embedding_sync(content_for_embedding)
+                            if not node_vector_list:
+                                self.logger.warning(f"SyncOrchestrator: Embedding generation returned None/empty for node ID {actual_node_id}. Content snippet: '{content_for_embedding[:100]}...'")
+                                failed_node_ids_during_embedding.append(actual_node_id)
+                            else:
+                                self.logger.debug(f"SyncOrchestrator: Successfully generated vector for node ID {actual_node_id}. Dim: {len(node_vector_list)}.")
+                        except Exception as e:
+                            self.logger.error(f"SyncOrchestrator: Failed to generate embedding for node {actual_node_id}: {e}")
+                            failed_node_ids_during_embedding.append(actual_node_id)
+                    elif not content_for_embedding:
+                        self.logger.info(f"SyncOrchestrator: No content for embedding for node ID {actual_node_id}. Skipping embedding generation.")
+                        failed_node_ids_during_embedding.append(actual_node_id)
+                    
+                    if vss_active:
+                        if node_vector_list:
+                            node_embeddings_for_vss.append({
+                                "node_id_ref": actual_node_id,
+                                "embedding": node_vector_list
+                            })
+                        # Metadata item already prepared, vector/vector_dim are not added to it when VSS is active
+                    else: # Fallback: VSS not active, add vector blob and dim to metadata item
+                        if node_vector_list:
+                            node_metadata_item['vector'] = np.array(node_vector_list, dtype=np.float32).tobytes()
+                            node_metadata_item['vector_dim'] = len(node_vector_list)
+                        else:
+                            node_metadata_item['vector'] = None
+                            node_metadata_item['vector_dim'] = 0
+
+                    processed_nodes_for_storage.append(node_metadata_item)
+                    nodes_processed_count += 1
+                else:
+                    self.logger.warning(f"SyncOrchestrator: Skipping malformed node entry (dict value not list or too short) for key {node_id_key}: value {properties_list!r}")
+        elif isinstance(raw_nodes, list):
+            for node_entry_list in raw_nodes:
+                if isinstance(node_entry_list, list) and len(node_entry_list) >= 5: # Base check, more specific indexing below
+                    actual_node_id = str(node_entry_list[0])
+                    title_val = node_entry_list[1]
+                    content_val = node_entry_list[2]
+                    tag_ids_list = node_entry_list[3] if isinstance(node_entry_list[3], list) else []
+                    file_path = node_entry_list[4]
+                    created_at = node_entry_list[5] if len(node_entry_list) > 5 else None
+                    modified_at = node_entry_list[6] if len(node_entry_list) > 6 else None
+                    pos_val = node_entry_list[7] if len(node_entry_list) > 7 else None
+                    olp_val = node_entry_list[8] if len(node_entry_list) > 8 else None
+                    level_val = node_entry_list[9] if len(node_entry_list) > 9 else None
+                    scheduled_val = node_entry_list[10] if len(node_entry_list) > 10 else None
+                    deadline_val = node_entry_list[11] if len(node_entry_list) > 11 else None
+                    todo_val = node_entry_list[12] if len(node_entry_list) > 12 else None
+                    priority_val = node_entry_list[13] if len(node_entry_list) > 13 else None
+                    props_json_val = node_entry_list[14] if len(node_entry_list) > 14 else None
+                    raw_value_val = node_entry_list[15] if len(node_entry_list) > 15 else None
+                    hash_val = node_entry_list[16] if len(node_entry_list) > 16 else None
+                    content_hash_val = node_entry_list[17] if len(node_entry_list) > 17 else None
+
+                    log_title = title_val if title_val is not None else "[No Title]"
+                    self.logger.info(f"SyncOrchestrator: Processing node ID: {actual_node_id}, Title: {log_title[:50]}...")
+
+                    # MODIFIED: Combine title and body for embedding
+                    title_for_embedding = title_val if title_val else ""
+                    body_for_embedding = content_val if content_val else ""
+
+                    if title_for_embedding and body_for_embedding:
+                        text_to_embed = title_for_embedding + "\n" + body_for_embedding
+                    elif title_for_embedding:
+                        text_to_embed = title_for_embedding
+                    elif body_for_embedding:
+                        text_to_embed = body_for_embedding
+                    else:
+                        text_to_embed = ""
+                        
+                    content_for_embedding = text_to_embed.strip()
+                    # END MODIFICATION
+
+                    node_metadata_item = {
+                        "node_id": actual_node_id,
+                        "title": title_val,
+                        "content": content_for_embedding,
+                        "tags": json.dumps(tag_ids_list),
+                        "file_path": file_path,
+                        "created_at": created_at,
+                        "modified_at": modified_at,
+                        "pos": pos_val,
+                        "olp": olp_val,
+                        "level": level_val,
+                        "scheduled": scheduled_val,
+                        "deadline": deadline_val,
+                        "todo": todo_val,
+                        "priority": priority_val,
+                        "properties": props_json_val if isinstance(props_json_val, str) else json.dumps(props_json_val),
+                        "raw_value": raw_value_val,
+                        "hash": hash_val,
+                        "content_hash": content_hash_val
+                    }
+
+                    # MODIFICATION: Collect incoming node ID
+                    incoming_node_ids_from_snapshot.add(actual_node_id)
+                    # END MODIFICATION
+
+                    node_vector_list = None
+                    if content_for_embedding and self.llm_client:
+                        try:
+                            self.logger.debug(f"SyncOrchestrator: Generating embedding for node {actual_node_id} (content length: {len(content_for_embedding)} chars)")
+                            node_vector_list = self.llm_client.get_embedding_sync(content_for_embedding)
+                            if not node_vector_list:
+                                self.logger.warning(f"SyncOrchestrator: Embedding generation returned None/empty for node ID {actual_node_id}. Content snippet: '{content_for_embedding[:100]}...'")
+                                failed_node_ids_during_embedding.append(actual_node_id)
+                            else:
+                                self.logger.debug(f"SyncOrchestrator: Successfully generated vector for node ID {actual_node_id}. Dim: {len(node_vector_list)}.")
+                        except Exception as e:
+                            self.logger.error(f"SyncOrchestrator: Failed to generate embedding for node {actual_node_id}: {e}")
+                            failed_node_ids_during_embedding.append(actual_node_id)
+                    elif not content_for_embedding:
+                        self.logger.info(f"SyncOrchestrator: No content for embedding for node ID {actual_node_id}. Skipping embedding generation.")
+                        failed_node_ids_during_embedding.append(actual_node_id)
+
+                    if vss_active:
+                        if node_vector_list:
+                            node_embeddings_for_vss.append({
+                                "node_id_ref": actual_node_id,
+                                "embedding": node_vector_list
+                            })
+                    else: # Fallback: VSS not active
+                        if node_vector_list:
+                            node_metadata_item['vector'] = np.array(node_vector_list, dtype=np.float32).tobytes()
+                            node_metadata_item['vector_dim'] = len(node_vector_list)
+                        else:
+                            node_metadata_item['vector'] = None
+                            node_metadata_item['vector_dim'] = 0
+                            
+                    processed_nodes_for_storage.append(node_metadata_item)
+                    nodes_processed_count += 1
+                else:
+                    self.logger.warning(f"SyncOrchestrator: Skipping malformed node entry (list item not list or too short): {node_entry_list!r}")
+        else:
+            self.logger.warning(f"SyncOrchestrator: raw_nodes is neither a dict nor a list (type: {type(raw_nodes)}). Cannot process nodes.")
+
+        if processed_nodes_for_storage:
+            self.logger.info(f"SyncOrchestrator: Upserting {len(processed_nodes_for_storage)} node metadata items into storage.")
+            try:
+                self.storage.bulk_upsert_nodes(processed_nodes_for_storage) # This will now handle VSS active/inactive for metadata
+                self.logger.info(f"SyncOrchestrator: Node metadata upsert successful for {len(processed_nodes_for_storage)} items.")
+            except Exception as e:
+                self.logger.error(f"SyncOrchestrator: Error during node metadata upsert: {e}")
+
+        # NEW: Upsert embeddings to VSS table if active and available
+        if vss_active and node_embeddings_for_vss:
+            self.logger.info(f"SyncOrchestrator: Preparing to upsert {len(node_embeddings_for_vss)} node embeddings into VSS table (contents of node_embeddings_for_vss count).")
+            try:
+                self.storage.bulk_upsert_node_embeddings(node_embeddings_for_vss)
+                self.logger.info(f"SyncOrchestrator: Node VSS embeddings upsert successful for {len(node_embeddings_for_vss)} items.")
+            except Exception as e:
+                self.logger.error(f"SyncOrchestrator: Error during node VSS embeddings upsert: {e}")
+        elif vss_active and not node_embeddings_for_vss:
+            self.logger.info("SyncOrchestrator: VSS is active, but no node embeddings were generated or prepared for VSS storage.")
+
+        # MODIFICATION: Calculate and delete orphaned nodes
+        nodes_deleted_count = 0
+        if existing_node_ids_in_db is not None: # Ensure the fetch was successful
+            node_ids_to_delete = list(existing_node_ids_in_db - incoming_node_ids_from_snapshot)
+            if node_ids_to_delete:
+                self.logger.info(f"SyncOrchestrator: Found {len(node_ids_to_delete)} orphaned node IDs to delete.")
+                try:
+                    self.storage.delete_nodes_by_ids(node_ids_to_delete)
+                    nodes_deleted_count = len(node_ids_to_delete) # Assume all requested deletions were successful if no error
+                    self.logger.info(f"SyncOrchestrator: Successfully deleted {nodes_deleted_count} orphaned nodes.")
+                except Exception as e:
+                    self.logger.error(f"SyncOrchestrator: Error during orphaned node deletion: {e}")
+            else:
+                self.logger.info("SyncOrchestrator: No orphaned nodes to delete.")
+        else:
+            self.logger.warning("SyncOrchestrator: Could not determine orphaned nodes as existing_node_ids_in_db was None (fetch might have failed).")
+        # END MODIFICATION
+
+        successfully_embedded_nodes_count = nodes_processed_count - len(failed_node_ids_during_embedding)
+        
+        # MODIFIED: Include the list of failed node IDs and deleted count in the log and return dict
+        log_message = (
+            f"SyncOrchestrator: Snapshot sync finished. "
+            f"Tags for storage: {len(processed_tags_for_storage)}, "
+            f"Nodes for storage: {nodes_processed_count}, "
+            f"Successfully embedded nodes: {successfully_embedded_nodes_count}, "
+            f"Failed embeddings: {len(failed_node_ids_during_embedding)}, "
+            f"Nodes deleted: {nodes_deleted_count}" # Added deleted count
+        )
+        if failed_node_ids_during_embedding:
+            # To avoid excessively long log messages if many IDs fail, log first few and total count
+            max_ids_to_log = 10 
+            failed_ids_snippet = failed_node_ids_during_embedding[:max_ids_to_log]
+            if len(failed_node_ids_during_embedding) > max_ids_to_log:
+                log_message += f". First {max_ids_to_log} Failed node IDs: {failed_ids_snippet}... (and {len(failed_node_ids_during_embedding) - max_ids_to_log} more)"
+            else:
+                log_message += f". Failed node IDs: {failed_ids_snippet}"
+        self.logger.info(log_message)
+        
+        return {
+            "status": "success", 
+            "message": "Snapshot processed.",
+            "processed_tags": len(processed_tags_for_storage), 
+            "processed_nodes": successfully_embedded_nodes_count, 
+            "failed_embedding_count": len(failed_node_ids_during_embedding),
+            "deleted_nodes_count": nodes_deleted_count, # Added deleted count
+            "failed_node_ids": failed_node_ids_during_embedding 
+        } 
\ No newline at end of file
diff --git a/simtag/core/tagging.py b/simtag/core/tagging.py
new file mode 100755
index 0000000..3cc329a
--- /dev/null
+++ b/simtag/core/tagging.py
@@ -0,0 +1,149 @@
+"""
+Tag Processing Core Engine
+Integrates entity extraction, tag generation, and relationship analysis functionality
+"""
+import logging
+from .graph_service import GraphService
+from .sync import SyncOrchestrator
+import time
+import traceback
+import sys
+import numpy as np
+from typing import List, Optional, TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from ..services.llm_client import LLMClient
+
+class TaggingEngine:
+    def __init__(self, config, graph_service: GraphService, llm_client):
+        self.config = config
+        self.graph_service = graph_service
+        self.llm_client = llm_client
+        self.logger = logging.getLogger("simtag_bridge.tagging_engine")
+        self.logger.info("TaggingEngine initialized.")
+
+        # Initialize SyncOrchestrator for node vectorization
+        # self.sync_orchestrator = SyncOrchestrator(storage=self.storage,
+        #                                         llm_client=self.llm_client,
+        #                                         logger=self.logger)
+        self.logger.warning("TaggingEngine: SyncOrchestrator is temporarily disabled during refactoring.")
+
+    def _generate_vector_for_text(self, text: str) -> Optional[np.ndarray]:
+        """Generates an embedding vector for a given text string.
+
+        Args:
+            text: The text to embed.
+
+        Returns:
+            A NumPy array of the embedding, or None if generation fails.
+        """
+        if not text or not text.strip():
+            self.logger.warning("_generate_vector_for_text: Input text is empty or whitespace. Skipping embedding.")
+            return None
+        try:
+            if self.llm_client:
+                self.logger.debug(f"Using LLMClient to generate semantic vector for text: '{text[:100]}...'")
+                # LLMClient might not have get_embedding_sync, let's assume it should be async
+                import asyncio
+                # This is a workaround for calling async from sync code
+                try:
+                    loop = asyncio.get_running_loop()
+                except RuntimeError:
+                    loop = asyncio.new_event_loop()
+                    asyncio.set_event_loop(loop)
+                
+                vector_list = loop.run_until_complete(self.llm_client.get_embedding(text))
+                
+                if vector_list:
+                    return np.array(vector_list, dtype=np.float32)
+                else:
+                    self.logger.warning(f"_generate_vector_for_text: LLMClient returned None or empty vector for text: '{text[:100]}...'")
+                    return None
+            else:
+                self.logger.error("LLMClient unavailable, cannot generate vector for text.")
+                return None
+        except Exception as e:
+            self.logger.error(f"Failed to generate text vector for '{text[:100]}...': {str(e)}\n{traceback.format_exc()}")
+            return None
+
+    def find_similar_nodes(self, query_input: str, top_k: int = 10) -> list:
+        """Finds similar nodes based on a query input (either a node_id or text).
+
+        Args:
+            query_input: A node_id (str) or a text string (str) to find similar nodes for.
+            top_k: The number of similar nodes to return.
+
+        Returns:
+            A list of tuples, where each tuple is (node_id, distance_score).
+            Returns an empty list if no query vector can be obtained or if an error occurs.
+        """
+        engine_start_time = time.time()
+        self.logger.info(f"[TaggingEngine.find_similar_nodes] Entered for query_input: '{query_input[:100]}...', top_k: {top_k}")
+
+        query_vector: Optional[np.ndarray] = None
+
+        # Attempt to retrieve vector if query_input might be a node_id
+        if self.graph_service.has_vector_ext:
+            self.logger.debug(f"[TaggingEngine.find_similar_nodes] Checking if '{query_input}' is a node_id with a stored embedding.")
+            potential_vector = self.graph_service.get_node_embedding_by_id(query_input)
+            if potential_vector is not None:
+                query_vector = potential_vector
+                self.logger.info(f"[TaggingEngine.find_similar_nodes] Using stored embedding for node_id: '{query_input}'.")
+
+        # If no stored vector was found, treat query_input as text and generate embedding
+        if query_vector is None:
+            self.logger.info(f"[TaggingEngine.find_similar_nodes] No stored vector found for '{query_input}'. Treating as text and generating new embedding.")
+            query_vector = self._generate_vector_for_text(query_input)
+
+        # If we have a vector, query the storage
+        if query_vector is not None and query_vector.size > 0:
+            self.logger.debug(f"[TaggingEngine.find_similar_nodes] Querying storage for similar nodes.")
+            storage_query_start_time = time.time()
+            similar_nodes = self.graph_service.find_similar_nodes(query_vector, top_k=top_k)
+            storage_query_end_time = time.time()
+            self.logger.info(f"[TaggingEngine.find_similar_nodes] Storage query took {storage_query_end_time - storage_query_start_time:.4f}s. Found {len(similar_nodes)} nodes.")
+            return similar_nodes
+        else:
+            self.logger.warning(f"[TaggingEngine.find_similar_nodes] Could not obtain a query vector for input: '{query_input[:100]}...'. Returning empty list.")
+            return []
+
+    def sync_full_snapshot(self, db_snapshot):
+        """
+        Processes a full database snapshot to create or update node embeddings.
+        This has been simplified to only handle node vectorization.
+        """
+        self.logger.warning("sync_full_snapshot is temporarily disabled during refactoring.")
+        return {"status": "disabled", "message": "This feature is disabled during refactoring."}
+        # self.logger.info("Starting simplified full snapshot sync for node embeddings.")
+        # start_time = time.time()
+        #
+        # nodes_data = db_snapshot.get('nodes', [])
+        # if not nodes_data:
+        #     self.logger.info("No nodes in the snapshot to process.")
+        #     return {"status": "success", "message": "No nodes to process."}
+        #
+        # # Use SyncOrchestrator to handle the node processing logic
+        # summary = self.sync_orchestrator.sync_nodes(nodes_data)
+        #
+        # end_time = time.time()
+        # self.logger.info(f"Full snapshot sync for nodes completed in {end_time - start_time:.2f} seconds. "
+        #                  f"Processed: {summary['processed']}, "
+        #                  f"Updated: {summary['updated']}, "
+        #                  f"Skipped: {summary['skipped']}, "
+        #                  f"Errors: {summary['errors']}.")
+        #
+        # return {
+        #     "status": "success",
+        #     "message": "Node embedding synchronization complete.",
+        #     "summary": summary
+        # }
+
+    def get_node_embedding(self, node_id: str) -> Optional[List[float]]:
+        """
+        Retrieves the embedding for a specific node ID.
+        """
+        self.logger.debug(f"Attempting to retrieve embedding for node_id: {node_id}")
+        vector = self.graph_service.get_node_embedding_by_id(node_id)
+        if vector is not None:
+            return vector.tolist()
+        return None
diff --git a/simtag/entity_extractor.py b/simtag/entity_extractor.py
new file mode 100644
index 0000000..d30762b
--- /dev/null
+++ b/simtag/entity_extractor.py
@@ -0,0 +1,105 @@
+"""
+SimTag 实体提取模块
+提供文本中实体的识别和分类功能
+"""
+
+import logging
+import json
+import traceback
+from typing import List, Dict, Any, Optional
+
+# 全局单例实例
+_extractor_instance = None
+
+def extract_entities(text: str) -> List[Dict[str, Any]]:
+    """从文本中提取实体的全局函数，供EPC服务器调用
+    
+    Args:
+        text: 要分析的文本
+        
+    Returns:
+        实体列表，每个实体包含:
+            - entity: 实体文本
+            - type: 实体类型
+            - start: 开始位置
+            - end: 结束位置
+    """
+    global _extractor_instance
+    
+    if _extractor_instance is None:
+        _extractor_instance = EntityExtractor()
+        
+    return _extractor_instance.extract(text)
+
+class EntityExtractor:
+    """实体提取器类"""
+    
+    def __init__(self, ollama_bridge: Any = None):
+        """初始化实体提取器
+        
+        Args:
+            ollama_bridge: Ollama桥接对象，用于实体提取
+        """
+        self.logger = logging.getLogger("simtag.entity_extractor")
+        self.ollama = ollama_bridge
+        
+    def extract(self, text: str) -> List[Dict[str, Any]]:
+        """从文本中提取实体
+        
+        Args:
+            text: 待分析文本
+            
+        Returns:
+            实体列表，每个实体包含:
+                - entity: 实体文本
+                - type: 实体类型
+                - start: 开始位置
+                - end: 结束位置
+        """
+        if not self.ollama:
+            self.logger.error("未提供Ollama实例，无法提取实体")
+            return []
+            
+        system = """You are an expert in Named Entity Recognition (NER). Your task is to identify and classify named entities in the given text. Focus on these entity types:
+
+1. PERSON - Names of people
+2. ORG - Organizations, companies, institutions
+3. PRODUCT - Products, software, technologies
+4. CONCEPT - Technical concepts, methodologies
+5. TECH - Programming languages, frameworks, tools
+
+For each entity found:
+1. Extract the exact text as it appears
+2. Classify its type from the above categories
+3. Find the start and end position in the text
+
+Return your result as a valid JSON array of entity objects:
+[
+  {"entity": "entity_text", "type": "ENTITY_TYPE", "start": start_pos, "end": end_pos},
+  ...
+]
+
+The start and end positions should be character indices where the entity appears in the text."""
+
+        prompt = f"""Extract all named entities from this text:
+
+{text}
+
+Return ONLY a valid JSON array of entities with no comments or explanations."""
+
+        try:
+            response = self.ollama.run(prompt)
+            entities = json.loads(response)
+            
+            # 验证和清理实体
+            valid_entities = []
+            for entity in entities:
+                if isinstance(entity, dict) and 'entity' in entity and 'type' in entity:
+                    valid_entities.append(entity)
+                    
+            return valid_entities
+            
+        except Exception as e:
+            self.logger.error(f"提取实体过程出错: {e}")
+            self.logger.error(traceback.format_exc())
+            return []
diff --git a/simtag/epc_server.py b/simtag/epc_server.py
new file mode 100644
index 0000000..349e5e6
--- /dev/null
+++ b/simtag/epc_server.py
@@ -0,0 +1,729 @@
+"""
+SimTag EPC Server Module
+Provides a unified EPC interface to connect Emacs with Python backend functionality
+"""
+
+import os
+import sys
+import json
+import logging
+import traceback
+import argparse
+import subprocess
+from typing import List, Dict, Any, Optional, Tuple
+
+from epc.server import EPCServer
+from .config import Config
+from .entity_extractor import EntityExtractor
+from .ollama_bridge import OllamaBridge
+from .tag_vectors import TagVectorEngine
+from .utils.logging import setup_logging
+from .utils.serialization import normalize_response
+from .tag_generator import TagGenerator
+from .tag_relation_analyzer import TagRelationAnalyzer, analyze_tag_relations
+
+logger = logging.getLogger("simtag.epc_server")
+
+class SimTagServer:
+    """SimTag EPC Server Class"""
+    
+    def __init__(self, config: Config):
+        """Initialize server
+        
+        Args:
+            config: Configuration object
+        """
+        self.logger = logging.getLogger("simtag.epc_server")
+        self.config = config
+        self._initialized = False  # Add initialization flag
+        
+        # Initialize base components as None
+        self.ollama = None
+        self.tag_generator = None
+        self.entity_extractor = None
+        self.vector_engine = None
+        
+        # Initialize EPC server
+        self.server = EPCServer((self.config.host, self.config.port))
+        self._register_methods()
+        
+    def _register_methods(self):
+        """Register EPC methods"""
+        methods = [
+            ('echo', self.echo),
+            ('status', self.status),
+            ('initialize', self.initialize),
+            ('find_similar', self.find_similar),
+            ('suggest_tags', self.suggest_tags),
+            ('suggest_tags_json', self.suggest_tags_json),
+            ('extract_entities', self.extract_entities),
+            ('check_imports', self.check_imports),
+            ('get_config', self.get_config),
+            ('test_engine', self.test_engine),
+            ('analyze_tag_relations', self.analyze_tag_relations),
+            ('run_ollama', self.run_ollama),
+            ('sync_library', self.sync_library),
+            ('add_tag', self.add_tag),
+            ('update_tag', self.update_tag),
+            ('remove_tag', self.remove_tag)
+        ]
+        
+        for name, method in methods:
+            self.server.register_function(method)
+            
+    def start(self):
+        """Start server"""
+        try:
+            port = self.server.server_address[1]
+            self.logger.info(f"Server port obtained: {port}")
+            
+            # Ensure clean stdout
+            sys.stdout.flush()  # Clear buffer
+            
+            # Important: Output port number on a separate line
+            print(f"{port}", flush=True)
+            self.logger.info(f"Port number output to stdout: {port}")
+            
+            # Start server
+            self.logger.info("Starting serve_forever()...")
+            self.server.serve_forever()
+        except Exception as e:
+            self.logger.error(f"Server startup failed: {e}")
+            raise
+        
+    def echo(self, message: str) -> str:
+        """Echo test method"""
+        self.logger.info(f"Echo test: {message}")
+        return f"Echo: {message}"
+        
+    def status(self) -> Dict[str, Any]:
+        """Get server status
+        
+        Returns:
+            Status information dictionary
+        """
+        status = {
+            "server": {
+                "running": True,
+                "port": self.server.server_address[1]
+            },
+            "components": {
+                "vector_engine": self.vector_engine.status() if self.vector_engine else None,
+                "ollama": self.ollama.status() if self.ollama else None
+            },
+            "config": self.config.to_dict()
+        }
+        return normalize_response(status)
+
+    """def initialize initialization function do not break update"""   
+    def initialize(self, vector_file: str = None, db_file: str = None) -> Dict[str, Any]:
+        """Initialize server components"""
+        try:
+            self.logger.info("Initializing server components...")
+            
+            # Update and validate file paths
+            if vector_file:
+                self.logger.info(f"Using specified vector file: {vector_file}")
+                # For vector file, we don't require it to exist - it can be created during initialization
+                self.config.vector_file = vector_file
+                # Ensure the directory exists
+                os.makedirs(os.path.dirname(vector_file), exist_ok=True)
+            
+            if db_file:
+                self.logger.info(f"Using specified database file: {db_file}")
+                if not os.path.exists(db_file):
+                    self.logger.error(f"Specified database file does not exist: {db_file}")
+                    return normalize_response(None, "error", f"Database file does not exist: {db_file}")
+                self.config.db_file = db_file
+            
+            # Ensure Ollama is available
+            if not self.config.ensure_ollama():
+                raise Exception("Ollama is not installed or not available")
+            
+            # 1. Initialize Ollama
+            self.logger.info("Initializing Ollama...")
+            self.ollama = OllamaBridge(model=self.config.model_name)
+            
+            # 2. Initialize tag generator
+            self.logger.info("Initializing tag generator...")
+            self.tag_generator = TagGenerator(self.ollama)
+            
+            # 3. Initialize other components
+            self.logger.info("Initializing other components...")
+            self.entity_extractor = EntityExtractor(self.ollama)
+            self.vector_engine = TagVectorEngine(vector_file=self.config.vector_file)
+            
+            # 4. Initialize the vector library with tag data
+            self.logger.info("Initializing vector library...")
+            vector_init_result = self.vector_engine.initialize(
+                self.config.db_file, 
+                self.config.vector_file
+            )
+            
+            if vector_init_result.get("status") != "success":
+                error_msg = f"Vector library initialization failed: {vector_init_result.get('message', 'Unknown error')}"
+                self.logger.error(error_msg)
+                return normalize_response(None, "error", error_msg)
+            
+            # Mark initialization complete
+            self._initialized = True
+            self.logger.info("All components initialized successfully")
+            
+            return normalize_response({
+                "status": "success",
+                "vector_file": self.config.vector_file,
+                "db_file": self.config.db_file,
+                "model": self.config.model_name,
+                "vector_stats": vector_init_result.get("result", {})
+            })
+            
+        except Exception as e:
+            self._initialized = False  # Ensure marked as uninitialized on failure
+            self.logger.error(f"Initialization failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+            
+    def find_similar(self, tag_name: str, content: str = "", top_k: int = 5) -> Dict[str, Any]:
+        """Find similar tags
+        
+        Args:
+            tag_name: Tag name
+            content: Related content
+            top_k: Number of results to return
+            
+        Returns:
+            List of similar tags
+        """
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+            
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+            
+            # Use hybrid search
+            self.logger.info(f"Finding tags similar to '{tag_name}'...")
+            results = self.vector_engine.find_similar(tag_name, top_k)
+            
+            return normalize_response(results)
+            
+        except Exception as e:
+            error_msg = f"Failed to find similar tags: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def suggest_tags(self, text: str, limit: int = 5) -> Dict[str, Any]:
+        """Generate tag suggestions"""
+        try:
+            # If not initialized, try auto-initialization
+            if not self._initialized:
+                self.logger.info("Service not initialized, attempting auto-initialization...")
+                init_result = self.initialize()
+                if init_result.get("status") != "success":
+                    self.logger.error("Auto-initialization failed")
+                    return normalize_response(None, "error", "Service initialization failed")
+            
+            if not self.tag_generator:
+                self.logger.error("Tag generator not initialized")
+                return normalize_response(None, "error", "Tag generator not initialized")
+            
+            # Get tag list
+            self.logger.info("Starting tag generation...")
+            self.logger.debug(f"Input text preview: {text[:100]}...")  # Add input text logging
+            
+            tags = self.tag_generator.suggest_tags(text)
+            
+            # Validate tag list
+            if not tags:
+                self.logger.warning("No tags generated")
+                return normalize_response([])  # Return empty list instead of None
+            
+            if not isinstance(tags, list):
+                self.logger.error(f"Tag generator returned non-list type: {type(tags)}")
+                return normalize_response(None, "error", "Invalid tag format")
+            
+            # Ensure all tags are strings
+            valid_tags = [str(tag).strip() for tag in tags if tag]
+            
+            self.logger.info(f"Successfully generated {len(valid_tags)} tags: {valid_tags}")
+            
+            # Return using normalize_response
+            return normalize_response(valid_tags)
+            
+        except Exception as e:
+            self.logger.error(f"Tag generation failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+            
+    def extract_entities(self, text: str) -> Dict[str, Any]:
+        """Extract entities (full version)
+        
+        Args:
+            text: Text content
+            
+        Returns:
+            List of entities
+        """
+        try:
+            if not self.entity_extractor:
+                raise Exception("Entity extractor not initialized")
+                
+            entities = self.entity_extractor.extract(text)
+            return normalize_response(entities)
+            
+        except Exception as e:
+            error_msg = f"Failed to extract entities: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+            
+
+    def check_imports(self):
+        """Check if required modules are properly imported."""
+        try:
+            import numpy
+            import torch
+            import sentence_transformers
+            import requests
+            return {
+                "status": "success",
+                "imports": {
+                    "numpy": numpy.__version__,
+                    "torch": torch.__version__,
+                    "sentence_transformers": sentence_transformers.__version__,
+                    "requests": requests.__version__
+                }
+            }
+        except ImportError as e:
+            return {
+                "status": "error",
+                "message": str(e)
+            }
+
+    def get_config(self):
+        """Return current configuration information."""
+        return {
+            "vector_file": self.config.vector_file,
+            "db_file": self.config.db_file,
+            "model_name": self.config.model_name,
+            "debug": self.config.debug
+        }
+
+    def test_engine(self, test_text: str) -> Dict[str, Any]:
+        """Test text vector engine functionality
+        
+        Args:
+            test_text: Test text
+            
+        Returns:
+            Vector data
+        """
+        try:
+            if not self.vector_engine:
+                raise Exception("Vector engine not initialized")
+                
+            # Generate text vector
+            self.logger.info(f"Starting text vector generation: {test_text}")
+            vector = self.vector_engine.model.encode(test_text)
+            
+            # Record vector details
+            self.logger.info(f"Vector type: {type(vector)}")
+            self.logger.info(f"Vector shape: {vector.shape if hasattr(vector, 'shape') else len(vector)}")
+            
+            # Get vector data
+            vector_data = vector.tolist() if hasattr(vector, 'tolist') else vector
+            self.logger.info(f"Vector data length: {len(vector_data)}")
+            
+            # Return result
+            result = {
+                "vector": vector_data,
+                "dimensions": len(vector_data),
+                "model": self.vector_engine.model_name if hasattr(self.vector_engine, 'model_name') else None
+            }
+            
+            return normalize_response(result)
+            
+        except Exception as e:
+            error_msg = f"Engine test failed: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def analyze_tag_relations(self, tag: str, tags: list) -> Dict[str, Any]:
+        """Analyze tag relationships
+        
+        Args:
+            tag: Target tag
+            tags: List of tags to analyze
+            
+        Returns:
+            List of tag relationships
+        """
+        try:
+            relations = self.tag_analyzer.analyze_relations(tag, tags)
+            return {
+                "status": "success",
+                "result": relations
+            }
+        except Exception as e:
+            return {
+                "status": "error",
+                "message": f"Failed to analyze tag relationships: {str(e)}"
+            }
+
+    def run_ollama(self, prompt, system=None):
+        """Send message to Ollama and get response
+        
+        Args:
+            prompt: User prompt text
+            system: Optional system prompt text
+            
+        Returns:
+            Dict: Dictionary containing processing results
+        """
+        self.logger.info(f"Received Ollama interaction request, prompt length: {len(prompt)}")
+        
+        # Check initialization status
+        if not self._initialized:
+            self.logger.error("Attempting to use Ollama before initialization")
+            return normalize_response(None, "error", "Service not initialized, please call initialize first")
+        
+        # Check Ollama instance
+        if not self.ollama:
+            self.logger.error("Ollama instance not initialized")
+            return normalize_response(None, "error", "Ollama instance not initialized")
+        
+        try:
+            # Record additional information, avoiding logging long prompts
+            prompt_preview = prompt[:100] + "..." if len(prompt) > 100 else prompt
+            self.logger.info(f"Sending request to Ollama, prompt preview: {prompt_preview}")
+            
+            # Call Ollama
+            response = self.ollama.run(prompt, system=system)
+            
+            # Check response
+            if not response:
+                return normalize_response(None, "error", "Ollama returned empty response")
+            
+            # Record response (partial to avoid large logs)
+            response_preview = response[:100] + "..." if len(response) > 100 else response
+            self.logger.info(f"Received Ollama response, length: {len(response)}, preview: {response_preview}")
+            
+            # Return success response
+            return normalize_response(response, "success")
+            
+        except Exception as e:
+            # Catch and log exception
+            error_message = f"Ollama interaction error: {str(e)}"
+            trace = traceback.format_exc()
+            self.logger.error(f"{error_message}\n{trace}")
+            return normalize_response(None, "error", error_message)
+            
+    def suggest_tags_json(self, json_data: str, limit: int = 5) -> Dict[str, Any]:
+        """Process tag generation request using JSON format
+        
+        Args:
+            json_data: JSON format request data containing text content to analyze
+            limit: Result count limit
+            
+        Returns:
+            List of tags
+        """
+        try:
+            # Record received JSON data length
+            self.logger.info(f"Received JSON format request, length: {len(json_data)}")
+            
+            # Parse JSON data
+            try:
+                import json
+                request = json.loads(json_data)
+                
+                # Ensure JSON format is correct, contains content field
+                if not isinstance(request, dict):
+                    self.logger.error(f"JSON data is not dictionary format: {type(request)}")
+                    return normalize_response(None, "error", "Invalid request format, should be JSON object")
+                
+                text = request.get("content")
+                
+                if not text:
+                    self.logger.error("Request missing content field or empty")
+                    return normalize_response(None, "error", "Request missing text content")
+                
+                self.logger.info(f"Text length extracted from JSON: {len(text)}")
+                text_preview = text[:100] + "..." if len(text) > 100 else text
+                self.logger.info(f"Text preview: {text_preview}")
+                
+            except json.JSONDecodeError as e:
+                self.logger.error(f"JSON parsing failed: {e}")
+                self.logger.error(f"Received JSON data: {json_data[:200]}..." if len(json_data) > 200 else json_data)
+                return normalize_response(None, "error", f"JSON parsing failed: {e}")
+            
+            # Process text using standard suggest_tags method
+            return self.suggest_tags(text, limit)
+            
+        except Exception as e:
+            self.logger.error(f"JSON request processing failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", str(e))
+
+    def sync_library(self, db_file: str, tag_data: List[Dict[str, str]]) -> Dict[str, Any]:
+        """Synchronize the vector library with the provided tag data.
+
+        Args:
+            db_file: The path to the source database file (for logging/reference).
+            tag_data: A list of dictionaries, each containing 'id' and 'name' of a tag.
+
+        Returns:
+            A dictionary indicating the status of the operation.
+        """
+        self.logger.info(f"Received sync_library request. DB file: {db_file}, {len(tag_data)} tags received.")
+        
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+
+            # Perform the synchronization
+            # Assuming vector_engine has a method like sync_from_tags
+            # You might need to adjust this method name based on your TagVectorEngine implementation
+            result = self.vector_engine.sync_from_tags(tag_data)
+            
+            self.logger.info(f"Library synchronization completed. Result: {result}")
+            return normalize_response({"status": "success", "details": result})
+
+        except Exception as e:
+            error_msg = f"Failed to synchronize library: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def update_tag(self, tag_data: List[Dict[str, str]]) -> Dict[str, Any]:
+        """Update the name of a tag
+        
+        Args:
+            tag_data: List containing tag information, format: [{"id": tag_id, "name": tag_name}]
+            
+        Returns:
+            Dictionary indicating the status of the operation
+        """
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+            
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+            
+            # Extract tag information
+            if not tag_data or not isinstance(tag_data, list) or len(tag_data) == 0:
+                self.logger.error("Invalid tag data format")
+                return normalize_response(None, "error", "Invalid tag data format")
+            
+            # Get the first tag data item
+            tag_info = tag_data[0]
+            if not isinstance(tag_info, dict):
+                # Handle case where it's passed as a list of tuples
+                if isinstance(tag_info, list):
+                    tag_dict = {}
+                    for item in tag_info:
+                        if isinstance(item, tuple) and len(item) == 2:
+                            key, value = item
+                            tag_dict[key] = value
+                    tag_info = tag_dict
+                else:
+                    self.logger.error(f"Invalid tag info format: {type(tag_info)}")
+                    return normalize_response(None, "error", f"Invalid tag info format: {type(tag_info)}")
+            
+            tag_id = tag_info.get("id")
+            tag_name = tag_info.get("name")
+            
+            if not tag_id:
+                self.logger.error("Missing tag ID")
+                return normalize_response(None, "error", "Missing tag ID")
+            
+            self.logger.info(f"Updating tag: {tag_id} -> '{tag_name}'")
+            
+            # Perform the update
+            result = self.vector_engine.update_tag(tag_id, tag_name)
+            
+            if result:
+                return normalize_response({"status": "success", "tag_id": tag_id, "updated": True})
+            else:
+                return normalize_response(None, "error", "Tag update failed")
+            
+        except Exception as e:
+            error_msg = f"Failed to update tag: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def remove_tag(self, tag_id: str) -> Dict[str, Any]:
+        """Remove a tag from the vector library
+        
+        Args:
+            tag_id: ID of the tag to remove
+            
+        Returns:
+            Dictionary indicating the status of the operation
+        """
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+            
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+            
+            # Perform the removal
+            result = self.vector_engine.remove_tag(tag_id)
+            
+            if result:
+                return normalize_response({"status": "success", "details": result})
+            else:
+                return normalize_response(None, "error", "Tag removal failed")
+            
+        except Exception as e:
+            error_msg = f"Failed to remove tag: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+    def add_tag(self, tag_data: List[Dict[str, str]]) -> Dict[str, Any]:
+        """Add a new tag to the vector library
+        
+        Args:
+            tag_data: List containing tag information, format: [{"id": tag_id, "name": tag_name}]
+            
+        Returns:
+            Dictionary indicating the status of the operation
+        """
+        try:
+            # Check initialization status
+            if not self._initialized:
+                self.logger.error("Service not initialized, please call initialize first")
+                return normalize_response(None, "error", "Service not initialized, please call initialize first")
+            
+            # Check vector engine
+            if not self.vector_engine:
+                self.logger.error("Vector engine not initialized")
+                return normalize_response(None, "error", "Vector engine not initialized")
+            
+            # Extract tag information
+            if not tag_data or not isinstance(tag_data, list) or len(tag_data) == 0:
+                self.logger.error("Invalid tag data format")
+                return normalize_response(None, "error", "Invalid tag data format")
+            
+            # Get the first tag data item
+            tag_info = tag_data[0]
+            if not isinstance(tag_info, dict):
+                # Handle case where it's passed as a list of tuples
+                if isinstance(tag_info, list):
+                    tag_dict = {}
+                    for item in tag_info:
+                        if isinstance(item, tuple) and len(item) == 2:
+                            key, value = item
+                            tag_dict[key] = value
+                    tag_info = tag_dict
+                else:
+                    self.logger.error(f"Invalid tag info format: {type(tag_info)}")
+                    return normalize_response(None, "error", f"Invalid tag info format: {type(tag_info)}")
+            
+            tag_id = tag_info.get("id")
+            tag_name = tag_info.get("name")
+            
+            if not tag_id:
+                self.logger.error("Missing tag ID")
+                return normalize_response(None, "error", "Missing tag ID")
+            
+            self.logger.info(f"Adding tag: {tag_id} -> '{tag_name}'")
+            
+            # Perform the addition
+            result = self.vector_engine.add_tag(tag_id, tag_name)
+            
+            if result:
+                return normalize_response({"status": "success", "tag_id": tag_id, "added": True})
+            else:
+                return normalize_response(None, "error", "Tag addition failed")
+            
+        except Exception as e:
+            error_msg = f"Failed to add tag: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            return normalize_response(None, "error", error_msg)
+
+def run_ollama_model(text, model_name="gemma-3b-it"):
+    """Run ollama command directly"""
+    try:
+        cmd = ["ollama", "run", model_name, text]
+        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
+        return result.stdout.strip()
+    except subprocess.CalledProcessError as e:
+        logger.error(f"Failed to run ollama command: {e}")
+        return None
+
+def main(config: Config):
+    """Main function"""
+    try:
+        # Initialize logging
+        log_level = logging.DEBUG if config.debug else logging.INFO
+        setup_logging(config.log_file, log_level)
+        
+        # Record configuration information
+        logger.info("SimTag EPC Server Configuration:")
+        logger.info(f"Vector file: {config.vector_file}")
+        logger.info(f"Database file: {config.db_file}")
+        logger.info(f"Log file: {config.log_file}")
+        logger.info(f"Debug mode: {config.debug}")
+        
+        # Create server instance
+        server = SimTagServer(config)
+        
+        # Start server
+        server.start()
+        
+    except Exception as e:
+        logger.error(f"Server startup failed: {e}")
+        logger.error(traceback.format_exc())
+        sys.exit(1)
+
+if __name__ == "__main__":
+    # Parse command line arguments
+    parser = argparse.ArgumentParser(description='SimTag EPC Server')
+    parser.add_argument('--vector-file', help='Vector file path')
+    parser.add_argument('--db-file', help='Database file path')
+    parser.add_argument('--model', help='Model name')
+    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
+    parser.add_argument('--log-file', help='Log file path')
+    parser.add_argument('--host', default='127.0.0.1', help='Server address')
+    parser.add_argument('--port', type=int, default=0, help='Server port')
+    args = parser.parse_args()
+
+    # Create configuration object
+    config = Config(
+        vector_file=args.vector_file,
+        db_file=args.db_file,
+        model_name=args.model,
+        debug=args.debug,
+        log_file=args.log_file,
+        host=args.host,
+        port=args.port
+    )
+    
+    main(config) 
\ No newline at end of file
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/152b56c8ff5229192e0b1f405f5bf07699854738.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/152b56c8ff5229192e0b1f405f5bf07699854738.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/5fd10429389515d3e5cccdeda08cae5fea1ae82e.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/5fd10429389515d3e5cccdeda08cae5fea1ae82e.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/7410db66f06de178beeadfdd11b1fc241b04f683.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/7410db66f06de178beeadfdd11b1fc241b04f683.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/b974b349cb2d419ada11181750a733ff82f291ad.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/b974b349cb2d419ada11181750a733ff82f291ad.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/d1514c3162bbe87b343f565fadc62e6c06f04f03.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/d1514c3162bbe87b343f565fadc62e6c06f04f03.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/f7640f94e81bb7f4f04daf1668850b38763a13d9.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/f7640f94e81bb7f4f04daf1668850b38763a13d9.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock b/simtag/models/.locks/models--sentence-transformers--paraphrase-MiniLM-L6-v2/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/adapter_config.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/adapter_config.json
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/added_tokens.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/added_tokens.json
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/chat_template.jinja b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/.no_exist/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/chat_template.jinja
new file mode 100644
index 0000000..e69de29
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/152b56c8ff5229192e0b1f405f5bf07699854738 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/152b56c8ff5229192e0b1f405f5bf07699854738
new file mode 100644
index 0000000..152b56c
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/152b56c8ff5229192e0b1f405f5bf07699854738
@@ -0,0 +1,100 @@
+---
+license: apache-2.0
+library_name: sentence-transformers
+tags:
+- sentence-transformers
+- feature-extraction
+- sentence-similarity
+- transformers
+pipeline_tag: sentence-similarity
+---
+
+# sentence-transformers/paraphrase-MiniLM-L6-v2
+
+This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.
+
+
+
+## Usage (Sentence-Transformers)
+
+Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:
+
+```
+pip install -U sentence-transformers
+```
+
+Then you can use the model like this:
+
+```python
+from sentence_transformers import SentenceTransformer
+sentences = ["This is an example sentence", "Each sentence is converted"]
+
+model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
+embeddings = model.encode(sentences)
+print(embeddings)
+```
+
+
+
+## Usage (HuggingFace Transformers)
+Without [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
+
+```python
+from transformers import AutoTokenizer, AutoModel
+import torch
+
+
+#Mean Pooling - Take attention mask into account for correct averaging
+def mean_pooling(model_output, attention_mask):
+    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
+    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
+    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
+
+
+# Sentences we want sentence embeddings for
+sentences = ['This is an example sentence', 'Each sentence is converted']
+
+# Load model from HuggingFace Hub
+tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')
+model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')
+
+# Tokenize sentences
+encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')
+
+# Compute token embeddings
+with torch.no_grad():
+    model_output = model(**encoded_input)
+
+# Perform pooling. In this case, max pooling.
+sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
+
+print("Sentence embeddings:")
+print(sentence_embeddings)
+```
+
+
+
+## Full Model Architecture
+```
+SentenceTransformer(
+  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
+  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
+)
+```
+
+## Citing & Authors
+
+This model was trained by [sentence-transformers](https://www.sbert.net/). 
+        
+If you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):
+```bibtex 
+@inproceedings{reimers-2019-sentence-bert,
+    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
+    author = "Reimers, Nils and Gurevych, Iryna",
+    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
+    month = "11",
+    year = "2019",
+    publisher = "Association for Computational Linguistics",
+    url = "http://arxiv.org/abs/1908.10084",
+}
+```
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298
new file mode 100644
index 0000000..e127b50
Binary files /dev/null and b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298 differ
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac
new file mode 100644
index 0000000..40c4a0f
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac
@@ -0,0 +1 @@
+{"version":"1.0","truncation":null,"padding":null,"added_tokens":[{"id":0,"special":true,"content":"[PAD]","single_word":false,"lstrip":false,"rstrip":false,"normalized":false},{"id":100,"special":true,"content":"[UNK]","single_word":false,"lstrip":false,"rstrip":false,"normalized":false},{"id":101,"special":true,"content":"[CLS]","single_word":false,"lstrip":false,"rstrip":false,"normalized":false},{"id":102,"special":true,"content":"[SEP]","single_word":false,"lstrip":false,"rstrip":false,"normalized":false},{"id":103,"special":true,"content":"[MASK]","single_word":false,"lstrip":false,"rstrip":false,"normalized":false}],"normalizer":{"type":"BertNormalizer","clean_text":true,"handle_chinese_chars":true,"strip_accents":null,"lowercase":true},"pre_tokenizer":{"type":"BertPreTokenizer"},"post_processor":{"type":"TemplateProcessing","single":[{"SpecialToken":{"id":"[CLS]","type_id":0}},{"Sequence":{"id":"A","type_id":0}},{"SpecialToken":{"id":"[SEP]","type_id":0}}],"pair":[{"SpecialToken":{"id":"[CLS]","type_id":0}},{"Sequence":{"id":"A","type_id":0}},{"SpecialToken":{"id":"[SEP]","type_id":0}},{"Sequence":{"id":"B","type_id":1}},{"SpecialToken":{"id":"[SEP]","type_id":1}}],"special_tokens":{"[CLS]":{"id":"[CLS]","ids":[101],"tokens":["[CLS]"]},"[SEP]":{"id":"[SEP]","ids":[102],"tokens":["[SEP]"]}}},"decoder":{"type":"WordPiece","prefix":"##","cleanup":true},"model":{"type":"WordPiece","unk_token":"[UNK]","continuing_subword_prefix":"##","max_input_chars_per_word":100,"vocab":{"[PAD]":0,"[unused0]":1,"[unused1]":2,"[unused2]":3,"[unused3]":4,"[unused4]":5,"[unused5]":6,"[unused6]":7,"[unused7]":8,"[unused8]":9,"[unused9]":10,"[unused10]":11,"[unused11]":12,"[unused12]":13,"[unused13]":14,"[unused14]":15,"[unused15]":16,"[unused16]":17,"[unused17]":18,"[unused18]":19,"[unused19]":20,"[unused20]":21,"[unused21]":22,"[unused22]":23,"[unused23]":24,"[unused24]":25,"[unused25]":26,"[unused26]":27,"[unused27]":28,"[unused28]":29,"[unused29]":30,"[unused30]":31,"[unused31]":32,"[unused32]":33,"[unused33]":34,"[unused34]":35,"[unused35]":36,"[unused36]":37,"[unused37]":38,"[unused38]":39,"[unused39]":40,"[unused40]":41,"[unused41]":42,"[unused42]":43,"[unused43]":44,"[unused44]":45,"[unused45]":46,"[unused46]":47,"[unused47]":48,"[unused48]":49,"[unused49]":50,"[unused50]":51,"[unused51]":52,"[unused52]":53,"[unused53]":54,"[unused54]":55,"[unused55]":56,"[unused56]":57,"[unused57]":58,"[unused58]":59,"[unused59]":60,"[unused60]":61,"[unused61]":62,"[unused62]":63,"[unused63]":64,"[unused64]":65,"[unused65]":66,"[unused66]":67,"[unused67]":68,"[unused68]":69,"[unused69]":70,"[unused70]":71,"[unused71]":72,"[unused72]":73,"[unused73]":74,"[unused74]":75,"[unused75]":76,"[unused76]":77,"[unused77]":78,"[unused78]":79,"[unused79]":80,"[unused80]":81,"[unused81]":82,"[unused82]":83,"[unused83]":84,"[unused84]":85,"[unused85]":86,"[unused86]":87,"[unused87]":88,"[unused88]":89,"[unused89]":90,"[unused90]":91,"[unused91]":92,"[unused92]":93,"[unused93]":94,"[unused94]":95,"[unused95]":96,"[unused96]":97,"[unused97]":98,"[unused98]":99,"[UNK]":100,"[CLS]":101,"[SEP]":102,"[MASK]":103,"[unused99]":104,"[unused100]":105,"[unused101]":106,"[unused102]":107,"[unused103]":108,"[unused104]":109,"[unused105]":110,"[unused106]":111,"[unused107]":112,"[unused108]":113,"[unused109]":114,"[unused110]":115,"[unused111]":116,"[unused112]":117,"[unused113]":118,"[unused114]":119,"[unused115]":120,"[unused116]":121,"[unused117]":122,"[unused118]":123,"[unused119]":124,"[unused120]":125,"[unused121]":126,"[unused122]":127,"[unused123]":128,"[unused124]":129,"[unused125]":130,"[unused126]":131,"[unused127]":132,"[unused128]":133,"[unused129]":134,"[unused130]":135,"[unused131]":136,"[unused132]":137,"[unused133]":138,"[unused134]":139,"[unused135]":140,"[unused136]":141,"[unused137]":142,"[unused138]":143,"[unused139]":144,"[unused140]":145,"[unused141]":146,"[unused142]":147,"[unused143]":148,"[unused144]":149,"[unused145]":150,"[unused146]":151,"[unused147]":152,"[unused148]":153,"[unused149]":154,"[unused150]":155,"[unused151]":156,"[unused152]":157,"[unused153]":158,"[unused154]":159,"[unused155]":160,"[unused156]":161,"[unused157]":162,"[unused158]":163,"[unused159]":164,"[unused160]":165,"[unused161]":166,"[unused162]":167,"[unused163]":168,"[unused164]":169,"[unused165]":170,"[unused166]":171,"[unused167]":172,"[unused168]":173,"[unused169]":174,"[unused170]":175,"[unused171]":176,"[unused172]":177,"[unused173]":178,"[unused174]":179,"[unused175]":180,"[unused176]":181,"[unused177]":182,"[unused178]":183,"[unused179]":184,"[unused180]":185,"[unused181]":186,"[unused182]":187,"[unused183]":188,"[unused184]":189,"[unused185]":190,"[unused186]":191,"[unused187]":192,"[unused188]":193,"[unused189]":194,"[unused190]":195,"[unused191]":196,"[unused192]":197,"[unused193]":198,"[unused194]":199,"[unused195]":200,"[unused196]":201,"[unused197]":202,"[unused198]":203,"[unused199]":204,"[unused200]":205,"[unused201]":206,"[unused202]":207,"[unused203]":208,"[unused204]":209,"[unused205]":210,"[unused206]":211,"[unused207]":212,"[unused208]":213,"[unused209]":214,"[unused210]":215,"[unused211]":216,"[unused212]":217,"[unused213]":218,"[unused214]":219,"[unused215]":220,"[unused216]":221,"[unused217]":222,"[unused218]":223,"[unused219]":224,"[unused220]":225,"[unused221]":226,"[unused222]":227,"[unused223]":228,"[unused224]":229,"[unused225]":230,"[unused226]":231,"[unused227]":232,"[unused228]":233,"[unused229]":234,"[unused230]":235,"[unused231]":236,"[unused232]":237,"[unused233]":238,"[unused234]":239,"[unused235]":240,"[unused236]":241,"[unused237]":242,"[unused238]":243,"[unused239]":244,"[unused240]":245,"[unused241]":246,"[unused242]":247,"[unused243]":248,"[unused244]":249,"[unused245]":250,"[unused246]":251,"[unused247]":252,"[unused248]":253,"[unused249]":254,"[unused250]":255,"[unused251]":256,"[unused252]":257,"[unused253]":258,"[unused254]":259,"[unused255]":260,"[unused256]":261,"[unused257]":262,"[unused258]":263,"[unused259]":264,"[unused260]":265,"[unused261]":266,"[unused262]":267,"[unused263]":268,"[unused264]":269,"[unused265]":270,"[unused266]":271,"[unused267]":272,"[unused268]":273,"[unused269]":274,"[unused270]":275,"[unused271]":276,"[unused272]":277,"[unused273]":278,"[unused274]":279,"[unused275]":280,"[unused276]":281,"[unused277]":282,"[unused278]":283,"[unused279]":284,"[unused280]":285,"[unused281]":286,"[unused282]":287,"[unused283]":288,"[unused284]":289,"[unused285]":290,"[unused286]":291,"[unused287]":292,"[unused288]":293,"[unused289]":294,"[unused290]":295,"[unused291]":296,"[unused292]":297,"[unused293]":298,"[unused294]":299,"[unused295]":300,"[unused296]":301,"[unused297]":302,"[unused298]":303,"[unused299]":304,"[unused300]":305,"[unused301]":306,"[unused302]":307,"[unused303]":308,"[unused304]":309,"[unused305]":310,"[unused306]":311,"[unused307]":312,"[unused308]":313,"[unused309]":314,"[unused310]":315,"[unused311]":316,"[unused312]":317,"[unused313]":318,"[unused314]":319,"[unused315]":320,"[unused316]":321,"[unused317]":322,"[unused318]":323,"[unused319]":324,"[unused320]":325,"[unused321]":326,"[unused322]":327,"[unused323]":328,"[unused324]":329,"[unused325]":330,"[unused326]":331,"[unused327]":332,"[unused328]":333,"[unused329]":334,"[unused330]":335,"[unused331]":336,"[unused332]":337,"[unused333]":338,"[unused334]":339,"[unused335]":340,"[unused336]":341,"[unused337]":342,"[unused338]":343,"[unused339]":344,"[unused340]":345,"[unused341]":346,"[unused342]":347,"[unused343]":348,"[unused344]":349,"[unused345]":350,"[unused346]":351,"[unused347]":352,"[unused348]":353,"[unused349]":354,"[unused350]":355,"[unused351]":356,"[unused352]":357,"[unused353]":358,"[unused354]":359,"[unused355]":360,"[unused356]":361,"[unused357]":362,"[unused358]":363,"[unused359]":364,"[unused360]":365,"[unused361]":366,"[unused362]":367,"[unused363]":368,"[unused364]":369,"[unused365]":370,"[unused366]":371,"[unused367]":372,"[unused368]":373,"[unused369]":374,"[unused370]":375,"[unused371]":376,"[unused372]":377,"[unused373]":378,"[unused374]":379,"[unused375]":380,"[unused376]":381,"[unused377]":382,"[unused378]":383,"[unused379]":384,"[unused380]":385,"[unused381]":386,"[unused382]":387,"[unused383]":388,"[unused384]":389,"[unused385]":390,"[unused386]":391,"[unused387]":392,"[unused388]":393,"[unused389]":394,"[unused390]":395,"[unused391]":396,"[unused392]":397,"[unused393]":398,"[unused394]":399,"[unused395]":400,"[unused396]":401,"[unused397]":402,"[unused398]":403,"[unused399]":404,"[unused400]":405,"[unused401]":406,"[unused402]":407,"[unused403]":408,"[unused404]":409,"[unused405]":410,"[unused406]":411,"[unused407]":412,"[unused408]":413,"[unused409]":414,"[unused410]":415,"[unused411]":416,"[unused412]":417,"[unused413]":418,"[unused414]":419,"[unused415]":420,"[unused416]":421,"[unused417]":422,"[unused418]":423,"[unused419]":424,"[unused420]":425,"[unused421]":426,"[unused422]":427,"[unused423]":428,"[unused424]":429,"[unused425]":430,"[unused426]":431,"[unused427]":432,"[unused428]":433,"[unused429]":434,"[unused430]":435,"[unused431]":436,"[unused432]":437,"[unused433]":438,"[unused434]":439,"[unused435]":440,"[unused436]":441,"[unused437]":442,"[unused438]":443,"[unused439]":444,"[unused440]":445,"[unused441]":446,"[unused442]":447,"[unused443]":448,"[unused444]":449,"[unused445]":450,"[unused446]":451,"[unused447]":452,"[unused448]":453,"[unused449]":454,"[unused450]":455,"[unused451]":456,"[unused452]":457,"[unused453]":458,"[unused454]":459,"[unused455]":460,"[unused456]":461,"[unused457]":462,"[unused458]":463,"[unused459]":464,"[unused460]":465,"[unused461]":466,"[unused462]":467,"[unused463]":468,"[unused464]":469,"[unused465]":470,"[unused466]":471,"[unused467]":472,"[unused468]":473,"[unused469]":474,"[unused470]":475,"[unused471]":476,"[unused472]":477,"[unused473]":478,"[unused474]":479,"[unused475]":480,"[unused476]":481,"[unused477]":482,"[unused478]":483,"[unused479]":484,"[unused480]":485,"[unused481]":486,"[unused482]":487,"[unused483]":488,"[unused484]":489,"[unused485]":490,"[unused486]":491,"[unused487]":492,"[unused488]":493,"[unused489]":494,"[unused490]":495,"[unused491]":496,"[unused492]":497,"[unused493]":498,"[unused494]":499,"[unused495]":500,"[unused496]":501,"[unused497]":502,"[unused498]":503,"[unused499]":504,"[unused500]":505,"[unused501]":506,"[unused502]":507,"[unused503]":508,"[unused504]":509,"[unused505]":510,"[unused506]":511,"[unused507]":512,"[unused508]":513,"[unused509]":514,"[unused510]":515,"[unused511]":516,"[unused512]":517,"[unused513]":518,"[unused514]":519,"[unused515]":520,"[unused516]":521,"[unused517]":522,"[unused518]":523,"[unused519]":524,"[unused520]":525,"[unused521]":526,"[unused522]":527,"[unused523]":528,"[unused524]":529,"[unused525]":530,"[unused526]":531,"[unused527]":532,"[unused528]":533,"[unused529]":534,"[unused530]":535,"[unused531]":536,"[unused532]":537,"[unused533]":538,"[unused534]":539,"[unused535]":540,"[unused536]":541,"[unused537]":542,"[unused538]":543,"[unused539]":544,"[unused540]":545,"[unused541]":546,"[unused542]":547,"[unused543]":548,"[unused544]":549,"[unused545]":550,"[unused546]":551,"[unused547]":552,"[unused548]":553,"[unused549]":554,"[unused550]":555,"[unused551]":556,"[unused552]":557,"[unused553]":558,"[unused554]":559,"[unused555]":560,"[unused556]":561,"[unused557]":562,"[unused558]":563,"[unused559]":564,"[unused560]":565,"[unused561]":566,"[unused562]":567,"[unused563]":568,"[unused564]":569,"[unused565]":570,"[unused566]":571,"[unused567]":572,"[unused568]":573,"[unused569]":574,"[unused570]":575,"[unused571]":576,"[unused572]":577,"[unused573]":578,"[unused574]":579,"[unused575]":580,"[unused576]":581,"[unused577]":582,"[unused578]":583,"[unused579]":584,"[unused580]":585,"[unused581]":586,"[unused582]":587,"[unused583]":588,"[unused584]":589,"[unused585]":590,"[unused586]":591,"[unused587]":592,"[unused588]":593,"[unused589]":594,"[unused590]":595,"[unused591]":596,"[unused592]":597,"[unused593]":598,"[unused594]":599,"[unused595]":600,"[unused596]":601,"[unused597]":602,"[unused598]":603,"[unused599]":604,"[unused600]":605,"[unused601]":606,"[unused602]":607,"[unused603]":608,"[unused604]":609,"[unused605]":610,"[unused606]":611,"[unused607]":612,"[unused608]":613,"[unused609]":614,"[unused610]":615,"[unused611]":616,"[unused612]":617,"[unused613]":618,"[unused614]":619,"[unused615]":620,"[unused616]":621,"[unused617]":622,"[unused618]":623,"[unused619]":624,"[unused620]":625,"[unused621]":626,"[unused622]":627,"[unused623]":628,"[unused624]":629,"[unused625]":630,"[unused626]":631,"[unused627]":632,"[unused628]":633,"[unused629]":634,"[unused630]":635,"[unused631]":636,"[unused632]":637,"[unused633]":638,"[unused634]":639,"[unused635]":640,"[unused636]":641,"[unused637]":642,"[unused638]":643,"[unused639]":644,"[unused640]":645,"[unused641]":646,"[unused642]":647,"[unused643]":648,"[unused644]":649,"[unused645]":650,"[unused646]":651,"[unused647]":652,"[unused648]":653,"[unused649]":654,"[unused650]":655,"[unused651]":656,"[unused652]":657,"[unused653]":658,"[unused654]":659,"[unused655]":660,"[unused656]":661,"[unused657]":662,"[unused658]":663,"[unused659]":664,"[unused660]":665,"[unused661]":666,"[unused662]":667,"[unused663]":668,"[unused664]":669,"[unused665]":670,"[unused666]":671,"[unused667]":672,"[unused668]":673,"[unused669]":674,"[unused670]":675,"[unused671]":676,"[unused672]":677,"[unused673]":678,"[unused674]":679,"[unused675]":680,"[unused676]":681,"[unused677]":682,"[unused678]":683,"[unused679]":684,"[unused680]":685,"[unused681]":686,"[unused682]":687,"[unused683]":688,"[unused684]":689,"[unused685]":690,"[unused686]":691,"[unused687]":692,"[unused688]":693,"[unused689]":694,"[unused690]":695,"[unused691]":696,"[unused692]":697,"[unused693]":698,"[unused694]":699,"[unused695]":700,"[unused696]":701,"[unused697]":702,"[unused698]":703,"[unused699]":704,"[unused700]":705,"[unused701]":706,"[unused702]":707,"[unused703]":708,"[unused704]":709,"[unused705]":710,"[unused706]":711,"[unused707]":712,"[unused708]":713,"[unused709]":714,"[unused710]":715,"[unused711]":716,"[unused712]":717,"[unused713]":718,"[unused714]":719,"[unused715]":720,"[unused716]":721,"[unused717]":722,"[unused718]":723,"[unused719]":724,"[unused720]":725,"[unused721]":726,"[unused722]":727,"[unused723]":728,"[unused724]":729,"[unused725]":730,"[unused726]":731,"[unused727]":732,"[unused728]":733,"[unused729]":734,"[unused730]":735,"[unused731]":736,"[unused732]":737,"[unused733]":738,"[unused734]":739,"[unused735]":740,"[unused736]":741,"[unused737]":742,"[unused738]":743,"[unused739]":744,"[unused740]":745,"[unused741]":746,"[unused742]":747,"[unused743]":748,"[unused744]":749,"[unused745]":750,"[unused746]":751,"[unused747]":752,"[unused748]":753,"[unused749]":754,"[unused750]":755,"[unused751]":756,"[unused752]":757,"[unused753]":758,"[unused754]":759,"[unused755]":760,"[unused756]":761,"[unused757]":762,"[unused758]":763,"[unused759]":764,"[unused760]":765,"[unused761]":766,"[unused762]":767,"[unused763]":768,"[unused764]":769,"[unused765]":770,"[unused766]":771,"[unused767]":772,"[unused768]":773,"[unused769]":774,"[unused770]":775,"[unused771]":776,"[unused772]":777,"[unused773]":778,"[unused774]":779,"[unused775]":780,"[unused776]":781,"[unused777]":782,"[unused778]":783,"[unused779]":784,"[unused780]":785,"[unused781]":786,"[unused782]":787,"[unused783]":788,"[unused784]":789,"[unused785]":790,"[unused786]":791,"[unused787]":792,"[unused788]":793,"[unused789]":794,"[unused790]":795,"[unused791]":796,"[unused792]":797,"[unused793]":798,"[unused794]":799,"[unused795]":800,"[unused796]":801,"[unused797]":802,"[unused798]":803,"[unused799]":804,"[unused800]":805,"[unused801]":806,"[unused802]":807,"[unused803]":808,"[unused804]":809,"[unused805]":810,"[unused806]":811,"[unused807]":812,"[unused808]":813,"[unused809]":814,"[unused810]":815,"[unused811]":816,"[unused812]":817,"[unused813]":818,"[unused814]":819,"[unused815]":820,"[unused816]":821,"[unused817]":822,"[unused818]":823,"[unused819]":824,"[unused820]":825,"[unused821]":826,"[unused822]":827,"[unused823]":828,"[unused824]":829,"[unused825]":830,"[unused826]":831,"[unused827]":832,"[unused828]":833,"[unused829]":834,"[unused830]":835,"[unused831]":836,"[unused832]":837,"[unused833]":838,"[unused834]":839,"[unused835]":840,"[unused836]":841,"[unused837]":842,"[unused838]":843,"[unused839]":844,"[unused840]":845,"[unused841]":846,"[unused842]":847,"[unused843]":848,"[unused844]":849,"[unused845]":850,"[unused846]":851,"[unused847]":852,"[unused848]":853,"[unused849]":854,"[unused850]":855,"[unused851]":856,"[unused852]":857,"[unused853]":858,"[unused854]":859,"[unused855]":860,"[unused856]":861,"[unused857]":862,"[unused858]":863,"[unused859]":864,"[unused860]":865,"[unused861]":866,"[unused862]":867,"[unused863]":868,"[unused864]":869,"[unused865]":870,"[unused866]":871,"[unused867]":872,"[unused868]":873,"[unused869]":874,"[unused870]":875,"[unused871]":876,"[unused872]":877,"[unused873]":878,"[unused874]":879,"[unused875]":880,"[unused876]":881,"[unused877]":882,"[unused878]":883,"[unused879]":884,"[unused880]":885,"[unused881]":886,"[unused882]":887,"[unused883]":888,"[unused884]":889,"[unused885]":890,"[unused886]":891,"[unused887]":892,"[unused888]":893,"[unused889]":894,"[unused890]":895,"[unused891]":896,"[unused892]":897,"[unused893]":898,"[unused894]":899,"[unused895]":900,"[unused896]":901,"[unused897]":902,"[unused898]":903,"[unused899]":904,"[unused900]":905,"[unused901]":906,"[unused902]":907,"[unused903]":908,"[unused904]":909,"[unused905]":910,"[unused906]":911,"[unused907]":912,"[unused908]":913,"[unused909]":914,"[unused910]":915,"[unused911]":916,"[unused912]":917,"[unused913]":918,"[unused914]":919,"[unused915]":920,"[unused916]":921,"[unused917]":922,"[unused918]":923,"[unused919]":924,"[unused920]":925,"[unused921]":926,"[unused922]":927,"[unused923]":928,"[unused924]":929,"[unused925]":930,"[unused926]":931,"[unused927]":932,"[unused928]":933,"[unused929]":934,"[unused930]":935,"[unused931]":936,"[unused932]":937,"[unused933]":938,"[unused934]":939,"[unused935]":940,"[unused936]":941,"[unused937]":942,"[unused938]":943,"[unused939]":944,"[unused940]":945,"[unused941]":946,"[unused942]":947,"[unused943]":948,"[unused944]":949,"[unused945]":950,"[unused946]":951,"[unused947]":952,"[unused948]":953,"[unused949]":954,"[unused950]":955,"[unused951]":956,"[unused952]":957,"[unused953]":958,"[unused954]":959,"[unused955]":960,"[unused956]":961,"[unused957]":962,"[unused958]":963,"[unused959]":964,"[unused960]":965,"[unused961]":966,"[unused962]":967,"[unused963]":968,"[unused964]":969,"[unused965]":970,"[unused966]":971,"[unused967]":972,"[unused968]":973,"[unused969]":974,"[unused970]":975,"[unused971]":976,"[unused972]":977,"[unused973]":978,"[unused974]":979,"[unused975]":980,"[unused976]":981,"[unused977]":982,"[unused978]":983,"[unused979]":984,"[unused980]":985,"[unused981]":986,"[unused982]":987,"[unused983]":988,"[unused984]":989,"[unused985]":990,"[unused986]":991,"[unused987]":992,"[unused988]":993,"[unused989]":994,"[unused990]":995,"[unused991]":996,"[unused992]":997,"[unused993]":998,"!":999,"\"":1000,"#":1001,"$":1002,"%":1003,"&":1004,"'":1005,"(":1006,")":1007,"*":1008,"+":1009,",":1010,"-":1011,".":1012,"/":1013,"0":1014,"1":1015,"2":1016,"3":1017,"4":1018,"5":1019,"6":1020,"7":1021,"8":1022,"9":1023,":":1024,";":1025,"<":1026,"=":1027,">":1028,"?":1029,"@":1030,"[":1031,"\\":1032,"]":1033,"^":1034,"_":1035,"`":1036,"a":1037,"b":1038,"c":1039,"d":1040,"e":1041,"f":1042,"g":1043,"h":1044,"i":1045,"j":1046,"k":1047,"l":1048,"m":1049,"n":1050,"o":1051,"p":1052,"q":1053,"r":1054,"s":1055,"t":1056,"u":1057,"v":1058,"w":1059,"x":1060,"y":1061,"z":1062,"{":1063,"|":1064,"}":1065,"~":1066,"¡":1067,"¢":1068,"£":1069,"¤":1070,"¥":1071,"¦":1072,"§":1073,"¨":1074,"©":1075,"ª":1076,"«":1077,"¬":1078,"®":1079,"°":1080,"±":1081,"²":1082,"³":1083,"´":1084,"µ":1085,"¶":1086,"·":1087,"¹":1088,"º":1089,"»":1090,"¼":1091,"½":1092,"¾":1093,"¿":1094,"×":1095,"ß":1096,"æ":1097,"ð":1098,"÷":1099,"ø":1100,"þ":1101,"đ":1102,"ħ":1103,"ı":1104,"ł":1105,"ŋ":1106,"œ":1107,"ƒ":1108,"ɐ":1109,"ɑ":1110,"ɒ":1111,"ɔ":1112,"ɕ":1113,"ə":1114,"ɛ":1115,"ɡ":1116,"ɣ":1117,"ɨ":1118,"ɪ":1119,"ɫ":1120,"ɬ":1121,"ɯ":1122,"ɲ":1123,"ɴ":1124,"ɹ":1125,"ɾ":1126,"ʀ":1127,"ʁ":1128,"ʂ":1129,"ʃ":1130,"ʉ":1131,"ʊ":1132,"ʋ":1133,"ʌ":1134,"ʎ":1135,"ʐ":1136,"ʑ":1137,"ʒ":1138,"ʔ":1139,"ʰ":1140,"ʲ":1141,"ʳ":1142,"ʷ":1143,"ʸ":1144,"ʻ":1145,"ʼ":1146,"ʾ":1147,"ʿ":1148,"ˈ":1149,"ː":1150,"ˡ":1151,"ˢ":1152,"ˣ":1153,"ˤ":1154,"α":1155,"β":1156,"γ":1157,"δ":1158,"ε":1159,"ζ":1160,"η":1161,"θ":1162,"ι":1163,"κ":1164,"λ":1165,"μ":1166,"ν":1167,"ξ":1168,"ο":1169,"π":1170,"ρ":1171,"ς":1172,"σ":1173,"τ":1174,"υ":1175,"φ":1176,"χ":1177,"ψ":1178,"ω":1179,"а":1180,"б":1181,"в":1182,"г":1183,"д":1184,"е":1185,"ж":1186,"з":1187,"и":1188,"к":1189,"л":1190,"м":1191,"н":1192,"о":1193,"п":1194,"р":1195,"с":1196,"т":1197,"у":1198,"ф":1199,"х":1200,"ц":1201,"ч":1202,"ш":1203,"щ":1204,"ъ":1205,"ы":1206,"ь":1207,"э":1208,"ю":1209,"я":1210,"ђ":1211,"є":1212,"і":1213,"ј":1214,"љ":1215,"њ":1216,"ћ":1217,"ӏ":1218,"ա":1219,"բ":1220,"գ":1221,"դ":1222,"ե":1223,"թ":1224,"ի":1225,"լ":1226,"կ":1227,"հ":1228,"մ":1229,"յ":1230,"ն":1231,"ո":1232,"պ":1233,"ս":1234,"վ":1235,"տ":1236,"ր":1237,"ւ":1238,"ք":1239,"־":1240,"א":1241,"ב":1242,"ג":1243,"ד":1244,"ה":1245,"ו":1246,"ז":1247,"ח":1248,"ט":1249,"י":1250,"ך":1251,"כ":1252,"ל":1253,"ם":1254,"מ":1255,"ן":1256,"נ":1257,"ס":1258,"ע":1259,"ף":1260,"פ":1261,"ץ":1262,"צ":1263,"ק":1264,"ר":1265,"ש":1266,"ת":1267,"،":1268,"ء":1269,"ا":1270,"ب":1271,"ة":1272,"ت":1273,"ث":1274,"ج":1275,"ح":1276,"خ":1277,"د":1278,"ذ":1279,"ر":1280,"ز":1281,"س":1282,"ش":1283,"ص":1284,"ض":1285,"ط":1286,"ظ":1287,"ع":1288,"غ":1289,"ـ":1290,"ف":1291,"ق":1292,"ك":1293,"ل":1294,"م":1295,"ن":1296,"ه":1297,"و":1298,"ى":1299,"ي":1300,"ٹ":1301,"پ":1302,"چ":1303,"ک":1304,"گ":1305,"ں":1306,"ھ":1307,"ہ":1308,"ی":1309,"ے":1310,"अ":1311,"आ":1312,"उ":1313,"ए":1314,"क":1315,"ख":1316,"ग":1317,"च":1318,"ज":1319,"ट":1320,"ड":1321,"ण":1322,"त":1323,"थ":1324,"द":1325,"ध":1326,"न":1327,"प":1328,"ब":1329,"भ":1330,"म":1331,"य":1332,"र":1333,"ल":1334,"व":1335,"श":1336,"ष":1337,"स":1338,"ह":1339,"ा":1340,"ि":1341,"ी":1342,"ो":1343,"।":1344,"॥":1345,"ং":1346,"অ":1347,"আ":1348,"ই":1349,"উ":1350,"এ":1351,"ও":1352,"ক":1353,"খ":1354,"গ":1355,"চ":1356,"ছ":1357,"জ":1358,"ট":1359,"ড":1360,"ণ":1361,"ত":1362,"থ":1363,"দ":1364,"ধ":1365,"ন":1366,"প":1367,"ব":1368,"ভ":1369,"ম":1370,"য":1371,"র":1372,"ল":1373,"শ":1374,"ষ":1375,"স":1376,"হ":1377,"া":1378,"ি":1379,"ী":1380,"ে":1381,"க":1382,"ச":1383,"ட":1384,"த":1385,"ந":1386,"ன":1387,"ப":1388,"ம":1389,"ய":1390,"ர":1391,"ல":1392,"ள":1393,"வ":1394,"ா":1395,"ி":1396,"ு":1397,"ே":1398,"ை":1399,"ನ":1400,"ರ":1401,"ಾ":1402,"ක":1403,"ය":1404,"ර":1405,"ල":1406,"ව":1407,"ා":1408,"ก":1409,"ง":1410,"ต":1411,"ท":1412,"น":1413,"พ":1414,"ม":1415,"ย":1416,"ร":1417,"ล":1418,"ว":1419,"ส":1420,"อ":1421,"า":1422,"เ":1423,"་":1424,"།":1425,"ག":1426,"ང":1427,"ད":1428,"ན":1429,"པ":1430,"བ":1431,"མ":1432,"འ":1433,"ར":1434,"ལ":1435,"ས":1436,"မ":1437,"ა":1438,"ბ":1439,"გ":1440,"დ":1441,"ე":1442,"ვ":1443,"თ":1444,"ი":1445,"კ":1446,"ლ":1447,"მ":1448,"ნ":1449,"ო":1450,"რ":1451,"ს":1452,"ტ":1453,"უ":1454,"ᄀ":1455,"ᄂ":1456,"ᄃ":1457,"ᄅ":1458,"ᄆ":1459,"ᄇ":1460,"ᄉ":1461,"ᄊ":1462,"ᄋ":1463,"ᄌ":1464,"ᄎ":1465,"ᄏ":1466,"ᄐ":1467,"ᄑ":1468,"ᄒ":1469,"ᅡ":1470,"ᅢ":1471,"ᅥ":1472,"ᅦ":1473,"ᅧ":1474,"ᅩ":1475,"ᅪ":1476,"ᅭ":1477,"ᅮ":1478,"ᅯ":1479,"ᅲ":1480,"ᅳ":1481,"ᅴ":1482,"ᅵ":1483,"ᆨ":1484,"ᆫ":1485,"ᆯ":1486,"ᆷ":1487,"ᆸ":1488,"ᆼ":1489,"ᴬ":1490,"ᴮ":1491,"ᴰ":1492,"ᴵ":1493,"ᴺ":1494,"ᵀ":1495,"ᵃ":1496,"ᵇ":1497,"ᵈ":1498,"ᵉ":1499,"ᵍ":1500,"ᵏ":1501,"ᵐ":1502,"ᵒ":1503,"ᵖ":1504,"ᵗ":1505,"ᵘ":1506,"ᵢ":1507,"ᵣ":1508,"ᵤ":1509,"ᵥ":1510,"ᶜ":1511,"ᶠ":1512,"‐":1513,"‑":1514,"‒":1515,"–":1516,"—":1517,"―":1518,"‖":1519,"‘":1520,"’":1521,"‚":1522,"“":1523,"”":1524,"„":1525,"†":1526,"‡":1527,"•":1528,"…":1529,"‰":1530,"′":1531,"″":1532,"›":1533,"‿":1534,"⁄":1535,"⁰":1536,"ⁱ":1537,"⁴":1538,"⁵":1539,"⁶":1540,"⁷":1541,"⁸":1542,"⁹":1543,"⁺":1544,"⁻":1545,"ⁿ":1546,"₀":1547,"₁":1548,"₂":1549,"₃":1550,"₄":1551,"₅":1552,"₆":1553,"₇":1554,"₈":1555,"₉":1556,"₊":1557,"₍":1558,"₎":1559,"ₐ":1560,"ₑ":1561,"ₒ":1562,"ₓ":1563,"ₕ":1564,"ₖ":1565,"ₗ":1566,"ₘ":1567,"ₙ":1568,"ₚ":1569,"ₛ":1570,"ₜ":1571,"₤":1572,"₩":1573,"€":1574,"₱":1575,"₹":1576,"ℓ":1577,"№":1578,"ℝ":1579,"™":1580,"⅓":1581,"⅔":1582,"←":1583,"↑":1584,"→":1585,"↓":1586,"↔":1587,"↦":1588,"⇄":1589,"⇌":1590,"⇒":1591,"∂":1592,"∅":1593,"∆":1594,"∇":1595,"∈":1596,"−":1597,"∗":1598,"∘":1599,"√":1600,"∞":1601,"∧":1602,"∨":1603,"∩":1604,"∪":1605,"≈":1606,"≡":1607,"≤":1608,"≥":1609,"⊂":1610,"⊆":1611,"⊕":1612,"⊗":1613,"⋅":1614,"─":1615,"│":1616,"■":1617,"▪":1618,"●":1619,"★":1620,"☆":1621,"☉":1622,"♠":1623,"♣":1624,"♥":1625,"♦":1626,"♭":1627,"♯":1628,"⟨":1629,"⟩":1630,"ⱼ":1631,"⺩":1632,"⺼":1633,"⽥":1634,"、":1635,"。":1636,"〈":1637,"〉":1638,"《":1639,"》":1640,"「":1641,"」":1642,"『":1643,"』":1644,"〜":1645,"あ":1646,"い":1647,"う":1648,"え":1649,"お":1650,"か":1651,"き":1652,"く":1653,"け":1654,"こ":1655,"さ":1656,"し":1657,"す":1658,"せ":1659,"そ":1660,"た":1661,"ち":1662,"っ":1663,"つ":1664,"て":1665,"と":1666,"な":1667,"に":1668,"ぬ":1669,"ね":1670,"の":1671,"は":1672,"ひ":1673,"ふ":1674,"へ":1675,"ほ":1676,"ま":1677,"み":1678,"む":1679,"め":1680,"も":1681,"や":1682,"ゆ":1683,"よ":1684,"ら":1685,"り":1686,"る":1687,"れ":1688,"ろ":1689,"を":1690,"ん":1691,"ァ":1692,"ア":1693,"ィ":1694,"イ":1695,"ウ":1696,"ェ":1697,"エ":1698,"オ":1699,"カ":1700,"キ":1701,"ク":1702,"ケ":1703,"コ":1704,"サ":1705,"シ":1706,"ス":1707,"セ":1708,"タ":1709,"チ":1710,"ッ":1711,"ツ":1712,"テ":1713,"ト":1714,"ナ":1715,"ニ":1716,"ノ":1717,"ハ":1718,"ヒ":1719,"フ":1720,"ヘ":1721,"ホ":1722,"マ":1723,"ミ":1724,"ム":1725,"メ":1726,"モ":1727,"ャ":1728,"ュ":1729,"ョ":1730,"ラ":1731,"リ":1732,"ル":1733,"レ":1734,"ロ":1735,"ワ":1736,"ン":1737,"・":1738,"ー":1739,"一":1740,"三":1741,"上":1742,"下":1743,"不":1744,"世":1745,"中":1746,"主":1747,"久":1748,"之":1749,"也":1750,"事":1751,"二":1752,"五":1753,"井":1754,"京":1755,"人":1756,"亻":1757,"仁":1758,"介":1759,"代":1760,"仮":1761,"伊":1762,"会":1763,"佐":1764,"侍":1765,"保":1766,"信":1767,"健":1768,"元":1769,"光":1770,"八":1771,"公":1772,"内":1773,"出":1774,"分":1775,"前":1776,"劉":1777,"力":1778,"加":1779,"勝":1780,"北":1781,"区":1782,"十":1783,"千":1784,"南":1785,"博":1786,"原":1787,"口":1788,"古":1789,"史":1790,"司":1791,"合":1792,"吉":1793,"同":1794,"名":1795,"和":1796,"囗":1797,"四":1798,"国":1799,"國":1800,"土":1801,"地":1802,"坂":1803,"城":1804,"堂":1805,"場":1806,"士":1807,"夏":1808,"外":1809,"大":1810,"天":1811,"太":1812,"夫":1813,"奈":1814,"女":1815,"子":1816,"学":1817,"宀":1818,"宇":1819,"安":1820,"宗":1821,"定":1822,"宣":1823,"宮":1824,"家":1825,"宿":1826,"寺":1827,"將":1828,"小":1829,"尚":1830,"山":1831,"岡":1832,"島":1833,"崎":1834,"川":1835,"州":1836,"巿":1837,"帝":1838,"平":1839,"年":1840,"幸":1841,"广":1842,"弘":1843,"張":1844,"彳":1845,"後":1846,"御":1847,"德":1848,"心":1849,"忄":1850,"志":1851,"忠":1852,"愛":1853,"成":1854,"我":1855,"戦":1856,"戸":1857,"手":1858,"扌":1859,"政":1860,"文":1861,"新":1862,"方":1863,"日":1864,"明":1865,"星":1866,"春":1867,"昭":1868,"智":1869,"曲":1870,"書":1871,"月":1872,"有":1873,"朝":1874,"木":1875,"本":1876,"李":1877,"村":1878,"東":1879,"松":1880,"林":1881,"森":1882,"楊":1883,"樹":1884,"橋":1885,"歌":1886,"止":1887,"正":1888,"武":1889,"比":1890,"氏":1891,"民":1892,"水":1893,"氵":1894,"氷":1895,"永":1896,"江":1897,"沢":1898,"河":1899,"治":1900,"法":1901,"海":1902,"清":1903,"漢":1904,"瀬":1905,"火":1906,"版":1907,"犬":1908,"王":1909,"生":1910,"田":1911,"男":1912,"疒":1913,"発":1914,"白":1915,"的":1916,"皇":1917,"目":1918,"相":1919,"省":1920,"真":1921,"石":1922,"示":1923,"社":1924,"神":1925,"福":1926,"禾":1927,"秀":1928,"秋":1929,"空":1930,"立":1931,"章":1932,"竹":1933,"糹":1934,"美":1935,"義":1936,"耳":1937,"良":1938,"艹":1939,"花":1940,"英":1941,"華":1942,"葉":1943,"藤":1944,"行":1945,"街":1946,"西":1947,"見":1948,"訁":1949,"語":1950,"谷":1951,"貝":1952,"貴":1953,"車":1954,"軍":1955,"辶":1956,"道":1957,"郎":1958,"郡":1959,"部":1960,"都":1961,"里":1962,"野":1963,"金":1964,"鈴":1965,"镇":1966,"長":1967,"門":1968,"間":1969,"阝":1970,"阿":1971,"陳":1972,"陽":1973,"雄":1974,"青":1975,"面":1976,"風":1977,"食":1978,"香":1979,"馬":1980,"高":1981,"龍":1982,"龸":1983,"ﬁ":1984,"ﬂ":1985,"！":1986,"（":1987,"）":1988,"，":1989,"－":1990,"．":1991,"／":1992,"：":1993,"？":1994,"～":1995,"the":1996,"of":1997,"and":1998,"in":1999,"to":2000,"was":2001,"he":2002,"is":2003,"as":2004,"for":2005,"on":2006,"with":2007,"that":2008,"it":2009,"his":2010,"by":2011,"at":2012,"from":2013,"her":2014,"##s":2015,"she":2016,"you":2017,"had":2018,"an":2019,"were":2020,"but":2021,"be":2022,"this":2023,"are":2024,"not":2025,"my":2026,"they":2027,"one":2028,"which":2029,"or":2030,"have":2031,"him":2032,"me":2033,"first":2034,"all":2035,"also":2036,"their":2037,"has":2038,"up":2039,"who":2040,"out":2041,"been":2042,"when":2043,"after":2044,"there":2045,"into":2046,"new":2047,"two":2048,"its":2049,"##a":2050,"time":2051,"would":2052,"no":2053,"what":2054,"about":2055,"said":2056,"we":2057,"over":2058,"then":2059,"other":2060,"so":2061,"more":2062,"##e":2063,"can":2064,"if":2065,"like":2066,"back":2067,"them":2068,"only":2069,"some":2070,"could":2071,"##i":2072,"where":2073,"just":2074,"##ing":2075,"during":2076,"before":2077,"##n":2078,"do":2079,"##o":2080,"made":2081,"school":2082,"through":2083,"than":2084,"now":2085,"years":2086,"most":2087,"world":2088,"may":2089,"between":2090,"down":2091,"well":2092,"three":2093,"##d":2094,"year":2095,"while":2096,"will":2097,"##ed":2098,"##r":2099,"##y":2100,"later":2101,"##t":2102,"city":2103,"under":2104,"around":2105,"did":2106,"such":2107,"being":2108,"used":2109,"state":2110,"people":2111,"part":2112,"know":2113,"against":2114,"your":2115,"many":2116,"second":2117,"university":2118,"both":2119,"national":2120,"##er":2121,"these":2122,"don":2123,"known":2124,"off":2125,"way":2126,"until":2127,"re":2128,"how":2129,"even":2130,"get":2131,"head":2132,"...":2133,"didn":2134,"##ly":2135,"team":2136,"american":2137,"because":2138,"de":2139,"##l":2140,"born":2141,"united":2142,"film":2143,"since":2144,"still":2145,"long":2146,"work":2147,"south":2148,"us":2149,"became":2150,"any":2151,"high":2152,"again":2153,"day":2154,"family":2155,"see":2156,"right":2157,"man":2158,"eyes":2159,"house":2160,"season":2161,"war":2162,"states":2163,"including":2164,"took":2165,"life":2166,"north":2167,"same":2168,"each":2169,"called":2170,"name":2171,"much":2172,"place":2173,"however":2174,"go":2175,"four":2176,"group":2177,"another":2178,"found":2179,"won":2180,"area":2181,"here":2182,"going":2183,"10":2184,"away":2185,"series":2186,"left":2187,"home":2188,"music":2189,"best":2190,"make":2191,"hand":2192,"number":2193,"company":2194,"several":2195,"never":2196,"last":2197,"john":2198,"000":2199,"very":2200,"album":2201,"take":2202,"end":2203,"good":2204,"too":2205,"following":2206,"released":2207,"game":2208,"played":2209,"little":2210,"began":2211,"district":2212,"##m":2213,"old":2214,"want":2215,"those":2216,"side":2217,"held":2218,"own":2219,"early":2220,"county":2221,"ll":2222,"league":2223,"use":2224,"west":2225,"##u":2226,"face":2227,"think":2228,"##es":2229,"2010":2230,"government":2231,"##h":2232,"march":2233,"came":2234,"small":2235,"general":2236,"town":2237,"june":2238,"##on":2239,"line":2240,"based":2241,"something":2242,"##k":2243,"september":2244,"thought":2245,"looked":2246,"along":2247,"international":2248,"2011":2249,"air":2250,"july":2251,"club":2252,"went":2253,"january":2254,"october":2255,"our":2256,"august":2257,"april":2258,"york":2259,"12":2260,"few":2261,"2012":2262,"2008":2263,"east":2264,"show":2265,"member":2266,"college":2267,"2009":2268,"father":2269,"public":2270,"##us":2271,"come":2272,"men":2273,"five":2274,"set":2275,"station":2276,"church":2277,"##c":2278,"next":2279,"former":2280,"november":2281,"room":2282,"party":2283,"located":2284,"december":2285,"2013":2286,"age":2287,"got":2288,"2007":2289,"##g":2290,"system":2291,"let":2292,"love":2293,"2006":2294,"though":2295,"every":2296,"2014":2297,"look":2298,"song":2299,"water":2300,"century":2301,"without":2302,"body":2303,"black":2304,"night":2305,"within":2306,"great":2307,"women":2308,"single":2309,"ve":2310,"building":2311,"large":2312,"population":2313,"river":2314,"named":2315,"band":2316,"white":2317,"started":2318,"##an":2319,"once":2320,"15":2321,"20":2322,"should":2323,"18":2324,"2015":2325,"service":2326,"top":2327,"built":2328,"british":2329,"open":2330,"death":2331,"king":2332,"moved":2333,"local":2334,"times":2335,"children":2336,"february":2337,"book":2338,"why":2339,"11":2340,"door":2341,"need":2342,"president":2343,"order":2344,"final":2345,"road":2346,"wasn":2347,"although":2348,"due":2349,"major":2350,"died":2351,"village":2352,"third":2353,"knew":2354,"2016":2355,"asked":2356,"turned":2357,"st":2358,"wanted":2359,"say":2360,"##p":2361,"together":2362,"received":2363,"main":2364,"son":2365,"served":2366,"different":2367,"##en":2368,"behind":2369,"himself":2370,"felt":2371,"members":2372,"power":2373,"football":2374,"law":2375,"voice":2376,"play":2377,"##in":2378,"near":2379,"park":2380,"history":2381,"30":2382,"having":2383,"2005":2384,"16":2385,"##man":2386,"saw":2387,"mother":2388,"##al":2389,"army":2390,"point":2391,"front":2392,"help":2393,"english":2394,"street":2395,"art":2396,"late":2397,"hands":2398,"games":2399,"award":2400,"##ia":2401,"young":2402,"14":2403,"put":2404,"published":2405,"country":2406,"division":2407,"across":2408,"told":2409,"13":2410,"often":2411,"ever":2412,"french":2413,"london":2414,"center":2415,"six":2416,"red":2417,"2017":2418,"led":2419,"days":2420,"include":2421,"light":2422,"25":2423,"find":2424,"tell":2425,"among":2426,"species":2427,"really":2428,"according":2429,"central":2430,"half":2431,"2004":2432,"form":2433,"original":2434,"gave":2435,"office":2436,"making":2437,"enough":2438,"lost":2439,"full":2440,"opened":2441,"must":2442,"included":2443,"live":2444,"given":2445,"german":2446,"player":2447,"run":2448,"business":2449,"woman":2450,"community":2451,"cup":2452,"might":2453,"million":2454,"land":2455,"2000":2456,"court":2457,"development":2458,"17":2459,"short":2460,"round":2461,"ii":2462,"km":2463,"seen":2464,"class":2465,"story":2466,"always":2467,"become":2468,"sure":2469,"research":2470,"almost":2471,"director":2472,"council":2473,"la":2474,"##2":2475,"career":2476,"things":2477,"using":2478,"island":2479,"##z":2480,"couldn":2481,"car":2482,"##is":2483,"24":2484,"close":2485,"force":2486,"##1":2487,"better":2488,"free":2489,"support":2490,"control":2491,"field":2492,"students":2493,"2003":2494,"education":2495,"married":2496,"##b":2497,"nothing":2498,"worked":2499,"others":2500,"record":2501,"big":2502,"inside":2503,"level":2504,"anything":2505,"continued":2506,"give":2507,"james":2508,"##3":2509,"military":2510,"established":2511,"non":2512,"returned":2513,"feel":2514,"does":2515,"title":2516,"written":2517,"thing":2518,"feet":2519,"william":2520,"far":2521,"co":2522,"association":2523,"hard":2524,"already":2525,"2002":2526,"##ra":2527,"championship":2528,"human":2529,"western":2530,"100":2531,"##na":2532,"department":2533,"hall":2534,"role":2535,"various":2536,"production":2537,"21":2538,"19":2539,"heart":2540,"2001":2541,"living":2542,"fire":2543,"version":2544,"##ers":2545,"##f":2546,"television":2547,"royal":2548,"##4":2549,"produced":2550,"working":2551,"act":2552,"case":2553,"society":2554,"region":2555,"present":2556,"radio":2557,"period":2558,"looking":2559,"least":2560,"total":2561,"keep":2562,"england":2563,"wife":2564,"program":2565,"per":2566,"brother":2567,"mind":2568,"special":2569,"22":2570,"##le":2571,"am":2572,"works":2573,"soon":2574,"##6":2575,"political":2576,"george":2577,"services":2578,"taken":2579,"created":2580,"##7":2581,"further":2582,"able":2583,"reached":2584,"david":2585,"union":2586,"joined":2587,"upon":2588,"done":2589,"important":2590,"social":2591,"information":2592,"either":2593,"##ic":2594,"##x":2595,"appeared":2596,"position":2597,"ground":2598,"lead":2599,"rock":2600,"dark":2601,"election":2602,"23":2603,"board":2604,"france":2605,"hair":2606,"course":2607,"arms":2608,"site":2609,"police":2610,"girl":2611,"instead":2612,"real":2613,"sound":2614,"##v":2615,"words":2616,"moment":2617,"##te":2618,"someone":2619,"##8":2620,"summer":2621,"project":2622,"announced":2623,"san":2624,"less":2625,"wrote":2626,"past":2627,"followed":2628,"##5":2629,"blue":2630,"founded":2631,"al":2632,"finally":2633,"india":2634,"taking":2635,"records":2636,"america":2637,"##ne":2638,"1999":2639,"design":2640,"considered":2641,"northern":2642,"god":2643,"stop":2644,"battle":2645,"toward":2646,"european":2647,"outside":2648,"described":2649,"track":2650,"today":2651,"playing":2652,"language":2653,"28":2654,"call":2655,"26":2656,"heard":2657,"professional":2658,"low":2659,"australia":2660,"miles":2661,"california":2662,"win":2663,"yet":2664,"green":2665,"##ie":2666,"trying":2667,"blood":2668,"##ton":2669,"southern":2670,"science":2671,"maybe":2672,"everything":2673,"match":2674,"square":2675,"27":2676,"mouth":2677,"video":2678,"race":2679,"recorded":2680,"leave":2681,"above":2682,"##9":2683,"daughter":2684,"points":2685,"space":2686,"1998":2687,"museum":2688,"change":2689,"middle":2690,"common":2691,"##0":2692,"move":2693,"tv":2694,"post":2695,"##ta":2696,"lake":2697,"seven":2698,"tried":2699,"elected":2700,"closed":2701,"ten":2702,"paul":2703,"minister":2704,"##th":2705,"months":2706,"start":2707,"chief":2708,"return":2709,"canada":2710,"person":2711,"sea":2712,"release":2713,"similar":2714,"modern":2715,"brought":2716,"rest":2717,"hit":2718,"formed":2719,"mr":2720,"##la":2721,"1997":2722,"floor":2723,"event":2724,"doing":2725,"thomas":2726,"1996":2727,"robert":2728,"care":2729,"killed":2730,"training":2731,"star":2732,"week":2733,"needed":2734,"turn":2735,"finished":2736,"railway":2737,"rather":2738,"news":2739,"health":2740,"sent":2741,"example":2742,"ran":2743,"term":2744,"michael":2745,"coming":2746,"currently":2747,"yes":2748,"forces":2749,"despite":2750,"gold":2751,"areas":2752,"50":2753,"stage":2754,"fact":2755,"29":2756,"dead":2757,"says":2758,"popular":2759,"2018":2760,"originally":2761,"germany":2762,"probably":2763,"developed":2764,"result":2765,"pulled":2766,"friend":2767,"stood":2768,"money":2769,"running":2770,"mi":2771,"signed":2772,"word":2773,"songs":2774,"child":2775,"eventually":2776,"met":2777,"tour":2778,"average":2779,"teams":2780,"minutes":2781,"festival":2782,"current":2783,"deep":2784,"kind":2785,"1995":2786,"decided":2787,"usually":2788,"eastern":2789,"seemed":2790,"##ness":2791,"episode":2792,"bed":2793,"added":2794,"table":2795,"indian":2796,"private":2797,"charles":2798,"route":2799,"available":2800,"idea":2801,"throughout":2802,"centre":2803,"addition":2804,"appointed":2805,"style":2806,"1994":2807,"books":2808,"eight":2809,"construction":2810,"press":2811,"mean":2812,"wall":2813,"friends":2814,"remained":2815,"schools":2816,"study":2817,"##ch":2818,"##um":2819,"institute":2820,"oh":2821,"chinese":2822,"sometimes":2823,"events":2824,"possible":2825,"1992":2826,"australian":2827,"type":2828,"brown":2829,"forward":2830,"talk":2831,"process":2832,"food":2833,"debut":2834,"seat":2835,"performance":2836,"committee":2837,"features":2838,"character":2839,"arts":2840,"herself":2841,"else":2842,"lot":2843,"strong":2844,"russian":2845,"range":2846,"hours":2847,"peter":2848,"arm":2849,"##da":2850,"morning":2851,"dr":2852,"sold":2853,"##ry":2854,"quickly":2855,"directed":2856,"1993":2857,"guitar":2858,"china":2859,"##w":2860,"31":2861,"list":2862,"##ma":2863,"performed":2864,"media":2865,"uk":2866,"players":2867,"smile":2868,"##rs":2869,"myself":2870,"40":2871,"placed":2872,"coach":2873,"province":2874,"towards":2875,"wouldn":2876,"leading":2877,"whole":2878,"boy":2879,"official":2880,"designed":2881,"grand":2882,"census":2883,"##el":2884,"europe":2885,"attack":2886,"japanese":2887,"henry":2888,"1991":2889,"##re":2890,"##os":2891,"cross":2892,"getting":2893,"alone":2894,"action":2895,"lower":2896,"network":2897,"wide":2898,"washington":2899,"japan":2900,"1990":2901,"hospital":2902,"believe":2903,"changed":2904,"sister":2905,"##ar":2906,"hold":2907,"gone":2908,"sir":2909,"hadn":2910,"ship":2911,"##ka":2912,"studies":2913,"academy":2914,"shot":2915,"rights":2916,"below":2917,"base":2918,"bad":2919,"involved":2920,"kept":2921,"largest":2922,"##ist":2923,"bank":2924,"future":2925,"especially":2926,"beginning":2927,"mark":2928,"movement":2929,"section":2930,"female":2931,"magazine":2932,"plan":2933,"professor":2934,"lord":2935,"longer":2936,"##ian":2937,"sat":2938,"walked":2939,"hill":2940,"actually":2941,"civil":2942,"energy":2943,"model":2944,"families":2945,"size":2946,"thus":2947,"aircraft":2948,"completed":2949,"includes":2950,"data":2951,"captain":2952,"##or":2953,"fight":2954,"vocals":2955,"featured":2956,"richard":2957,"bridge":2958,"fourth":2959,"1989":2960,"officer":2961,"stone":2962,"hear":2963,"##ism":2964,"means":2965,"medical":2966,"groups":2967,"management":2968,"self":2969,"lips":2970,"competition":2971,"entire":2972,"lived":2973,"technology":2974,"leaving":2975,"federal":2976,"tournament":2977,"bit":2978,"passed":2979,"hot":2980,"independent":2981,"awards":2982,"kingdom":2983,"mary":2984,"spent":2985,"fine":2986,"doesn":2987,"reported":2988,"##ling":2989,"jack":2990,"fall":2991,"raised":2992,"itself":2993,"stay":2994,"true":2995,"studio":2996,"1988":2997,"sports":2998,"replaced":2999,"paris":3000,"systems":3001,"saint":3002,"leader":3003,"theatre":3004,"whose":3005,"market":3006,"capital":3007,"parents":3008,"spanish":3009,"canadian":3010,"earth":3011,"##ity":3012,"cut":3013,"degree":3014,"writing":3015,"bay":3016,"christian":3017,"awarded":3018,"natural":3019,"higher":3020,"bill":3021,"##as":3022,"coast":3023,"provided":3024,"previous":3025,"senior":3026,"ft":3027,"valley":3028,"organization":3029,"stopped":3030,"onto":3031,"countries":3032,"parts":3033,"conference":3034,"queen":3035,"security":3036,"interest":3037,"saying":3038,"allowed":3039,"master":3040,"earlier":3041,"phone":3042,"matter":3043,"smith":3044,"winning":3045,"try":3046,"happened":3047,"moving":3048,"campaign":3049,"los":3050,"##ley":3051,"breath":3052,"nearly":3053,"mid":3054,"1987":3055,"certain":3056,"girls":3057,"date":3058,"italian":3059,"african":3060,"standing":3061,"fell":3062,"artist":3063,"##ted":3064,"shows":3065,"deal":3066,"mine":3067,"industry":3068,"1986":3069,"##ng":3070,"everyone":3071,"republic":3072,"provide":3073,"collection":3074,"library":3075,"student":3076,"##ville":3077,"primary":3078,"owned":3079,"older":3080,"via":3081,"heavy":3082,"1st":3083,"makes":3084,"##able":3085,"attention":3086,"anyone":3087,"africa":3088,"##ri":3089,"stated":3090,"length":3091,"ended":3092,"fingers":3093,"command":3094,"staff":3095,"skin":3096,"foreign":3097,"opening":3098,"governor":3099,"okay":3100,"medal":3101,"kill":3102,"sun":3103,"cover":3104,"job":3105,"1985":3106,"introduced":3107,"chest":3108,"hell":3109,"feeling":3110,"##ies":3111,"success":3112,"meet":3113,"reason":3114,"standard":3115,"meeting":3116,"novel":3117,"1984":3118,"trade":3119,"source":3120,"buildings":3121,"##land":3122,"rose":3123,"guy":3124,"goal":3125,"##ur":3126,"chapter":3127,"native":3128,"husband":3129,"previously":3130,"unit":3131,"limited":3132,"entered":3133,"weeks":3134,"producer":3135,"operations":3136,"mountain":3137,"takes":3138,"covered":3139,"forced":3140,"related":3141,"roman":3142,"complete":3143,"successful":3144,"key":3145,"texas":3146,"cold":3147,"##ya":3148,"channel":3149,"1980":3150,"traditional":3151,"films":3152,"dance":3153,"clear":3154,"approximately":3155,"500":3156,"nine":3157,"van":3158,"prince":3159,"question":3160,"active":3161,"tracks":3162,"ireland":3163,"regional":3164,"silver":3165,"author":3166,"personal":3167,"sense":3168,"operation":3169,"##ine":3170,"economic":3171,"1983":3172,"holding":3173,"twenty":3174,"isbn":3175,"additional":3176,"speed":3177,"hour":3178,"edition":3179,"regular":3180,"historic":3181,"places":3182,"whom":3183,"shook":3184,"movie":3185,"km²":3186,"secretary":3187,"prior":3188,"report":3189,"chicago":3190,"read":3191,"foundation":3192,"view":3193,"engine":3194,"scored":3195,"1982":3196,"units":3197,"ask":3198,"airport":3199,"property":3200,"ready":3201,"immediately":3202,"lady":3203,"month":3204,"listed":3205,"contract":3206,"##de":3207,"manager":3208,"themselves":3209,"lines":3210,"##ki":3211,"navy":3212,"writer":3213,"meant":3214,"##ts":3215,"runs":3216,"##ro":3217,"practice":3218,"championships":3219,"singer":3220,"glass":3221,"commission":3222,"required":3223,"forest":3224,"starting":3225,"culture":3226,"generally":3227,"giving":3228,"access":3229,"attended":3230,"test":3231,"couple":3232,"stand":3233,"catholic":3234,"martin":3235,"caught":3236,"executive":3237,"##less":3238,"eye":3239,"##ey":3240,"thinking":3241,"chair":3242,"quite":3243,"shoulder":3244,"1979":3245,"hope":3246,"decision":3247,"plays":3248,"defeated":3249,"municipality":3250,"whether":3251,"structure":3252,"offered":3253,"slowly":3254,"pain":3255,"ice":3256,"direction":3257,"##ion":3258,"paper":3259,"mission":3260,"1981":3261,"mostly":3262,"200":3263,"noted":3264,"individual":3265,"managed":3266,"nature":3267,"lives":3268,"plant":3269,"##ha":3270,"helped":3271,"except":3272,"studied":3273,"computer":3274,"figure":3275,"relationship":3276,"issue":3277,"significant":3278,"loss":3279,"die":3280,"smiled":3281,"gun":3282,"ago":3283,"highest":3284,"1972":3285,"##am":3286,"male":3287,"bring":3288,"goals":3289,"mexico":3290,"problem":3291,"distance":3292,"commercial":3293,"completely":3294,"location":3295,"annual":3296,"famous":3297,"drive":3298,"1976":3299,"neck":3300,"1978":3301,"surface":3302,"caused":3303,"italy":3304,"understand":3305,"greek":3306,"highway":3307,"wrong":3308,"hotel":3309,"comes":3310,"appearance":3311,"joseph":3312,"double":3313,"issues":3314,"musical":3315,"companies":3316,"castle":3317,"income":3318,"review":3319,"assembly":3320,"bass":3321,"initially":3322,"parliament":3323,"artists":3324,"experience":3325,"1974":3326,"particular":3327,"walk":3328,"foot":3329,"engineering":3330,"talking":3331,"window":3332,"dropped":3333,"##ter":3334,"miss":3335,"baby":3336,"boys":3337,"break":3338,"1975":3339,"stars":3340,"edge":3341,"remember":3342,"policy":3343,"carried":3344,"train":3345,"stadium":3346,"bar":3347,"sex":3348,"angeles":3349,"evidence":3350,"##ge":3351,"becoming":3352,"assistant":3353,"soviet":3354,"1977":3355,"upper":3356,"step":3357,"wing":3358,"1970":3359,"youth":3360,"financial":3361,"reach":3362,"##ll":3363,"actor":3364,"numerous":3365,"##se":3366,"##st":3367,"nodded":3368,"arrived":3369,"##ation":3370,"minute":3371,"##nt":3372,"believed":3373,"sorry":3374,"complex":3375,"beautiful":3376,"victory":3377,"associated":3378,"temple":3379,"1968":3380,"1973":3381,"chance":3382,"perhaps":3383,"metal":3384,"##son":3385,"1945":3386,"bishop":3387,"##et":3388,"lee":3389,"launched":3390,"particularly":3391,"tree":3392,"le":3393,"retired":3394,"subject":3395,"prize":3396,"contains":3397,"yeah":3398,"theory":3399,"empire":3400,"##ce":3401,"suddenly":3402,"waiting":3403,"trust":3404,"recording":3405,"##to":3406,"happy":3407,"terms":3408,"camp":3409,"champion":3410,"1971":3411,"religious":3412,"pass":3413,"zealand":3414,"names":3415,"2nd":3416,"port":3417,"ancient":3418,"tom":3419,"corner":3420,"represented":3421,"watch":3422,"legal":3423,"anti":3424,"justice":3425,"cause":3426,"watched":3427,"brothers":3428,"45":3429,"material":3430,"changes":3431,"simply":3432,"response":3433,"louis":3434,"fast":3435,"##ting":3436,"answer":3437,"60":3438,"historical":3439,"1969":3440,"stories":3441,"straight":3442,"create":3443,"feature":3444,"increased":3445,"rate":3446,"administration":3447,"virginia":3448,"el":3449,"activities":3450,"cultural":3451,"overall":3452,"winner":3453,"programs":3454,"basketball":3455,"legs":3456,"guard":3457,"beyond":3458,"cast":3459,"doctor":3460,"mm":3461,"flight":3462,"results":3463,"remains":3464,"cost":3465,"effect":3466,"winter":3467,"##ble":3468,"larger":3469,"islands":3470,"problems":3471,"chairman":3472,"grew":3473,"commander":3474,"isn":3475,"1967":3476,"pay":3477,"failed":3478,"selected":3479,"hurt":3480,"fort":3481,"box":3482,"regiment":3483,"majority":3484,"journal":3485,"35":3486,"edward":3487,"plans":3488,"##ke":3489,"##ni":3490,"shown":3491,"pretty":3492,"irish":3493,"characters":3494,"directly":3495,"scene":3496,"likely":3497,"operated":3498,"allow":3499,"spring":3500,"##j":3501,"junior":3502,"matches":3503,"looks":3504,"mike":3505,"houses":3506,"fellow":3507,"##tion":3508,"beach":3509,"marriage":3510,"##ham":3511,"##ive":3512,"rules":3513,"oil":3514,"65":3515,"florida":3516,"expected":3517,"nearby":3518,"congress":3519,"sam":3520,"peace":3521,"recent":3522,"iii":3523,"wait":3524,"subsequently":3525,"cell":3526,"##do":3527,"variety":3528,"serving":3529,"agreed":3530,"please":3531,"poor":3532,"joe":3533,"pacific":3534,"attempt":3535,"wood":3536,"democratic":3537,"piece":3538,"prime":3539,"##ca":3540,"rural":3541,"mile":3542,"touch":3543,"appears":3544,"township":3545,"1964":3546,"1966":3547,"soldiers":3548,"##men":3549,"##ized":3550,"1965":3551,"pennsylvania":3552,"closer":3553,"fighting":3554,"claimed":3555,"score":3556,"jones":3557,"physical":3558,"editor":3559,"##ous":3560,"filled":3561,"genus":3562,"specific":3563,"sitting":3564,"super":3565,"mom":3566,"##va":3567,"therefore":3568,"supported":3569,"status":3570,"fear":3571,"cases":3572,"store":3573,"meaning":3574,"wales":3575,"minor":3576,"spain":3577,"tower":3578,"focus":3579,"vice":3580,"frank":3581,"follow":3582,"parish":3583,"separate":3584,"golden":3585,"horse":3586,"fifth":3587,"remaining":3588,"branch":3589,"32":3590,"presented":3591,"stared":3592,"##id":3593,"uses":3594,"secret":3595,"forms":3596,"##co":3597,"baseball":3598,"exactly":3599,"##ck":3600,"choice":3601,"note":3602,"discovered":3603,"travel":3604,"composed":3605,"truth":3606,"russia":3607,"ball":3608,"color":3609,"kiss":3610,"dad":3611,"wind":3612,"continue":3613,"ring":3614,"referred":3615,"numbers":3616,"digital":3617,"greater":3618,"##ns":3619,"metres":3620,"slightly":3621,"direct":3622,"increase":3623,"1960":3624,"responsible":3625,"crew":3626,"rule":3627,"trees":3628,"troops":3629,"##no":3630,"broke":3631,"goes":3632,"individuals":3633,"hundred":3634,"weight":3635,"creek":3636,"sleep":3637,"memory":3638,"defense":3639,"provides":3640,"ordered":3641,"code":3642,"value":3643,"jewish":3644,"windows":3645,"1944":3646,"safe":3647,"judge":3648,"whatever":3649,"corps":3650,"realized":3651,"growing":3652,"pre":3653,"##ga":3654,"cities":3655,"alexander":3656,"gaze":3657,"lies":3658,"spread":3659,"scott":3660,"letter":3661,"showed":3662,"situation":3663,"mayor":3664,"transport":3665,"watching":3666,"workers":3667,"extended":3668,"##li":3669,"expression":3670,"normal":3671,"##ment":3672,"chart":3673,"multiple":3674,"border":3675,"##ba":3676,"host":3677,"##ner":3678,"daily":3679,"mrs":3680,"walls":3681,"piano":3682,"##ko":3683,"heat":3684,"cannot":3685,"##ate":3686,"earned":3687,"products":3688,"drama":3689,"era":3690,"authority":3691,"seasons":3692,"join":3693,"grade":3694,"##io":3695,"sign":3696,"difficult":3697,"machine":3698,"1963":3699,"territory":3700,"mainly":3701,"##wood":3702,"stations":3703,"squadron":3704,"1962":3705,"stepped":3706,"iron":3707,"19th":3708,"##led":3709,"serve":3710,"appear":3711,"sky":3712,"speak":3713,"broken":3714,"charge":3715,"knowledge":3716,"kilometres":3717,"removed":3718,"ships":3719,"article":3720,"campus":3721,"simple":3722,"##ty":3723,"pushed":3724,"britain":3725,"##ve":3726,"leaves":3727,"recently":3728,"cd":3729,"soft":3730,"boston":3731,"latter":3732,"easy":3733,"acquired":3734,"poland":3735,"##sa":3736,"quality":3737,"officers":3738,"presence":3739,"planned":3740,"nations":3741,"mass":3742,"broadcast":3743,"jean":3744,"share":3745,"image":3746,"influence":3747,"wild":3748,"offer":3749,"emperor":3750,"electric":3751,"reading":3752,"headed":3753,"ability":3754,"promoted":3755,"yellow":3756,"ministry":3757,"1942":3758,"throat":3759,"smaller":3760,"politician":3761,"##by":3762,"latin":3763,"spoke":3764,"cars":3765,"williams":3766,"males":3767,"lack":3768,"pop":3769,"80":3770,"##ier":3771,"acting":3772,"seeing":3773,"consists":3774,"##ti":3775,"estate":3776,"1961":3777,"pressure":3778,"johnson":3779,"newspaper":3780,"jr":3781,"chris":3782,"olympics":3783,"online":3784,"conditions":3785,"beat":3786,"elements":3787,"walking":3788,"vote":3789,"##field":3790,"needs":3791,"carolina":3792,"text":3793,"featuring":3794,"global":3795,"block":3796,"shirt":3797,"levels":3798,"francisco":3799,"purpose":3800,"females":3801,"et":3802,"dutch":3803,"duke":3804,"ahead":3805,"gas":3806,"twice":3807,"safety":3808,"serious":3809,"turning":3810,"highly":3811,"lieutenant":3812,"firm":3813,"maria":3814,"amount":3815,"mixed":3816,"daniel":3817,"proposed":3818,"perfect":3819,"agreement":3820,"affairs":3821,"3rd":3822,"seconds":3823,"contemporary":3824,"paid":3825,"1943":3826,"prison":3827,"save":3828,"kitchen":3829,"label":3830,"administrative":3831,"intended":3832,"constructed":3833,"academic":3834,"nice":3835,"teacher":3836,"races":3837,"1956":3838,"formerly":3839,"corporation":3840,"ben":3841,"nation":3842,"issued":3843,"shut":3844,"1958":3845,"drums":3846,"housing":3847,"victoria":3848,"seems":3849,"opera":3850,"1959":3851,"graduated":3852,"function":3853,"von":3854,"mentioned":3855,"picked":3856,"build":3857,"recognized":3858,"shortly":3859,"protection":3860,"picture":3861,"notable":3862,"exchange":3863,"elections":3864,"1980s":3865,"loved":3866,"percent":3867,"racing":3868,"fish":3869,"elizabeth":3870,"garden":3871,"volume":3872,"hockey":3873,"1941":3874,"beside":3875,"settled":3876,"##ford":3877,"1940":3878,"competed":3879,"replied":3880,"drew":3881,"1948":3882,"actress":3883,"marine":3884,"scotland":3885,"steel":3886,"glanced":3887,"farm":3888,"steve":3889,"1957":3890,"risk":3891,"tonight":3892,"positive":3893,"magic":3894,"singles":3895,"effects":3896,"gray":3897,"screen":3898,"dog":3899,"##ja":3900,"residents":3901,"bus":3902,"sides":3903,"none":3904,"secondary":3905,"literature":3906,"polish":3907,"destroyed":3908,"flying":3909,"founder":3910,"households":3911,"1939":3912,"lay":3913,"reserve":3914,"usa":3915,"gallery":3916,"##ler":3917,"1946":3918,"industrial":3919,"younger":3920,"approach":3921,"appearances":3922,"urban":3923,"ones":3924,"1950":3925,"finish":3926,"avenue":3927,"powerful":3928,"fully":3929,"growth":3930,"page":3931,"honor":3932,"jersey":3933,"projects":3934,"advanced":3935,"revealed":3936,"basic":3937,"90":3938,"infantry":3939,"pair":3940,"equipment":3941,"visit":3942,"33":3943,"evening":3944,"search":3945,"grant":3946,"effort":3947,"solo":3948,"treatment":3949,"buried":3950,"republican":3951,"primarily":3952,"bottom":3953,"owner":3954,"1970s":3955,"israel":3956,"gives":3957,"jim":3958,"dream":3959,"bob":3960,"remain":3961,"spot":3962,"70":3963,"notes":3964,"produce":3965,"champions":3966,"contact":3967,"ed":3968,"soul":3969,"accepted":3970,"ways":3971,"del":3972,"##ally":3973,"losing":3974,"split":3975,"price":3976,"capacity":3977,"basis":3978,"trial":3979,"questions":3980,"##ina":3981,"1955":3982,"20th":3983,"guess":3984,"officially":3985,"memorial":3986,"naval":3987,"initial":3988,"##ization":3989,"whispered":3990,"median":3991,"engineer":3992,"##ful":3993,"sydney":3994,"##go":3995,"columbia":3996,"strength":3997,"300":3998,"1952":3999,"tears":4000,"senate":4001,"00":4002,"card":4003,"asian":4004,"agent":4005,"1947":4006,"software":4007,"44":4008,"draw":4009,"warm":4010,"supposed":4011,"com":4012,"pro":4013,"##il":4014,"transferred":4015,"leaned":4016,"##at":4017,"candidate":4018,"escape":4019,"mountains":4020,"asia":4021,"potential":4022,"activity":4023,"entertainment":4024,"seem":4025,"traffic":4026,"jackson":4027,"murder":4028,"36":4029,"slow":4030,"product":4031,"orchestra":4032,"haven":4033,"agency":4034,"bbc":4035,"taught":4036,"website":4037,"comedy":4038,"unable":4039,"storm":4040,"planning":4041,"albums":4042,"rugby":4043,"environment":4044,"scientific":4045,"grabbed":4046,"protect":4047,"##hi":4048,"boat":4049,"typically":4050,"1954":4051,"1953":4052,"damage":4053,"principal":4054,"divided":4055,"dedicated":4056,"mount":4057,"ohio":4058,"##berg":4059,"pick":4060,"fought":4061,"driver":4062,"##der":4063,"empty":4064,"shoulders":4065,"sort":4066,"thank":4067,"berlin":4068,"prominent":4069,"account":4070,"freedom":4071,"necessary":4072,"efforts":4073,"alex":4074,"headquarters":4075,"follows":4076,"alongside":4077,"des":4078,"simon":4079,"andrew":4080,"suggested":4081,"operating":4082,"learning":4083,"steps":4084,"1949":4085,"sweet":4086,"technical":4087,"begin":4088,"easily":4089,"34":4090,"teeth":4091,"speaking":4092,"settlement":4093,"scale":4094,"##sh":4095,"renamed":4096,"ray":4097,"max":4098,"enemy":4099,"semi":4100,"joint":4101,"compared":4102,"##rd":4103,"scottish":4104,"leadership":4105,"analysis":4106,"offers":4107,"georgia":4108,"pieces":4109,"captured":4110,"animal":4111,"deputy":4112,"guest":4113,"organized":4114,"##lin":4115,"tony":4116,"combined":4117,"method":4118,"challenge":4119,"1960s":4120,"huge":4121,"wants":4122,"battalion":4123,"sons":4124,"rise":4125,"crime":4126,"types":4127,"facilities":4128,"telling":4129,"path":4130,"1951":4131,"platform":4132,"sit":4133,"1990s":4134,"##lo":4135,"tells":4136,"assigned":4137,"rich":4138,"pull":4139,"##ot":4140,"commonly":4141,"alive":4142,"##za":4143,"letters":4144,"concept":4145,"conducted":4146,"wearing":4147,"happen":4148,"bought":4149,"becomes":4150,"holy":4151,"gets":4152,"ocean":4153,"defeat":4154,"languages":4155,"purchased":4156,"coffee":4157,"occurred":4158,"titled":4159,"##q":4160,"declared":4161,"applied":4162,"sciences":4163,"concert":4164,"sounds":4165,"jazz":4166,"brain":4167,"##me":4168,"painting":4169,"fleet":4170,"tax":4171,"nick":4172,"##ius":4173,"michigan":4174,"count":4175,"animals":4176,"leaders":4177,"episodes":4178,"##line":4179,"content":4180,"##den":4181,"birth":4182,"##it":4183,"clubs":4184,"64":4185,"palace":4186,"critical":4187,"refused":4188,"fair":4189,"leg":4190,"laughed":4191,"returning":4192,"surrounding":4193,"participated":4194,"formation":4195,"lifted":4196,"pointed":4197,"connected":4198,"rome":4199,"medicine":4200,"laid":4201,"taylor":4202,"santa":4203,"powers":4204,"adam":4205,"tall":4206,"shared":4207,"focused":4208,"knowing":4209,"yards":4210,"entrance":4211,"falls":4212,"##wa":4213,"calling":4214,"##ad":4215,"sources":4216,"chosen":4217,"beneath":4218,"resources":4219,"yard":4220,"##ite":4221,"nominated":4222,"silence":4223,"zone":4224,"defined":4225,"##que":4226,"gained":4227,"thirty":4228,"38":4229,"bodies":4230,"moon":4231,"##ard":4232,"adopted":4233,"christmas":4234,"widely":4235,"register":4236,"apart":4237,"iran":4238,"premier":4239,"serves":4240,"du":4241,"unknown":4242,"parties":4243,"##les":4244,"generation":4245,"##ff":4246,"continues":4247,"quick":4248,"fields":4249,"brigade":4250,"quiet":4251,"teaching":4252,"clothes":4253,"impact":4254,"weapons":4255,"partner":4256,"flat":4257,"theater":4258,"supreme":4259,"1938":4260,"37":4261,"relations":4262,"##tor":4263,"plants":4264,"suffered":4265,"1936":4266,"wilson":4267,"kids":4268,"begins":4269,"##age":4270,"1918":4271,"seats":4272,"armed":4273,"internet":4274,"models":4275,"worth":4276,"laws":4277,"400":4278,"communities":4279,"classes":4280,"background":4281,"knows":4282,"thanks":4283,"quarter":4284,"reaching":4285,"humans":4286,"carry":4287,"killing":4288,"format":4289,"kong":4290,"hong":4291,"setting":4292,"75":4293,"architecture":4294,"disease":4295,"railroad":4296,"inc":4297,"possibly":4298,"wish":4299,"arthur":4300,"thoughts":4301,"harry":4302,"doors":4303,"density":4304,"##di":4305,"crowd":4306,"illinois":4307,"stomach":4308,"tone":4309,"unique":4310,"reports":4311,"anyway":4312,"##ir":4313,"liberal":4314,"der":4315,"vehicle":4316,"thick":4317,"dry":4318,"drug":4319,"faced":4320,"largely":4321,"facility":4322,"theme":4323,"holds":4324,"creation":4325,"strange":4326,"colonel":4327,"##mi":4328,"revolution":4329,"bell":4330,"politics":4331,"turns":4332,"silent":4333,"rail":4334,"relief":4335,"independence":4336,"combat":4337,"shape":4338,"write":4339,"determined":4340,"sales":4341,"learned":4342,"4th":4343,"finger":4344,"oxford":4345,"providing":4346,"1937":4347,"heritage":4348,"fiction":4349,"situated":4350,"designated":4351,"allowing":4352,"distribution":4353,"hosted":4354,"##est":4355,"sight":4356,"interview":4357,"estimated":4358,"reduced":4359,"##ria":4360,"toronto":4361,"footballer":4362,"keeping":4363,"guys":4364,"damn":4365,"claim":4366,"motion":4367,"sport":4368,"sixth":4369,"stayed":4370,"##ze":4371,"en":4372,"rear":4373,"receive":4374,"handed":4375,"twelve":4376,"dress":4377,"audience":4378,"granted":4379,"brazil":4380,"##well":4381,"spirit":4382,"##ated":4383,"noticed":4384,"etc":4385,"olympic":4386,"representative":4387,"eric":4388,"tight":4389,"trouble":4390,"reviews":4391,"drink":4392,"vampire":4393,"missing":4394,"roles":4395,"ranked":4396,"newly":4397,"household":4398,"finals":4399,"wave":4400,"critics":4401,"##ee":4402,"phase":4403,"massachusetts":4404,"pilot":4405,"unlike":4406,"philadelphia":4407,"bright":4408,"guns":4409,"crown":4410,"organizations":4411,"roof":4412,"42":4413,"respectively":4414,"clearly":4415,"tongue":4416,"marked":4417,"circle":4418,"fox":4419,"korea":4420,"bronze":4421,"brian":4422,"expanded":4423,"sexual":4424,"supply":4425,"yourself":4426,"inspired":4427,"labour":4428,"fc":4429,"##ah":4430,"reference":4431,"vision":4432,"draft":4433,"connection":4434,"brand":4435,"reasons":4436,"1935":4437,"classic":4438,"driving":4439,"trip":4440,"jesus":4441,"cells":4442,"entry":4443,"1920":4444,"neither":4445,"trail":4446,"claims":4447,"atlantic":4448,"orders":4449,"labor":4450,"nose":4451,"afraid":4452,"identified":4453,"intelligence":4454,"calls":4455,"cancer":4456,"attacked":4457,"passing":4458,"stephen":4459,"positions":4460,"imperial":4461,"grey":4462,"jason":4463,"39":4464,"sunday":4465,"48":4466,"swedish":4467,"avoid":4468,"extra":4469,"uncle":4470,"message":4471,"covers":4472,"allows":4473,"surprise":4474,"materials":4475,"fame":4476,"hunter":4477,"##ji":4478,"1930":4479,"citizens":4480,"figures":4481,"davis":4482,"environmental":4483,"confirmed":4484,"shit":4485,"titles":4486,"di":4487,"performing":4488,"difference":4489,"acts":4490,"attacks":4491,"##ov":4492,"existing":4493,"votes":4494,"opportunity":4495,"nor":4496,"shop":4497,"entirely":4498,"trains":4499,"opposite":4500,"pakistan":4501,"##pa":4502,"develop":4503,"resulted":4504,"representatives":4505,"actions":4506,"reality":4507,"pressed":4508,"##ish":4509,"barely":4510,"wine":4511,"conversation":4512,"faculty":4513,"northwest":4514,"ends":4515,"documentary":4516,"nuclear":4517,"stock":4518,"grace":4519,"sets":4520,"eat":4521,"alternative":4522,"##ps":4523,"bag":4524,"resulting":4525,"creating":4526,"surprised":4527,"cemetery":4528,"1919":4529,"drop":4530,"finding":4531,"sarah":4532,"cricket":4533,"streets":4534,"tradition":4535,"ride":4536,"1933":4537,"exhibition":4538,"target":4539,"ear":4540,"explained":4541,"rain":4542,"composer":4543,"injury":4544,"apartment":4545,"municipal":4546,"educational":4547,"occupied":4548,"netherlands":4549,"clean":4550,"billion":4551,"constitution":4552,"learn":4553,"1914":4554,"maximum":4555,"classical":4556,"francis":4557,"lose":4558,"opposition":4559,"jose":4560,"ontario":4561,"bear":4562,"core":4563,"hills":4564,"rolled":4565,"ending":4566,"drawn":4567,"permanent":4568,"fun":4569,"##tes":4570,"##lla":4571,"lewis":4572,"sites":4573,"chamber":4574,"ryan":4575,"##way":4576,"scoring":4577,"height":4578,"1934":4579,"##house":4580,"lyrics":4581,"staring":4582,"55":4583,"officials":4584,"1917":4585,"snow":4586,"oldest":4587,"##tic":4588,"orange":4589,"##ger":4590,"qualified":4591,"interior":4592,"apparently":4593,"succeeded":4594,"thousand":4595,"dinner":4596,"lights":4597,"existence":4598,"fans":4599,"heavily":4600,"41":4601,"greatest":4602,"conservative":4603,"send":4604,"bowl":4605,"plus":4606,"enter":4607,"catch":4608,"##un":4609,"economy":4610,"duty":4611,"1929":4612,"speech":4613,"authorities":4614,"princess":4615,"performances":4616,"versions":4617,"shall":4618,"graduate":4619,"pictures":4620,"effective":4621,"remembered":4622,"poetry":4623,"desk":4624,"crossed":4625,"starring":4626,"starts":4627,"passenger":4628,"sharp":4629,"##ant":4630,"acres":4631,"ass":4632,"weather":4633,"falling":4634,"rank":4635,"fund":4636,"supporting":4637,"check":4638,"adult":4639,"publishing":4640,"heads":4641,"cm":4642,"southeast":4643,"lane":4644,"##burg":4645,"application":4646,"bc":4647,"##ura":4648,"les":4649,"condition":4650,"transfer":4651,"prevent":4652,"display":4653,"ex":4654,"regions":4655,"earl":4656,"federation":4657,"cool":4658,"relatively":4659,"answered":4660,"besides":4661,"1928":4662,"obtained":4663,"portion":4664,"##town":4665,"mix":4666,"##ding":4667,"reaction":4668,"liked":4669,"dean":4670,"express":4671,"peak":4672,"1932":4673,"##tte":4674,"counter":4675,"religion":4676,"chain":4677,"rare":4678,"miller":4679,"convention":4680,"aid":4681,"lie":4682,"vehicles":4683,"mobile":4684,"perform":4685,"squad":4686,"wonder":4687,"lying":4688,"crazy":4689,"sword":4690,"##ping":4691,"attempted":4692,"centuries":4693,"weren":4694,"philosophy":4695,"category":4696,"##ize":4697,"anna":4698,"interested":4699,"47":4700,"sweden":4701,"wolf":4702,"frequently":4703,"abandoned":4704,"kg":4705,"literary":4706,"alliance":4707,"task":4708,"entitled":4709,"##ay":4710,"threw":4711,"promotion":4712,"factory":4713,"tiny":4714,"soccer":4715,"visited":4716,"matt":4717,"fm":4718,"achieved":4719,"52":4720,"defence":4721,"internal":4722,"persian":4723,"43":4724,"methods":4725,"##ging":4726,"arrested":4727,"otherwise":4728,"cambridge":4729,"programming":4730,"villages":4731,"elementary":4732,"districts":4733,"rooms":4734,"criminal":4735,"conflict":4736,"worry":4737,"trained":4738,"1931":4739,"attempts":4740,"waited":4741,"signal":4742,"bird":4743,"truck":4744,"subsequent":4745,"programme":4746,"##ol":4747,"ad":4748,"49":4749,"communist":4750,"details":4751,"faith":4752,"sector":4753,"patrick":4754,"carrying":4755,"laugh":4756,"##ss":4757,"controlled":4758,"korean":4759,"showing":4760,"origin":4761,"fuel":4762,"evil":4763,"1927":4764,"##ent":4765,"brief":4766,"identity":4767,"darkness":4768,"address":4769,"pool":4770,"missed":4771,"publication":4772,"web":4773,"planet":4774,"ian":4775,"anne":4776,"wings":4777,"invited":4778,"##tt":4779,"briefly":4780,"standards":4781,"kissed":4782,"##be":4783,"ideas":4784,"climate":4785,"causing":4786,"walter":4787,"worse":4788,"albert":4789,"articles":4790,"winners":4791,"desire":4792,"aged":4793,"northeast":4794,"dangerous":4795,"gate":4796,"doubt":4797,"1922":4798,"wooden":4799,"multi":4800,"##ky":4801,"poet":4802,"rising":4803,"funding":4804,"46":4805,"communications":4806,"communication":4807,"violence":4808,"copies":4809,"prepared":4810,"ford":4811,"investigation":4812,"skills":4813,"1924":4814,"pulling":4815,"electronic":4816,"##ak":4817,"##ial":4818,"##han":4819,"containing":4820,"ultimately":4821,"offices":4822,"singing":4823,"understanding":4824,"restaurant":4825,"tomorrow":4826,"fashion":4827,"christ":4828,"ward":4829,"da":4830,"pope":4831,"stands":4832,"5th":4833,"flow":4834,"studios":4835,"aired":4836,"commissioned":4837,"contained":4838,"exist":4839,"fresh":4840,"americans":4841,"##per":4842,"wrestling":4843,"approved":4844,"kid":4845,"employed":4846,"respect":4847,"suit":4848,"1925":4849,"angel":4850,"asking":4851,"increasing":4852,"frame":4853,"angry":4854,"selling":4855,"1950s":4856,"thin":4857,"finds":4858,"##nd":4859,"temperature":4860,"statement":4861,"ali":4862,"explain":4863,"inhabitants":4864,"towns":4865,"extensive":4866,"narrow":4867,"51":4868,"jane":4869,"flowers":4870,"images":4871,"promise":4872,"somewhere":4873,"object":4874,"fly":4875,"closely":4876,"##ls":4877,"1912":4878,"bureau":4879,"cape":4880,"1926":4881,"weekly":4882,"presidential":4883,"legislative":4884,"1921":4885,"##ai":4886,"##au":4887,"launch":4888,"founding":4889,"##ny":4890,"978":4891,"##ring":4892,"artillery":4893,"strike":4894,"un":4895,"institutions":4896,"roll":4897,"writers":4898,"landing":4899,"chose":4900,"kevin":4901,"anymore":4902,"pp":4903,"##ut":4904,"attorney":4905,"fit":4906,"dan":4907,"billboard":4908,"receiving":4909,"agricultural":4910,"breaking":4911,"sought":4912,"dave":4913,"admitted":4914,"lands":4915,"mexican":4916,"##bury":4917,"charlie":4918,"specifically":4919,"hole":4920,"iv":4921,"howard":4922,"credit":4923,"moscow":4924,"roads":4925,"accident":4926,"1923":4927,"proved":4928,"wear":4929,"struck":4930,"hey":4931,"guards":4932,"stuff":4933,"slid":4934,"expansion":4935,"1915":4936,"cat":4937,"anthony":4938,"##kin":4939,"melbourne":4940,"opposed":4941,"sub":4942,"southwest":4943,"architect":4944,"failure":4945,"plane":4946,"1916":4947,"##ron":4948,"map":4949,"camera":4950,"tank":4951,"listen":4952,"regarding":4953,"wet":4954,"introduction":4955,"metropolitan":4956,"link":4957,"ep":4958,"fighter":4959,"inch":4960,"grown":4961,"gene":4962,"anger":4963,"fixed":4964,"buy":4965,"dvd":4966,"khan":4967,"domestic":4968,"worldwide":4969,"chapel":4970,"mill":4971,"functions":4972,"examples":4973,"##head":4974,"developing":4975,"1910":4976,"turkey":4977,"hits":4978,"pocket":4979,"antonio":4980,"papers":4981,"grow":4982,"unless":4983,"circuit":4984,"18th":4985,"concerned":4986,"attached":4987,"journalist":4988,"selection":4989,"journey":4990,"converted":4991,"provincial":4992,"painted":4993,"hearing":4994,"aren":4995,"bands":4996,"negative":4997,"aside":4998,"wondered":4999,"knight":5000,"lap":5001,"survey":5002,"ma":5003,"##ow":5004,"noise":5005,"billy":5006,"##ium":5007,"shooting":5008,"guide":5009,"bedroom":5010,"priest":5011,"resistance":5012,"motor":5013,"homes":5014,"sounded":5015,"giant":5016,"##mer":5017,"150":5018,"scenes":5019,"equal":5020,"comic":5021,"patients":5022,"hidden":5023,"solid":5024,"actual":5025,"bringing":5026,"afternoon":5027,"touched":5028,"funds":5029,"wedding":5030,"consisted":5031,"marie":5032,"canal":5033,"sr":5034,"kim":5035,"treaty":5036,"turkish":5037,"recognition":5038,"residence":5039,"cathedral":5040,"broad":5041,"knees":5042,"incident":5043,"shaped":5044,"fired":5045,"norwegian":5046,"handle":5047,"cheek":5048,"contest":5049,"represent":5050,"##pe":5051,"representing":5052,"beauty":5053,"##sen":5054,"birds":5055,"advantage":5056,"emergency":5057,"wrapped":5058,"drawing":5059,"notice":5060,"pink":5061,"broadcasting":5062,"##ong":5063,"somehow":5064,"bachelor":5065,"seventh":5066,"collected":5067,"registered":5068,"establishment":5069,"alan":5070,"assumed":5071,"chemical":5072,"personnel":5073,"roger":5074,"retirement":5075,"jeff":5076,"portuguese":5077,"wore":5078,"tied":5079,"device":5080,"threat":5081,"progress":5082,"advance":5083,"##ised":5084,"banks":5085,"hired":5086,"manchester":5087,"nfl":5088,"teachers":5089,"structures":5090,"forever":5091,"##bo":5092,"tennis":5093,"helping":5094,"saturday":5095,"sale":5096,"applications":5097,"junction":5098,"hip":5099,"incorporated":5100,"neighborhood":5101,"dressed":5102,"ceremony":5103,"##ds":5104,"influenced":5105,"hers":5106,"visual":5107,"stairs":5108,"decades":5109,"inner":5110,"kansas":5111,"hung":5112,"hoped":5113,"gain":5114,"scheduled":5115,"downtown":5116,"engaged":5117,"austria":5118,"clock":5119,"norway":5120,"certainly":5121,"pale":5122,"protected":5123,"1913":5124,"victor":5125,"employees":5126,"plate":5127,"putting":5128,"surrounded":5129,"##ists":5130,"finishing":5131,"blues":5132,"tropical":5133,"##ries":5134,"minnesota":5135,"consider":5136,"philippines":5137,"accept":5138,"54":5139,"retrieved":5140,"1900":5141,"concern":5142,"anderson":5143,"properties":5144,"institution":5145,"gordon":5146,"successfully":5147,"vietnam":5148,"##dy":5149,"backing":5150,"outstanding":5151,"muslim":5152,"crossing":5153,"folk":5154,"producing":5155,"usual":5156,"demand":5157,"occurs":5158,"observed":5159,"lawyer":5160,"educated":5161,"##ana":5162,"kelly":5163,"string":5164,"pleasure":5165,"budget":5166,"items":5167,"quietly":5168,"colorado":5169,"philip":5170,"typical":5171,"##worth":5172,"derived":5173,"600":5174,"survived":5175,"asks":5176,"mental":5177,"##ide":5178,"56":5179,"jake":5180,"jews":5181,"distinguished":5182,"ltd":5183,"1911":5184,"sri":5185,"extremely":5186,"53":5187,"athletic":5188,"loud":5189,"thousands":5190,"worried":5191,"shadow":5192,"transportation":5193,"horses":5194,"weapon":5195,"arena":5196,"importance":5197,"users":5198,"tim":5199,"objects":5200,"contributed":5201,"dragon":5202,"douglas":5203,"aware":5204,"senator":5205,"johnny":5206,"jordan":5207,"sisters":5208,"engines":5209,"flag":5210,"investment":5211,"samuel":5212,"shock":5213,"capable":5214,"clark":5215,"row":5216,"wheel":5217,"refers":5218,"session":5219,"familiar":5220,"biggest":5221,"wins":5222,"hate":5223,"maintained":5224,"drove":5225,"hamilton":5226,"request":5227,"expressed":5228,"injured":5229,"underground":5230,"churches":5231,"walker":5232,"wars":5233,"tunnel":5234,"passes":5235,"stupid":5236,"agriculture":5237,"softly":5238,"cabinet":5239,"regarded":5240,"joining":5241,"indiana":5242,"##ea":5243,"##ms":5244,"push":5245,"dates":5246,"spend":5247,"behavior":5248,"woods":5249,"protein":5250,"gently":5251,"chase":5252,"morgan":5253,"mention":5254,"burning":5255,"wake":5256,"combination":5257,"occur":5258,"mirror":5259,"leads":5260,"jimmy":5261,"indeed":5262,"impossible":5263,"singapore":5264,"paintings":5265,"covering":5266,"##nes":5267,"soldier":5268,"locations":5269,"attendance":5270,"sell":5271,"historian":5272,"wisconsin":5273,"invasion":5274,"argued":5275,"painter":5276,"diego":5277,"changing":5278,"egypt":5279,"##don":5280,"experienced":5281,"inches":5282,"##ku":5283,"missouri":5284,"vol":5285,"grounds":5286,"spoken":5287,"switzerland":5288,"##gan":5289,"reform":5290,"rolling":5291,"ha":5292,"forget":5293,"massive":5294,"resigned":5295,"burned":5296,"allen":5297,"tennessee":5298,"locked":5299,"values":5300,"improved":5301,"##mo":5302,"wounded":5303,"universe":5304,"sick":5305,"dating":5306,"facing":5307,"pack":5308,"purchase":5309,"user":5310,"##pur":5311,"moments":5312,"##ul":5313,"merged":5314,"anniversary":5315,"1908":5316,"coal":5317,"brick":5318,"understood":5319,"causes":5320,"dynasty":5321,"queensland":5322,"establish":5323,"stores":5324,"crisis":5325,"promote":5326,"hoping":5327,"views":5328,"cards":5329,"referee":5330,"extension":5331,"##si":5332,"raise":5333,"arizona":5334,"improve":5335,"colonial":5336,"formal":5337,"charged":5338,"##rt":5339,"palm":5340,"lucky":5341,"hide":5342,"rescue":5343,"faces":5344,"95":5345,"feelings":5346,"candidates":5347,"juan":5348,"##ell":5349,"goods":5350,"6th":5351,"courses":5352,"weekend":5353,"59":5354,"luke":5355,"cash":5356,"fallen":5357,"##om":5358,"delivered":5359,"affected":5360,"installed":5361,"carefully":5362,"tries":5363,"swiss":5364,"hollywood":5365,"costs":5366,"lincoln":5367,"responsibility":5368,"##he":5369,"shore":5370,"file":5371,"proper":5372,"normally":5373,"maryland":5374,"assistance":5375,"jump":5376,"constant":5377,"offering":5378,"friendly":5379,"waters":5380,"persons":5381,"realize":5382,"contain":5383,"trophy":5384,"800":5385,"partnership":5386,"factor":5387,"58":5388,"musicians":5389,"cry":5390,"bound":5391,"oregon":5392,"indicated":5393,"hero":5394,"houston":5395,"medium":5396,"##ure":5397,"consisting":5398,"somewhat":5399,"##ara":5400,"57":5401,"cycle":5402,"##che":5403,"beer":5404,"moore":5405,"frederick":5406,"gotten":5407,"eleven":5408,"worst":5409,"weak":5410,"approached":5411,"arranged":5412,"chin":5413,"loan":5414,"universal":5415,"bond":5416,"fifteen":5417,"pattern":5418,"disappeared":5419,"##ney":5420,"translated":5421,"##zed":5422,"lip":5423,"arab":5424,"capture":5425,"interests":5426,"insurance":5427,"##chi":5428,"shifted":5429,"cave":5430,"prix":5431,"warning":5432,"sections":5433,"courts":5434,"coat":5435,"plot":5436,"smell":5437,"feed":5438,"golf":5439,"favorite":5440,"maintain":5441,"knife":5442,"vs":5443,"voted":5444,"degrees":5445,"finance":5446,"quebec":5447,"opinion":5448,"translation":5449,"manner":5450,"ruled":5451,"operate":5452,"productions":5453,"choose":5454,"musician":5455,"discovery":5456,"confused":5457,"tired":5458,"separated":5459,"stream":5460,"techniques":5461,"committed":5462,"attend":5463,"ranking":5464,"kings":5465,"throw":5466,"passengers":5467,"measure":5468,"horror":5469,"fan":5470,"mining":5471,"sand":5472,"danger":5473,"salt":5474,"calm":5475,"decade":5476,"dam":5477,"require":5478,"runner":5479,"##ik":5480,"rush":5481,"associate":5482,"greece":5483,"##ker":5484,"rivers":5485,"consecutive":5486,"matthew":5487,"##ski":5488,"sighed":5489,"sq":5490,"documents":5491,"steam":5492,"edited":5493,"closing":5494,"tie":5495,"accused":5496,"1905":5497,"##ini":5498,"islamic":5499,"distributed":5500,"directors":5501,"organisation":5502,"bruce":5503,"7th":5504,"breathing":5505,"mad":5506,"lit":5507,"arrival":5508,"concrete":5509,"taste":5510,"08":5511,"composition":5512,"shaking":5513,"faster":5514,"amateur":5515,"adjacent":5516,"stating":5517,"1906":5518,"twin":5519,"flew":5520,"##ran":5521,"tokyo":5522,"publications":5523,"##tone":5524,"obviously":5525,"ridge":5526,"storage":5527,"1907":5528,"carl":5529,"pages":5530,"concluded":5531,"desert":5532,"driven":5533,"universities":5534,"ages":5535,"terminal":5536,"sequence":5537,"borough":5538,"250":5539,"constituency":5540,"creative":5541,"cousin":5542,"economics":5543,"dreams":5544,"margaret":5545,"notably":5546,"reduce":5547,"montreal":5548,"mode":5549,"17th":5550,"ears":5551,"saved":5552,"jan":5553,"vocal":5554,"##ica":5555,"1909":5556,"andy":5557,"##jo":5558,"riding":5559,"roughly":5560,"threatened":5561,"##ise":5562,"meters":5563,"meanwhile":5564,"landed":5565,"compete":5566,"repeated":5567,"grass":5568,"czech":5569,"regularly":5570,"charges":5571,"tea":5572,"sudden":5573,"appeal":5574,"##ung":5575,"solution":5576,"describes":5577,"pierre":5578,"classification":5579,"glad":5580,"parking":5581,"##ning":5582,"belt":5583,"physics":5584,"99":5585,"rachel":5586,"add":5587,"hungarian":5588,"participate":5589,"expedition":5590,"damaged":5591,"gift":5592,"childhood":5593,"85":5594,"fifty":5595,"##red":5596,"mathematics":5597,"jumped":5598,"letting":5599,"defensive":5600,"mph":5601,"##ux":5602,"##gh":5603,"testing":5604,"##hip":5605,"hundreds":5606,"shoot":5607,"owners":5608,"matters":5609,"smoke":5610,"israeli":5611,"kentucky":5612,"dancing":5613,"mounted":5614,"grandfather":5615,"emma":5616,"designs":5617,"profit":5618,"argentina":5619,"##gs":5620,"truly":5621,"li":5622,"lawrence":5623,"cole":5624,"begun":5625,"detroit":5626,"willing":5627,"branches":5628,"smiling":5629,"decide":5630,"miami":5631,"enjoyed":5632,"recordings":5633,"##dale":5634,"poverty":5635,"ethnic":5636,"gay":5637,"##bi":5638,"gary":5639,"arabic":5640,"09":5641,"accompanied":5642,"##one":5643,"##ons":5644,"fishing":5645,"determine":5646,"residential":5647,"acid":5648,"##ary":5649,"alice":5650,"returns":5651,"starred":5652,"mail":5653,"##ang":5654,"jonathan":5655,"strategy":5656,"##ue":5657,"net":5658,"forty":5659,"cook":5660,"businesses":5661,"equivalent":5662,"commonwealth":5663,"distinct":5664,"ill":5665,"##cy":5666,"seriously":5667,"##ors":5668,"##ped":5669,"shift":5670,"harris":5671,"replace":5672,"rio":5673,"imagine":5674,"formula":5675,"ensure":5676,"##ber":5677,"additionally":5678,"scheme":5679,"conservation":5680,"occasionally":5681,"purposes":5682,"feels":5683,"favor":5684,"##and":5685,"##ore":5686,"1930s":5687,"contrast":5688,"hanging":5689,"hunt":5690,"movies":5691,"1904":5692,"instruments":5693,"victims":5694,"danish":5695,"christopher":5696,"busy":5697,"demon":5698,"sugar":5699,"earliest":5700,"colony":5701,"studying":5702,"balance":5703,"duties":5704,"##ks":5705,"belgium":5706,"slipped":5707,"carter":5708,"05":5709,"visible":5710,"stages":5711,"iraq":5712,"fifa":5713,"##im":5714,"commune":5715,"forming":5716,"zero":5717,"07":5718,"continuing":5719,"talked":5720,"counties":5721,"legend":5722,"bathroom":5723,"option":5724,"tail":5725,"clay":5726,"daughters":5727,"afterwards":5728,"severe":5729,"jaw":5730,"visitors":5731,"##ded":5732,"devices":5733,"aviation":5734,"russell":5735,"kate":5736,"##vi":5737,"entering":5738,"subjects":5739,"##ino":5740,"temporary":5741,"swimming":5742,"forth":5743,"smooth":5744,"ghost":5745,"audio":5746,"bush":5747,"operates":5748,"rocks":5749,"movements":5750,"signs":5751,"eddie":5752,"##tz":5753,"ann":5754,"voices":5755,"honorary":5756,"06":5757,"memories":5758,"dallas":5759,"pure":5760,"measures":5761,"racial":5762,"promised":5763,"66":5764,"harvard":5765,"ceo":5766,"16th":5767,"parliamentary":5768,"indicate":5769,"benefit":5770,"flesh":5771,"dublin":5772,"louisiana":5773,"1902":5774,"1901":5775,"patient":5776,"sleeping":5777,"1903":5778,"membership":5779,"coastal":5780,"medieval":5781,"wanting":5782,"element":5783,"scholars":5784,"rice":5785,"62":5786,"limit":5787,"survive":5788,"makeup":5789,"rating":5790,"definitely":5791,"collaboration":5792,"obvious":5793,"##tan":5794,"boss":5795,"ms":5796,"baron":5797,"birthday":5798,"linked":5799,"soil":5800,"diocese":5801,"##lan":5802,"ncaa":5803,"##mann":5804,"offensive":5805,"shell":5806,"shouldn":5807,"waist":5808,"##tus":5809,"plain":5810,"ross":5811,"organ":5812,"resolution":5813,"manufacturing":5814,"adding":5815,"relative":5816,"kennedy":5817,"98":5818,"whilst":5819,"moth":5820,"marketing":5821,"gardens":5822,"crash":5823,"72":5824,"heading":5825,"partners":5826,"credited":5827,"carlos":5828,"moves":5829,"cable":5830,"##zi":5831,"marshall":5832,"##out":5833,"depending":5834,"bottle":5835,"represents":5836,"rejected":5837,"responded":5838,"existed":5839,"04":5840,"jobs":5841,"denmark":5842,"lock":5843,"##ating":5844,"treated":5845,"graham":5846,"routes":5847,"talent":5848,"commissioner":5849,"drugs":5850,"secure":5851,"tests":5852,"reign":5853,"restored":5854,"photography":5855,"##gi":5856,"contributions":5857,"oklahoma":5858,"designer":5859,"disc":5860,"grin":5861,"seattle":5862,"robin":5863,"paused":5864,"atlanta":5865,"unusual":5866,"##gate":5867,"praised":5868,"las":5869,"laughing":5870,"satellite":5871,"hungary":5872,"visiting":5873,"##sky":5874,"interesting":5875,"factors":5876,"deck":5877,"poems":5878,"norman":5879,"##water":5880,"stuck":5881,"speaker":5882,"rifle":5883,"domain":5884,"premiered":5885,"##her":5886,"dc":5887,"comics":5888,"actors":5889,"01":5890,"reputation":5891,"eliminated":5892,"8th":5893,"ceiling":5894,"prisoners":5895,"script":5896,"##nce":5897,"leather":5898,"austin":5899,"mississippi":5900,"rapidly":5901,"admiral":5902,"parallel":5903,"charlotte":5904,"guilty":5905,"tools":5906,"gender":5907,"divisions":5908,"fruit":5909,"##bs":5910,"laboratory":5911,"nelson":5912,"fantasy":5913,"marry":5914,"rapid":5915,"aunt":5916,"tribe":5917,"requirements":5918,"aspects":5919,"suicide":5920,"amongst":5921,"adams":5922,"bone":5923,"ukraine":5924,"abc":5925,"kick":5926,"sees":5927,"edinburgh":5928,"clothing":5929,"column":5930,"rough":5931,"gods":5932,"hunting":5933,"broadway":5934,"gathered":5935,"concerns":5936,"##ek":5937,"spending":5938,"ty":5939,"12th":5940,"snapped":5941,"requires":5942,"solar":5943,"bones":5944,"cavalry":5945,"##tta":5946,"iowa":5947,"drinking":5948,"waste":5949,"index":5950,"franklin":5951,"charity":5952,"thompson":5953,"stewart":5954,"tip":5955,"flash":5956,"landscape":5957,"friday":5958,"enjoy":5959,"singh":5960,"poem":5961,"listening":5962,"##back":5963,"eighth":5964,"fred":5965,"differences":5966,"adapted":5967,"bomb":5968,"ukrainian":5969,"surgery":5970,"corporate":5971,"masters":5972,"anywhere":5973,"##more":5974,"waves":5975,"odd":5976,"sean":5977,"portugal":5978,"orleans":5979,"dick":5980,"debate":5981,"kent":5982,"eating":5983,"puerto":5984,"cleared":5985,"96":5986,"expect":5987,"cinema":5988,"97":5989,"guitarist":5990,"blocks":5991,"electrical":5992,"agree":5993,"involving":5994,"depth":5995,"dying":5996,"panel":5997,"struggle":5998,"##ged":5999,"peninsula":6000,"adults":6001,"novels":6002,"emerged":6003,"vienna":6004,"metro":6005,"debuted":6006,"shoes":6007,"tamil":6008,"songwriter":6009,"meets":6010,"prove":6011,"beating":6012,"instance":6013,"heaven":6014,"scared":6015,"sending":6016,"marks":6017,"artistic":6018,"passage":6019,"superior":6020,"03":6021,"significantly":6022,"shopping":6023,"##tive":6024,"retained":6025,"##izing":6026,"malaysia":6027,"technique":6028,"cheeks":6029,"##ola":6030,"warren":6031,"maintenance":6032,"destroy":6033,"extreme":6034,"allied":6035,"120":6036,"appearing":6037,"##yn":6038,"fill":6039,"advice":6040,"alabama":6041,"qualifying":6042,"policies":6043,"cleveland":6044,"hat":6045,"battery":6046,"smart":6047,"authors":6048,"10th":6049,"soundtrack":6050,"acted":6051,"dated":6052,"lb":6053,"glance":6054,"equipped":6055,"coalition":6056,"funny":6057,"outer":6058,"ambassador":6059,"roy":6060,"possibility":6061,"couples":6062,"campbell":6063,"dna":6064,"loose":6065,"ethan":6066,"supplies":6067,"1898":6068,"gonna":6069,"88":6070,"monster":6071,"##res":6072,"shake":6073,"agents":6074,"frequency":6075,"springs":6076,"dogs":6077,"practices":6078,"61":6079,"gang":6080,"plastic":6081,"easier":6082,"suggests":6083,"gulf":6084,"blade":6085,"exposed":6086,"colors":6087,"industries":6088,"markets":6089,"pan":6090,"nervous":6091,"electoral":6092,"charts":6093,"legislation":6094,"ownership":6095,"##idae":6096,"mac":6097,"appointment":6098,"shield":6099,"copy":6100,"assault":6101,"socialist":6102,"abbey":6103,"monument":6104,"license":6105,"throne":6106,"employment":6107,"jay":6108,"93":6109,"replacement":6110,"charter":6111,"cloud":6112,"powered":6113,"suffering":6114,"accounts":6115,"oak":6116,"connecticut":6117,"strongly":6118,"wright":6119,"colour":6120,"crystal":6121,"13th":6122,"context":6123,"welsh":6124,"networks":6125,"voiced":6126,"gabriel":6127,"jerry":6128,"##cing":6129,"forehead":6130,"mp":6131,"##ens":6132,"manage":6133,"schedule":6134,"totally":6135,"remix":6136,"##ii":6137,"forests":6138,"occupation":6139,"print":6140,"nicholas":6141,"brazilian":6142,"strategic":6143,"vampires":6144,"engineers":6145,"76":6146,"roots":6147,"seek":6148,"correct":6149,"instrumental":6150,"und":6151,"alfred":6152,"backed":6153,"hop":6154,"##des":6155,"stanley":6156,"robinson":6157,"traveled":6158,"wayne":6159,"welcome":6160,"austrian":6161,"achieve":6162,"67":6163,"exit":6164,"rates":6165,"1899":6166,"strip":6167,"whereas":6168,"##cs":6169,"sing":6170,"deeply":6171,"adventure":6172,"bobby":6173,"rick":6174,"jamie":6175,"careful":6176,"components":6177,"cap":6178,"useful":6179,"personality":6180,"knee":6181,"##shi":6182,"pushing":6183,"hosts":6184,"02":6185,"protest":6186,"ca":6187,"ottoman":6188,"symphony":6189,"##sis":6190,"63":6191,"boundary":6192,"1890":6193,"processes":6194,"considering":6195,"considerable":6196,"tons":6197,"##work":6198,"##ft":6199,"##nia":6200,"cooper":6201,"trading":6202,"dear":6203,"conduct":6204,"91":6205,"illegal":6206,"apple":6207,"revolutionary":6208,"holiday":6209,"definition":6210,"harder":6211,"##van":6212,"jacob":6213,"circumstances":6214,"destruction":6215,"##lle":6216,"popularity":6217,"grip":6218,"classified":6219,"liverpool":6220,"donald":6221,"baltimore":6222,"flows":6223,"seeking":6224,"honour":6225,"approval":6226,"92":6227,"mechanical":6228,"till":6229,"happening":6230,"statue":6231,"critic":6232,"increasingly":6233,"immediate":6234,"describe":6235,"commerce":6236,"stare":6237,"##ster":6238,"indonesia":6239,"meat":6240,"rounds":6241,"boats":6242,"baker":6243,"orthodox":6244,"depression":6245,"formally":6246,"worn":6247,"naked":6248,"claire":6249,"muttered":6250,"sentence":6251,"11th":6252,"emily":6253,"document":6254,"77":6255,"criticism":6256,"wished":6257,"vessel":6258,"spiritual":6259,"bent":6260,"virgin":6261,"parker":6262,"minimum":6263,"murray":6264,"lunch":6265,"danny":6266,"printed":6267,"compilation":6268,"keyboards":6269,"false":6270,"blow":6271,"belonged":6272,"68":6273,"raising":6274,"78":6275,"cutting":6276,"##board":6277,"pittsburgh":6278,"##up":6279,"9th":6280,"shadows":6281,"81":6282,"hated":6283,"indigenous":6284,"jon":6285,"15th":6286,"barry":6287,"scholar":6288,"ah":6289,"##zer":6290,"oliver":6291,"##gy":6292,"stick":6293,"susan":6294,"meetings":6295,"attracted":6296,"spell":6297,"romantic":6298,"##ver":6299,"ye":6300,"1895":6301,"photo":6302,"demanded":6303,"customers":6304,"##ac":6305,"1896":6306,"logan":6307,"revival":6308,"keys":6309,"modified":6310,"commanded":6311,"jeans":6312,"##ious":6313,"upset":6314,"raw":6315,"phil":6316,"detective":6317,"hiding":6318,"resident":6319,"vincent":6320,"##bly":6321,"experiences":6322,"diamond":6323,"defeating":6324,"coverage":6325,"lucas":6326,"external":6327,"parks":6328,"franchise":6329,"helen":6330,"bible":6331,"successor":6332,"percussion":6333,"celebrated":6334,"il":6335,"lift":6336,"profile":6337,"clan":6338,"romania":6339,"##ied":6340,"mills":6341,"##su":6342,"nobody":6343,"achievement":6344,"shrugged":6345,"fault":6346,"1897":6347,"rhythm":6348,"initiative":6349,"breakfast":6350,"carbon":6351,"700":6352,"69":6353,"lasted":6354,"violent":6355,"74":6356,"wound":6357,"ken":6358,"killer":6359,"gradually":6360,"filmed":6361,"°c":6362,"dollars":6363,"processing":6364,"94":6365,"remove":6366,"criticized":6367,"guests":6368,"sang":6369,"chemistry":6370,"##vin":6371,"legislature":6372,"disney":6373,"##bridge":6374,"uniform":6375,"escaped":6376,"integrated":6377,"proposal":6378,"purple":6379,"denied":6380,"liquid":6381,"karl":6382,"influential":6383,"morris":6384,"nights":6385,"stones":6386,"intense":6387,"experimental":6388,"twisted":6389,"71":6390,"84":6391,"##ld":6392,"pace":6393,"nazi":6394,"mitchell":6395,"ny":6396,"blind":6397,"reporter":6398,"newspapers":6399,"14th":6400,"centers":6401,"burn":6402,"basin":6403,"forgotten":6404,"surviving":6405,"filed":6406,"collections":6407,"monastery":6408,"losses":6409,"manual":6410,"couch":6411,"description":6412,"appropriate":6413,"merely":6414,"tag":6415,"missions":6416,"sebastian":6417,"restoration":6418,"replacing":6419,"triple":6420,"73":6421,"elder":6422,"julia":6423,"warriors":6424,"benjamin":6425,"julian":6426,"convinced":6427,"stronger":6428,"amazing":6429,"declined":6430,"versus":6431,"merchant":6432,"happens":6433,"output":6434,"finland":6435,"bare":6436,"barbara":6437,"absence":6438,"ignored":6439,"dawn":6440,"injuries":6441,"##port":6442,"producers":6443,"##ram":6444,"82":6445,"luis":6446,"##ities":6447,"kw":6448,"admit":6449,"expensive":6450,"electricity":6451,"nba":6452,"exception":6453,"symbol":6454,"##ving":6455,"ladies":6456,"shower":6457,"sheriff":6458,"characteristics":6459,"##je":6460,"aimed":6461,"button":6462,"ratio":6463,"effectively":6464,"summit":6465,"angle":6466,"jury":6467,"bears":6468,"foster":6469,"vessels":6470,"pants":6471,"executed":6472,"evans":6473,"dozen":6474,"advertising":6475,"kicked":6476,"patrol":6477,"1889":6478,"competitions":6479,"lifetime":6480,"principles":6481,"athletics":6482,"##logy":6483,"birmingham":6484,"sponsored":6485,"89":6486,"rob":6487,"nomination":6488,"1893":6489,"acoustic":6490,"##sm":6491,"creature":6492,"longest":6493,"##tra":6494,"credits":6495,"harbor":6496,"dust":6497,"josh":6498,"##so":6499,"territories":6500,"milk":6501,"infrastructure":6502,"completion":6503,"thailand":6504,"indians":6505,"leon":6506,"archbishop":6507,"##sy":6508,"assist":6509,"pitch":6510,"blake":6511,"arrangement":6512,"girlfriend":6513,"serbian":6514,"operational":6515,"hence":6516,"sad":6517,"scent":6518,"fur":6519,"dj":6520,"sessions":6521,"hp":6522,"refer":6523,"rarely":6524,"##ora":6525,"exists":6526,"1892":6527,"##ten":6528,"scientists":6529,"dirty":6530,"penalty":6531,"burst":6532,"portrait":6533,"seed":6534,"79":6535,"pole":6536,"limits":6537,"rival":6538,"1894":6539,"stable":6540,"alpha":6541,"grave":6542,"constitutional":6543,"alcohol":6544,"arrest":6545,"flower":6546,"mystery":6547,"devil":6548,"architectural":6549,"relationships":6550,"greatly":6551,"habitat":6552,"##istic":6553,"larry":6554,"progressive":6555,"remote":6556,"cotton":6557,"##ics":6558,"##ok":6559,"preserved":6560,"reaches":6561,"##ming":6562,"cited":6563,"86":6564,"vast":6565,"scholarship":6566,"decisions":6567,"cbs":6568,"joy":6569,"teach":6570,"1885":6571,"editions":6572,"knocked":6573,"eve":6574,"searching":6575,"partly":6576,"participation":6577,"gap":6578,"animated":6579,"fate":6580,"excellent":6581,"##ett":6582,"na":6583,"87":6584,"alternate":6585,"saints":6586,"youngest":6587,"##ily":6588,"climbed":6589,"##ita":6590,"##tors":6591,"suggest":6592,"##ct":6593,"discussion":6594,"staying":6595,"choir":6596,"lakes":6597,"jacket":6598,"revenue":6599,"nevertheless":6600,"peaked":6601,"instrument":6602,"wondering":6603,"annually":6604,"managing":6605,"neil":6606,"1891":6607,"signing":6608,"terry":6609,"##ice":6610,"apply":6611,"clinical":6612,"brooklyn":6613,"aim":6614,"catherine":6615,"fuck":6616,"farmers":6617,"figured":6618,"ninth":6619,"pride":6620,"hugh":6621,"evolution":6622,"ordinary":6623,"involvement":6624,"comfortable":6625,"shouted":6626,"tech":6627,"encouraged":6628,"taiwan":6629,"representation":6630,"sharing":6631,"##lia":6632,"##em":6633,"panic":6634,"exact":6635,"cargo":6636,"competing":6637,"fat":6638,"cried":6639,"83":6640,"1920s":6641,"occasions":6642,"pa":6643,"cabin":6644,"borders":6645,"utah":6646,"marcus":6647,"##isation":6648,"badly":6649,"muscles":6650,"##ance":6651,"victorian":6652,"transition":6653,"warner":6654,"bet":6655,"permission":6656,"##rin":6657,"slave":6658,"terrible":6659,"similarly":6660,"shares":6661,"seth":6662,"uefa":6663,"possession":6664,"medals":6665,"benefits":6666,"colleges":6667,"lowered":6668,"perfectly":6669,"mall":6670,"transit":6671,"##ye":6672,"##kar":6673,"publisher":6674,"##ened":6675,"harrison":6676,"deaths":6677,"elevation":6678,"##ae":6679,"asleep":6680,"machines":6681,"sigh":6682,"ash":6683,"hardly":6684,"argument":6685,"occasion":6686,"parent":6687,"leo":6688,"decline":6689,"1888":6690,"contribution":6691,"##ua":6692,"concentration":6693,"1000":6694,"opportunities":6695,"hispanic":6696,"guardian":6697,"extent":6698,"emotions":6699,"hips":6700,"mason":6701,"volumes":6702,"bloody":6703,"controversy":6704,"diameter":6705,"steady":6706,"mistake":6707,"phoenix":6708,"identify":6709,"violin":6710,"##sk":6711,"departure":6712,"richmond":6713,"spin":6714,"funeral":6715,"enemies":6716,"1864":6717,"gear":6718,"literally":6719,"connor":6720,"random":6721,"sergeant":6722,"grab":6723,"confusion":6724,"1865":6725,"transmission":6726,"informed":6727,"op":6728,"leaning":6729,"sacred":6730,"suspended":6731,"thinks":6732,"gates":6733,"portland":6734,"luck":6735,"agencies":6736,"yours":6737,"hull":6738,"expert":6739,"muscle":6740,"layer":6741,"practical":6742,"sculpture":6743,"jerusalem":6744,"latest":6745,"lloyd":6746,"statistics":6747,"deeper":6748,"recommended":6749,"warrior":6750,"arkansas":6751,"mess":6752,"supports":6753,"greg":6754,"eagle":6755,"1880":6756,"recovered":6757,"rated":6758,"concerts":6759,"rushed":6760,"##ano":6761,"stops":6762,"eggs":6763,"files":6764,"premiere":6765,"keith":6766,"##vo":6767,"delhi":6768,"turner":6769,"pit":6770,"affair":6771,"belief":6772,"paint":6773,"##zing":6774,"mate":6775,"##ach":6776,"##ev":6777,"victim":6778,"##ology":6779,"withdrew":6780,"bonus":6781,"styles":6782,"fled":6783,"##ud":6784,"glasgow":6785,"technologies":6786,"funded":6787,"nbc":6788,"adaptation":6789,"##ata":6790,"portrayed":6791,"cooperation":6792,"supporters":6793,"judges":6794,"bernard":6795,"justin":6796,"hallway":6797,"ralph":6798,"##ick":6799,"graduating":6800,"controversial":6801,"distant":6802,"continental":6803,"spider":6804,"bite":6805,"##ho":6806,"recognize":6807,"intention":6808,"mixing":6809,"##ese":6810,"egyptian":6811,"bow":6812,"tourism":6813,"suppose":6814,"claiming":6815,"tiger":6816,"dominated":6817,"participants":6818,"vi":6819,"##ru":6820,"nurse":6821,"partially":6822,"tape":6823,"##rum":6824,"psychology":6825,"##rn":6826,"essential":6827,"touring":6828,"duo":6829,"voting":6830,"civilian":6831,"emotional":6832,"channels":6833,"##king":6834,"apparent":6835,"hebrew":6836,"1887":6837,"tommy":6838,"carrier":6839,"intersection":6840,"beast":6841,"hudson":6842,"##gar":6843,"##zo":6844,"lab":6845,"nova":6846,"bench":6847,"discuss":6848,"costa":6849,"##ered":6850,"detailed":6851,"behalf":6852,"drivers":6853,"unfortunately":6854,"obtain":6855,"##lis":6856,"rocky":6857,"##dae":6858,"siege":6859,"friendship":6860,"honey":6861,"##rian":6862,"1861":6863,"amy":6864,"hang":6865,"posted":6866,"governments":6867,"collins":6868,"respond":6869,"wildlife":6870,"preferred":6871,"operator":6872,"##po":6873,"laura":6874,"pregnant":6875,"videos":6876,"dennis":6877,"suspected":6878,"boots":6879,"instantly":6880,"weird":6881,"automatic":6882,"businessman":6883,"alleged":6884,"placing":6885,"throwing":6886,"ph":6887,"mood":6888,"1862":6889,"perry":6890,"venue":6891,"jet":6892,"remainder":6893,"##lli":6894,"##ci":6895,"passion":6896,"biological":6897,"boyfriend":6898,"1863":6899,"dirt":6900,"buffalo":6901,"ron":6902,"segment":6903,"fa":6904,"abuse":6905,"##era":6906,"genre":6907,"thrown":6908,"stroke":6909,"colored":6910,"stress":6911,"exercise":6912,"displayed":6913,"##gen":6914,"struggled":6915,"##tti":6916,"abroad":6917,"dramatic":6918,"wonderful":6919,"thereafter":6920,"madrid":6921,"component":6922,"widespread":6923,"##sed":6924,"tale":6925,"citizen":6926,"todd":6927,"monday":6928,"1886":6929,"vancouver":6930,"overseas":6931,"forcing":6932,"crying":6933,"descent":6934,"##ris":6935,"discussed":6936,"substantial":6937,"ranks":6938,"regime":6939,"1870":6940,"provinces":6941,"switch":6942,"drum":6943,"zane":6944,"ted":6945,"tribes":6946,"proof":6947,"lp":6948,"cream":6949,"researchers":6950,"volunteer":6951,"manor":6952,"silk":6953,"milan":6954,"donated":6955,"allies":6956,"venture":6957,"principle":6958,"delivery":6959,"enterprise":6960,"##ves":6961,"##ans":6962,"bars":6963,"traditionally":6964,"witch":6965,"reminded":6966,"copper":6967,"##uk":6968,"pete":6969,"inter":6970,"links":6971,"colin":6972,"grinned":6973,"elsewhere":6974,"competitive":6975,"frequent":6976,"##oy":6977,"scream":6978,"##hu":6979,"tension":6980,"texts":6981,"submarine":6982,"finnish":6983,"defending":6984,"defend":6985,"pat":6986,"detail":6987,"1884":6988,"affiliated":6989,"stuart":6990,"themes":6991,"villa":6992,"periods":6993,"tool":6994,"belgian":6995,"ruling":6996,"crimes":6997,"answers":6998,"folded":6999,"licensed":7000,"resort":7001,"demolished":7002,"hans":7003,"lucy":7004,"1881":7005,"lion":7006,"traded":7007,"photographs":7008,"writes":7009,"craig":7010,"##fa":7011,"trials":7012,"generated":7013,"beth":7014,"noble":7015,"debt":7016,"percentage":7017,"yorkshire":7018,"erected":7019,"ss":7020,"viewed":7021,"grades":7022,"confidence":7023,"ceased":7024,"islam":7025,"telephone":7026,"retail":7027,"##ible":7028,"chile":7029,"m²":7030,"roberts":7031,"sixteen":7032,"##ich":7033,"commented":7034,"hampshire":7035,"innocent":7036,"dual":7037,"pounds":7038,"checked":7039,"regulations":7040,"afghanistan":7041,"sung":7042,"rico":7043,"liberty":7044,"assets":7045,"bigger":7046,"options":7047,"angels":7048,"relegated":7049,"tribute":7050,"wells":7051,"attending":7052,"leaf":7053,"##yan":7054,"butler":7055,"romanian":7056,"forum":7057,"monthly":7058,"lisa":7059,"patterns":7060,"gmina":7061,"##tory":7062,"madison":7063,"hurricane":7064,"rev":7065,"##ians":7066,"bristol":7067,"##ula":7068,"elite":7069,"valuable":7070,"disaster":7071,"democracy":7072,"awareness":7073,"germans":7074,"freyja":7075,"##ins":7076,"loop":7077,"absolutely":7078,"paying":7079,"populations":7080,"maine":7081,"sole":7082,"prayer":7083,"spencer":7084,"releases":7085,"doorway":7086,"bull":7087,"##ani":7088,"lover":7089,"midnight":7090,"conclusion":7091,"##sson":7092,"thirteen":7093,"lily":7094,"mediterranean":7095,"##lt":7096,"nhl":7097,"proud":7098,"sample":7099,"##hill":7100,"drummer":7101,"guinea":7102,"##ova":7103,"murphy":7104,"climb":7105,"##ston":7106,"instant":7107,"attributed":7108,"horn":7109,"ain":7110,"railways":7111,"steven":7112,"##ao":7113,"autumn":7114,"ferry":7115,"opponent":7116,"root":7117,"traveling":7118,"secured":7119,"corridor":7120,"stretched":7121,"tales":7122,"sheet":7123,"trinity":7124,"cattle":7125,"helps":7126,"indicates":7127,"manhattan":7128,"murdered":7129,"fitted":7130,"1882":7131,"gentle":7132,"grandmother":7133,"mines":7134,"shocked":7135,"vegas":7136,"produces":7137,"##light":7138,"caribbean":7139,"##ou":7140,"belong":7141,"continuous":7142,"desperate":7143,"drunk":7144,"historically":7145,"trio":7146,"waved":7147,"raf":7148,"dealing":7149,"nathan":7150,"bat":7151,"murmured":7152,"interrupted":7153,"residing":7154,"scientist":7155,"pioneer":7156,"harold":7157,"aaron":7158,"##net":7159,"delta":7160,"attempting":7161,"minority":7162,"mini":7163,"believes":7164,"chorus":7165,"tend":7166,"lots":7167,"eyed":7168,"indoor":7169,"load":7170,"shots":7171,"updated":7172,"jail":7173,"##llo":7174,"concerning":7175,"connecting":7176,"wealth":7177,"##ved":7178,"slaves":7179,"arrive":7180,"rangers":7181,"sufficient":7182,"rebuilt":7183,"##wick":7184,"cardinal":7185,"flood":7186,"muhammad":7187,"whenever":7188,"relation":7189,"runners":7190,"moral":7191,"repair":7192,"viewers":7193,"arriving":7194,"revenge":7195,"punk":7196,"assisted":7197,"bath":7198,"fairly":7199,"breathe":7200,"lists":7201,"innings":7202,"illustrated":7203,"whisper":7204,"nearest":7205,"voters":7206,"clinton":7207,"ties":7208,"ultimate":7209,"screamed":7210,"beijing":7211,"lions":7212,"andre":7213,"fictional":7214,"gathering":7215,"comfort":7216,"radar":7217,"suitable":7218,"dismissed":7219,"hms":7220,"ban":7221,"pine":7222,"wrist":7223,"atmosphere":7224,"voivodeship":7225,"bid":7226,"timber":7227,"##ned":7228,"##nan":7229,"giants":7230,"##ane":7231,"cameron":7232,"recovery":7233,"uss":7234,"identical":7235,"categories":7236,"switched":7237,"serbia":7238,"laughter":7239,"noah":7240,"ensemble":7241,"therapy":7242,"peoples":7243,"touching":7244,"##off":7245,"locally":7246,"pearl":7247,"platforms":7248,"everywhere":7249,"ballet":7250,"tables":7251,"lanka":7252,"herbert":7253,"outdoor":7254,"toured":7255,"derek":7256,"1883":7257,"spaces":7258,"contested":7259,"swept":7260,"1878":7261,"exclusive":7262,"slight":7263,"connections":7264,"##dra":7265,"winds":7266,"prisoner":7267,"collective":7268,"bangladesh":7269,"tube":7270,"publicly":7271,"wealthy":7272,"thai":7273,"##ys":7274,"isolated":7275,"select":7276,"##ric":7277,"insisted":7278,"pen":7279,"fortune":7280,"ticket":7281,"spotted":7282,"reportedly":7283,"animation":7284,"enforcement":7285,"tanks":7286,"110":7287,"decides":7288,"wider":7289,"lowest":7290,"owen":7291,"##time":7292,"nod":7293,"hitting":7294,"##hn":7295,"gregory":7296,"furthermore":7297,"magazines":7298,"fighters":7299,"solutions":7300,"##ery":7301,"pointing":7302,"requested":7303,"peru":7304,"reed":7305,"chancellor":7306,"knights":7307,"mask":7308,"worker":7309,"eldest":7310,"flames":7311,"reduction":7312,"1860":7313,"volunteers":7314,"##tis":7315,"reporting":7316,"##hl":7317,"wire":7318,"advisory":7319,"endemic":7320,"origins":7321,"settlers":7322,"pursue":7323,"knock":7324,"consumer":7325,"1876":7326,"eu":7327,"compound":7328,"creatures":7329,"mansion":7330,"sentenced":7331,"ivan":7332,"deployed":7333,"guitars":7334,"frowned":7335,"involves":7336,"mechanism":7337,"kilometers":7338,"perspective":7339,"shops":7340,"maps":7341,"terminus":7342,"duncan":7343,"alien":7344,"fist":7345,"bridges":7346,"##pers":7347,"heroes":7348,"fed":7349,"derby":7350,"swallowed":7351,"##ros":7352,"patent":7353,"sara":7354,"illness":7355,"characterized":7356,"adventures":7357,"slide":7358,"hawaii":7359,"jurisdiction":7360,"##op":7361,"organised":7362,"##side":7363,"adelaide":7364,"walks":7365,"biology":7366,"se":7367,"##ties":7368,"rogers":7369,"swing":7370,"tightly":7371,"boundaries":7372,"##rie":7373,"prepare":7374,"implementation":7375,"stolen":7376,"##sha":7377,"certified":7378,"colombia":7379,"edwards":7380,"garage":7381,"##mm":7382,"recalled":7383,"##ball":7384,"rage":7385,"harm":7386,"nigeria":7387,"breast":7388,"##ren":7389,"furniture":7390,"pupils":7391,"settle":7392,"##lus":7393,"cuba":7394,"balls":7395,"client":7396,"alaska":7397,"21st":7398,"linear":7399,"thrust":7400,"celebration":7401,"latino":7402,"genetic":7403,"terror":7404,"##cia":7405,"##ening":7406,"lightning":7407,"fee":7408,"witness":7409,"lodge":7410,"establishing":7411,"skull":7412,"##ique":7413,"earning":7414,"hood":7415,"##ei":7416,"rebellion":7417,"wang":7418,"sporting":7419,"warned":7420,"missile":7421,"devoted":7422,"activist":7423,"porch":7424,"worship":7425,"fourteen":7426,"package":7427,"1871":7428,"decorated":7429,"##shire":7430,"housed":7431,"##ock":7432,"chess":7433,"sailed":7434,"doctors":7435,"oscar":7436,"joan":7437,"treat":7438,"garcia":7439,"harbour":7440,"jeremy":7441,"##ire":7442,"traditions":7443,"dominant":7444,"jacques":7445,"##gon":7446,"##wan":7447,"relocated":7448,"1879":7449,"amendment":7450,"sized":7451,"companion":7452,"simultaneously":7453,"volleyball":7454,"spun":7455,"acre":7456,"increases":7457,"stopping":7458,"loves":7459,"belongs":7460,"affect":7461,"drafted":7462,"tossed":7463,"scout":7464,"battles":7465,"1875":7466,"filming":7467,"shoved":7468,"munich":7469,"tenure":7470,"vertical":7471,"romance":7472,"pc":7473,"##cher":7474,"argue":7475,"##ical":7476,"craft":7477,"ranging":7478,"www":7479,"opens":7480,"honest":7481,"tyler":7482,"yesterday":7483,"virtual":7484,"##let":7485,"muslims":7486,"reveal":7487,"snake":7488,"immigrants":7489,"radical":7490,"screaming":7491,"speakers":7492,"firing":7493,"saving":7494,"belonging":7495,"ease":7496,"lighting":7497,"prefecture":7498,"blame":7499,"farmer":7500,"hungry":7501,"grows":7502,"rubbed":7503,"beam":7504,"sur":7505,"subsidiary":7506,"##cha":7507,"armenian":7508,"sao":7509,"dropping":7510,"conventional":7511,"##fer":7512,"microsoft":7513,"reply":7514,"qualify":7515,"spots":7516,"1867":7517,"sweat":7518,"festivals":7519,"##ken":7520,"immigration":7521,"physician":7522,"discover":7523,"exposure":7524,"sandy":7525,"explanation":7526,"isaac":7527,"implemented":7528,"##fish":7529,"hart":7530,"initiated":7531,"connect":7532,"stakes":7533,"presents":7534,"heights":7535,"householder":7536,"pleased":7537,"tourist":7538,"regardless":7539,"slip":7540,"closest":7541,"##ction":7542,"surely":7543,"sultan":7544,"brings":7545,"riley":7546,"preparation":7547,"aboard":7548,"slammed":7549,"baptist":7550,"experiment":7551,"ongoing":7552,"interstate":7553,"organic":7554,"playoffs":7555,"##ika":7556,"1877":7557,"130":7558,"##tar":7559,"hindu":7560,"error":7561,"tours":7562,"tier":7563,"plenty":7564,"arrangements":7565,"talks":7566,"trapped":7567,"excited":7568,"sank":7569,"ho":7570,"athens":7571,"1872":7572,"denver":7573,"welfare":7574,"suburb":7575,"athletes":7576,"trick":7577,"diverse":7578,"belly":7579,"exclusively":7580,"yelled":7581,"1868":7582,"##med":7583,"conversion":7584,"##ette":7585,"1874":7586,"internationally":7587,"computers":7588,"conductor":7589,"abilities":7590,"sensitive":7591,"hello":7592,"dispute":7593,"measured":7594,"globe":7595,"rocket":7596,"prices":7597,"amsterdam":7598,"flights":7599,"tigers":7600,"inn":7601,"municipalities":7602,"emotion":7603,"references":7604,"3d":7605,"##mus":7606,"explains":7607,"airlines":7608,"manufactured":7609,"pm":7610,"archaeological":7611,"1873":7612,"interpretation":7613,"devon":7614,"comment":7615,"##ites":7616,"settlements":7617,"kissing":7618,"absolute":7619,"improvement":7620,"suite":7621,"impressed":7622,"barcelona":7623,"sullivan":7624,"jefferson":7625,"towers":7626,"jesse":7627,"julie":7628,"##tin":7629,"##lu":7630,"grandson":7631,"hi":7632,"gauge":7633,"regard":7634,"rings":7635,"interviews":7636,"trace":7637,"raymond":7638,"thumb":7639,"departments":7640,"burns":7641,"serial":7642,"bulgarian":7643,"scores":7644,"demonstrated":7645,"##ix":7646,"1866":7647,"kyle":7648,"alberta":7649,"underneath":7650,"romanized":7651,"##ward":7652,"relieved":7653,"acquisition":7654,"phrase":7655,"cliff":7656,"reveals":7657,"han":7658,"cuts":7659,"merger":7660,"custom":7661,"##dar":7662,"nee":7663,"gilbert":7664,"graduation":7665,"##nts":7666,"assessment":7667,"cafe":7668,"difficulty":7669,"demands":7670,"swung":7671,"democrat":7672,"jennifer":7673,"commons":7674,"1940s":7675,"grove":7676,"##yo":7677,"completing":7678,"focuses":7679,"sum":7680,"substitute":7681,"bearing":7682,"stretch":7683,"reception":7684,"##py":7685,"reflected":7686,"essentially":7687,"destination":7688,"pairs":7689,"##ched":7690,"survival":7691,"resource":7692,"##bach":7693,"promoting":7694,"doubles":7695,"messages":7696,"tear":7697,"##down":7698,"##fully":7699,"parade":7700,"florence":7701,"harvey":7702,"incumbent":7703,"partial":7704,"framework":7705,"900":7706,"pedro":7707,"frozen":7708,"procedure":7709,"olivia":7710,"controls":7711,"##mic":7712,"shelter":7713,"personally":7714,"temperatures":7715,"##od":7716,"brisbane":7717,"tested":7718,"sits":7719,"marble":7720,"comprehensive":7721,"oxygen":7722,"leonard":7723,"##kov":7724,"inaugural":7725,"iranian":7726,"referring":7727,"quarters":7728,"attitude":7729,"##ivity":7730,"mainstream":7731,"lined":7732,"mars":7733,"dakota":7734,"norfolk":7735,"unsuccessful":7736,"##°":7737,"explosion":7738,"helicopter":7739,"congressional":7740,"##sing":7741,"inspector":7742,"bitch":7743,"seal":7744,"departed":7745,"divine":7746,"##ters":7747,"coaching":7748,"examination":7749,"punishment":7750,"manufacturer":7751,"sink":7752,"columns":7753,"unincorporated":7754,"signals":7755,"nevada":7756,"squeezed":7757,"dylan":7758,"dining":7759,"photos":7760,"martial":7761,"manuel":7762,"eighteen":7763,"elevator":7764,"brushed":7765,"plates":7766,"ministers":7767,"ivy":7768,"congregation":7769,"##len":7770,"slept":7771,"specialized":7772,"taxes":7773,"curve":7774,"restricted":7775,"negotiations":7776,"likes":7777,"statistical":7778,"arnold":7779,"inspiration":7780,"execution":7781,"bold":7782,"intermediate":7783,"significance":7784,"margin":7785,"ruler":7786,"wheels":7787,"gothic":7788,"intellectual":7789,"dependent":7790,"listened":7791,"eligible":7792,"buses":7793,"widow":7794,"syria":7795,"earn":7796,"cincinnati":7797,"collapsed":7798,"recipient":7799,"secrets":7800,"accessible":7801,"philippine":7802,"maritime":7803,"goddess":7804,"clerk":7805,"surrender":7806,"breaks":7807,"playoff":7808,"database":7809,"##ified":7810,"##lon":7811,"ideal":7812,"beetle":7813,"aspect":7814,"soap":7815,"regulation":7816,"strings":7817,"expand":7818,"anglo":7819,"shorter":7820,"crosses":7821,"retreat":7822,"tough":7823,"coins":7824,"wallace":7825,"directions":7826,"pressing":7827,"##oon":7828,"shipping":7829,"locomotives":7830,"comparison":7831,"topics":7832,"nephew":7833,"##mes":7834,"distinction":7835,"honors":7836,"travelled":7837,"sierra":7838,"ibn":7839,"##over":7840,"fortress":7841,"sa":7842,"recognised":7843,"carved":7844,"1869":7845,"clients":7846,"##dan":7847,"intent":7848,"##mar":7849,"coaches":7850,"describing":7851,"bread":7852,"##ington":7853,"beaten":7854,"northwestern":7855,"##ona":7856,"merit":7857,"youtube":7858,"collapse":7859,"challenges":7860,"em":7861,"historians":7862,"objective":7863,"submitted":7864,"virus":7865,"attacking":7866,"drake":7867,"assume":7868,"##ere":7869,"diseases":7870,"marc":7871,"stem":7872,"leeds":7873,"##cus":7874,"##ab":7875,"farming":7876,"glasses":7877,"##lock":7878,"visits":7879,"nowhere":7880,"fellowship":7881,"relevant":7882,"carries":7883,"restaurants":7884,"experiments":7885,"101":7886,"constantly":7887,"bases":7888,"targets":7889,"shah":7890,"tenth":7891,"opponents":7892,"verse":7893,"territorial":7894,"##ira":7895,"writings":7896,"corruption":7897,"##hs":7898,"instruction":7899,"inherited":7900,"reverse":7901,"emphasis":7902,"##vic":7903,"employee":7904,"arch":7905,"keeps":7906,"rabbi":7907,"watson":7908,"payment":7909,"uh":7910,"##ala":7911,"nancy":7912,"##tre":7913,"venice":7914,"fastest":7915,"sexy":7916,"banned":7917,"adrian":7918,"properly":7919,"ruth":7920,"touchdown":7921,"dollar":7922,"boards":7923,"metre":7924,"circles":7925,"edges":7926,"favour":7927,"comments":7928,"ok":7929,"travels":7930,"liberation":7931,"scattered":7932,"firmly":7933,"##ular":7934,"holland":7935,"permitted":7936,"diesel":7937,"kenya":7938,"den":7939,"originated":7940,"##ral":7941,"demons":7942,"resumed":7943,"dragged":7944,"rider":7945,"##rus":7946,"servant":7947,"blinked":7948,"extend":7949,"torn":7950,"##ias":7951,"##sey":7952,"input":7953,"meal":7954,"everybody":7955,"cylinder":7956,"kinds":7957,"camps":7958,"##fe":7959,"bullet":7960,"logic":7961,"##wn":7962,"croatian":7963,"evolved":7964,"healthy":7965,"fool":7966,"chocolate":7967,"wise":7968,"preserve":7969,"pradesh":7970,"##ess":7971,"respective":7972,"1850":7973,"##ew":7974,"chicken":7975,"artificial":7976,"gross":7977,"corresponding":7978,"convicted":7979,"cage":7980,"caroline":7981,"dialogue":7982,"##dor":7983,"narrative":7984,"stranger":7985,"mario":7986,"br":7987,"christianity":7988,"failing":7989,"trent":7990,"commanding":7991,"buddhist":7992,"1848":7993,"maurice":7994,"focusing":7995,"yale":7996,"bike":7997,"altitude":7998,"##ering":7999,"mouse":8000,"revised":8001,"##sley":8002,"veteran":8003,"##ig":8004,"pulls":8005,"theology":8006,"crashed":8007,"campaigns":8008,"legion":8009,"##ability":8010,"drag":8011,"excellence":8012,"customer":8013,"cancelled":8014,"intensity":8015,"excuse":8016,"##lar":8017,"liga":8018,"participating":8019,"contributing":8020,"printing":8021,"##burn":8022,"variable":8023,"##rk":8024,"curious":8025,"bin":8026,"legacy":8027,"renaissance":8028,"##my":8029,"symptoms":8030,"binding":8031,"vocalist":8032,"dancer":8033,"##nie":8034,"grammar":8035,"gospel":8036,"democrats":8037,"ya":8038,"enters":8039,"sc":8040,"diplomatic":8041,"hitler":8042,"##ser":8043,"clouds":8044,"mathematical":8045,"quit":8046,"defended":8047,"oriented":8048,"##heim":8049,"fundamental":8050,"hardware":8051,"impressive":8052,"equally":8053,"convince":8054,"confederate":8055,"guilt":8056,"chuck":8057,"sliding":8058,"##ware":8059,"magnetic":8060,"narrowed":8061,"petersburg":8062,"bulgaria":8063,"otto":8064,"phd":8065,"skill":8066,"##ama":8067,"reader":8068,"hopes":8069,"pitcher":8070,"reservoir":8071,"hearts":8072,"automatically":8073,"expecting":8074,"mysterious":8075,"bennett":8076,"extensively":8077,"imagined":8078,"seeds":8079,"monitor":8080,"fix":8081,"##ative":8082,"journalism":8083,"struggling":8084,"signature":8085,"ranch":8086,"encounter":8087,"photographer":8088,"observation":8089,"protests":8090,"##pin":8091,"influences":8092,"##hr":8093,"calendar":8094,"##all":8095,"cruz":8096,"croatia":8097,"locomotive":8098,"hughes":8099,"naturally":8100,"shakespeare":8101,"basement":8102,"hook":8103,"uncredited":8104,"faded":8105,"theories":8106,"approaches":8107,"dare":8108,"phillips":8109,"filling":8110,"fury":8111,"obama":8112,"##ain":8113,"efficient":8114,"arc":8115,"deliver":8116,"min":8117,"raid":8118,"breeding":8119,"inducted":8120,"leagues":8121,"efficiency":8122,"axis":8123,"montana":8124,"eagles":8125,"##ked":8126,"supplied":8127,"instructions":8128,"karen":8129,"picking":8130,"indicating":8131,"trap":8132,"anchor":8133,"practically":8134,"christians":8135,"tomb":8136,"vary":8137,"occasional":8138,"electronics":8139,"lords":8140,"readers":8141,"newcastle":8142,"faint":8143,"innovation":8144,"collect":8145,"situations":8146,"engagement":8147,"160":8148,"claude":8149,"mixture":8150,"##feld":8151,"peer":8152,"tissue":8153,"logo":8154,"lean":8155,"##ration":8156,"°f":8157,"floors":8158,"##ven":8159,"architects":8160,"reducing":8161,"##our":8162,"##ments":8163,"rope":8164,"1859":8165,"ottawa":8166,"##har":8167,"samples":8168,"banking":8169,"declaration":8170,"proteins":8171,"resignation":8172,"francois":8173,"saudi":8174,"advocate":8175,"exhibited":8176,"armor":8177,"twins":8178,"divorce":8179,"##ras":8180,"abraham":8181,"reviewed":8182,"jo":8183,"temporarily":8184,"matrix":8185,"physically":8186,"pulse":8187,"curled":8188,"##ena":8189,"difficulties":8190,"bengal":8191,"usage":8192,"##ban":8193,"annie":8194,"riders":8195,"certificate":8196,"##pi":8197,"holes":8198,"warsaw":8199,"distinctive":8200,"jessica":8201,"##mon":8202,"mutual":8203,"1857":8204,"customs":8205,"circular":8206,"eugene":8207,"removal":8208,"loaded":8209,"mere":8210,"vulnerable":8211,"depicted":8212,"generations":8213,"dame":8214,"heir":8215,"enormous":8216,"lightly":8217,"climbing":8218,"pitched":8219,"lessons":8220,"pilots":8221,"nepal":8222,"ram":8223,"google":8224,"preparing":8225,"brad":8226,"louise":8227,"renowned":8228,"##₂":8229,"liam":8230,"##ably":8231,"plaza":8232,"shaw":8233,"sophie":8234,"brilliant":8235,"bills":8236,"##bar":8237,"##nik":8238,"fucking":8239,"mainland":8240,"server":8241,"pleasant":8242,"seized":8243,"veterans":8244,"jerked":8245,"fail":8246,"beta":8247,"brush":8248,"radiation":8249,"stored":8250,"warmth":8251,"southeastern":8252,"nate":8253,"sin":8254,"raced":8255,"berkeley":8256,"joke":8257,"athlete":8258,"designation":8259,"trunk":8260,"##low":8261,"roland":8262,"qualification":8263,"archives":8264,"heels":8265,"artwork":8266,"receives":8267,"judicial":8268,"reserves":8269,"##bed":8270,"woke":8271,"installation":8272,"abu":8273,"floating":8274,"fake":8275,"lesser":8276,"excitement":8277,"interface":8278,"concentrated":8279,"addressed":8280,"characteristic":8281,"amanda":8282,"saxophone":8283,"monk":8284,"auto":8285,"##bus":8286,"releasing":8287,"egg":8288,"dies":8289,"interaction":8290,"defender":8291,"ce":8292,"outbreak":8293,"glory":8294,"loving":8295,"##bert":8296,"sequel":8297,"consciousness":8298,"http":8299,"awake":8300,"ski":8301,"enrolled":8302,"##ress":8303,"handling":8304,"rookie":8305,"brow":8306,"somebody":8307,"biography":8308,"warfare":8309,"amounts":8310,"contracts":8311,"presentation":8312,"fabric":8313,"dissolved":8314,"challenged":8315,"meter":8316,"psychological":8317,"lt":8318,"elevated":8319,"rally":8320,"accurate":8321,"##tha":8322,"hospitals":8323,"undergraduate":8324,"specialist":8325,"venezuela":8326,"exhibit":8327,"shed":8328,"nursing":8329,"protestant":8330,"fluid":8331,"structural":8332,"footage":8333,"jared":8334,"consistent":8335,"prey":8336,"##ska":8337,"succession":8338,"reflect":8339,"exile":8340,"lebanon":8341,"wiped":8342,"suspect":8343,"shanghai":8344,"resting":8345,"integration":8346,"preservation":8347,"marvel":8348,"variant":8349,"pirates":8350,"sheep":8351,"rounded":8352,"capita":8353,"sailing":8354,"colonies":8355,"manuscript":8356,"deemed":8357,"variations":8358,"clarke":8359,"functional":8360,"emerging":8361,"boxing":8362,"relaxed":8363,"curse":8364,"azerbaijan":8365,"heavyweight":8366,"nickname":8367,"editorial":8368,"rang":8369,"grid":8370,"tightened":8371,"earthquake":8372,"flashed":8373,"miguel":8374,"rushing":8375,"##ches":8376,"improvements":8377,"boxes":8378,"brooks":8379,"180":8380,"consumption":8381,"molecular":8382,"felix":8383,"societies":8384,"repeatedly":8385,"variation":8386,"aids":8387,"civic":8388,"graphics":8389,"professionals":8390,"realm":8391,"autonomous":8392,"receiver":8393,"delayed":8394,"workshop":8395,"militia":8396,"chairs":8397,"trump":8398,"canyon":8399,"##point":8400,"harsh":8401,"extending":8402,"lovely":8403,"happiness":8404,"##jan":8405,"stake":8406,"eyebrows":8407,"embassy":8408,"wellington":8409,"hannah":8410,"##ella":8411,"sony":8412,"corners":8413,"bishops":8414,"swear":8415,"cloth":8416,"contents":8417,"xi":8418,"namely":8419,"commenced":8420,"1854":8421,"stanford":8422,"nashville":8423,"courage":8424,"graphic":8425,"commitment":8426,"garrison":8427,"##bin":8428,"hamlet":8429,"clearing":8430,"rebels":8431,"attraction":8432,"literacy":8433,"cooking":8434,"ruins":8435,"temples":8436,"jenny":8437,"humanity":8438,"celebrate":8439,"hasn":8440,"freight":8441,"sixty":8442,"rebel":8443,"bastard":8444,"##art":8445,"newton":8446,"##ada":8447,"deer":8448,"##ges":8449,"##ching":8450,"smiles":8451,"delaware":8452,"singers":8453,"##ets":8454,"approaching":8455,"assists":8456,"flame":8457,"##ph":8458,"boulevard":8459,"barrel":8460,"planted":8461,"##ome":8462,"pursuit":8463,"##sia":8464,"consequences":8465,"posts":8466,"shallow":8467,"invitation":8468,"rode":8469,"depot":8470,"ernest":8471,"kane":8472,"rod":8473,"concepts":8474,"preston":8475,"topic":8476,"chambers":8477,"striking":8478,"blast":8479,"arrives":8480,"descendants":8481,"montgomery":8482,"ranges":8483,"worlds":8484,"##lay":8485,"##ari":8486,"span":8487,"chaos":8488,"praise":8489,"##ag":8490,"fewer":8491,"1855":8492,"sanctuary":8493,"mud":8494,"fbi":8495,"##ions":8496,"programmes":8497,"maintaining":8498,"unity":8499,"harper":8500,"bore":8501,"handsome":8502,"closure":8503,"tournaments":8504,"thunder":8505,"nebraska":8506,"linda":8507,"facade":8508,"puts":8509,"satisfied":8510,"argentine":8511,"dale":8512,"cork":8513,"dome":8514,"panama":8515,"##yl":8516,"1858":8517,"tasks":8518,"experts":8519,"##ates":8520,"feeding":8521,"equation":8522,"##las":8523,"##ida":8524,"##tu":8525,"engage":8526,"bryan":8527,"##ax":8528,"um":8529,"quartet":8530,"melody":8531,"disbanded":8532,"sheffield":8533,"blocked":8534,"gasped":8535,"delay":8536,"kisses":8537,"maggie":8538,"connects":8539,"##non":8540,"sts":8541,"poured":8542,"creator":8543,"publishers":8544,"##we":8545,"guided":8546,"ellis":8547,"extinct":8548,"hug":8549,"gaining":8550,"##ord":8551,"complicated":8552,"##bility":8553,"poll":8554,"clenched":8555,"investigate":8556,"##use":8557,"thereby":8558,"quantum":8559,"spine":8560,"cdp":8561,"humor":8562,"kills":8563,"administered":8564,"semifinals":8565,"##du":8566,"encountered":8567,"ignore":8568,"##bu":8569,"commentary":8570,"##maker":8571,"bother":8572,"roosevelt":8573,"140":8574,"plains":8575,"halfway":8576,"flowing":8577,"cultures":8578,"crack":8579,"imprisoned":8580,"neighboring":8581,"airline":8582,"##ses":8583,"##view":8584,"##mate":8585,"##ec":8586,"gather":8587,"wolves":8588,"marathon":8589,"transformed":8590,"##ill":8591,"cruise":8592,"organisations":8593,"carol":8594,"punch":8595,"exhibitions":8596,"numbered":8597,"alarm":8598,"ratings":8599,"daddy":8600,"silently":8601,"##stein":8602,"queens":8603,"colours":8604,"impression":8605,"guidance":8606,"liu":8607,"tactical":8608,"##rat":8609,"marshal":8610,"della":8611,"arrow":8612,"##ings":8613,"rested":8614,"feared":8615,"tender":8616,"owns":8617,"bitter":8618,"advisor":8619,"escort":8620,"##ides":8621,"spare":8622,"farms":8623,"grants":8624,"##ene":8625,"dragons":8626,"encourage":8627,"colleagues":8628,"cameras":8629,"##und":8630,"sucked":8631,"pile":8632,"spirits":8633,"prague":8634,"statements":8635,"suspension":8636,"landmark":8637,"fence":8638,"torture":8639,"recreation":8640,"bags":8641,"permanently":8642,"survivors":8643,"pond":8644,"spy":8645,"predecessor":8646,"bombing":8647,"coup":8648,"##og":8649,"protecting":8650,"transformation":8651,"glow":8652,"##lands":8653,"##book":8654,"dug":8655,"priests":8656,"andrea":8657,"feat":8658,"barn":8659,"jumping":8660,"##chen":8661,"##ologist":8662,"##con":8663,"casualties":8664,"stern":8665,"auckland":8666,"pipe":8667,"serie":8668,"revealing":8669,"ba":8670,"##bel":8671,"trevor":8672,"mercy":8673,"spectrum":8674,"yang":8675,"consist":8676,"governing":8677,"collaborated":8678,"possessed":8679,"epic":8680,"comprises":8681,"blew":8682,"shane":8683,"##ack":8684,"lopez":8685,"honored":8686,"magical":8687,"sacrifice":8688,"judgment":8689,"perceived":8690,"hammer":8691,"mtv":8692,"baronet":8693,"tune":8694,"das":8695,"missionary":8696,"sheets":8697,"350":8698,"neutral":8699,"oral":8700,"threatening":8701,"attractive":8702,"shade":8703,"aims":8704,"seminary":8705,"##master":8706,"estates":8707,"1856":8708,"michel":8709,"wounds":8710,"refugees":8711,"manufacturers":8712,"##nic":8713,"mercury":8714,"syndrome":8715,"porter":8716,"##iya":8717,"##din":8718,"hamburg":8719,"identification":8720,"upstairs":8721,"purse":8722,"widened":8723,"pause":8724,"cared":8725,"breathed":8726,"affiliate":8727,"santiago":8728,"prevented":8729,"celtic":8730,"fisher":8731,"125":8732,"recruited":8733,"byzantine":8734,"reconstruction":8735,"farther":8736,"##mp":8737,"diet":8738,"sake":8739,"au":8740,"spite":8741,"sensation":8742,"##ert":8743,"blank":8744,"separation":8745,"105":8746,"##hon":8747,"vladimir":8748,"armies":8749,"anime":8750,"##lie":8751,"accommodate":8752,"orbit":8753,"cult":8754,"sofia":8755,"archive":8756,"##ify":8757,"##box":8758,"founders":8759,"sustained":8760,"disorder":8761,"honours":8762,"northeastern":8763,"mia":8764,"crops":8765,"violet":8766,"threats":8767,"blanket":8768,"fires":8769,"canton":8770,"followers":8771,"southwestern":8772,"prototype":8773,"voyage":8774,"assignment":8775,"altered":8776,"moderate":8777,"protocol":8778,"pistol":8779,"##eo":8780,"questioned":8781,"brass":8782,"lifting":8783,"1852":8784,"math":8785,"authored":8786,"##ual":8787,"doug":8788,"dimensional":8789,"dynamic":8790,"##san":8791,"1851":8792,"pronounced":8793,"grateful":8794,"quest":8795,"uncomfortable":8796,"boom":8797,"presidency":8798,"stevens":8799,"relating":8800,"politicians":8801,"chen":8802,"barrier":8803,"quinn":8804,"diana":8805,"mosque":8806,"tribal":8807,"cheese":8808,"palmer":8809,"portions":8810,"sometime":8811,"chester":8812,"treasure":8813,"wu":8814,"bend":8815,"download":8816,"millions":8817,"reforms":8818,"registration":8819,"##osa":8820,"consequently":8821,"monitoring":8822,"ate":8823,"preliminary":8824,"brandon":8825,"invented":8826,"ps":8827,"eaten":8828,"exterior":8829,"intervention":8830,"ports":8831,"documented":8832,"log":8833,"displays":8834,"lecture":8835,"sally":8836,"favourite":8837,"##itz":8838,"vermont":8839,"lo":8840,"invisible":8841,"isle":8842,"breed":8843,"##ator":8844,"journalists":8845,"relay":8846,"speaks":8847,"backward":8848,"explore":8849,"midfielder":8850,"actively":8851,"stefan":8852,"procedures":8853,"cannon":8854,"blond":8855,"kenneth":8856,"centered":8857,"servants":8858,"chains":8859,"libraries":8860,"malcolm":8861,"essex":8862,"henri":8863,"slavery":8864,"##hal":8865,"facts":8866,"fairy":8867,"coached":8868,"cassie":8869,"cats":8870,"washed":8871,"cop":8872,"##fi":8873,"announcement":8874,"item":8875,"2000s":8876,"vinyl":8877,"activated":8878,"marco":8879,"frontier":8880,"growled":8881,"curriculum":8882,"##das":8883,"loyal":8884,"accomplished":8885,"leslie":8886,"ritual":8887,"kenny":8888,"##00":8889,"vii":8890,"napoleon":8891,"hollow":8892,"hybrid":8893,"jungle":8894,"stationed":8895,"friedrich":8896,"counted":8897,"##ulated":8898,"platinum":8899,"theatrical":8900,"seated":8901,"col":8902,"rubber":8903,"glen":8904,"1840":8905,"diversity":8906,"healing":8907,"extends":8908,"id":8909,"provisions":8910,"administrator":8911,"columbus":8912,"##oe":8913,"tributary":8914,"te":8915,"assured":8916,"org":8917,"##uous":8918,"prestigious":8919,"examined":8920,"lectures":8921,"grammy":8922,"ronald":8923,"associations":8924,"bailey":8925,"allan":8926,"essays":8927,"flute":8928,"believing":8929,"consultant":8930,"proceedings":8931,"travelling":8932,"1853":8933,"kit":8934,"kerala":8935,"yugoslavia":8936,"buddy":8937,"methodist":8938,"##ith":8939,"burial":8940,"centres":8941,"batman":8942,"##nda":8943,"discontinued":8944,"bo":8945,"dock":8946,"stockholm":8947,"lungs":8948,"severely":8949,"##nk":8950,"citing":8951,"manga":8952,"##ugh":8953,"steal":8954,"mumbai":8955,"iraqi":8956,"robot":8957,"celebrity":8958,"bride":8959,"broadcasts":8960,"abolished":8961,"pot":8962,"joel":8963,"overhead":8964,"franz":8965,"packed":8966,"reconnaissance":8967,"johann":8968,"acknowledged":8969,"introduce":8970,"handled":8971,"doctorate":8972,"developments":8973,"drinks":8974,"alley":8975,"palestine":8976,"##nis":8977,"##aki":8978,"proceeded":8979,"recover":8980,"bradley":8981,"grain":8982,"patch":8983,"afford":8984,"infection":8985,"nationalist":8986,"legendary":8987,"##ath":8988,"interchange":8989,"virtually":8990,"gen":8991,"gravity":8992,"exploration":8993,"amber":8994,"vital":8995,"wishes":8996,"powell":8997,"doctrine":8998,"elbow":8999,"screenplay":9000,"##bird":9001,"contribute":9002,"indonesian":9003,"pet":9004,"creates":9005,"##com":9006,"enzyme":9007,"kylie":9008,"discipline":9009,"drops":9010,"manila":9011,"hunger":9012,"##ien":9013,"layers":9014,"suffer":9015,"fever":9016,"bits":9017,"monica":9018,"keyboard":9019,"manages":9020,"##hood":9021,"searched":9022,"appeals":9023,"##bad":9024,"testament":9025,"grande":9026,"reid":9027,"##war":9028,"beliefs":9029,"congo":9030,"##ification":9031,"##dia":9032,"si":9033,"requiring":9034,"##via":9035,"casey":9036,"1849":9037,"regret":9038,"streak":9039,"rape":9040,"depends":9041,"syrian":9042,"sprint":9043,"pound":9044,"tourists":9045,"upcoming":9046,"pub":9047,"##xi":9048,"tense":9049,"##els":9050,"practiced":9051,"echo":9052,"nationwide":9053,"guild":9054,"motorcycle":9055,"liz":9056,"##zar":9057,"chiefs":9058,"desired":9059,"elena":9060,"bye":9061,"precious":9062,"absorbed":9063,"relatives":9064,"booth":9065,"pianist":9066,"##mal":9067,"citizenship":9068,"exhausted":9069,"wilhelm":9070,"##ceae":9071,"##hed":9072,"noting":9073,"quarterback":9074,"urge":9075,"hectares":9076,"##gue":9077,"ace":9078,"holly":9079,"##tal":9080,"blonde":9081,"davies":9082,"parked":9083,"sustainable":9084,"stepping":9085,"twentieth":9086,"airfield":9087,"galaxy":9088,"nest":9089,"chip":9090,"##nell":9091,"tan":9092,"shaft":9093,"paulo":9094,"requirement":9095,"##zy":9096,"paradise":9097,"tobacco":9098,"trans":9099,"renewed":9100,"vietnamese":9101,"##cker":9102,"##ju":9103,"suggesting":9104,"catching":9105,"holmes":9106,"enjoying":9107,"md":9108,"trips":9109,"colt":9110,"holder":9111,"butterfly":9112,"nerve":9113,"reformed":9114,"cherry":9115,"bowling":9116,"trailer":9117,"carriage":9118,"goodbye":9119,"appreciate":9120,"toy":9121,"joshua":9122,"interactive":9123,"enabled":9124,"involve":9125,"##kan":9126,"collar":9127,"determination":9128,"bunch":9129,"facebook":9130,"recall":9131,"shorts":9132,"superintendent":9133,"episcopal":9134,"frustration":9135,"giovanni":9136,"nineteenth":9137,"laser":9138,"privately":9139,"array":9140,"circulation":9141,"##ovic":9142,"armstrong":9143,"deals":9144,"painful":9145,"permit":9146,"discrimination":9147,"##wi":9148,"aires":9149,"retiring":9150,"cottage":9151,"ni":9152,"##sta":9153,"horizon":9154,"ellen":9155,"jamaica":9156,"ripped":9157,"fernando":9158,"chapters":9159,"playstation":9160,"patron":9161,"lecturer":9162,"navigation":9163,"behaviour":9164,"genes":9165,"georgian":9166,"export":9167,"solomon":9168,"rivals":9169,"swift":9170,"seventeen":9171,"rodriguez":9172,"princeton":9173,"independently":9174,"sox":9175,"1847":9176,"arguing":9177,"entity":9178,"casting":9179,"hank":9180,"criteria":9181,"oakland":9182,"geographic":9183,"milwaukee":9184,"reflection":9185,"expanding":9186,"conquest":9187,"dubbed":9188,"##tv":9189,"halt":9190,"brave":9191,"brunswick":9192,"doi":9193,"arched":9194,"curtis":9195,"divorced":9196,"predominantly":9197,"somerset":9198,"streams":9199,"ugly":9200,"zoo":9201,"horrible":9202,"curved":9203,"buenos":9204,"fierce":9205,"dictionary":9206,"vector":9207,"theological":9208,"unions":9209,"handful":9210,"stability":9211,"chan":9212,"punjab":9213,"segments":9214,"##lly":9215,"altar":9216,"ignoring":9217,"gesture":9218,"monsters":9219,"pastor":9220,"##stone":9221,"thighs":9222,"unexpected":9223,"operators":9224,"abruptly":9225,"coin":9226,"compiled":9227,"associates":9228,"improving":9229,"migration":9230,"pin":9231,"##ose":9232,"compact":9233,"collegiate":9234,"reserved":9235,"##urs":9236,"quarterfinals":9237,"roster":9238,"restore":9239,"assembled":9240,"hurry":9241,"oval":9242,"##cies":9243,"1846":9244,"flags":9245,"martha":9246,"##del":9247,"victories":9248,"sharply":9249,"##rated":9250,"argues":9251,"deadly":9252,"neo":9253,"drawings":9254,"symbols":9255,"performer":9256,"##iel":9257,"griffin":9258,"restrictions":9259,"editing":9260,"andrews":9261,"java":9262,"journals":9263,"arabia":9264,"compositions":9265,"dee":9266,"pierce":9267,"removing":9268,"hindi":9269,"casino":9270,"runway":9271,"civilians":9272,"minds":9273,"nasa":9274,"hotels":9275,"##zation":9276,"refuge":9277,"rent":9278,"retain":9279,"potentially":9280,"conferences":9281,"suburban":9282,"conducting":9283,"##tto":9284,"##tions":9285,"##tle":9286,"descended":9287,"massacre":9288,"##cal":9289,"ammunition":9290,"terrain":9291,"fork":9292,"souls":9293,"counts":9294,"chelsea":9295,"durham":9296,"drives":9297,"cab":9298,"##bank":9299,"perth":9300,"realizing":9301,"palestinian":9302,"finn":9303,"simpson":9304,"##dal":9305,"betty":9306,"##ule":9307,"moreover":9308,"particles":9309,"cardinals":9310,"tent":9311,"evaluation":9312,"extraordinary":9313,"##oid":9314,"inscription":9315,"##works":9316,"wednesday":9317,"chloe":9318,"maintains":9319,"panels":9320,"ashley":9321,"trucks":9322,"##nation":9323,"cluster":9324,"sunlight":9325,"strikes":9326,"zhang":9327,"##wing":9328,"dialect":9329,"canon":9330,"##ap":9331,"tucked":9332,"##ws":9333,"collecting":9334,"##mas":9335,"##can":9336,"##sville":9337,"maker":9338,"quoted":9339,"evan":9340,"franco":9341,"aria":9342,"buying":9343,"cleaning":9344,"eva":9345,"closet":9346,"provision":9347,"apollo":9348,"clinic":9349,"rat":9350,"##ez":9351,"necessarily":9352,"ac":9353,"##gle":9354,"##ising":9355,"venues":9356,"flipped":9357,"cent":9358,"spreading":9359,"trustees":9360,"checking":9361,"authorized":9362,"##sco":9363,"disappointed":9364,"##ado":9365,"notion":9366,"duration":9367,"trumpet":9368,"hesitated":9369,"topped":9370,"brussels":9371,"rolls":9372,"theoretical":9373,"hint":9374,"define":9375,"aggressive":9376,"repeat":9377,"wash":9378,"peaceful":9379,"optical":9380,"width":9381,"allegedly":9382,"mcdonald":9383,"strict":9384,"copyright":9385,"##illa":9386,"investors":9387,"mar":9388,"jam":9389,"witnesses":9390,"sounding":9391,"miranda":9392,"michelle":9393,"privacy":9394,"hugo":9395,"harmony":9396,"##pp":9397,"valid":9398,"lynn":9399,"glared":9400,"nina":9401,"102":9402,"headquartered":9403,"diving":9404,"boarding":9405,"gibson":9406,"##ncy":9407,"albanian":9408,"marsh":9409,"routine":9410,"dealt":9411,"enhanced":9412,"er":9413,"intelligent":9414,"substance":9415,"targeted":9416,"enlisted":9417,"discovers":9418,"spinning":9419,"observations":9420,"pissed":9421,"smoking":9422,"rebecca":9423,"capitol":9424,"visa":9425,"varied":9426,"costume":9427,"seemingly":9428,"indies":9429,"compensation":9430,"surgeon":9431,"thursday":9432,"arsenal":9433,"westminster":9434,"suburbs":9435,"rid":9436,"anglican":9437,"##ridge":9438,"knots":9439,"foods":9440,"alumni":9441,"lighter":9442,"fraser":9443,"whoever":9444,"portal":9445,"scandal":9446,"##ray":9447,"gavin":9448,"advised":9449,"instructor":9450,"flooding":9451,"terrorist":9452,"##ale":9453,"teenage":9454,"interim":9455,"senses":9456,"duck":9457,"teen":9458,"thesis":9459,"abby":9460,"eager":9461,"overcome":9462,"##ile":9463,"newport":9464,"glenn":9465,"rises":9466,"shame":9467,"##cc":9468,"prompted":9469,"priority":9470,"forgot":9471,"bomber":9472,"nicolas":9473,"protective":9474,"360":9475,"cartoon":9476,"katherine":9477,"breeze":9478,"lonely":9479,"trusted":9480,"henderson":9481,"richardson":9482,"relax":9483,"banner":9484,"candy":9485,"palms":9486,"remarkable":9487,"##rio":9488,"legends":9489,"cricketer":9490,"essay":9491,"ordained":9492,"edmund":9493,"rifles":9494,"trigger":9495,"##uri":9496,"##away":9497,"sail":9498,"alert":9499,"1830":9500,"audiences":9501,"penn":9502,"sussex":9503,"siblings":9504,"pursued":9505,"indianapolis":9506,"resist":9507,"rosa":9508,"consequence":9509,"succeed":9510,"avoided":9511,"1845":9512,"##ulation":9513,"inland":9514,"##tie":9515,"##nna":9516,"counsel":9517,"profession":9518,"chronicle":9519,"hurried":9520,"##una":9521,"eyebrow":9522,"eventual":9523,"bleeding":9524,"innovative":9525,"cure":9526,"##dom":9527,"committees":9528,"accounting":9529,"con":9530,"scope":9531,"hardy":9532,"heather":9533,"tenor":9534,"gut":9535,"herald":9536,"codes":9537,"tore":9538,"scales":9539,"wagon":9540,"##oo":9541,"luxury":9542,"tin":9543,"prefer":9544,"fountain":9545,"triangle":9546,"bonds":9547,"darling":9548,"convoy":9549,"dried":9550,"traced":9551,"beings":9552,"troy":9553,"accidentally":9554,"slam":9555,"findings":9556,"smelled":9557,"joey":9558,"lawyers":9559,"outcome":9560,"steep":9561,"bosnia":9562,"configuration":9563,"shifting":9564,"toll":9565,"brook":9566,"performers":9567,"lobby":9568,"philosophical":9569,"construct":9570,"shrine":9571,"aggregate":9572,"boot":9573,"cox":9574,"phenomenon":9575,"savage":9576,"insane":9577,"solely":9578,"reynolds":9579,"lifestyle":9580,"##ima":9581,"nationally":9582,"holdings":9583,"consideration":9584,"enable":9585,"edgar":9586,"mo":9587,"mama":9588,"##tein":9589,"fights":9590,"relegation":9591,"chances":9592,"atomic":9593,"hub":9594,"conjunction":9595,"awkward":9596,"reactions":9597,"currency":9598,"finale":9599,"kumar":9600,"underwent":9601,"steering":9602,"elaborate":9603,"gifts":9604,"comprising":9605,"melissa":9606,"veins":9607,"reasonable":9608,"sunshine":9609,"chi":9610,"solve":9611,"trails":9612,"inhabited":9613,"elimination":9614,"ethics":9615,"huh":9616,"ana":9617,"molly":9618,"consent":9619,"apartments":9620,"layout":9621,"marines":9622,"##ces":9623,"hunters":9624,"bulk":9625,"##oma":9626,"hometown":9627,"##wall":9628,"##mont":9629,"cracked":9630,"reads":9631,"neighbouring":9632,"withdrawn":9633,"admission":9634,"wingspan":9635,"damned":9636,"anthology":9637,"lancashire":9638,"brands":9639,"batting":9640,"forgive":9641,"cuban":9642,"awful":9643,"##lyn":9644,"104":9645,"dimensions":9646,"imagination":9647,"##ade":9648,"dante":9649,"##ship":9650,"tracking":9651,"desperately":9652,"goalkeeper":9653,"##yne":9654,"groaned":9655,"workshops":9656,"confident":9657,"burton":9658,"gerald":9659,"milton":9660,"circus":9661,"uncertain":9662,"slope":9663,"copenhagen":9664,"sophia":9665,"fog":9666,"philosopher":9667,"portraits":9668,"accent":9669,"cycling":9670,"varying":9671,"gripped":9672,"larvae":9673,"garrett":9674,"specified":9675,"scotia":9676,"mature":9677,"luther":9678,"kurt":9679,"rap":9680,"##kes":9681,"aerial":9682,"750":9683,"ferdinand":9684,"heated":9685,"es":9686,"transported":9687,"##shan":9688,"safely":9689,"nonetheless":9690,"##orn":9691,"##gal":9692,"motors":9693,"demanding":9694,"##sburg":9695,"startled":9696,"##brook":9697,"ally":9698,"generate":9699,"caps":9700,"ghana":9701,"stained":9702,"demo":9703,"mentions":9704,"beds":9705,"ap":9706,"afterward":9707,"diary":9708,"##bling":9709,"utility":9710,"##iro":9711,"richards":9712,"1837":9713,"conspiracy":9714,"conscious":9715,"shining":9716,"footsteps":9717,"observer":9718,"cyprus":9719,"urged":9720,"loyalty":9721,"developer":9722,"probability":9723,"olive":9724,"upgraded":9725,"gym":9726,"miracle":9727,"insects":9728,"graves":9729,"1844":9730,"ourselves":9731,"hydrogen":9732,"amazon":9733,"katie":9734,"tickets":9735,"poets":9736,"##pm":9737,"planes":9738,"##pan":9739,"prevention":9740,"witnessed":9741,"dense":9742,"jin":9743,"randy":9744,"tang":9745,"warehouse":9746,"monroe":9747,"bang":9748,"archived":9749,"elderly":9750,"investigations":9751,"alec":9752,"granite":9753,"mineral":9754,"conflicts":9755,"controlling":9756,"aboriginal":9757,"carlo":9758,"##zu":9759,"mechanics":9760,"stan":9761,"stark":9762,"rhode":9763,"skirt":9764,"est":9765,"##berry":9766,"bombs":9767,"respected":9768,"##horn":9769,"imposed":9770,"limestone":9771,"deny":9772,"nominee":9773,"memphis":9774,"grabbing":9775,"disabled":9776,"##als":9777,"amusement":9778,"aa":9779,"frankfurt":9780,"corn":9781,"referendum":9782,"varies":9783,"slowed":9784,"disk":9785,"firms":9786,"unconscious":9787,"incredible":9788,"clue":9789,"sue":9790,"##zhou":9791,"twist":9792,"##cio":9793,"joins":9794,"idaho":9795,"chad":9796,"developers":9797,"computing":9798,"destroyer":9799,"103":9800,"mortal":9801,"tucker":9802,"kingston":9803,"choices":9804,"yu":9805,"carson":9806,"1800":9807,"os":9808,"whitney":9809,"geneva":9810,"pretend":9811,"dimension":9812,"staged":9813,"plateau":9814,"maya":9815,"##une":9816,"freestyle":9817,"##bc":9818,"rovers":9819,"hiv":9820,"##ids":9821,"tristan":9822,"classroom":9823,"prospect":9824,"##hus":9825,"honestly":9826,"diploma":9827,"lied":9828,"thermal":9829,"auxiliary":9830,"feast":9831,"unlikely":9832,"iata":9833,"##tel":9834,"morocco":9835,"pounding":9836,"treasury":9837,"lithuania":9838,"considerably":9839,"1841":9840,"dish":9841,"1812":9842,"geological":9843,"matching":9844,"stumbled":9845,"destroying":9846,"marched":9847,"brien":9848,"advances":9849,"cake":9850,"nicole":9851,"belle":9852,"settling":9853,"measuring":9854,"directing":9855,"##mie":9856,"tuesday":9857,"bassist":9858,"capabilities":9859,"stunned":9860,"fraud":9861,"torpedo":9862,"##list":9863,"##phone":9864,"anton":9865,"wisdom":9866,"surveillance":9867,"ruined":9868,"##ulate":9869,"lawsuit":9870,"healthcare":9871,"theorem":9872,"halls":9873,"trend":9874,"aka":9875,"horizontal":9876,"dozens":9877,"acquire":9878,"lasting":9879,"swim":9880,"hawk":9881,"gorgeous":9882,"fees":9883,"vicinity":9884,"decrease":9885,"adoption":9886,"tactics":9887,"##ography":9888,"pakistani":9889,"##ole":9890,"draws":9891,"##hall":9892,"willie":9893,"burke":9894,"heath":9895,"algorithm":9896,"integral":9897,"powder":9898,"elliott":9899,"brigadier":9900,"jackie":9901,"tate":9902,"varieties":9903,"darker":9904,"##cho":9905,"lately":9906,"cigarette":9907,"specimens":9908,"adds":9909,"##ree":9910,"##ensis":9911,"##inger":9912,"exploded":9913,"finalist":9914,"cia":9915,"murders":9916,"wilderness":9917,"arguments":9918,"nicknamed":9919,"acceptance":9920,"onwards":9921,"manufacture":9922,"robertson":9923,"jets":9924,"tampa":9925,"enterprises":9926,"blog":9927,"loudly":9928,"composers":9929,"nominations":9930,"1838":9931,"ai":9932,"malta":9933,"inquiry":9934,"automobile":9935,"hosting":9936,"viii":9937,"rays":9938,"tilted":9939,"grief":9940,"museums":9941,"strategies":9942,"furious":9943,"euro":9944,"equality":9945,"cohen":9946,"poison":9947,"surrey":9948,"wireless":9949,"governed":9950,"ridiculous":9951,"moses":9952,"##esh":9953,"##room":9954,"vanished":9955,"##ito":9956,"barnes":9957,"attract":9958,"morrison":9959,"istanbul":9960,"##iness":9961,"absent":9962,"rotation":9963,"petition":9964,"janet":9965,"##logical":9966,"satisfaction":9967,"custody":9968,"deliberately":9969,"observatory":9970,"comedian":9971,"surfaces":9972,"pinyin":9973,"novelist":9974,"strictly":9975,"canterbury":9976,"oslo":9977,"monks":9978,"embrace":9979,"ibm":9980,"jealous":9981,"photograph":9982,"continent":9983,"dorothy":9984,"marina":9985,"doc":9986,"excess":9987,"holden":9988,"allegations":9989,"explaining":9990,"stack":9991,"avoiding":9992,"lance":9993,"storyline":9994,"majesty":9995,"poorly":9996,"spike":9997,"dos":9998,"bradford":9999,"raven":10000,"travis":10001,"classics":10002,"proven":10003,"voltage":10004,"pillow":10005,"fists":10006,"butt":10007,"1842":10008,"interpreted":10009,"##car":10010,"1839":10011,"gage":10012,"telegraph":10013,"lens":10014,"promising":10015,"expelled":10016,"casual":10017,"collector":10018,"zones":10019,"##min":10020,"silly":10021,"nintendo":10022,"##kh":10023,"##bra":10024,"downstairs":10025,"chef":10026,"suspicious":10027,"afl":10028,"flies":10029,"vacant":10030,"uganda":10031,"pregnancy":10032,"condemned":10033,"lutheran":10034,"estimates":10035,"cheap":10036,"decree":10037,"saxon":10038,"proximity":10039,"stripped":10040,"idiot":10041,"deposits":10042,"contrary":10043,"presenter":10044,"magnus":10045,"glacier":10046,"im":10047,"offense":10048,"edwin":10049,"##ori":10050,"upright":10051,"##long":10052,"bolt":10053,"##ois":10054,"toss":10055,"geographical":10056,"##izes":10057,"environments":10058,"delicate":10059,"marking":10060,"abstract":10061,"xavier":10062,"nails":10063,"windsor":10064,"plantation":10065,"occurring":10066,"equity":10067,"saskatchewan":10068,"fears":10069,"drifted":10070,"sequences":10071,"vegetation":10072,"revolt":10073,"##stic":10074,"1843":10075,"sooner":10076,"fusion":10077,"opposing":10078,"nato":10079,"skating":10080,"1836":10081,"secretly":10082,"ruin":10083,"lease":10084,"##oc":10085,"edit":10086,"##nne":10087,"flora":10088,"anxiety":10089,"ruby":10090,"##ological":10091,"##mia":10092,"tel":10093,"bout":10094,"taxi":10095,"emmy":10096,"frost":10097,"rainbow":10098,"compounds":10099,"foundations":10100,"rainfall":10101,"assassination":10102,"nightmare":10103,"dominican":10104,"##win":10105,"achievements":10106,"deserve":10107,"orlando":10108,"intact":10109,"armenia":10110,"##nte":10111,"calgary":10112,"valentine":10113,"106":10114,"marion":10115,"proclaimed":10116,"theodore":10117,"bells":10118,"courtyard":10119,"thigh":10120,"gonzalez":10121,"console":10122,"troop":10123,"minimal":10124,"monte":10125,"everyday":10126,"##ence":10127,"##if":10128,"supporter":10129,"terrorism":10130,"buck":10131,"openly":10132,"presbyterian":10133,"activists":10134,"carpet":10135,"##iers":10136,"rubbing":10137,"uprising":10138,"##yi":10139,"cute":10140,"conceived":10141,"legally":10142,"##cht":10143,"millennium":10144,"cello":10145,"velocity":10146,"ji":10147,"rescued":10148,"cardiff":10149,"1835":10150,"rex":10151,"concentrate":10152,"senators":10153,"beard":10154,"rendered":10155,"glowing":10156,"battalions":10157,"scouts":10158,"competitors":10159,"sculptor":10160,"catalogue":10161,"arctic":10162,"ion":10163,"raja":10164,"bicycle":10165,"wow":10166,"glancing":10167,"lawn":10168,"##woman":10169,"gentleman":10170,"lighthouse":10171,"publish":10172,"predicted":10173,"calculated":10174,"##val":10175,"variants":10176,"##gne":10177,"strain":10178,"##ui":10179,"winston":10180,"deceased":10181,"##nus":10182,"touchdowns":10183,"brady":10184,"caleb":10185,"sinking":10186,"echoed":10187,"crush":10188,"hon":10189,"blessed":10190,"protagonist":10191,"hayes":10192,"endangered":10193,"magnitude":10194,"editors":10195,"##tine":10196,"estimate":10197,"responsibilities":10198,"##mel":10199,"backup":10200,"laying":10201,"consumed":10202,"sealed":10203,"zurich":10204,"lovers":10205,"frustrated":10206,"##eau":10207,"ahmed":10208,"kicking":10209,"mit":10210,"treasurer":10211,"1832":10212,"biblical":10213,"refuse":10214,"terrified":10215,"pump":10216,"agrees":10217,"genuine":10218,"imprisonment":10219,"refuses":10220,"plymouth":10221,"##hen":10222,"lou":10223,"##nen":10224,"tara":10225,"trembling":10226,"antarctic":10227,"ton":10228,"learns":10229,"##tas":10230,"crap":10231,"crucial":10232,"faction":10233,"atop":10234,"##borough":10235,"wrap":10236,"lancaster":10237,"odds":10238,"hopkins":10239,"erik":10240,"lyon":10241,"##eon":10242,"bros":10243,"##ode":10244,"snap":10245,"locality":10246,"tips":10247,"empress":10248,"crowned":10249,"cal":10250,"acclaimed":10251,"chuckled":10252,"##ory":10253,"clara":10254,"sends":10255,"mild":10256,"towel":10257,"##fl":10258,"##day":10259,"##а":10260,"wishing":10261,"assuming":10262,"interviewed":10263,"##bal":10264,"##die":10265,"interactions":10266,"eden":10267,"cups":10268,"helena":10269,"##lf":10270,"indie":10271,"beck":10272,"##fire":10273,"batteries":10274,"filipino":10275,"wizard":10276,"parted":10277,"##lam":10278,"traces":10279,"##born":10280,"rows":10281,"idol":10282,"albany":10283,"delegates":10284,"##ees":10285,"##sar":10286,"discussions":10287,"##ex":10288,"notre":10289,"instructed":10290,"belgrade":10291,"highways":10292,"suggestion":10293,"lauren":10294,"possess":10295,"orientation":10296,"alexandria":10297,"abdul":10298,"beats":10299,"salary":10300,"reunion":10301,"ludwig":10302,"alright":10303,"wagner":10304,"intimate":10305,"pockets":10306,"slovenia":10307,"hugged":10308,"brighton":10309,"merchants":10310,"cruel":10311,"stole":10312,"trek":10313,"slopes":10314,"repairs":10315,"enrollment":10316,"politically":10317,"underlying":10318,"promotional":10319,"counting":10320,"boeing":10321,"##bb":10322,"isabella":10323,"naming":10324,"##и":10325,"keen":10326,"bacteria":10327,"listing":10328,"separately":10329,"belfast":10330,"ussr":10331,"450":10332,"lithuanian":10333,"anybody":10334,"ribs":10335,"sphere":10336,"martinez":10337,"cock":10338,"embarrassed":10339,"proposals":10340,"fragments":10341,"nationals":10342,"##fs":10343,"##wski":10344,"premises":10345,"fin":10346,"1500":10347,"alpine":10348,"matched":10349,"freely":10350,"bounded":10351,"jace":10352,"sleeve":10353,"##af":10354,"gaming":10355,"pier":10356,"populated":10357,"evident":10358,"##like":10359,"frances":10360,"flooded":10361,"##dle":10362,"frightened":10363,"pour":10364,"trainer":10365,"framed":10366,"visitor":10367,"challenging":10368,"pig":10369,"wickets":10370,"##fold":10371,"infected":10372,"email":10373,"##pes":10374,"arose":10375,"##aw":10376,"reward":10377,"ecuador":10378,"oblast":10379,"vale":10380,"ch":10381,"shuttle":10382,"##usa":10383,"bach":10384,"rankings":10385,"forbidden":10386,"cornwall":10387,"accordance":10388,"salem":10389,"consumers":10390,"bruno":10391,"fantastic":10392,"toes":10393,"machinery":10394,"resolved":10395,"julius":10396,"remembering":10397,"propaganda":10398,"iceland":10399,"bombardment":10400,"tide":10401,"contacts":10402,"wives":10403,"##rah":10404,"concerto":10405,"macdonald":10406,"albania":10407,"implement":10408,"daisy":10409,"tapped":10410,"sudan":10411,"helmet":10412,"angela":10413,"mistress":10414,"##lic":10415,"crop":10416,"sunk":10417,"finest":10418,"##craft":10419,"hostile":10420,"##ute":10421,"##tsu":10422,"boxer":10423,"fr":10424,"paths":10425,"adjusted":10426,"habit":10427,"ballot":10428,"supervision":10429,"soprano":10430,"##zen":10431,"bullets":10432,"wicked":10433,"sunset":10434,"regiments":10435,"disappear":10436,"lamp":10437,"performs":10438,"app":10439,"##gia":10440,"##oa":10441,"rabbit":10442,"digging":10443,"incidents":10444,"entries":10445,"##cion":10446,"dishes":10447,"##oi":10448,"introducing":10449,"##ati":10450,"##fied":10451,"freshman":10452,"slot":10453,"jill":10454,"tackles":10455,"baroque":10456,"backs":10457,"##iest":10458,"lone":10459,"sponsor":10460,"destiny":10461,"altogether":10462,"convert":10463,"##aro":10464,"consensus":10465,"shapes":10466,"demonstration":10467,"basically":10468,"feminist":10469,"auction":10470,"artifacts":10471,"##bing":10472,"strongest":10473,"twitter":10474,"halifax":10475,"2019":10476,"allmusic":10477,"mighty":10478,"smallest":10479,"precise":10480,"alexandra":10481,"viola":10482,"##los":10483,"##ille":10484,"manuscripts":10485,"##illo":10486,"dancers":10487,"ari":10488,"managers":10489,"monuments":10490,"blades":10491,"barracks":10492,"springfield":10493,"maiden":10494,"consolidated":10495,"electron":10496,"##end":10497,"berry":10498,"airing":10499,"wheat":10500,"nobel":10501,"inclusion":10502,"blair":10503,"payments":10504,"geography":10505,"bee":10506,"cc":10507,"eleanor":10508,"react":10509,"##hurst":10510,"afc":10511,"manitoba":10512,"##yu":10513,"su":10514,"lineup":10515,"fitness":10516,"recreational":10517,"investments":10518,"airborne":10519,"disappointment":10520,"##dis":10521,"edmonton":10522,"viewing":10523,"##row":10524,"renovation":10525,"##cast":10526,"infant":10527,"bankruptcy":10528,"roses":10529,"aftermath":10530,"pavilion":10531,"##yer":10532,"carpenter":10533,"withdrawal":10534,"ladder":10535,"##hy":10536,"discussing":10537,"popped":10538,"reliable":10539,"agreements":10540,"rochester":10541,"##abad":10542,"curves":10543,"bombers":10544,"220":10545,"rao":10546,"reverend":10547,"decreased":10548,"choosing":10549,"107":10550,"stiff":10551,"consulting":10552,"naples":10553,"crawford":10554,"tracy":10555,"ka":10556,"ribbon":10557,"cops":10558,"##lee":10559,"crushed":10560,"deciding":10561,"unified":10562,"teenager":10563,"accepting":10564,"flagship":10565,"explorer":10566,"poles":10567,"sanchez":10568,"inspection":10569,"revived":10570,"skilled":10571,"induced":10572,"exchanged":10573,"flee":10574,"locals":10575,"tragedy":10576,"swallow":10577,"loading":10578,"hanna":10579,"demonstrate":10580,"##ela":10581,"salvador":10582,"flown":10583,"contestants":10584,"civilization":10585,"##ines":10586,"wanna":10587,"rhodes":10588,"fletcher":10589,"hector":10590,"knocking":10591,"considers":10592,"##ough":10593,"nash":10594,"mechanisms":10595,"sensed":10596,"mentally":10597,"walt":10598,"unclear":10599,"##eus":10600,"renovated":10601,"madame":10602,"##cks":10603,"crews":10604,"governmental":10605,"##hin":10606,"undertaken":10607,"monkey":10608,"##ben":10609,"##ato":10610,"fatal":10611,"armored":10612,"copa":10613,"caves":10614,"governance":10615,"grasp":10616,"perception":10617,"certification":10618,"froze":10619,"damp":10620,"tugged":10621,"wyoming":10622,"##rg":10623,"##ero":10624,"newman":10625,"##lor":10626,"nerves":10627,"curiosity":10628,"graph":10629,"115":10630,"##ami":10631,"withdraw":10632,"tunnels":10633,"dull":10634,"meredith":10635,"moss":10636,"exhibits":10637,"neighbors":10638,"communicate":10639,"accuracy":10640,"explored":10641,"raiders":10642,"republicans":10643,"secular":10644,"kat":10645,"superman":10646,"penny":10647,"criticised":10648,"##tch":10649,"freed":10650,"update":10651,"conviction":10652,"wade":10653,"ham":10654,"likewise":10655,"delegation":10656,"gotta":10657,"doll":10658,"promises":10659,"technological":10660,"myth":10661,"nationality":10662,"resolve":10663,"convent":10664,"##mark":10665,"sharon":10666,"dig":10667,"sip":10668,"coordinator":10669,"entrepreneur":10670,"fold":10671,"##dine":10672,"capability":10673,"councillor":10674,"synonym":10675,"blown":10676,"swan":10677,"cursed":10678,"1815":10679,"jonas":10680,"haired":10681,"sofa":10682,"canvas":10683,"keeper":10684,"rivalry":10685,"##hart":10686,"rapper":10687,"speedway":10688,"swords":10689,"postal":10690,"maxwell":10691,"estonia":10692,"potter":10693,"recurring":10694,"##nn":10695,"##ave":10696,"errors":10697,"##oni":10698,"cognitive":10699,"1834":10700,"##²":10701,"claws":10702,"nadu":10703,"roberto":10704,"bce":10705,"wrestler":10706,"ellie":10707,"##ations":10708,"infinite":10709,"ink":10710,"##tia":10711,"presumably":10712,"finite":10713,"staircase":10714,"108":10715,"noel":10716,"patricia":10717,"nacional":10718,"##cation":10719,"chill":10720,"eternal":10721,"tu":10722,"preventing":10723,"prussia":10724,"fossil":10725,"limbs":10726,"##logist":10727,"ernst":10728,"frog":10729,"perez":10730,"rene":10731,"##ace":10732,"pizza":10733,"prussian":10734,"##ios":10735,"##vy":10736,"molecules":10737,"regulatory":10738,"answering":10739,"opinions":10740,"sworn":10741,"lengths":10742,"supposedly":10743,"hypothesis":10744,"upward":10745,"habitats":10746,"seating":10747,"ancestors":10748,"drank":10749,"yield":10750,"hd":10751,"synthesis":10752,"researcher":10753,"modest":10754,"##var":10755,"mothers":10756,"peered":10757,"voluntary":10758,"homeland":10759,"##the":10760,"acclaim":10761,"##igan":10762,"static":10763,"valve":10764,"luxembourg":10765,"alto":10766,"carroll":10767,"fe":10768,"receptor":10769,"norton":10770,"ambulance":10771,"##tian":10772,"johnston":10773,"catholics":10774,"depicting":10775,"jointly":10776,"elephant":10777,"gloria":10778,"mentor":10779,"badge":10780,"ahmad":10781,"distinguish":10782,"remarked":10783,"councils":10784,"precisely":10785,"allison":10786,"advancing":10787,"detection":10788,"crowded":10789,"##10":10790,"cooperative":10791,"ankle":10792,"mercedes":10793,"dagger":10794,"surrendered":10795,"pollution":10796,"commit":10797,"subway":10798,"jeffrey":10799,"lesson":10800,"sculptures":10801,"provider":10802,"##fication":10803,"membrane":10804,"timothy":10805,"rectangular":10806,"fiscal":10807,"heating":10808,"teammate":10809,"basket":10810,"particle":10811,"anonymous":10812,"deployment":10813,"##ple":10814,"missiles":10815,"courthouse":10816,"proportion":10817,"shoe":10818,"sec":10819,"##ller":10820,"complaints":10821,"forbes":10822,"blacks":10823,"abandon":10824,"remind":10825,"sizes":10826,"overwhelming":10827,"autobiography":10828,"natalie":10829,"##awa":10830,"risks":10831,"contestant":10832,"countryside":10833,"babies":10834,"scorer":10835,"invaded":10836,"enclosed":10837,"proceed":10838,"hurling":10839,"disorders":10840,"##cu":10841,"reflecting":10842,"continuously":10843,"cruiser":10844,"graduates":10845,"freeway":10846,"investigated":10847,"ore":10848,"deserved":10849,"maid":10850,"blocking":10851,"phillip":10852,"jorge":10853,"shakes":10854,"dove":10855,"mann":10856,"variables":10857,"lacked":10858,"burden":10859,"accompanying":10860,"que":10861,"consistently":10862,"organizing":10863,"provisional":10864,"complained":10865,"endless":10866,"##rm":10867,"tubes":10868,"juice":10869,"georges":10870,"krishna":10871,"mick":10872,"labels":10873,"thriller":10874,"##uch":10875,"laps":10876,"arcade":10877,"sage":10878,"snail":10879,"##table":10880,"shannon":10881,"fi":10882,"laurence":10883,"seoul":10884,"vacation":10885,"presenting":10886,"hire":10887,"churchill":10888,"surprisingly":10889,"prohibited":10890,"savannah":10891,"technically":10892,"##oli":10893,"170":10894,"##lessly":10895,"testimony":10896,"suited":10897,"speeds":10898,"toys":10899,"romans":10900,"mlb":10901,"flowering":10902,"measurement":10903,"talented":10904,"kay":10905,"settings":10906,"charleston":10907,"expectations":10908,"shattered":10909,"achieving":10910,"triumph":10911,"ceremonies":10912,"portsmouth":10913,"lanes":10914,"mandatory":10915,"loser":10916,"stretching":10917,"cologne":10918,"realizes":10919,"seventy":10920,"cornell":10921,"careers":10922,"webb":10923,"##ulating":10924,"americas":10925,"budapest":10926,"ava":10927,"suspicion":10928,"##ison":10929,"yo":10930,"conrad":10931,"##hai":10932,"sterling":10933,"jessie":10934,"rector":10935,"##az":10936,"1831":10937,"transform":10938,"organize":10939,"loans":10940,"christine":10941,"volcanic":10942,"warrant":10943,"slender":10944,"summers":10945,"subfamily":10946,"newer":10947,"danced":10948,"dynamics":10949,"rhine":10950,"proceeds":10951,"heinrich":10952,"gastropod":10953,"commands":10954,"sings":10955,"facilitate":10956,"easter":10957,"ra":10958,"positioned":10959,"responses":10960,"expense":10961,"fruits":10962,"yanked":10963,"imported":10964,"25th":10965,"velvet":10966,"vic":10967,"primitive":10968,"tribune":10969,"baldwin":10970,"neighbourhood":10971,"donna":10972,"rip":10973,"hay":10974,"pr":10975,"##uro":10976,"1814":10977,"espn":10978,"welcomed":10979,"##aria":10980,"qualifier":10981,"glare":10982,"highland":10983,"timing":10984,"##cted":10985,"shells":10986,"eased":10987,"geometry":10988,"louder":10989,"exciting":10990,"slovakia":10991,"##sion":10992,"##iz":10993,"##lot":10994,"savings":10995,"prairie":10996,"##ques":10997,"marching":10998,"rafael":10999,"tonnes":11000,"##lled":11001,"curtain":11002,"preceding":11003,"shy":11004,"heal":11005,"greene":11006,"worthy":11007,"##pot":11008,"detachment":11009,"bury":11010,"sherman":11011,"##eck":11012,"reinforced":11013,"seeks":11014,"bottles":11015,"contracted":11016,"duchess":11017,"outfit":11018,"walsh":11019,"##sc":11020,"mickey":11021,"##ase":11022,"geoffrey":11023,"archer":11024,"squeeze":11025,"dawson":11026,"eliminate":11027,"invention":11028,"##enberg":11029,"neal":11030,"##eth":11031,"stance":11032,"dealer":11033,"coral":11034,"maple":11035,"retire":11036,"polo":11037,"simplified":11038,"##ht":11039,"1833":11040,"hid":11041,"watts":11042,"backwards":11043,"jules":11044,"##oke":11045,"genesis":11046,"mt":11047,"frames":11048,"rebounds":11049,"burma":11050,"woodland":11051,"moist":11052,"santos":11053,"whispers":11054,"drained":11055,"subspecies":11056,"##aa":11057,"streaming":11058,"ulster":11059,"burnt":11060,"correspondence":11061,"maternal":11062,"gerard":11063,"denis":11064,"stealing":11065,"##load":11066,"genius":11067,"duchy":11068,"##oria":11069,"inaugurated":11070,"momentum":11071,"suits":11072,"placement":11073,"sovereign":11074,"clause":11075,"thames":11076,"##hara":11077,"confederation":11078,"reservation":11079,"sketch":11080,"yankees":11081,"lets":11082,"rotten":11083,"charm":11084,"hal":11085,"verses":11086,"ultra":11087,"commercially":11088,"dot":11089,"salon":11090,"citation":11091,"adopt":11092,"winnipeg":11093,"mist":11094,"allocated":11095,"cairo":11096,"##boy":11097,"jenkins":11098,"interference":11099,"objectives":11100,"##wind":11101,"1820":11102,"portfolio":11103,"armoured":11104,"sectors":11105,"##eh":11106,"initiatives":11107,"##world":11108,"integrity":11109,"exercises":11110,"robe":11111,"tap":11112,"ab":11113,"gazed":11114,"##tones":11115,"distracted":11116,"rulers":11117,"111":11118,"favorable":11119,"jerome":11120,"tended":11121,"cart":11122,"factories":11123,"##eri":11124,"diplomat":11125,"valued":11126,"gravel":11127,"charitable":11128,"##try":11129,"calvin":11130,"exploring":11131,"chang":11132,"shepherd":11133,"terrace":11134,"pdf":11135,"pupil":11136,"##ural":11137,"reflects":11138,"ups":11139,"##rch":11140,"governors":11141,"shelf":11142,"depths":11143,"##nberg":11144,"trailed":11145,"crest":11146,"tackle":11147,"##nian":11148,"##ats":11149,"hatred":11150,"##kai":11151,"clare":11152,"makers":11153,"ethiopia":11154,"longtime":11155,"detected":11156,"embedded":11157,"lacking":11158,"slapped":11159,"rely":11160,"thomson":11161,"anticipation":11162,"iso":11163,"morton":11164,"successive":11165,"agnes":11166,"screenwriter":11167,"straightened":11168,"philippe":11169,"playwright":11170,"haunted":11171,"licence":11172,"iris":11173,"intentions":11174,"sutton":11175,"112":11176,"logical":11177,"correctly":11178,"##weight":11179,"branded":11180,"licked":11181,"tipped":11182,"silva":11183,"ricky":11184,"narrator":11185,"requests":11186,"##ents":11187,"greeted":11188,"supernatural":11189,"cow":11190,"##wald":11191,"lung":11192,"refusing":11193,"employer":11194,"strait":11195,"gaelic":11196,"liner":11197,"##piece":11198,"zoe":11199,"sabha":11200,"##mba":11201,"driveway":11202,"harvest":11203,"prints":11204,"bates":11205,"reluctantly":11206,"threshold":11207,"algebra":11208,"ira":11209,"wherever":11210,"coupled":11211,"240":11212,"assumption":11213,"picks":11214,"##air":11215,"designers":11216,"raids":11217,"gentlemen":11218,"##ean":11219,"roller":11220,"blowing":11221,"leipzig":11222,"locks":11223,"screw":11224,"dressing":11225,"strand":11226,"##lings":11227,"scar":11228,"dwarf":11229,"depicts":11230,"##nu":11231,"nods":11232,"##mine":11233,"differ":11234,"boris":11235,"##eur":11236,"yuan":11237,"flip":11238,"##gie":11239,"mob":11240,"invested":11241,"questioning":11242,"applying":11243,"##ture":11244,"shout":11245,"##sel":11246,"gameplay":11247,"blamed":11248,"illustrations":11249,"bothered":11250,"weakness":11251,"rehabilitation":11252,"##of":11253,"##zes":11254,"envelope":11255,"rumors":11256,"miners":11257,"leicester":11258,"subtle":11259,"kerry":11260,"##ico":11261,"ferguson":11262,"##fu":11263,"premiership":11264,"ne":11265,"##cat":11266,"bengali":11267,"prof":11268,"catches":11269,"remnants":11270,"dana":11271,"##rily":11272,"shouting":11273,"presidents":11274,"baltic":11275,"ought":11276,"ghosts":11277,"dances":11278,"sailors":11279,"shirley":11280,"fancy":11281,"dominic":11282,"##bie":11283,"madonna":11284,"##rick":11285,"bark":11286,"buttons":11287,"gymnasium":11288,"ashes":11289,"liver":11290,"toby":11291,"oath":11292,"providence":11293,"doyle":11294,"evangelical":11295,"nixon":11296,"cement":11297,"carnegie":11298,"embarked":11299,"hatch":11300,"surroundings":11301,"guarantee":11302,"needing":11303,"pirate":11304,"essence":11305,"##bee":11306,"filter":11307,"crane":11308,"hammond":11309,"projected":11310,"immune":11311,"percy":11312,"twelfth":11313,"##ult":11314,"regent":11315,"doctoral":11316,"damon":11317,"mikhail":11318,"##ichi":11319,"lu":11320,"critically":11321,"elect":11322,"realised":11323,"abortion":11324,"acute":11325,"screening":11326,"mythology":11327,"steadily":11328,"##fc":11329,"frown":11330,"nottingham":11331,"kirk":11332,"wa":11333,"minneapolis":11334,"##rra":11335,"module":11336,"algeria":11337,"mc":11338,"nautical":11339,"encounters":11340,"surprising":11341,"statues":11342,"availability":11343,"shirts":11344,"pie":11345,"alma":11346,"brows":11347,"munster":11348,"mack":11349,"soup":11350,"crater":11351,"tornado":11352,"sanskrit":11353,"cedar":11354,"explosive":11355,"bordered":11356,"dixon":11357,"planets":11358,"stamp":11359,"exam":11360,"happily":11361,"##bble":11362,"carriers":11363,"kidnapped":11364,"##vis":11365,"accommodation":11366,"emigrated":11367,"##met":11368,"knockout":11369,"correspondent":11370,"violation":11371,"profits":11372,"peaks":11373,"lang":11374,"specimen":11375,"agenda":11376,"ancestry":11377,"pottery":11378,"spelling":11379,"equations":11380,"obtaining":11381,"ki":11382,"linking":11383,"1825":11384,"debris":11385,"asylum":11386,"##20":11387,"buddhism":11388,"teddy":11389,"##ants":11390,"gazette":11391,"##nger":11392,"##sse":11393,"dental":11394,"eligibility":11395,"utc":11396,"fathers":11397,"averaged":11398,"zimbabwe":11399,"francesco":11400,"coloured":11401,"hissed":11402,"translator":11403,"lynch":11404,"mandate":11405,"humanities":11406,"mackenzie":11407,"uniforms":11408,"lin":11409,"##iana":11410,"##gio":11411,"asset":11412,"mhz":11413,"fitting":11414,"samantha":11415,"genera":11416,"wei":11417,"rim":11418,"beloved":11419,"shark":11420,"riot":11421,"entities":11422,"expressions":11423,"indo":11424,"carmen":11425,"slipping":11426,"owing":11427,"abbot":11428,"neighbor":11429,"sidney":11430,"##av":11431,"rats":11432,"recommendations":11433,"encouraging":11434,"squadrons":11435,"anticipated":11436,"commanders":11437,"conquered":11438,"##oto":11439,"donations":11440,"diagnosed":11441,"##mond":11442,"divide":11443,"##iva":11444,"guessed":11445,"decoration":11446,"vernon":11447,"auditorium":11448,"revelation":11449,"conversations":11450,"##kers":11451,"##power":11452,"herzegovina":11453,"dash":11454,"alike":11455,"protested":11456,"lateral":11457,"herman":11458,"accredited":11459,"mg":11460,"##gent":11461,"freeman":11462,"mel":11463,"fiji":11464,"crow":11465,"crimson":11466,"##rine":11467,"livestock":11468,"##pped":11469,"humanitarian":11470,"bored":11471,"oz":11472,"whip":11473,"##lene":11474,"##ali":11475,"legitimate":11476,"alter":11477,"grinning":11478,"spelled":11479,"anxious":11480,"oriental":11481,"wesley":11482,"##nin":11483,"##hole":11484,"carnival":11485,"controller":11486,"detect":11487,"##ssa":11488,"bowed":11489,"educator":11490,"kosovo":11491,"macedonia":11492,"##sin":11493,"occupy":11494,"mastering":11495,"stephanie":11496,"janeiro":11497,"para":11498,"unaware":11499,"nurses":11500,"noon":11501,"135":11502,"cam":11503,"hopefully":11504,"ranger":11505,"combine":11506,"sociology":11507,"polar":11508,"rica":11509,"##eer":11510,"neill":11511,"##sman":11512,"holocaust":11513,"##ip":11514,"doubled":11515,"lust":11516,"1828":11517,"109":11518,"decent":11519,"cooling":11520,"unveiled":11521,"##card":11522,"1829":11523,"nsw":11524,"homer":11525,"chapman":11526,"meyer":11527,"##gin":11528,"dive":11529,"mae":11530,"reagan":11531,"expertise":11532,"##gled":11533,"darwin":11534,"brooke":11535,"sided":11536,"prosecution":11537,"investigating":11538,"comprised":11539,"petroleum":11540,"genres":11541,"reluctant":11542,"differently":11543,"trilogy":11544,"johns":11545,"vegetables":11546,"corpse":11547,"highlighted":11548,"lounge":11549,"pension":11550,"unsuccessfully":11551,"elegant":11552,"aided":11553,"ivory":11554,"beatles":11555,"amelia":11556,"cain":11557,"dubai":11558,"sunny":11559,"immigrant":11560,"babe":11561,"click":11562,"##nder":11563,"underwater":11564,"pepper":11565,"combining":11566,"mumbled":11567,"atlas":11568,"horns":11569,"accessed":11570,"ballad":11571,"physicians":11572,"homeless":11573,"gestured":11574,"rpm":11575,"freak":11576,"louisville":11577,"corporations":11578,"patriots":11579,"prizes":11580,"rational":11581,"warn":11582,"modes":11583,"decorative":11584,"overnight":11585,"din":11586,"troubled":11587,"phantom":11588,"##ort":11589,"monarch":11590,"sheer":11591,"##dorf":11592,"generals":11593,"guidelines":11594,"organs":11595,"addresses":11596,"##zon":11597,"enhance":11598,"curling":11599,"parishes":11600,"cord":11601,"##kie":11602,"linux":11603,"caesar":11604,"deutsche":11605,"bavaria":11606,"##bia":11607,"coleman":11608,"cyclone":11609,"##eria":11610,"bacon":11611,"petty":11612,"##yama":11613,"##old":11614,"hampton":11615,"diagnosis":11616,"1824":11617,"throws":11618,"complexity":11619,"rita":11620,"disputed":11621,"##₃":11622,"pablo":11623,"##sch":11624,"marketed":11625,"trafficking":11626,"##ulus":11627,"examine":11628,"plague":11629,"formats":11630,"##oh":11631,"vault":11632,"faithful":11633,"##bourne":11634,"webster":11635,"##ox":11636,"highlights":11637,"##ient":11638,"##ann":11639,"phones":11640,"vacuum":11641,"sandwich":11642,"modeling":11643,"##gated":11644,"bolivia":11645,"clergy":11646,"qualities":11647,"isabel":11648,"##nas":11649,"##ars":11650,"wears":11651,"screams":11652,"reunited":11653,"annoyed":11654,"bra":11655,"##ancy":11656,"##rate":11657,"differential":11658,"transmitter":11659,"tattoo":11660,"container":11661,"poker":11662,"##och":11663,"excessive":11664,"resides":11665,"cowboys":11666,"##tum":11667,"augustus":11668,"trash":11669,"providers":11670,"statute":11671,"retreated":11672,"balcony":11673,"reversed":11674,"void":11675,"storey":11676,"preceded":11677,"masses":11678,"leap":11679,"laughs":11680,"neighborhoods":11681,"wards":11682,"schemes":11683,"falcon":11684,"santo":11685,"battlefield":11686,"pad":11687,"ronnie":11688,"thread":11689,"lesbian":11690,"venus":11691,"##dian":11692,"beg":11693,"sandstone":11694,"daylight":11695,"punched":11696,"gwen":11697,"analog":11698,"stroked":11699,"wwe":11700,"acceptable":11701,"measurements":11702,"dec":11703,"toxic":11704,"##kel":11705,"adequate":11706,"surgical":11707,"economist":11708,"parameters":11709,"varsity":11710,"##sberg":11711,"quantity":11712,"ella":11713,"##chy":11714,"##rton":11715,"countess":11716,"generating":11717,"precision":11718,"diamonds":11719,"expressway":11720,"ga":11721,"##ı":11722,"1821":11723,"uruguay":11724,"talents":11725,"galleries":11726,"expenses":11727,"scanned":11728,"colleague":11729,"outlets":11730,"ryder":11731,"lucien":11732,"##ila":11733,"paramount":11734,"##bon":11735,"syracuse":11736,"dim":11737,"fangs":11738,"gown":11739,"sweep":11740,"##sie":11741,"toyota":11742,"missionaries":11743,"websites":11744,"##nsis":11745,"sentences":11746,"adviser":11747,"val":11748,"trademark":11749,"spells":11750,"##plane":11751,"patience":11752,"starter":11753,"slim":11754,"##borg":11755,"toe":11756,"incredibly":11757,"shoots":11758,"elliot":11759,"nobility":11760,"##wyn":11761,"cowboy":11762,"endorsed":11763,"gardner":11764,"tendency":11765,"persuaded":11766,"organisms":11767,"emissions":11768,"kazakhstan":11769,"amused":11770,"boring":11771,"chips":11772,"themed":11773,"##hand":11774,"llc":11775,"constantinople":11776,"chasing":11777,"systematic":11778,"guatemala":11779,"borrowed":11780,"erin":11781,"carey":11782,"##hard":11783,"highlands":11784,"struggles":11785,"1810":11786,"##ifying":11787,"##ced":11788,"wong":11789,"exceptions":11790,"develops":11791,"enlarged":11792,"kindergarten":11793,"castro":11794,"##ern":11795,"##rina":11796,"leigh":11797,"zombie":11798,"juvenile":11799,"##most":11800,"consul":11801,"##nar":11802,"sailor":11803,"hyde":11804,"clarence":11805,"intensive":11806,"pinned":11807,"nasty":11808,"useless":11809,"jung":11810,"clayton":11811,"stuffed":11812,"exceptional":11813,"ix":11814,"apostolic":11815,"230":11816,"transactions":11817,"##dge":11818,"exempt":11819,"swinging":11820,"cove":11821,"religions":11822,"##ash":11823,"shields":11824,"dairy":11825,"bypass":11826,"190":11827,"pursuing":11828,"bug":11829,"joyce":11830,"bombay":11831,"chassis":11832,"southampton":11833,"chat":11834,"interact":11835,"redesignated":11836,"##pen":11837,"nascar":11838,"pray":11839,"salmon":11840,"rigid":11841,"regained":11842,"malaysian":11843,"grim":11844,"publicity":11845,"constituted":11846,"capturing":11847,"toilet":11848,"delegate":11849,"purely":11850,"tray":11851,"drift":11852,"loosely":11853,"striker":11854,"weakened":11855,"trinidad":11856,"mitch":11857,"itv":11858,"defines":11859,"transmitted":11860,"ming":11861,"scarlet":11862,"nodding":11863,"fitzgerald":11864,"fu":11865,"narrowly":11866,"sp":11867,"tooth":11868,"standings":11869,"virtue":11870,"##₁":11871,"##wara":11872,"##cting":11873,"chateau":11874,"gloves":11875,"lid":11876,"##nel":11877,"hurting":11878,"conservatory":11879,"##pel":11880,"sinclair":11881,"reopened":11882,"sympathy":11883,"nigerian":11884,"strode":11885,"advocated":11886,"optional":11887,"chronic":11888,"discharge":11889,"##rc":11890,"suck":11891,"compatible":11892,"laurel":11893,"stella":11894,"shi":11895,"fails":11896,"wage":11897,"dodge":11898,"128":11899,"informal":11900,"sorts":11901,"levi":11902,"buddha":11903,"villagers":11904,"##aka":11905,"chronicles":11906,"heavier":11907,"summoned":11908,"gateway":11909,"3000":11910,"eleventh":11911,"jewelry":11912,"translations":11913,"accordingly":11914,"seas":11915,"##ency":11916,"fiber":11917,"pyramid":11918,"cubic":11919,"dragging":11920,"##ista":11921,"caring":11922,"##ops":11923,"android":11924,"contacted":11925,"lunar":11926,"##dt":11927,"kai":11928,"lisbon":11929,"patted":11930,"1826":11931,"sacramento":11932,"theft":11933,"madagascar":11934,"subtropical":11935,"disputes":11936,"ta":11937,"holidays":11938,"piper":11939,"willow":11940,"mare":11941,"cane":11942,"itunes":11943,"newfoundland":11944,"benny":11945,"companions":11946,"dong":11947,"raj":11948,"observe":11949,"roar":11950,"charming":11951,"plaque":11952,"tibetan":11953,"fossils":11954,"enacted":11955,"manning":11956,"bubble":11957,"tina":11958,"tanzania":11959,"##eda":11960,"##hir":11961,"funk":11962,"swamp":11963,"deputies":11964,"cloak":11965,"ufc":11966,"scenario":11967,"par":11968,"scratch":11969,"metals":11970,"anthem":11971,"guru":11972,"engaging":11973,"specially":11974,"##boat":11975,"dialects":11976,"nineteen":11977,"cecil":11978,"duet":11979,"disability":11980,"messenger":11981,"unofficial":11982,"##lies":11983,"defunct":11984,"eds":11985,"moonlight":11986,"drainage":11987,"surname":11988,"puzzle":11989,"honda":11990,"switching":11991,"conservatives":11992,"mammals":11993,"knox":11994,"broadcaster":11995,"sidewalk":11996,"cope":11997,"##ried":11998,"benson":11999,"princes":12000,"peterson":12001,"##sal":12002,"bedford":12003,"sharks":12004,"eli":12005,"wreck":12006,"alberto":12007,"gasp":12008,"archaeology":12009,"lgbt":12010,"teaches":12011,"securities":12012,"madness":12013,"compromise":12014,"waving":12015,"coordination":12016,"davidson":12017,"visions":12018,"leased":12019,"possibilities":12020,"eighty":12021,"jun":12022,"fernandez":12023,"enthusiasm":12024,"assassin":12025,"sponsorship":12026,"reviewer":12027,"kingdoms":12028,"estonian":12029,"laboratories":12030,"##fy":12031,"##nal":12032,"applies":12033,"verb":12034,"celebrations":12035,"##zzo":12036,"rowing":12037,"lightweight":12038,"sadness":12039,"submit":12040,"mvp":12041,"balanced":12042,"dude":12043,"##vas":12044,"explicitly":12045,"metric":12046,"magnificent":12047,"mound":12048,"brett":12049,"mohammad":12050,"mistakes":12051,"irregular":12052,"##hing":12053,"##ass":12054,"sanders":12055,"betrayed":12056,"shipped":12057,"surge":12058,"##enburg":12059,"reporters":12060,"termed":12061,"georg":12062,"pity":12063,"verbal":12064,"bulls":12065,"abbreviated":12066,"enabling":12067,"appealed":12068,"##are":12069,"##atic":12070,"sicily":12071,"sting":12072,"heel":12073,"sweetheart":12074,"bart":12075,"spacecraft":12076,"brutal":12077,"monarchy":12078,"##tter":12079,"aberdeen":12080,"cameo":12081,"diane":12082,"##ub":12083,"survivor":12084,"clyde":12085,"##aries":12086,"complaint":12087,"##makers":12088,"clarinet":12089,"delicious":12090,"chilean":12091,"karnataka":12092,"coordinates":12093,"1818":12094,"panties":12095,"##rst":12096,"pretending":12097,"ar":12098,"dramatically":12099,"kiev":12100,"bella":12101,"tends":12102,"distances":12103,"113":12104,"catalog":12105,"launching":12106,"instances":12107,"telecommunications":12108,"portable":12109,"lindsay":12110,"vatican":12111,"##eim":12112,"angles":12113,"aliens":12114,"marker":12115,"stint":12116,"screens":12117,"bolton":12118,"##rne":12119,"judy":12120,"wool":12121,"benedict":12122,"plasma":12123,"europa":12124,"spark":12125,"imaging":12126,"filmmaker":12127,"swiftly":12128,"##een":12129,"contributor":12130,"##nor":12131,"opted":12132,"stamps":12133,"apologize":12134,"financing":12135,"butter":12136,"gideon":12137,"sophisticated":12138,"alignment":12139,"avery":12140,"chemicals":12141,"yearly":12142,"speculation":12143,"prominence":12144,"professionally":12145,"##ils":12146,"immortal":12147,"institutional":12148,"inception":12149,"wrists":12150,"identifying":12151,"tribunal":12152,"derives":12153,"gains":12154,"##wo":12155,"papal":12156,"preference":12157,"linguistic":12158,"vince":12159,"operative":12160,"brewery":12161,"##ont":12162,"unemployment":12163,"boyd":12164,"##ured":12165,"##outs":12166,"albeit":12167,"prophet":12168,"1813":12169,"bi":12170,"##rr":12171,"##face":12172,"##rad":12173,"quarterly":12174,"asteroid":12175,"cleaned":12176,"radius":12177,"temper":12178,"##llen":12179,"telugu":12180,"jerk":12181,"viscount":12182,"menu":12183,"##ote":12184,"glimpse":12185,"##aya":12186,"yacht":12187,"hawaiian":12188,"baden":12189,"##rl":12190,"laptop":12191,"readily":12192,"##gu":12193,"monetary":12194,"offshore":12195,"scots":12196,"watches":12197,"##yang":12198,"##arian":12199,"upgrade":12200,"needle":12201,"xbox":12202,"lea":12203,"encyclopedia":12204,"flank":12205,"fingertips":12206,"##pus":12207,"delight":12208,"teachings":12209,"confirm":12210,"roth":12211,"beaches":12212,"midway":12213,"winters":12214,"##iah":12215,"teasing":12216,"daytime":12217,"beverly":12218,"gambling":12219,"bonnie":12220,"##backs":12221,"regulated":12222,"clement":12223,"hermann":12224,"tricks":12225,"knot":12226,"##shing":12227,"##uring":12228,"##vre":12229,"detached":12230,"ecological":12231,"owed":12232,"specialty":12233,"byron":12234,"inventor":12235,"bats":12236,"stays":12237,"screened":12238,"unesco":12239,"midland":12240,"trim":12241,"affection":12242,"##ander":12243,"##rry":12244,"jess":12245,"thoroughly":12246,"feedback":12247,"##uma":12248,"chennai":12249,"strained":12250,"heartbeat":12251,"wrapping":12252,"overtime":12253,"pleaded":12254,"##sworth":12255,"mon":12256,"leisure":12257,"oclc":12258,"##tate":12259,"##ele":12260,"feathers":12261,"angelo":12262,"thirds":12263,"nuts":12264,"surveys":12265,"clever":12266,"gill":12267,"commentator":12268,"##dos":12269,"darren":12270,"rides":12271,"gibraltar":12272,"##nc":12273,"##mu":12274,"dissolution":12275,"dedication":12276,"shin":12277,"meals":12278,"saddle":12279,"elvis":12280,"reds":12281,"chaired":12282,"taller":12283,"appreciation":12284,"functioning":12285,"niece":12286,"favored":12287,"advocacy":12288,"robbie":12289,"criminals":12290,"suffolk":12291,"yugoslav":12292,"passport":12293,"constable":12294,"congressman":12295,"hastings":12296,"vera":12297,"##rov":12298,"consecrated":12299,"sparks":12300,"ecclesiastical":12301,"confined":12302,"##ovich":12303,"muller":12304,"floyd":12305,"nora":12306,"1822":12307,"paved":12308,"1827":12309,"cumberland":12310,"ned":12311,"saga":12312,"spiral":12313,"##flow":12314,"appreciated":12315,"yi":12316,"collaborative":12317,"treating":12318,"similarities":12319,"feminine":12320,"finishes":12321,"##ib":12322,"jade":12323,"import":12324,"##nse":12325,"##hot":12326,"champagne":12327,"mice":12328,"securing":12329,"celebrities":12330,"helsinki":12331,"attributes":12332,"##gos":12333,"cousins":12334,"phases":12335,"ache":12336,"lucia":12337,"gandhi":12338,"submission":12339,"vicar":12340,"spear":12341,"shine":12342,"tasmania":12343,"biting":12344,"detention":12345,"constitute":12346,"tighter":12347,"seasonal":12348,"##gus":12349,"terrestrial":12350,"matthews":12351,"##oka":12352,"effectiveness":12353,"parody":12354,"philharmonic":12355,"##onic":12356,"1816":12357,"strangers":12358,"encoded":12359,"consortium":12360,"guaranteed":12361,"regards":12362,"shifts":12363,"tortured":12364,"collision":12365,"supervisor":12366,"inform":12367,"broader":12368,"insight":12369,"theaters":12370,"armour":12371,"emeritus":12372,"blink":12373,"incorporates":12374,"mapping":12375,"##50":12376,"##ein":12377,"handball":12378,"flexible":12379,"##nta":12380,"substantially":12381,"generous":12382,"thief":12383,"##own":12384,"carr":12385,"loses":12386,"1793":12387,"prose":12388,"ucla":12389,"romeo":12390,"generic":12391,"metallic":12392,"realization":12393,"damages":12394,"mk":12395,"commissioners":12396,"zach":12397,"default":12398,"##ther":12399,"helicopters":12400,"lengthy":12401,"stems":12402,"spa":12403,"partnered":12404,"spectators":12405,"rogue":12406,"indication":12407,"penalties":12408,"teresa":12409,"1801":12410,"sen":12411,"##tric":12412,"dalton":12413,"##wich":12414,"irving":12415,"photographic":12416,"##vey":12417,"dell":12418,"deaf":12419,"peters":12420,"excluded":12421,"unsure":12422,"##vable":12423,"patterson":12424,"crawled":12425,"##zio":12426,"resided":12427,"whipped":12428,"latvia":12429,"slower":12430,"ecole":12431,"pipes":12432,"employers":12433,"maharashtra":12434,"comparable":12435,"va":12436,"textile":12437,"pageant":12438,"##gel":12439,"alphabet":12440,"binary":12441,"irrigation":12442,"chartered":12443,"choked":12444,"antoine":12445,"offs":12446,"waking":12447,"supplement":12448,"##wen":12449,"quantities":12450,"demolition":12451,"regain":12452,"locate":12453,"urdu":12454,"folks":12455,"alt":12456,"114":12457,"##mc":12458,"scary":12459,"andreas":12460,"whites":12461,"##ava":12462,"classrooms":12463,"mw":12464,"aesthetic":12465,"publishes":12466,"valleys":12467,"guides":12468,"cubs":12469,"johannes":12470,"bryant":12471,"conventions":12472,"affecting":12473,"##itt":12474,"drain":12475,"awesome":12476,"isolation":12477,"prosecutor":12478,"ambitious":12479,"apology":12480,"captive":12481,"downs":12482,"atmospheric":12483,"lorenzo":12484,"aisle":12485,"beef":12486,"foul":12487,"##onia":12488,"kidding":12489,"composite":12490,"disturbed":12491,"illusion":12492,"natives":12493,"##ffer":12494,"emi":12495,"rockets":12496,"riverside":12497,"wartime":12498,"painters":12499,"adolf":12500,"melted":12501,"##ail":12502,"uncertainty":12503,"simulation":12504,"hawks":12505,"progressed":12506,"meantime":12507,"builder":12508,"spray":12509,"breach":12510,"unhappy":12511,"regina":12512,"russians":12513,"##urg":12514,"determining":12515,"##tation":12516,"tram":12517,"1806":12518,"##quin":12519,"aging":12520,"##12":12521,"1823":12522,"garion":12523,"rented":12524,"mister":12525,"diaz":12526,"terminated":12527,"clip":12528,"1817":12529,"depend":12530,"nervously":12531,"disco":12532,"owe":12533,"defenders":12534,"shiva":12535,"notorious":12536,"disbelief":12537,"shiny":12538,"worcester":12539,"##gation":12540,"##yr":12541,"trailing":12542,"undertook":12543,"islander":12544,"belarus":12545,"limitations":12546,"watershed":12547,"fuller":12548,"overlooking":12549,"utilized":12550,"raphael":12551,"1819":12552,"synthetic":12553,"breakdown":12554,"klein":12555,"##nate":12556,"moaned":12557,"memoir":12558,"lamb":12559,"practicing":12560,"##erly":12561,"cellular":12562,"arrows":12563,"exotic":12564,"##graphy":12565,"witches":12566,"117":12567,"charted":12568,"rey":12569,"hut":12570,"hierarchy":12571,"subdivision":12572,"freshwater":12573,"giuseppe":12574,"aloud":12575,"reyes":12576,"qatar":12577,"marty":12578,"sideways":12579,"utterly":12580,"sexually":12581,"jude":12582,"prayers":12583,"mccarthy":12584,"softball":12585,"blend":12586,"damien":12587,"##gging":12588,"##metric":12589,"wholly":12590,"erupted":12591,"lebanese":12592,"negro":12593,"revenues":12594,"tasted":12595,"comparative":12596,"teamed":12597,"transaction":12598,"labeled":12599,"maori":12600,"sovereignty":12601,"parkway":12602,"trauma":12603,"gran":12604,"malay":12605,"121":12606,"advancement":12607,"descendant":12608,"2020":12609,"buzz":12610,"salvation":12611,"inventory":12612,"symbolic":12613,"##making":12614,"antarctica":12615,"mps":12616,"##gas":12617,"##bro":12618,"mohammed":12619,"myanmar":12620,"holt":12621,"submarines":12622,"tones":12623,"##lman":12624,"locker":12625,"patriarch":12626,"bangkok":12627,"emerson":12628,"remarks":12629,"predators":12630,"kin":12631,"afghan":12632,"confession":12633,"norwich":12634,"rental":12635,"emerge":12636,"advantages":12637,"##zel":12638,"rca":12639,"##hold":12640,"shortened":12641,"storms":12642,"aidan":12643,"##matic":12644,"autonomy":12645,"compliance":12646,"##quet":12647,"dudley":12648,"atp":12649,"##osis":12650,"1803":12651,"motto":12652,"documentation":12653,"summary":12654,"professors":12655,"spectacular":12656,"christina":12657,"archdiocese":12658,"flashing":12659,"innocence":12660,"remake":12661,"##dell":12662,"psychic":12663,"reef":12664,"scare":12665,"employ":12666,"rs":12667,"sticks":12668,"meg":12669,"gus":12670,"leans":12671,"##ude":12672,"accompany":12673,"bergen":12674,"tomas":12675,"##iko":12676,"doom":12677,"wages":12678,"pools":12679,"##nch":12680,"##bes":12681,"breasts":12682,"scholarly":12683,"alison":12684,"outline":12685,"brittany":12686,"breakthrough":12687,"willis":12688,"realistic":12689,"##cut":12690,"##boro":12691,"competitor":12692,"##stan":12693,"pike":12694,"picnic":12695,"icon":12696,"designing":12697,"commercials":12698,"washing":12699,"villain":12700,"skiing":12701,"micro":12702,"costumes":12703,"auburn":12704,"halted":12705,"executives":12706,"##hat":12707,"logistics":12708,"cycles":12709,"vowel":12710,"applicable":12711,"barrett":12712,"exclaimed":12713,"eurovision":12714,"eternity":12715,"ramon":12716,"##umi":12717,"##lls":12718,"modifications":12719,"sweeping":12720,"disgust":12721,"##uck":12722,"torch":12723,"aviv":12724,"ensuring":12725,"rude":12726,"dusty":12727,"sonic":12728,"donovan":12729,"outskirts":12730,"cu":12731,"pathway":12732,"##band":12733,"##gun":12734,"##lines":12735,"disciplines":12736,"acids":12737,"cadet":12738,"paired":12739,"##40":12740,"sketches":12741,"##sive":12742,"marriages":12743,"##⁺":12744,"folding":12745,"peers":12746,"slovak":12747,"implies":12748,"admired":12749,"##beck":12750,"1880s":12751,"leopold":12752,"instinct":12753,"attained":12754,"weston":12755,"megan":12756,"horace":12757,"##ination":12758,"dorsal":12759,"ingredients":12760,"evolutionary":12761,"##its":12762,"complications":12763,"deity":12764,"lethal":12765,"brushing":12766,"levy":12767,"deserted":12768,"institutes":12769,"posthumously":12770,"delivering":12771,"telescope":12772,"coronation":12773,"motivated":12774,"rapids":12775,"luc":12776,"flicked":12777,"pays":12778,"volcano":12779,"tanner":12780,"weighed":12781,"##nica":12782,"crowds":12783,"frankie":12784,"gifted":12785,"addressing":12786,"granddaughter":12787,"winding":12788,"##rna":12789,"constantine":12790,"gomez":12791,"##front":12792,"landscapes":12793,"rudolf":12794,"anthropology":12795,"slate":12796,"werewolf":12797,"##lio":12798,"astronomy":12799,"circa":12800,"rouge":12801,"dreaming":12802,"sack":12803,"knelt":12804,"drowned":12805,"naomi":12806,"prolific":12807,"tracked":12808,"freezing":12809,"herb":12810,"##dium":12811,"agony":12812,"randall":12813,"twisting":12814,"wendy":12815,"deposit":12816,"touches":12817,"vein":12818,"wheeler":12819,"##bbled":12820,"##bor":12821,"batted":12822,"retaining":12823,"tire":12824,"presently":12825,"compare":12826,"specification":12827,"daemon":12828,"nigel":12829,"##grave":12830,"merry":12831,"recommendation":12832,"czechoslovakia":12833,"sandra":12834,"ng":12835,"roma":12836,"##sts":12837,"lambert":12838,"inheritance":12839,"sheikh":12840,"winchester":12841,"cries":12842,"examining":12843,"##yle":12844,"comeback":12845,"cuisine":12846,"nave":12847,"##iv":12848,"ko":12849,"retrieve":12850,"tomatoes":12851,"barker":12852,"polished":12853,"defining":12854,"irene":12855,"lantern":12856,"personalities":12857,"begging":12858,"tract":12859,"swore":12860,"1809":12861,"175":12862,"##gic":12863,"omaha":12864,"brotherhood":12865,"##rley":12866,"haiti":12867,"##ots":12868,"exeter":12869,"##ete":12870,"##zia":12871,"steele":12872,"dumb":12873,"pearson":12874,"210":12875,"surveyed":12876,"elisabeth":12877,"trends":12878,"##ef":12879,"fritz":12880,"##rf":12881,"premium":12882,"bugs":12883,"fraction":12884,"calmly":12885,"viking":12886,"##birds":12887,"tug":12888,"inserted":12889,"unusually":12890,"##ield":12891,"confronted":12892,"distress":12893,"crashing":12894,"brent":12895,"turks":12896,"resign":12897,"##olo":12898,"cambodia":12899,"gabe":12900,"sauce":12901,"##kal":12902,"evelyn":12903,"116":12904,"extant":12905,"clusters":12906,"quarry":12907,"teenagers":12908,"luna":12909,"##lers":12910,"##ister":12911,"affiliation":12912,"drill":12913,"##ashi":12914,"panthers":12915,"scenic":12916,"libya":12917,"anita":12918,"strengthen":12919,"inscriptions":12920,"##cated":12921,"lace":12922,"sued":12923,"judith":12924,"riots":12925,"##uted":12926,"mint":12927,"##eta":12928,"preparations":12929,"midst":12930,"dub":12931,"challenger":12932,"##vich":12933,"mock":12934,"cf":12935,"displaced":12936,"wicket":12937,"breaths":12938,"enables":12939,"schmidt":12940,"analyst":12941,"##lum":12942,"ag":12943,"highlight":12944,"automotive":12945,"axe":12946,"josef":12947,"newark":12948,"sufficiently":12949,"resembles":12950,"50th":12951,"##pal":12952,"flushed":12953,"mum":12954,"traits":12955,"##ante":12956,"commodore":12957,"incomplete":12958,"warming":12959,"titular":12960,"ceremonial":12961,"ethical":12962,"118":12963,"celebrating":12964,"eighteenth":12965,"cao":12966,"lima":12967,"medalist":12968,"mobility":12969,"strips":12970,"snakes":12971,"##city":12972,"miniature":12973,"zagreb":12974,"barton":12975,"escapes":12976,"umbrella":12977,"automated":12978,"doubted":12979,"differs":12980,"cooled":12981,"georgetown":12982,"dresden":12983,"cooked":12984,"fade":12985,"wyatt":12986,"rna":12987,"jacobs":12988,"carlton":12989,"abundant":12990,"stereo":12991,"boost":12992,"madras":12993,"inning":12994,"##hia":12995,"spur":12996,"ip":12997,"malayalam":12998,"begged":12999,"osaka":13000,"groan":13001,"escaping":13002,"charging":13003,"dose":13004,"vista":13005,"##aj":13006,"bud":13007,"papa":13008,"communists":13009,"advocates":13010,"edged":13011,"tri":13012,"##cent":13013,"resemble":13014,"peaking":13015,"necklace":13016,"fried":13017,"montenegro":13018,"saxony":13019,"goose":13020,"glances":13021,"stuttgart":13022,"curator":13023,"recruit":13024,"grocery":13025,"sympathetic":13026,"##tting":13027,"##fort":13028,"127":13029,"lotus":13030,"randolph":13031,"ancestor":13032,"##rand":13033,"succeeding":13034,"jupiter":13035,"1798":13036,"macedonian":13037,"##heads":13038,"hiking":13039,"1808":13040,"handing":13041,"fischer":13042,"##itive":13043,"garbage":13044,"node":13045,"##pies":13046,"prone":13047,"singular":13048,"papua":13049,"inclined":13050,"attractions":13051,"italia":13052,"pouring":13053,"motioned":13054,"grandma":13055,"garnered":13056,"jacksonville":13057,"corp":13058,"ego":13059,"ringing":13060,"aluminum":13061,"##hausen":13062,"ordering":13063,"##foot":13064,"drawer":13065,"traders":13066,"synagogue":13067,"##play":13068,"##kawa":13069,"resistant":13070,"wandering":13071,"fragile":13072,"fiona":13073,"teased":13074,"var":13075,"hardcore":13076,"soaked":13077,"jubilee":13078,"decisive":13079,"exposition":13080,"mercer":13081,"poster":13082,"valencia":13083,"hale":13084,"kuwait":13085,"1811":13086,"##ises":13087,"##wr":13088,"##eed":13089,"tavern":13090,"gamma":13091,"122":13092,"johan":13093,"##uer":13094,"airways":13095,"amino":13096,"gil":13097,"##ury":13098,"vocational":13099,"domains":13100,"torres":13101,"##sp":13102,"generator":13103,"folklore":13104,"outcomes":13105,"##keeper":13106,"canberra":13107,"shooter":13108,"fl":13109,"beams":13110,"confrontation":13111,"##lling":13112,"##gram":13113,"feb":13114,"aligned":13115,"forestry":13116,"pipeline":13117,"jax":13118,"motorway":13119,"conception":13120,"decay":13121,"##tos":13122,"coffin":13123,"##cott":13124,"stalin":13125,"1805":13126,"escorted":13127,"minded":13128,"##nam":13129,"sitcom":13130,"purchasing":13131,"twilight":13132,"veronica":13133,"additions":13134,"passive":13135,"tensions":13136,"straw":13137,"123":13138,"frequencies":13139,"1804":13140,"refugee":13141,"cultivation":13142,"##iate":13143,"christie":13144,"clary":13145,"bulletin":13146,"crept":13147,"disposal":13148,"##rich":13149,"##zong":13150,"processor":13151,"crescent":13152,"##rol":13153,"bmw":13154,"emphasized":13155,"whale":13156,"nazis":13157,"aurora":13158,"##eng":13159,"dwelling":13160,"hauled":13161,"sponsors":13162,"toledo":13163,"mega":13164,"ideology":13165,"theatres":13166,"tessa":13167,"cerambycidae":13168,"saves":13169,"turtle":13170,"cone":13171,"suspects":13172,"kara":13173,"rusty":13174,"yelling":13175,"greeks":13176,"mozart":13177,"shades":13178,"cocked":13179,"participant":13180,"##tro":13181,"shire":13182,"spit":13183,"freeze":13184,"necessity":13185,"##cos":13186,"inmates":13187,"nielsen":13188,"councillors":13189,"loaned":13190,"uncommon":13191,"omar":13192,"peasants":13193,"botanical":13194,"offspring":13195,"daniels":13196,"formations":13197,"jokes":13198,"1794":13199,"pioneers":13200,"sigma":13201,"licensing":13202,"##sus":13203,"wheelchair":13204,"polite":13205,"1807":13206,"liquor":13207,"pratt":13208,"trustee":13209,"##uta":13210,"forewings":13211,"balloon":13212,"##zz":13213,"kilometre":13214,"camping":13215,"explicit":13216,"casually":13217,"shawn":13218,"foolish":13219,"teammates":13220,"nm":13221,"hassan":13222,"carrie":13223,"judged":13224,"satisfy":13225,"vanessa":13226,"knives":13227,"selective":13228,"cnn":13229,"flowed":13230,"##lice":13231,"eclipse":13232,"stressed":13233,"eliza":13234,"mathematician":13235,"cease":13236,"cultivated":13237,"##roy":13238,"commissions":13239,"browns":13240,"##ania":13241,"destroyers":13242,"sheridan":13243,"meadow":13244,"##rius":13245,"minerals":13246,"##cial":13247,"downstream":13248,"clash":13249,"gram":13250,"memoirs":13251,"ventures":13252,"baha":13253,"seymour":13254,"archie":13255,"midlands":13256,"edith":13257,"fare":13258,"flynn":13259,"invite":13260,"canceled":13261,"tiles":13262,"stabbed":13263,"boulder":13264,"incorporate":13265,"amended":13266,"camden":13267,"facial":13268,"mollusk":13269,"unreleased":13270,"descriptions":13271,"yoga":13272,"grabs":13273,"550":13274,"raises":13275,"ramp":13276,"shiver":13277,"##rose":13278,"coined":13279,"pioneering":13280,"tunes":13281,"qing":13282,"warwick":13283,"tops":13284,"119":13285,"melanie":13286,"giles":13287,"##rous":13288,"wandered":13289,"##inal":13290,"annexed":13291,"nov":13292,"30th":13293,"unnamed":13294,"##ished":13295,"organizational":13296,"airplane":13297,"normandy":13298,"stoke":13299,"whistle":13300,"blessing":13301,"violations":13302,"chased":13303,"holders":13304,"shotgun":13305,"##ctic":13306,"outlet":13307,"reactor":13308,"##vik":13309,"tires":13310,"tearing":13311,"shores":13312,"fortified":13313,"mascot":13314,"constituencies":13315,"nc":13316,"columnist":13317,"productive":13318,"tibet":13319,"##rta":13320,"lineage":13321,"hooked":13322,"oct":13323,"tapes":13324,"judging":13325,"cody":13326,"##gger":13327,"hansen":13328,"kashmir":13329,"triggered":13330,"##eva":13331,"solved":13332,"cliffs":13333,"##tree":13334,"resisted":13335,"anatomy":13336,"protesters":13337,"transparent":13338,"implied":13339,"##iga":13340,"injection":13341,"mattress":13342,"excluding":13343,"##mbo":13344,"defenses":13345,"helpless":13346,"devotion":13347,"##elli":13348,"growl":13349,"liberals":13350,"weber":13351,"phenomena":13352,"atoms":13353,"plug":13354,"##iff":13355,"mortality":13356,"apprentice":13357,"howe":13358,"convincing":13359,"aaa":13360,"swimmer":13361,"barber":13362,"leone":13363,"promptly":13364,"sodium":13365,"def":13366,"nowadays":13367,"arise":13368,"##oning":13369,"gloucester":13370,"corrected":13371,"dignity":13372,"norm":13373,"erie":13374,"##ders":13375,"elders":13376,"evacuated":13377,"sylvia":13378,"compression":13379,"##yar":13380,"hartford":13381,"pose":13382,"backpack":13383,"reasoning":13384,"accepts":13385,"24th":13386,"wipe":13387,"millimetres":13388,"marcel":13389,"##oda":13390,"dodgers":13391,"albion":13392,"1790":13393,"overwhelmed":13394,"aerospace":13395,"oaks":13396,"1795":13397,"showcase":13398,"acknowledge":13399,"recovering":13400,"nolan":13401,"ashe":13402,"hurts":13403,"geology":13404,"fashioned":13405,"disappearance":13406,"farewell":13407,"swollen":13408,"shrug":13409,"marquis":13410,"wimbledon":13411,"124":13412,"rue":13413,"1792":13414,"commemorate":13415,"reduces":13416,"experiencing":13417,"inevitable":13418,"calcutta":13419,"intel":13420,"##court":13421,"murderer":13422,"sticking":13423,"fisheries":13424,"imagery":13425,"bloom":13426,"280":13427,"brake":13428,"##inus":13429,"gustav":13430,"hesitation":13431,"memorable":13432,"po":13433,"viral":13434,"beans":13435,"accidents":13436,"tunisia":13437,"antenna":13438,"spilled":13439,"consort":13440,"treatments":13441,"aye":13442,"perimeter":13443,"##gard":13444,"donation":13445,"hostage":13446,"migrated":13447,"banker":13448,"addiction":13449,"apex":13450,"lil":13451,"trout":13452,"##ously":13453,"conscience":13454,"##nova":13455,"rams":13456,"sands":13457,"genome":13458,"passionate":13459,"troubles":13460,"##lets":13461,"##set":13462,"amid":13463,"##ibility":13464,"##ret":13465,"higgins":13466,"exceed":13467,"vikings":13468,"##vie":13469,"payne":13470,"##zan":13471,"muscular":13472,"##ste":13473,"defendant":13474,"sucking":13475,"##wal":13476,"ibrahim":13477,"fuselage":13478,"claudia":13479,"vfl":13480,"europeans":13481,"snails":13482,"interval":13483,"##garh":13484,"preparatory":13485,"statewide":13486,"tasked":13487,"lacrosse":13488,"viktor":13489,"##lation":13490,"angola":13491,"##hra":13492,"flint":13493,"implications":13494,"employs":13495,"teens":13496,"patrons":13497,"stall":13498,"weekends":13499,"barriers":13500,"scrambled":13501,"nucleus":13502,"tehran":13503,"jenna":13504,"parsons":13505,"lifelong":13506,"robots":13507,"displacement":13508,"5000":13509,"##bles":13510,"precipitation":13511,"##gt":13512,"knuckles":13513,"clutched":13514,"1802":13515,"marrying":13516,"ecology":13517,"marx":13518,"accusations":13519,"declare":13520,"scars":13521,"kolkata":13522,"mat":13523,"meadows":13524,"bermuda":13525,"skeleton":13526,"finalists":13527,"vintage":13528,"crawl":13529,"coordinate":13530,"affects":13531,"subjected":13532,"orchestral":13533,"mistaken":13534,"##tc":13535,"mirrors":13536,"dipped":13537,"relied":13538,"260":13539,"arches":13540,"candle":13541,"##nick":13542,"incorporating":13543,"wildly":13544,"fond":13545,"basilica":13546,"owl":13547,"fringe":13548,"rituals":13549,"whispering":13550,"stirred":13551,"feud":13552,"tertiary":13553,"slick":13554,"goat":13555,"honorable":13556,"whereby":13557,"skip":13558,"ricardo":13559,"stripes":13560,"parachute":13561,"adjoining":13562,"submerged":13563,"synthesizer":13564,"##gren":13565,"intend":13566,"positively":13567,"ninety":13568,"phi":13569,"beaver":13570,"partition":13571,"fellows":13572,"alexis":13573,"prohibition":13574,"carlisle":13575,"bizarre":13576,"fraternity":13577,"##bre":13578,"doubts":13579,"icy":13580,"cbc":13581,"aquatic":13582,"sneak":13583,"sonny":13584,"combines":13585,"airports":13586,"crude":13587,"supervised":13588,"spatial":13589,"merge":13590,"alfonso":13591,"##bic":13592,"corrupt":13593,"scan":13594,"undergo":13595,"##ams":13596,"disabilities":13597,"colombian":13598,"comparing":13599,"dolphins":13600,"perkins":13601,"##lish":13602,"reprinted":13603,"unanimous":13604,"bounced":13605,"hairs":13606,"underworld":13607,"midwest":13608,"semester":13609,"bucket":13610,"paperback":13611,"miniseries":13612,"coventry":13613,"demise":13614,"##leigh":13615,"demonstrations":13616,"sensor":13617,"rotating":13618,"yan":13619,"##hler":13620,"arrange":13621,"soils":13622,"##idge":13623,"hyderabad":13624,"labs":13625,"##dr":13626,"brakes":13627,"grandchildren":13628,"##nde":13629,"negotiated":13630,"rover":13631,"ferrari":13632,"continuation":13633,"directorate":13634,"augusta":13635,"stevenson":13636,"counterpart":13637,"gore":13638,"##rda":13639,"nursery":13640,"rican":13641,"ave":13642,"collectively":13643,"broadly":13644,"pastoral":13645,"repertoire":13646,"asserted":13647,"discovering":13648,"nordic":13649,"styled":13650,"fiba":13651,"cunningham":13652,"harley":13653,"middlesex":13654,"survives":13655,"tumor":13656,"tempo":13657,"zack":13658,"aiming":13659,"lok":13660,"urgent":13661,"##rade":13662,"##nto":13663,"devils":13664,"##ement":13665,"contractor":13666,"turin":13667,"##wl":13668,"##ool":13669,"bliss":13670,"repaired":13671,"simmons":13672,"moan":13673,"astronomical":13674,"cr":13675,"negotiate":13676,"lyric":13677,"1890s":13678,"lara":13679,"bred":13680,"clad":13681,"angus":13682,"pbs":13683,"##ience":13684,"engineered":13685,"posed":13686,"##lk":13687,"hernandez":13688,"possessions":13689,"elbows":13690,"psychiatric":13691,"strokes":13692,"confluence":13693,"electorate":13694,"lifts":13695,"campuses":13696,"lava":13697,"alps":13698,"##ep":13699,"##ution":13700,"##date":13701,"physicist":13702,"woody":13703,"##page":13704,"##ographic":13705,"##itis":13706,"juliet":13707,"reformation":13708,"sparhawk":13709,"320":13710,"complement":13711,"suppressed":13712,"jewel":13713,"##½":13714,"floated":13715,"##kas":13716,"continuity":13717,"sadly":13718,"##ische":13719,"inability":13720,"melting":13721,"scanning":13722,"paula":13723,"flour":13724,"judaism":13725,"safer":13726,"vague":13727,"##lm":13728,"solving":13729,"curb":13730,"##stown":13731,"financially":13732,"gable":13733,"bees":13734,"expired":13735,"miserable":13736,"cassidy":13737,"dominion":13738,"1789":13739,"cupped":13740,"145":13741,"robbery":13742,"facto":13743,"amos":13744,"warden":13745,"resume":13746,"tallest":13747,"marvin":13748,"ing":13749,"pounded":13750,"usd":13751,"declaring":13752,"gasoline":13753,"##aux":13754,"darkened":13755,"270":13756,"650":13757,"sophomore":13758,"##mere":13759,"erection":13760,"gossip":13761,"televised":13762,"risen":13763,"dial":13764,"##eu":13765,"pillars":13766,"##link":13767,"passages":13768,"profound":13769,"##tina":13770,"arabian":13771,"ashton":13772,"silicon":13773,"nail":13774,"##ead":13775,"##lated":13776,"##wer":13777,"##hardt":13778,"fleming":13779,"firearms":13780,"ducked":13781,"circuits":13782,"blows":13783,"waterloo":13784,"titans":13785,"##lina":13786,"atom":13787,"fireplace":13788,"cheshire":13789,"financed":13790,"activation":13791,"algorithms":13792,"##zzi":13793,"constituent":13794,"catcher":13795,"cherokee":13796,"partnerships":13797,"sexuality":13798,"platoon":13799,"tragic":13800,"vivian":13801,"guarded":13802,"whiskey":13803,"meditation":13804,"poetic":13805,"##late":13806,"##nga":13807,"##ake":13808,"porto":13809,"listeners":13810,"dominance":13811,"kendra":13812,"mona":13813,"chandler":13814,"factions":13815,"22nd":13816,"salisbury":13817,"attitudes":13818,"derivative":13819,"##ido":13820,"##haus":13821,"intake":13822,"paced":13823,"javier":13824,"illustrator":13825,"barrels":13826,"bias":13827,"cockpit":13828,"burnett":13829,"dreamed":13830,"ensuing":13831,"##anda":13832,"receptors":13833,"someday":13834,"hawkins":13835,"mattered":13836,"##lal":13837,"slavic":13838,"1799":13839,"jesuit":13840,"cameroon":13841,"wasted":13842,"tai":13843,"wax":13844,"lowering":13845,"victorious":13846,"freaking":13847,"outright":13848,"hancock":13849,"librarian":13850,"sensing":13851,"bald":13852,"calcium":13853,"myers":13854,"tablet":13855,"announcing":13856,"barack":13857,"shipyard":13858,"pharmaceutical":13859,"##uan":13860,"greenwich":13861,"flush":13862,"medley":13863,"patches":13864,"wolfgang":13865,"pt":13866,"speeches":13867,"acquiring":13868,"exams":13869,"nikolai":13870,"##gg":13871,"hayden":13872,"kannada":13873,"##type":13874,"reilly":13875,"##pt":13876,"waitress":13877,"abdomen":13878,"devastated":13879,"capped":13880,"pseudonym":13881,"pharmacy":13882,"fulfill":13883,"paraguay":13884,"1796":13885,"clicked":13886,"##trom":13887,"archipelago":13888,"syndicated":13889,"##hman":13890,"lumber":13891,"orgasm":13892,"rejection":13893,"clifford":13894,"lorraine":13895,"advent":13896,"mafia":13897,"rodney":13898,"brock":13899,"##ght":13900,"##used":13901,"##elia":13902,"cassette":13903,"chamberlain":13904,"despair":13905,"mongolia":13906,"sensors":13907,"developmental":13908,"upstream":13909,"##eg":13910,"##alis":13911,"spanning":13912,"165":13913,"trombone":13914,"basque":13915,"seeded":13916,"interred":13917,"renewable":13918,"rhys":13919,"leapt":13920,"revision":13921,"molecule":13922,"##ages":13923,"chord":13924,"vicious":13925,"nord":13926,"shivered":13927,"23rd":13928,"arlington":13929,"debts":13930,"corpus":13931,"sunrise":13932,"bays":13933,"blackburn":13934,"centimetres":13935,"##uded":13936,"shuddered":13937,"gm":13938,"strangely":13939,"gripping":13940,"cartoons":13941,"isabelle":13942,"orbital":13943,"##ppa":13944,"seals":13945,"proving":13946,"##lton":13947,"refusal":13948,"strengthened":13949,"bust":13950,"assisting":13951,"baghdad":13952,"batsman":13953,"portrayal":13954,"mara":13955,"pushes":13956,"spears":13957,"og":13958,"##cock":13959,"reside":13960,"nathaniel":13961,"brennan":13962,"1776":13963,"confirmation":13964,"caucus":13965,"##worthy":13966,"markings":13967,"yemen":13968,"nobles":13969,"ku":13970,"lazy":13971,"viewer":13972,"catalan":13973,"encompasses":13974,"sawyer":13975,"##fall":13976,"sparked":13977,"substances":13978,"patents":13979,"braves":13980,"arranger":13981,"evacuation":13982,"sergio":13983,"persuade":13984,"dover":13985,"tolerance":13986,"penguin":13987,"cum":13988,"jockey":13989,"insufficient":13990,"townships":13991,"occupying":13992,"declining":13993,"plural":13994,"processed":13995,"projection":13996,"puppet":13997,"flanders":13998,"introduces":13999,"liability":14000,"##yon":14001,"gymnastics":14002,"antwerp":14003,"taipei":14004,"hobart":14005,"candles":14006,"jeep":14007,"wes":14008,"observers":14009,"126":14010,"chaplain":14011,"bundle":14012,"glorious":14013,"##hine":14014,"hazel":14015,"flung":14016,"sol":14017,"excavations":14018,"dumped":14019,"stares":14020,"sh":14021,"bangalore":14022,"triangular":14023,"icelandic":14024,"intervals":14025,"expressing":14026,"turbine":14027,"##vers":14028,"songwriting":14029,"crafts":14030,"##igo":14031,"jasmine":14032,"ditch":14033,"rite":14034,"##ways":14035,"entertaining":14036,"comply":14037,"sorrow":14038,"wrestlers":14039,"basel":14040,"emirates":14041,"marian":14042,"rivera":14043,"helpful":14044,"##some":14045,"caution":14046,"downward":14047,"networking":14048,"##atory":14049,"##tered":14050,"darted":14051,"genocide":14052,"emergence":14053,"replies":14054,"specializing":14055,"spokesman":14056,"convenient":14057,"unlocked":14058,"fading":14059,"augustine":14060,"concentrations":14061,"resemblance":14062,"elijah":14063,"investigator":14064,"andhra":14065,"##uda":14066,"promotes":14067,"bean":14068,"##rrell":14069,"fleeing":14070,"wan":14071,"simone":14072,"announcer":14073,"##ame":14074,"##bby":14075,"lydia":14076,"weaver":14077,"132":14078,"residency":14079,"modification":14080,"##fest":14081,"stretches":14082,"##ast":14083,"alternatively":14084,"nat":14085,"lowe":14086,"lacks":14087,"##ented":14088,"pam":14089,"tile":14090,"concealed":14091,"inferior":14092,"abdullah":14093,"residences":14094,"tissues":14095,"vengeance":14096,"##ided":14097,"moisture":14098,"peculiar":14099,"groove":14100,"zip":14101,"bologna":14102,"jennings":14103,"ninja":14104,"oversaw":14105,"zombies":14106,"pumping":14107,"batch":14108,"livingston":14109,"emerald":14110,"installations":14111,"1797":14112,"peel":14113,"nitrogen":14114,"rama":14115,"##fying":14116,"##star":14117,"schooling":14118,"strands":14119,"responding":14120,"werner":14121,"##ost":14122,"lime":14123,"casa":14124,"accurately":14125,"targeting":14126,"##rod":14127,"underway":14128,"##uru":14129,"hemisphere":14130,"lester":14131,"##yard":14132,"occupies":14133,"2d":14134,"griffith":14135,"angrily":14136,"reorganized":14137,"##owing":14138,"courtney":14139,"deposited":14140,"##dd":14141,"##30":14142,"estadio":14143,"##ifies":14144,"dunn":14145,"exiled":14146,"##ying":14147,"checks":14148,"##combe":14149,"##о":14150,"##fly":14151,"successes":14152,"unexpectedly":14153,"blu":14154,"assessed":14155,"##flower":14156,"##ه":14157,"observing":14158,"sacked":14159,"spiders":14160,"kn":14161,"##tail":14162,"mu":14163,"nodes":14164,"prosperity":14165,"audrey":14166,"divisional":14167,"155":14168,"broncos":14169,"tangled":14170,"adjust":14171,"feeds":14172,"erosion":14173,"paolo":14174,"surf":14175,"directory":14176,"snatched":14177,"humid":14178,"admiralty":14179,"screwed":14180,"gt":14181,"reddish":14182,"##nese":14183,"modules":14184,"trench":14185,"lamps":14186,"bind":14187,"leah":14188,"bucks":14189,"competes":14190,"##nz":14191,"##form":14192,"transcription":14193,"##uc":14194,"isles":14195,"violently":14196,"clutching":14197,"pga":14198,"cyclist":14199,"inflation":14200,"flats":14201,"ragged":14202,"unnecessary":14203,"##hian":14204,"stubborn":14205,"coordinated":14206,"harriet":14207,"baba":14208,"disqualified":14209,"330":14210,"insect":14211,"wolfe":14212,"##fies":14213,"reinforcements":14214,"rocked":14215,"duel":14216,"winked":14217,"embraced":14218,"bricks":14219,"##raj":14220,"hiatus":14221,"defeats":14222,"pending":14223,"brightly":14224,"jealousy":14225,"##xton":14226,"##hm":14227,"##uki":14228,"lena":14229,"gdp":14230,"colorful":14231,"##dley":14232,"stein":14233,"kidney":14234,"##shu":14235,"underwear":14236,"wanderers":14237,"##haw":14238,"##icus":14239,"guardians":14240,"m³":14241,"roared":14242,"habits":14243,"##wise":14244,"permits":14245,"gp":14246,"uranium":14247,"punished":14248,"disguise":14249,"bundesliga":14250,"elise":14251,"dundee":14252,"erotic":14253,"partisan":14254,"pi":14255,"collectors":14256,"float":14257,"individually":14258,"rendering":14259,"behavioral":14260,"bucharest":14261,"ser":14262,"hare":14263,"valerie":14264,"corporal":14265,"nutrition":14266,"proportional":14267,"##isa":14268,"immense":14269,"##kis":14270,"pavement":14271,"##zie":14272,"##eld":14273,"sutherland":14274,"crouched":14275,"1775":14276,"##lp":14277,"suzuki":14278,"trades":14279,"endurance":14280,"operas":14281,"crosby":14282,"prayed":14283,"priory":14284,"rory":14285,"socially":14286,"##urn":14287,"gujarat":14288,"##pu":14289,"walton":14290,"cube":14291,"pasha":14292,"privilege":14293,"lennon":14294,"floods":14295,"thorne":14296,"waterfall":14297,"nipple":14298,"scouting":14299,"approve":14300,"##lov":14301,"minorities":14302,"voter":14303,"dwight":14304,"extensions":14305,"assure":14306,"ballroom":14307,"slap":14308,"dripping":14309,"privileges":14310,"rejoined":14311,"confessed":14312,"demonstrating":14313,"patriotic":14314,"yell":14315,"investor":14316,"##uth":14317,"pagan":14318,"slumped":14319,"squares":14320,"##cle":14321,"##kins":14322,"confront":14323,"bert":14324,"embarrassment":14325,"##aid":14326,"aston":14327,"urging":14328,"sweater":14329,"starr":14330,"yuri":14331,"brains":14332,"williamson":14333,"commuter":14334,"mortar":14335,"structured":14336,"selfish":14337,"exports":14338,"##jon":14339,"cds":14340,"##him":14341,"unfinished":14342,"##rre":14343,"mortgage":14344,"destinations":14345,"##nagar":14346,"canoe":14347,"solitary":14348,"buchanan":14349,"delays":14350,"magistrate":14351,"fk":14352,"##pling":14353,"motivation":14354,"##lier":14355,"##vier":14356,"recruiting":14357,"assess":14358,"##mouth":14359,"malik":14360,"antique":14361,"1791":14362,"pius":14363,"rahman":14364,"reich":14365,"tub":14366,"zhou":14367,"smashed":14368,"airs":14369,"galway":14370,"xii":14371,"conditioning":14372,"honduras":14373,"discharged":14374,"dexter":14375,"##pf":14376,"lionel":14377,"129":14378,"debates":14379,"lemon":14380,"tiffany":14381,"volunteered":14382,"dom":14383,"dioxide":14384,"procession":14385,"devi":14386,"sic":14387,"tremendous":14388,"advertisements":14389,"colts":14390,"transferring":14391,"verdict":14392,"hanover":14393,"decommissioned":14394,"utter":14395,"relate":14396,"pac":14397,"racism":14398,"##top":14399,"beacon":14400,"limp":14401,"similarity":14402,"terra":14403,"occurrence":14404,"ant":14405,"##how":14406,"becky":14407,"capt":14408,"updates":14409,"armament":14410,"richie":14411,"pal":14412,"##graph":14413,"halloween":14414,"mayo":14415,"##ssen":14416,"##bone":14417,"cara":14418,"serena":14419,"fcc":14420,"dolls":14421,"obligations":14422,"##dling":14423,"violated":14424,"lafayette":14425,"jakarta":14426,"exploitation":14427,"##ime":14428,"infamous":14429,"iconic":14430,"##lah":14431,"##park":14432,"kitty":14433,"moody":14434,"reginald":14435,"dread":14436,"spill":14437,"crystals":14438,"olivier":14439,"modeled":14440,"bluff":14441,"equilibrium":14442,"separating":14443,"notices":14444,"ordnance":14445,"extinction":14446,"onset":14447,"cosmic":14448,"attachment":14449,"sammy":14450,"expose":14451,"privy":14452,"anchored":14453,"##bil":14454,"abbott":14455,"admits":14456,"bending":14457,"baritone":14458,"emmanuel":14459,"policeman":14460,"vaughan":14461,"winged":14462,"climax":14463,"dresses":14464,"denny":14465,"polytechnic":14466,"mohamed":14467,"burmese":14468,"authentic":14469,"nikki":14470,"genetics":14471,"grandparents":14472,"homestead":14473,"gaza":14474,"postponed":14475,"metacritic":14476,"una":14477,"##sby":14478,"##bat":14479,"unstable":14480,"dissertation":14481,"##rial":14482,"##cian":14483,"curls":14484,"obscure":14485,"uncovered":14486,"bronx":14487,"praying":14488,"disappearing":14489,"##hoe":14490,"prehistoric":14491,"coke":14492,"turret":14493,"mutations":14494,"nonprofit":14495,"pits":14496,"monaco":14497,"##ي":14498,"##usion":14499,"prominently":14500,"dispatched":14501,"podium":14502,"##mir":14503,"uci":14504,"##uation":14505,"133":14506,"fortifications":14507,"birthplace":14508,"kendall":14509,"##lby":14510,"##oll":14511,"preacher":14512,"rack":14513,"goodman":14514,"##rman":14515,"persistent":14516,"##ott":14517,"countless":14518,"jaime":14519,"recorder":14520,"lexington":14521,"persecution":14522,"jumps":14523,"renewal":14524,"wagons":14525,"##11":14526,"crushing":14527,"##holder":14528,"decorations":14529,"##lake":14530,"abundance":14531,"wrath":14532,"laundry":14533,"£1":14534,"garde":14535,"##rp":14536,"jeanne":14537,"beetles":14538,"peasant":14539,"##sl":14540,"splitting":14541,"caste":14542,"sergei":14543,"##rer":14544,"##ema":14545,"scripts":14546,"##ively":14547,"rub":14548,"satellites":14549,"##vor":14550,"inscribed":14551,"verlag":14552,"scrapped":14553,"gale":14554,"packages":14555,"chick":14556,"potato":14557,"slogan":14558,"kathleen":14559,"arabs":14560,"##culture":14561,"counterparts":14562,"reminiscent":14563,"choral":14564,"##tead":14565,"rand":14566,"retains":14567,"bushes":14568,"dane":14569,"accomplish":14570,"courtesy":14571,"closes":14572,"##oth":14573,"slaughter":14574,"hague":14575,"krakow":14576,"lawson":14577,"tailed":14578,"elias":14579,"ginger":14580,"##ttes":14581,"canopy":14582,"betrayal":14583,"rebuilding":14584,"turf":14585,"##hof":14586,"frowning":14587,"allegiance":14588,"brigades":14589,"kicks":14590,"rebuild":14591,"polls":14592,"alias":14593,"nationalism":14594,"td":14595,"rowan":14596,"audition":14597,"bowie":14598,"fortunately":14599,"recognizes":14600,"harp":14601,"dillon":14602,"horrified":14603,"##oro":14604,"renault":14605,"##tics":14606,"ropes":14607,"##α":14608,"presumed":14609,"rewarded":14610,"infrared":14611,"wiping":14612,"accelerated":14613,"illustration":14614,"##rid":14615,"presses":14616,"practitioners":14617,"badminton":14618,"##iard":14619,"detained":14620,"##tera":14621,"recognizing":14622,"relates":14623,"misery":14624,"##sies":14625,"##tly":14626,"reproduction":14627,"piercing":14628,"potatoes":14629,"thornton":14630,"esther":14631,"manners":14632,"hbo":14633,"##aan":14634,"ours":14635,"bullshit":14636,"ernie":14637,"perennial":14638,"sensitivity":14639,"illuminated":14640,"rupert":14641,"##jin":14642,"##iss":14643,"##ear":14644,"rfc":14645,"nassau":14646,"##dock":14647,"staggered":14648,"socialism":14649,"##haven":14650,"appointments":14651,"nonsense":14652,"prestige":14653,"sharma":14654,"haul":14655,"##tical":14656,"solidarity":14657,"gps":14658,"##ook":14659,"##rata":14660,"igor":14661,"pedestrian":14662,"##uit":14663,"baxter":14664,"tenants":14665,"wires":14666,"medication":14667,"unlimited":14668,"guiding":14669,"impacts":14670,"diabetes":14671,"##rama":14672,"sasha":14673,"pas":14674,"clive":14675,"extraction":14676,"131":14677,"continually":14678,"constraints":14679,"##bilities":14680,"sonata":14681,"hunted":14682,"sixteenth":14683,"chu":14684,"planting":14685,"quote":14686,"mayer":14687,"pretended":14688,"abs":14689,"spat":14690,"##hua":14691,"ceramic":14692,"##cci":14693,"curtains":14694,"pigs":14695,"pitching":14696,"##dad":14697,"latvian":14698,"sore":14699,"dayton":14700,"##sted":14701,"##qi":14702,"patrols":14703,"slice":14704,"playground":14705,"##nted":14706,"shone":14707,"stool":14708,"apparatus":14709,"inadequate":14710,"mates":14711,"treason":14712,"##ija":14713,"desires":14714,"##liga":14715,"##croft":14716,"somalia":14717,"laurent":14718,"mir":14719,"leonardo":14720,"oracle":14721,"grape":14722,"obliged":14723,"chevrolet":14724,"thirteenth":14725,"stunning":14726,"enthusiastic":14727,"##ede":14728,"accounted":14729,"concludes":14730,"currents":14731,"basil":14732,"##kovic":14733,"drought":14734,"##rica":14735,"mai":14736,"##aire":14737,"shove":14738,"posting":14739,"##shed":14740,"pilgrimage":14741,"humorous":14742,"packing":14743,"fry":14744,"pencil":14745,"wines":14746,"smells":14747,"144":14748,"marilyn":14749,"aching":14750,"newest":14751,"clung":14752,"bon":14753,"neighbours":14754,"sanctioned":14755,"##pie":14756,"mug":14757,"##stock":14758,"drowning":14759,"##mma":14760,"hydraulic":14761,"##vil":14762,"hiring":14763,"reminder":14764,"lilly":14765,"investigators":14766,"##ncies":14767,"sour":14768,"##eous":14769,"compulsory":14770,"packet":14771,"##rion":14772,"##graphic":14773,"##elle":14774,"cannes":14775,"##inate":14776,"depressed":14777,"##rit":14778,"heroic":14779,"importantly":14780,"theresa":14781,"##tled":14782,"conway":14783,"saturn":14784,"marginal":14785,"rae":14786,"##xia":14787,"corresponds":14788,"royce":14789,"pact":14790,"jasper":14791,"explosives":14792,"packaging":14793,"aluminium":14794,"##ttered":14795,"denotes":14796,"rhythmic":14797,"spans":14798,"assignments":14799,"hereditary":14800,"outlined":14801,"originating":14802,"sundays":14803,"lad":14804,"reissued":14805,"greeting":14806,"beatrice":14807,"##dic":14808,"pillar":14809,"marcos":14810,"plots":14811,"handbook":14812,"alcoholic":14813,"judiciary":14814,"avant":14815,"slides":14816,"extract":14817,"masculine":14818,"blur":14819,"##eum":14820,"##force":14821,"homage":14822,"trembled":14823,"owens":14824,"hymn":14825,"trey":14826,"omega":14827,"signaling":14828,"socks":14829,"accumulated":14830,"reacted":14831,"attic":14832,"theo":14833,"lining":14834,"angie":14835,"distraction":14836,"primera":14837,"talbot":14838,"##key":14839,"1200":14840,"ti":14841,"creativity":14842,"billed":14843,"##hey":14844,"deacon":14845,"eduardo":14846,"identifies":14847,"proposition":14848,"dizzy":14849,"gunner":14850,"hogan":14851,"##yam":14852,"##pping":14853,"##hol":14854,"ja":14855,"##chan":14856,"jensen":14857,"reconstructed":14858,"##berger":14859,"clearance":14860,"darius":14861,"##nier":14862,"abe":14863,"harlem":14864,"plea":14865,"dei":14866,"circled":14867,"emotionally":14868,"notation":14869,"fascist":14870,"neville":14871,"exceeded":14872,"upwards":14873,"viable":14874,"ducks":14875,"##fo":14876,"workforce":14877,"racer":14878,"limiting":14879,"shri":14880,"##lson":14881,"possesses":14882,"1600":14883,"kerr":14884,"moths":14885,"devastating":14886,"laden":14887,"disturbing":14888,"locking":14889,"##cture":14890,"gal":14891,"fearing":14892,"accreditation":14893,"flavor":14894,"aide":14895,"1870s":14896,"mountainous":14897,"##baum":14898,"melt":14899,"##ures":14900,"motel":14901,"texture":14902,"servers":14903,"soda":14904,"##mb":14905,"herd":14906,"##nium":14907,"erect":14908,"puzzled":14909,"hum":14910,"peggy":14911,"examinations":14912,"gould":14913,"testified":14914,"geoff":14915,"ren":14916,"devised":14917,"sacks":14918,"##law":14919,"denial":14920,"posters":14921,"grunted":14922,"cesar":14923,"tutor":14924,"ec":14925,"gerry":14926,"offerings":14927,"byrne":14928,"falcons":14929,"combinations":14930,"ct":14931,"incoming":14932,"pardon":14933,"rocking":14934,"26th":14935,"avengers":14936,"flared":14937,"mankind":14938,"seller":14939,"uttar":14940,"loch":14941,"nadia":14942,"stroking":14943,"exposing":14944,"##hd":14945,"fertile":14946,"ancestral":14947,"instituted":14948,"##has":14949,"noises":14950,"prophecy":14951,"taxation":14952,"eminent":14953,"vivid":14954,"pol":14955,"##bol":14956,"dart":14957,"indirect":14958,"multimedia":14959,"notebook":14960,"upside":14961,"displaying":14962,"adrenaline":14963,"referenced":14964,"geometric":14965,"##iving":14966,"progression":14967,"##ddy":14968,"blunt":14969,"announce":14970,"##far":14971,"implementing":14972,"##lav":14973,"aggression":14974,"liaison":14975,"cooler":14976,"cares":14977,"headache":14978,"plantations":14979,"gorge":14980,"dots":14981,"impulse":14982,"thickness":14983,"ashamed":14984,"averaging":14985,"kathy":14986,"obligation":14987,"precursor":14988,"137":14989,"fowler":14990,"symmetry":14991,"thee":14992,"225":14993,"hears":14994,"##rai":14995,"undergoing":14996,"ads":14997,"butcher":14998,"bowler":14999,"##lip":15000,"cigarettes":15001,"subscription":15002,"goodness":15003,"##ically":15004,"browne":15005,"##hos":15006,"##tech":15007,"kyoto":15008,"donor":15009,"##erty":15010,"damaging":15011,"friction":15012,"drifting":15013,"expeditions":15014,"hardened":15015,"prostitution":15016,"152":15017,"fauna":15018,"blankets":15019,"claw":15020,"tossing":15021,"snarled":15022,"butterflies":15023,"recruits":15024,"investigative":15025,"coated":15026,"healed":15027,"138":15028,"communal":15029,"hai":15030,"xiii":15031,"academics":15032,"boone":15033,"psychologist":15034,"restless":15035,"lahore":15036,"stephens":15037,"mba":15038,"brendan":15039,"foreigners":15040,"printer":15041,"##pc":15042,"ached":15043,"explode":15044,"27th":15045,"deed":15046,"scratched":15047,"dared":15048,"##pole":15049,"cardiac":15050,"1780":15051,"okinawa":15052,"proto":15053,"commando":15054,"compelled":15055,"oddly":15056,"electrons":15057,"##base":15058,"replica":15059,"thanksgiving":15060,"##rist":15061,"sheila":15062,"deliberate":15063,"stafford":15064,"tidal":15065,"representations":15066,"hercules":15067,"ou":15068,"##path":15069,"##iated":15070,"kidnapping":15071,"lenses":15072,"##tling":15073,"deficit":15074,"samoa":15075,"mouths":15076,"consuming":15077,"computational":15078,"maze":15079,"granting":15080,"smirk":15081,"razor":15082,"fixture":15083,"ideals":15084,"inviting":15085,"aiden":15086,"nominal":15087,"##vs":15088,"issuing":15089,"julio":15090,"pitt":15091,"ramsey":15092,"docks":15093,"##oss":15094,"exhaust":15095,"##owed":15096,"bavarian":15097,"draped":15098,"anterior":15099,"mating":15100,"ethiopian":15101,"explores":15102,"noticing":15103,"##nton":15104,"discarded":15105,"convenience":15106,"hoffman":15107,"endowment":15108,"beasts":15109,"cartridge":15110,"mormon":15111,"paternal":15112,"probe":15113,"sleeves":15114,"interfere":15115,"lump":15116,"deadline":15117,"##rail":15118,"jenks":15119,"bulldogs":15120,"scrap":15121,"alternating":15122,"justified":15123,"reproductive":15124,"nam":15125,"seize":15126,"descending":15127,"secretariat":15128,"kirby":15129,"coupe":15130,"grouped":15131,"smash":15132,"panther":15133,"sedan":15134,"tapping":15135,"##18":15136,"lola":15137,"cheer":15138,"germanic":15139,"unfortunate":15140,"##eter":15141,"unrelated":15142,"##fan":15143,"subordinate":15144,"##sdale":15145,"suzanne":15146,"advertisement":15147,"##ility":15148,"horsepower":15149,"##lda":15150,"cautiously":15151,"discourse":15152,"luigi":15153,"##mans":15154,"##fields":15155,"noun":15156,"prevalent":15157,"mao":15158,"schneider":15159,"everett":15160,"surround":15161,"governorate":15162,"kira":15163,"##avia":15164,"westward":15165,"##take":15166,"misty":15167,"rails":15168,"sustainability":15169,"134":15170,"unused":15171,"##rating":15172,"packs":15173,"toast":15174,"unwilling":15175,"regulate":15176,"thy":15177,"suffrage":15178,"nile":15179,"awe":15180,"assam":15181,"definitions":15182,"travelers":15183,"affordable":15184,"##rb":15185,"conferred":15186,"sells":15187,"undefeated":15188,"beneficial":15189,"torso":15190,"basal":15191,"repeating":15192,"remixes":15193,"##pass":15194,"bahrain":15195,"cables":15196,"fang":15197,"##itated":15198,"excavated":15199,"numbering":15200,"statutory":15201,"##rey":15202,"deluxe":15203,"##lian":15204,"forested":15205,"ramirez":15206,"derbyshire":15207,"zeus":15208,"slamming":15209,"transfers":15210,"astronomer":15211,"banana":15212,"lottery":15213,"berg":15214,"histories":15215,"bamboo":15216,"##uchi":15217,"resurrection":15218,"posterior":15219,"bowls":15220,"vaguely":15221,"##thi":15222,"thou":15223,"preserving":15224,"tensed":15225,"offence":15226,"##inas":15227,"meyrick":15228,"callum":15229,"ridden":15230,"watt":15231,"langdon":15232,"tying":15233,"lowland":15234,"snorted":15235,"daring":15236,"truman":15237,"##hale":15238,"##girl":15239,"aura":15240,"overly":15241,"filing":15242,"weighing":15243,"goa":15244,"infections":15245,"philanthropist":15246,"saunders":15247,"eponymous":15248,"##owski":15249,"latitude":15250,"perspectives":15251,"reviewing":15252,"mets":15253,"commandant":15254,"radial":15255,"##kha":15256,"flashlight":15257,"reliability":15258,"koch":15259,"vowels":15260,"amazed":15261,"ada":15262,"elaine":15263,"supper":15264,"##rth":15265,"##encies":15266,"predator":15267,"debated":15268,"soviets":15269,"cola":15270,"##boards":15271,"##nah":15272,"compartment":15273,"crooked":15274,"arbitrary":15275,"fourteenth":15276,"##ctive":15277,"havana":15278,"majors":15279,"steelers":15280,"clips":15281,"profitable":15282,"ambush":15283,"exited":15284,"packers":15285,"##tile":15286,"nude":15287,"cracks":15288,"fungi":15289,"##е":15290,"limb":15291,"trousers":15292,"josie":15293,"shelby":15294,"tens":15295,"frederic":15296,"##ος":15297,"definite":15298,"smoothly":15299,"constellation":15300,"insult":15301,"baton":15302,"discs":15303,"lingering":15304,"##nco":15305,"conclusions":15306,"lent":15307,"staging":15308,"becker":15309,"grandpa":15310,"shaky":15311,"##tron":15312,"einstein":15313,"obstacles":15314,"sk":15315,"adverse":15316,"elle":15317,"economically":15318,"##moto":15319,"mccartney":15320,"thor":15321,"dismissal":15322,"motions":15323,"readings":15324,"nostrils":15325,"treatise":15326,"##pace":15327,"squeezing":15328,"evidently":15329,"prolonged":15330,"1783":15331,"venezuelan":15332,"je":15333,"marguerite":15334,"beirut":15335,"takeover":15336,"shareholders":15337,"##vent":15338,"denise":15339,"digit":15340,"airplay":15341,"norse":15342,"##bbling":15343,"imaginary":15344,"pills":15345,"hubert":15346,"blaze":15347,"vacated":15348,"eliminating":15349,"##ello":15350,"vine":15351,"mansfield":15352,"##tty":15353,"retrospective":15354,"barrow":15355,"borne":15356,"clutch":15357,"bail":15358,"forensic":15359,"weaving":15360,"##nett":15361,"##witz":15362,"desktop":15363,"citadel":15364,"promotions":15365,"worrying":15366,"dorset":15367,"ieee":15368,"subdivided":15369,"##iating":15370,"manned":15371,"expeditionary":15372,"pickup":15373,"synod":15374,"chuckle":15375,"185":15376,"barney":15377,"##rz":15378,"##ffin":15379,"functionality":15380,"karachi":15381,"litigation":15382,"meanings":15383,"uc":15384,"lick":15385,"turbo":15386,"anders":15387,"##ffed":15388,"execute":15389,"curl":15390,"oppose":15391,"ankles":15392,"typhoon":15393,"##د":15394,"##ache":15395,"##asia":15396,"linguistics":15397,"compassion":15398,"pressures":15399,"grazing":15400,"perfection":15401,"##iting":15402,"immunity":15403,"monopoly":15404,"muddy":15405,"backgrounds":15406,"136":15407,"namibia":15408,"francesca":15409,"monitors":15410,"attracting":15411,"stunt":15412,"tuition":15413,"##ии":15414,"vegetable":15415,"##mates":15416,"##quent":15417,"mgm":15418,"jen":15419,"complexes":15420,"forts":15421,"##ond":15422,"cellar":15423,"bites":15424,"seventeenth":15425,"royals":15426,"flemish":15427,"failures":15428,"mast":15429,"charities":15430,"##cular":15431,"peruvian":15432,"capitals":15433,"macmillan":15434,"ipswich":15435,"outward":15436,"frigate":15437,"postgraduate":15438,"folds":15439,"employing":15440,"##ouse":15441,"concurrently":15442,"fiery":15443,"##tai":15444,"contingent":15445,"nightmares":15446,"monumental":15447,"nicaragua":15448,"##kowski":15449,"lizard":15450,"mal":15451,"fielding":15452,"gig":15453,"reject":15454,"##pad":15455,"harding":15456,"##ipe":15457,"coastline":15458,"##cin":15459,"##nos":15460,"beethoven":15461,"humphrey":15462,"innovations":15463,"##tam":15464,"##nge":15465,"norris":15466,"doris":15467,"solicitor":15468,"huang":15469,"obey":15470,"141":15471,"##lc":15472,"niagara":15473,"##tton":15474,"shelves":15475,"aug":15476,"bourbon":15477,"curry":15478,"nightclub":15479,"specifications":15480,"hilton":15481,"##ndo":15482,"centennial":15483,"dispersed":15484,"worm":15485,"neglected":15486,"briggs":15487,"sm":15488,"font":15489,"kuala":15490,"uneasy":15491,"plc":15492,"##nstein":15493,"##bound":15494,"##aking":15495,"##burgh":15496,"awaiting":15497,"pronunciation":15498,"##bbed":15499,"##quest":15500,"eh":15501,"optimal":15502,"zhu":15503,"raped":15504,"greens":15505,"presided":15506,"brenda":15507,"worries":15508,"##life":15509,"venetian":15510,"marxist":15511,"turnout":15512,"##lius":15513,"refined":15514,"braced":15515,"sins":15516,"grasped":15517,"sunderland":15518,"nickel":15519,"speculated":15520,"lowell":15521,"cyrillic":15522,"communism":15523,"fundraising":15524,"resembling":15525,"colonists":15526,"mutant":15527,"freddie":15528,"usc":15529,"##mos":15530,"gratitude":15531,"##run":15532,"mural":15533,"##lous":15534,"chemist":15535,"wi":15536,"reminds":15537,"28th":15538,"steals":15539,"tess":15540,"pietro":15541,"##ingen":15542,"promoter":15543,"ri":15544,"microphone":15545,"honoured":15546,"rai":15547,"sant":15548,"##qui":15549,"feather":15550,"##nson":15551,"burlington":15552,"kurdish":15553,"terrorists":15554,"deborah":15555,"sickness":15556,"##wed":15557,"##eet":15558,"hazard":15559,"irritated":15560,"desperation":15561,"veil":15562,"clarity":15563,"##rik":15564,"jewels":15565,"xv":15566,"##gged":15567,"##ows":15568,"##cup":15569,"berkshire":15570,"unfair":15571,"mysteries":15572,"orchid":15573,"winced":15574,"exhaustion":15575,"renovations":15576,"stranded":15577,"obe":15578,"infinity":15579,"##nies":15580,"adapt":15581,"redevelopment":15582,"thanked":15583,"registry":15584,"olga":15585,"domingo":15586,"noir":15587,"tudor":15588,"ole":15589,"##atus":15590,"commenting":15591,"behaviors":15592,"##ais":15593,"crisp":15594,"pauline":15595,"probable":15596,"stirling":15597,"wigan":15598,"##bian":15599,"paralympics":15600,"panting":15601,"surpassed":15602,"##rew":15603,"luca":15604,"barred":15605,"pony":15606,"famed":15607,"##sters":15608,"cassandra":15609,"waiter":15610,"carolyn":15611,"exported":15612,"##orted":15613,"andres":15614,"destructive":15615,"deeds":15616,"jonah":15617,"castles":15618,"vacancy":15619,"suv":15620,"##glass":15621,"1788":15622,"orchard":15623,"yep":15624,"famine":15625,"belarusian":15626,"sprang":15627,"##forth":15628,"skinny":15629,"##mis":15630,"administrators":15631,"rotterdam":15632,"zambia":15633,"zhao":15634,"boiler":15635,"discoveries":15636,"##ride":15637,"##physics":15638,"lucius":15639,"disappointing":15640,"outreach":15641,"spoon":15642,"##frame":15643,"qualifications":15644,"unanimously":15645,"enjoys":15646,"regency":15647,"##iidae":15648,"stade":15649,"realism":15650,"veterinary":15651,"rodgers":15652,"dump":15653,"alain":15654,"chestnut":15655,"castile":15656,"censorship":15657,"rumble":15658,"gibbs":15659,"##itor":15660,"communion":15661,"reggae":15662,"inactivated":15663,"logs":15664,"loads":15665,"##houses":15666,"homosexual":15667,"##iano":15668,"ale":15669,"informs":15670,"##cas":15671,"phrases":15672,"plaster":15673,"linebacker":15674,"ambrose":15675,"kaiser":15676,"fascinated":15677,"850":15678,"limerick":15679,"recruitment":15680,"forge":15681,"mastered":15682,"##nding":15683,"leinster":15684,"rooted":15685,"threaten":15686,"##strom":15687,"borneo":15688,"##hes":15689,"suggestions":15690,"scholarships":15691,"propeller":15692,"documentaries":15693,"patronage":15694,"coats":15695,"constructing":15696,"invest":15697,"neurons":15698,"comet":15699,"entirety":15700,"shouts":15701,"identities":15702,"annoying":15703,"unchanged":15704,"wary":15705,"##antly":15706,"##ogy":15707,"neat":15708,"oversight":15709,"##kos":15710,"phillies":15711,"replay":15712,"constance":15713,"##kka":15714,"incarnation":15715,"humble":15716,"skies":15717,"minus":15718,"##acy":15719,"smithsonian":15720,"##chel":15721,"guerrilla":15722,"jar":15723,"cadets":15724,"##plate":15725,"surplus":15726,"audit":15727,"##aru":15728,"cracking":15729,"joanna":15730,"louisa":15731,"pacing":15732,"##lights":15733,"intentionally":15734,"##iri":15735,"diner":15736,"nwa":15737,"imprint":15738,"australians":15739,"tong":15740,"unprecedented":15741,"bunker":15742,"naive":15743,"specialists":15744,"ark":15745,"nichols":15746,"railing":15747,"leaked":15748,"pedal":15749,"##uka":15750,"shrub":15751,"longing":15752,"roofs":15753,"v8":15754,"captains":15755,"neural":15756,"tuned":15757,"##ntal":15758,"##jet":15759,"emission":15760,"medina":15761,"frantic":15762,"codex":15763,"definitive":15764,"sid":15765,"abolition":15766,"intensified":15767,"stocks":15768,"enrique":15769,"sustain":15770,"genoa":15771,"oxide":15772,"##written":15773,"clues":15774,"cha":15775,"##gers":15776,"tributaries":15777,"fragment":15778,"venom":15779,"##rity":15780,"##ente":15781,"##sca":15782,"muffled":15783,"vain":15784,"sire":15785,"laos":15786,"##ingly":15787,"##hana":15788,"hastily":15789,"snapping":15790,"surfaced":15791,"sentiment":15792,"motive":15793,"##oft":15794,"contests":15795,"approximate":15796,"mesa":15797,"luckily":15798,"dinosaur":15799,"exchanges":15800,"propelled":15801,"accord":15802,"bourne":15803,"relieve":15804,"tow":15805,"masks":15806,"offended":15807,"##ues":15808,"cynthia":15809,"##mmer":15810,"rains":15811,"bartender":15812,"zinc":15813,"reviewers":15814,"lois":15815,"##sai":15816,"legged":15817,"arrogant":15818,"rafe":15819,"rosie":15820,"comprise":15821,"handicap":15822,"blockade":15823,"inlet":15824,"lagoon":15825,"copied":15826,"drilling":15827,"shelley":15828,"petals":15829,"##inian":15830,"mandarin":15831,"obsolete":15832,"##inated":15833,"onward":15834,"arguably":15835,"productivity":15836,"cindy":15837,"praising":15838,"seldom":15839,"busch":15840,"discusses":15841,"raleigh":15842,"shortage":15843,"ranged":15844,"stanton":15845,"encouragement":15846,"firstly":15847,"conceded":15848,"overs":15849,"temporal":15850,"##uke":15851,"cbe":15852,"##bos":15853,"woo":15854,"certainty":15855,"pumps":15856,"##pton":15857,"stalked":15858,"##uli":15859,"lizzie":15860,"periodic":15861,"thieves":15862,"weaker":15863,"##night":15864,"gases":15865,"shoving":15866,"chooses":15867,"wc":15868,"##chemical":15869,"prompting":15870,"weights":15871,"##kill":15872,"robust":15873,"flanked":15874,"sticky":15875,"hu":15876,"tuberculosis":15877,"##eb":15878,"##eal":15879,"christchurch":15880,"resembled":15881,"wallet":15882,"reese":15883,"inappropriate":15884,"pictured":15885,"distract":15886,"fixing":15887,"fiddle":15888,"giggled":15889,"burger":15890,"heirs":15891,"hairy":15892,"mechanic":15893,"torque":15894,"apache":15895,"obsessed":15896,"chiefly":15897,"cheng":15898,"logging":15899,"##tag":15900,"extracted":15901,"meaningful":15902,"numb":15903,"##vsky":15904,"gloucestershire":15905,"reminding":15906,"##bay":15907,"unite":15908,"##lit":15909,"breeds":15910,"diminished":15911,"clown":15912,"glove":15913,"1860s":15914,"##ن":15915,"##ug":15916,"archibald":15917,"focal":15918,"freelance":15919,"sliced":15920,"depiction":15921,"##yk":15922,"organism":15923,"switches":15924,"sights":15925,"stray":15926,"crawling":15927,"##ril":15928,"lever":15929,"leningrad":15930,"interpretations":15931,"loops":15932,"anytime":15933,"reel":15934,"alicia":15935,"delighted":15936,"##ech":15937,"inhaled":15938,"xiv":15939,"suitcase":15940,"bernie":15941,"vega":15942,"licenses":15943,"northampton":15944,"exclusion":15945,"induction":15946,"monasteries":15947,"racecourse":15948,"homosexuality":15949,"##right":15950,"##sfield":15951,"##rky":15952,"dimitri":15953,"michele":15954,"alternatives":15955,"ions":15956,"commentators":15957,"genuinely":15958,"objected":15959,"pork":15960,"hospitality":15961,"fencing":15962,"stephan":15963,"warships":15964,"peripheral":15965,"wit":15966,"drunken":15967,"wrinkled":15968,"quentin":15969,"spends":15970,"departing":15971,"chung":15972,"numerical":15973,"spokesperson":15974,"##zone":15975,"johannesburg":15976,"caliber":15977,"killers":15978,"##udge":15979,"assumes":15980,"neatly":15981,"demographic":15982,"abigail":15983,"bloc":15984,"##vel":15985,"mounting":15986,"##lain":15987,"bentley":15988,"slightest":15989,"xu":15990,"recipients":15991,"##jk":15992,"merlin":15993,"##writer":15994,"seniors":15995,"prisons":15996,"blinking":15997,"hindwings":15998,"flickered":15999,"kappa":16000,"##hel":16001,"80s":16002,"strengthening":16003,"appealing":16004,"brewing":16005,"gypsy":16006,"mali":16007,"lashes":16008,"hulk":16009,"unpleasant":16010,"harassment":16011,"bio":16012,"treaties":16013,"predict":16014,"instrumentation":16015,"pulp":16016,"troupe":16017,"boiling":16018,"mantle":16019,"##ffe":16020,"ins":16021,"##vn":16022,"dividing":16023,"handles":16024,"verbs":16025,"##onal":16026,"coconut":16027,"senegal":16028,"340":16029,"thorough":16030,"gum":16031,"momentarily":16032,"##sto":16033,"cocaine":16034,"panicked":16035,"destined":16036,"##turing":16037,"teatro":16038,"denying":16039,"weary":16040,"captained":16041,"mans":16042,"##hawks":16043,"##code":16044,"wakefield":16045,"bollywood":16046,"thankfully":16047,"##16":16048,"cyril":16049,"##wu":16050,"amendments":16051,"##bahn":16052,"consultation":16053,"stud":16054,"reflections":16055,"kindness":16056,"1787":16057,"internally":16058,"##ovo":16059,"tex":16060,"mosaic":16061,"distribute":16062,"paddy":16063,"seeming":16064,"143":16065,"##hic":16066,"piers":16067,"##15":16068,"##mura":16069,"##verse":16070,"popularly":16071,"winger":16072,"kang":16073,"sentinel":16074,"mccoy":16075,"##anza":16076,"covenant":16077,"##bag":16078,"verge":16079,"fireworks":16080,"suppress":16081,"thrilled":16082,"dominate":16083,"##jar":16084,"swansea":16085,"##60":16086,"142":16087,"reconciliation":16088,"##ndi":16089,"stiffened":16090,"cue":16091,"dorian":16092,"##uf":16093,"damascus":16094,"amor":16095,"ida":16096,"foremost":16097,"##aga":16098,"porsche":16099,"unseen":16100,"dir":16101,"##had":16102,"##azi":16103,"stony":16104,"lexi":16105,"melodies":16106,"##nko":16107,"angular":16108,"integer":16109,"podcast":16110,"ants":16111,"inherent":16112,"jaws":16113,"justify":16114,"persona":16115,"##olved":16116,"josephine":16117,"##nr":16118,"##ressed":16119,"customary":16120,"flashes":16121,"gala":16122,"cyrus":16123,"glaring":16124,"backyard":16125,"ariel":16126,"physiology":16127,"greenland":16128,"html":16129,"stir":16130,"avon":16131,"atletico":16132,"finch":16133,"methodology":16134,"ked":16135,"##lent":16136,"mas":16137,"catholicism":16138,"townsend":16139,"branding":16140,"quincy":16141,"fits":16142,"containers":16143,"1777":16144,"ashore":16145,"aragon":16146,"##19":16147,"forearm":16148,"poisoning":16149,"##sd":16150,"adopting":16151,"conquer":16152,"grinding":16153,"amnesty":16154,"keller":16155,"finances":16156,"evaluate":16157,"forged":16158,"lankan":16159,"instincts":16160,"##uto":16161,"guam":16162,"bosnian":16163,"photographed":16164,"workplace":16165,"desirable":16166,"protector":16167,"##dog":16168,"allocation":16169,"intently":16170,"encourages":16171,"willy":16172,"##sten":16173,"bodyguard":16174,"electro":16175,"brighter":16176,"##ν":16177,"bihar":16178,"##chev":16179,"lasts":16180,"opener":16181,"amphibious":16182,"sal":16183,"verde":16184,"arte":16185,"##cope":16186,"captivity":16187,"vocabulary":16188,"yields":16189,"##tted":16190,"agreeing":16191,"desmond":16192,"pioneered":16193,"##chus":16194,"strap":16195,"campaigned":16196,"railroads":16197,"##ович":16198,"emblem":16199,"##dre":16200,"stormed":16201,"501":16202,"##ulous":16203,"marijuana":16204,"northumberland":16205,"##gn":16206,"##nath":16207,"bowen":16208,"landmarks":16209,"beaumont":16210,"##qua":16211,"danube":16212,"##bler":16213,"attorneys":16214,"th":16215,"ge":16216,"flyers":16217,"critique":16218,"villains":16219,"cass":16220,"mutation":16221,"acc":16222,"##0s":16223,"colombo":16224,"mckay":16225,"motif":16226,"sampling":16227,"concluding":16228,"syndicate":16229,"##rell":16230,"neon":16231,"stables":16232,"ds":16233,"warnings":16234,"clint":16235,"mourning":16236,"wilkinson":16237,"##tated":16238,"merrill":16239,"leopard":16240,"evenings":16241,"exhaled":16242,"emil":16243,"sonia":16244,"ezra":16245,"discrete":16246,"stove":16247,"farrell":16248,"fifteenth":16249,"prescribed":16250,"superhero":16251,"##rier":16252,"worms":16253,"helm":16254,"wren":16255,"##duction":16256,"##hc":16257,"expo":16258,"##rator":16259,"hq":16260,"unfamiliar":16261,"antony":16262,"prevents":16263,"acceleration":16264,"fiercely":16265,"mari":16266,"painfully":16267,"calculations":16268,"cheaper":16269,"ign":16270,"clifton":16271,"irvine":16272,"davenport":16273,"mozambique":16274,"##np":16275,"pierced":16276,"##evich":16277,"wonders":16278,"##wig":16279,"##cate":16280,"##iling":16281,"crusade":16282,"ware":16283,"##uel":16284,"enzymes":16285,"reasonably":16286,"mls":16287,"##coe":16288,"mater":16289,"ambition":16290,"bunny":16291,"eliot":16292,"kernel":16293,"##fin":16294,"asphalt":16295,"headmaster":16296,"torah":16297,"aden":16298,"lush":16299,"pins":16300,"waived":16301,"##care":16302,"##yas":16303,"joao":16304,"substrate":16305,"enforce":16306,"##grad":16307,"##ules":16308,"alvarez":16309,"selections":16310,"epidemic":16311,"tempted":16312,"##bit":16313,"bremen":16314,"translates":16315,"ensured":16316,"waterfront":16317,"29th":16318,"forrest":16319,"manny":16320,"malone":16321,"kramer":16322,"reigning":16323,"cookies":16324,"simpler":16325,"absorption":16326,"205":16327,"engraved":16328,"##ffy":16329,"evaluated":16330,"1778":16331,"haze":16332,"146":16333,"comforting":16334,"crossover":16335,"##abe":16336,"thorn":16337,"##rift":16338,"##imo":16339,"##pop":16340,"suppression":16341,"fatigue":16342,"cutter":16343,"##tr":16344,"201":16345,"wurttemberg":16346,"##orf":16347,"enforced":16348,"hovering":16349,"proprietary":16350,"gb":16351,"samurai":16352,"syllable":16353,"ascent":16354,"lacey":16355,"tick":16356,"lars":16357,"tractor":16358,"merchandise":16359,"rep":16360,"bouncing":16361,"defendants":16362,"##yre":16363,"huntington":16364,"##ground":16365,"##oko":16366,"standardized":16367,"##hor":16368,"##hima":16369,"assassinated":16370,"nu":16371,"predecessors":16372,"rainy":16373,"liar":16374,"assurance":16375,"lyrical":16376,"##uga":16377,"secondly":16378,"flattened":16379,"ios":16380,"parameter":16381,"undercover":16382,"##mity":16383,"bordeaux":16384,"punish":16385,"ridges":16386,"markers":16387,"exodus":16388,"inactive":16389,"hesitate":16390,"debbie":16391,"nyc":16392,"pledge":16393,"savoy":16394,"nagar":16395,"offset":16396,"organist":16397,"##tium":16398,"hesse":16399,"marin":16400,"converting":16401,"##iver":16402,"diagram":16403,"propulsion":16404,"pu":16405,"validity":16406,"reverted":16407,"supportive":16408,"##dc":16409,"ministries":16410,"clans":16411,"responds":16412,"proclamation":16413,"##inae":16414,"##ø":16415,"##rea":16416,"ein":16417,"pleading":16418,"patriot":16419,"sf":16420,"birch":16421,"islanders":16422,"strauss":16423,"hates":16424,"##dh":16425,"brandenburg":16426,"concession":16427,"rd":16428,"##ob":16429,"1900s":16430,"killings":16431,"textbook":16432,"antiquity":16433,"cinematography":16434,"wharf":16435,"embarrassing":16436,"setup":16437,"creed":16438,"farmland":16439,"inequality":16440,"centred":16441,"signatures":16442,"fallon":16443,"370":16444,"##ingham":16445,"##uts":16446,"ceylon":16447,"gazing":16448,"directive":16449,"laurie":16450,"##tern":16451,"globally":16452,"##uated":16453,"##dent":16454,"allah":16455,"excavation":16456,"threads":16457,"##cross":16458,"148":16459,"frantically":16460,"icc":16461,"utilize":16462,"determines":16463,"respiratory":16464,"thoughtful":16465,"receptions":16466,"##dicate":16467,"merging":16468,"chandra":16469,"seine":16470,"147":16471,"builders":16472,"builds":16473,"diagnostic":16474,"dev":16475,"visibility":16476,"goddamn":16477,"analyses":16478,"dhaka":16479,"cho":16480,"proves":16481,"chancel":16482,"concurrent":16483,"curiously":16484,"canadians":16485,"pumped":16486,"restoring":16487,"1850s":16488,"turtles":16489,"jaguar":16490,"sinister":16491,"spinal":16492,"traction":16493,"declan":16494,"vows":16495,"1784":16496,"glowed":16497,"capitalism":16498,"swirling":16499,"install":16500,"universidad":16501,"##lder":16502,"##oat":16503,"soloist":16504,"##genic":16505,"##oor":16506,"coincidence":16507,"beginnings":16508,"nissan":16509,"dip":16510,"resorts":16511,"caucasus":16512,"combustion":16513,"infectious":16514,"##eno":16515,"pigeon":16516,"serpent":16517,"##itating":16518,"conclude":16519,"masked":16520,"salad":16521,"jew":16522,"##gr":16523,"surreal":16524,"toni":16525,"##wc":16526,"harmonica":16527,"151":16528,"##gins":16529,"##etic":16530,"##coat":16531,"fishermen":16532,"intending":16533,"bravery":16534,"##wave":16535,"klaus":16536,"titan":16537,"wembley":16538,"taiwanese":16539,"ransom":16540,"40th":16541,"incorrect":16542,"hussein":16543,"eyelids":16544,"jp":16545,"cooke":16546,"dramas":16547,"utilities":16548,"##etta":16549,"##print":16550,"eisenhower":16551,"principally":16552,"granada":16553,"lana":16554,"##rak":16555,"openings":16556,"concord":16557,"##bl":16558,"bethany":16559,"connie":16560,"morality":16561,"sega":16562,"##mons":16563,"##nard":16564,"earnings":16565,"##kara":16566,"##cine":16567,"wii":16568,"communes":16569,"##rel":16570,"coma":16571,"composing":16572,"softened":16573,"severed":16574,"grapes":16575,"##17":16576,"nguyen":16577,"analyzed":16578,"warlord":16579,"hubbard":16580,"heavenly":16581,"behave":16582,"slovenian":16583,"##hit":16584,"##ony":16585,"hailed":16586,"filmmakers":16587,"trance":16588,"caldwell":16589,"skye":16590,"unrest":16591,"coward":16592,"likelihood":16593,"##aging":16594,"bern":16595,"sci":16596,"taliban":16597,"honolulu":16598,"propose":16599,"##wang":16600,"1700":16601,"browser":16602,"imagining":16603,"cobra":16604,"contributes":16605,"dukes":16606,"instinctively":16607,"conan":16608,"violinist":16609,"##ores":16610,"accessories":16611,"gradual":16612,"##amp":16613,"quotes":16614,"sioux":16615,"##dating":16616,"undertake":16617,"intercepted":16618,"sparkling":16619,"compressed":16620,"139":16621,"fungus":16622,"tombs":16623,"haley":16624,"imposing":16625,"rests":16626,"degradation":16627,"lincolnshire":16628,"retailers":16629,"wetlands":16630,"tulsa":16631,"distributor":16632,"dungeon":16633,"nun":16634,"greenhouse":16635,"convey":16636,"atlantis":16637,"aft":16638,"exits":16639,"oman":16640,"dresser":16641,"lyons":16642,"##sti":16643,"joking":16644,"eddy":16645,"judgement":16646,"omitted":16647,"digits":16648,"##cts":16649,"##game":16650,"juniors":16651,"##rae":16652,"cents":16653,"stricken":16654,"une":16655,"##ngo":16656,"wizards":16657,"weir":16658,"breton":16659,"nan":16660,"technician":16661,"fibers":16662,"liking":16663,"royalty":16664,"##cca":16665,"154":16666,"persia":16667,"terribly":16668,"magician":16669,"##rable":16670,"##unt":16671,"vance":16672,"cafeteria":16673,"booker":16674,"camille":16675,"warmer":16676,"##static":16677,"consume":16678,"cavern":16679,"gaps":16680,"compass":16681,"contemporaries":16682,"foyer":16683,"soothing":16684,"graveyard":16685,"maj":16686,"plunged":16687,"blush":16688,"##wear":16689,"cascade":16690,"demonstrates":16691,"ordinance":16692,"##nov":16693,"boyle":16694,"##lana":16695,"rockefeller":16696,"shaken":16697,"banjo":16698,"izzy":16699,"##ense":16700,"breathless":16701,"vines":16702,"##32":16703,"##eman":16704,"alterations":16705,"chromosome":16706,"dwellings":16707,"feudal":16708,"mole":16709,"153":16710,"catalonia":16711,"relics":16712,"tenant":16713,"mandated":16714,"##fm":16715,"fridge":16716,"hats":16717,"honesty":16718,"patented":16719,"raul":16720,"heap":16721,"cruisers":16722,"accusing":16723,"enlightenment":16724,"infants":16725,"wherein":16726,"chatham":16727,"contractors":16728,"zen":16729,"affinity":16730,"hc":16731,"osborne":16732,"piston":16733,"156":16734,"traps":16735,"maturity":16736,"##rana":16737,"lagos":16738,"##zal":16739,"peering":16740,"##nay":16741,"attendant":16742,"dealers":16743,"protocols":16744,"subset":16745,"prospects":16746,"biographical":16747,"##cre":16748,"artery":16749,"##zers":16750,"insignia":16751,"nuns":16752,"endured":16753,"##eration":16754,"recommend":16755,"schwartz":16756,"serbs":16757,"berger":16758,"cromwell":16759,"crossroads":16760,"##ctor":16761,"enduring":16762,"clasped":16763,"grounded":16764,"##bine":16765,"marseille":16766,"twitched":16767,"abel":16768,"choke":16769,"https":16770,"catalyst":16771,"moldova":16772,"italians":16773,"##tist":16774,"disastrous":16775,"wee":16776,"##oured":16777,"##nti":16778,"wwf":16779,"nope":16780,"##piration":16781,"##asa":16782,"expresses":16783,"thumbs":16784,"167":16785,"##nza":16786,"coca":16787,"1781":16788,"cheating":16789,"##ption":16790,"skipped":16791,"sensory":16792,"heidelberg":16793,"spies":16794,"satan":16795,"dangers":16796,"semifinal":16797,"202":16798,"bohemia":16799,"whitish":16800,"confusing":16801,"shipbuilding":16802,"relies":16803,"surgeons":16804,"landings":16805,"ravi":16806,"baku":16807,"moor":16808,"suffix":16809,"alejandro":16810,"##yana":16811,"litre":16812,"upheld":16813,"##unk":16814,"rajasthan":16815,"##rek":16816,"coaster":16817,"insists":16818,"posture":16819,"scenarios":16820,"etienne":16821,"favoured":16822,"appoint":16823,"transgender":16824,"elephants":16825,"poked":16826,"greenwood":16827,"defences":16828,"fulfilled":16829,"militant":16830,"somali":16831,"1758":16832,"chalk":16833,"potent":16834,"##ucci":16835,"migrants":16836,"wink":16837,"assistants":16838,"nos":16839,"restriction":16840,"activism":16841,"niger":16842,"##ario":16843,"colon":16844,"shaun":16845,"##sat":16846,"daphne":16847,"##erated":16848,"swam":16849,"congregations":16850,"reprise":16851,"considerations":16852,"magnet":16853,"playable":16854,"xvi":16855,"##р":16856,"overthrow":16857,"tobias":16858,"knob":16859,"chavez":16860,"coding":16861,"##mers":16862,"propped":16863,"katrina":16864,"orient":16865,"newcomer":16866,"##suke":16867,"temperate":16868,"##pool":16869,"farmhouse":16870,"interrogation":16871,"##vd":16872,"committing":16873,"##vert":16874,"forthcoming":16875,"strawberry":16876,"joaquin":16877,"macau":16878,"ponds":16879,"shocking":16880,"siberia":16881,"##cellular":16882,"chant":16883,"contributors":16884,"##nant":16885,"##ologists":16886,"sped":16887,"absorb":16888,"hail":16889,"1782":16890,"spared":16891,"##hore":16892,"barbados":16893,"karate":16894,"opus":16895,"originates":16896,"saul":16897,"##xie":16898,"evergreen":16899,"leaped":16900,"##rock":16901,"correlation":16902,"exaggerated":16903,"weekday":16904,"unification":16905,"bump":16906,"tracing":16907,"brig":16908,"afb":16909,"pathways":16910,"utilizing":16911,"##ners":16912,"mod":16913,"mb":16914,"disturbance":16915,"kneeling":16916,"##stad":16917,"##guchi":16918,"100th":16919,"pune":16920,"##thy":16921,"decreasing":16922,"168":16923,"manipulation":16924,"miriam":16925,"academia":16926,"ecosystem":16927,"occupational":16928,"rbi":16929,"##lem":16930,"rift":16931,"##14":16932,"rotary":16933,"stacked":16934,"incorporation":16935,"awakening":16936,"generators":16937,"guerrero":16938,"racist":16939,"##omy":16940,"cyber":16941,"derivatives":16942,"culminated":16943,"allie":16944,"annals":16945,"panzer":16946,"sainte":16947,"wikipedia":16948,"pops":16949,"zu":16950,"austro":16951,"##vate":16952,"algerian":16953,"politely":16954,"nicholson":16955,"mornings":16956,"educate":16957,"tastes":16958,"thrill":16959,"dartmouth":16960,"##gating":16961,"db":16962,"##jee":16963,"regan":16964,"differing":16965,"concentrating":16966,"choreography":16967,"divinity":16968,"##media":16969,"pledged":16970,"alexandre":16971,"routing":16972,"gregor":16973,"madeline":16974,"##idal":16975,"apocalypse":16976,"##hora":16977,"gunfire":16978,"culminating":16979,"elves":16980,"fined":16981,"liang":16982,"lam":16983,"programmed":16984,"tar":16985,"guessing":16986,"transparency":16987,"gabrielle":16988,"##gna":16989,"cancellation":16990,"flexibility":16991,"##lining":16992,"accession":16993,"shea":16994,"stronghold":16995,"nets":16996,"specializes":16997,"##rgan":16998,"abused":16999,"hasan":17000,"sgt":17001,"ling":17002,"exceeding":17003,"##₄":17004,"admiration":17005,"supermarket":17006,"##ark":17007,"photographers":17008,"specialised":17009,"tilt":17010,"resonance":17011,"hmm":17012,"perfume":17013,"380":17014,"sami":17015,"threatens":17016,"garland":17017,"botany":17018,"guarding":17019,"boiled":17020,"greet":17021,"puppy":17022,"russo":17023,"supplier":17024,"wilmington":17025,"vibrant":17026,"vijay":17027,"##bius":17028,"paralympic":17029,"grumbled":17030,"paige":17031,"faa":17032,"licking":17033,"margins":17034,"hurricanes":17035,"##gong":17036,"fest":17037,"grenade":17038,"ripping":17039,"##uz":17040,"counseling":17041,"weigh":17042,"##sian":17043,"needles":17044,"wiltshire":17045,"edison":17046,"costly":17047,"##not":17048,"fulton":17049,"tramway":17050,"redesigned":17051,"staffordshire":17052,"cache":17053,"gasping":17054,"watkins":17055,"sleepy":17056,"candidacy":17057,"##group":17058,"monkeys":17059,"timeline":17060,"throbbing":17061,"##bid":17062,"##sos":17063,"berth":17064,"uzbekistan":17065,"vanderbilt":17066,"bothering":17067,"overturned":17068,"ballots":17069,"gem":17070,"##iger":17071,"sunglasses":17072,"subscribers":17073,"hooker":17074,"compelling":17075,"ang":17076,"exceptionally":17077,"saloon":17078,"stab":17079,"##rdi":17080,"carla":17081,"terrifying":17082,"rom":17083,"##vision":17084,"coil":17085,"##oids":17086,"satisfying":17087,"vendors":17088,"31st":17089,"mackay":17090,"deities":17091,"overlooked":17092,"ambient":17093,"bahamas":17094,"felipe":17095,"olympia":17096,"whirled":17097,"botanist":17098,"advertised":17099,"tugging":17100,"##dden":17101,"disciples":17102,"morales":17103,"unionist":17104,"rites":17105,"foley":17106,"morse":17107,"motives":17108,"creepy":17109,"##₀":17110,"soo":17111,"##sz":17112,"bargain":17113,"highness":17114,"frightening":17115,"turnpike":17116,"tory":17117,"reorganization":17118,"##cer":17119,"depict":17120,"biographer":17121,"##walk":17122,"unopposed":17123,"manifesto":17124,"##gles":17125,"institut":17126,"emile":17127,"accidental":17128,"kapoor":17129,"##dam":17130,"kilkenny":17131,"cortex":17132,"lively":17133,"##13":17134,"romanesque":17135,"jain":17136,"shan":17137,"cannons":17138,"##ood":17139,"##ske":17140,"petrol":17141,"echoing":17142,"amalgamated":17143,"disappears":17144,"cautious":17145,"proposes":17146,"sanctions":17147,"trenton":17148,"##ر":17149,"flotilla":17150,"aus":17151,"contempt":17152,"tor":17153,"canary":17154,"cote":17155,"theirs":17156,"##hun":17157,"conceptual":17158,"deleted":17159,"fascinating":17160,"paso":17161,"blazing":17162,"elf":17163,"honourable":17164,"hutchinson":17165,"##eiro":17166,"##outh":17167,"##zin":17168,"surveyor":17169,"tee":17170,"amidst":17171,"wooded":17172,"reissue":17173,"intro":17174,"##ono":17175,"cobb":17176,"shelters":17177,"newsletter":17178,"hanson":17179,"brace":17180,"encoding":17181,"confiscated":17182,"dem":17183,"caravan":17184,"marino":17185,"scroll":17186,"melodic":17187,"cows":17188,"imam":17189,"##adi":17190,"##aneous":17191,"northward":17192,"searches":17193,"biodiversity":17194,"cora":17195,"310":17196,"roaring":17197,"##bers":17198,"connell":17199,"theologian":17200,"halo":17201,"compose":17202,"pathetic":17203,"unmarried":17204,"dynamo":17205,"##oot":17206,"az":17207,"calculation":17208,"toulouse":17209,"deserves":17210,"humour":17211,"nr":17212,"forgiveness":17213,"tam":17214,"undergone":17215,"martyr":17216,"pamela":17217,"myths":17218,"whore":17219,"counselor":17220,"hicks":17221,"290":17222,"heavens":17223,"battleship":17224,"electromagnetic":17225,"##bbs":17226,"stellar":17227,"establishments":17228,"presley":17229,"hopped":17230,"##chin":17231,"temptation":17232,"90s":17233,"wills":17234,"nas":17235,"##yuan":17236,"nhs":17237,"##nya":17238,"seminars":17239,"##yev":17240,"adaptations":17241,"gong":17242,"asher":17243,"lex":17244,"indicator":17245,"sikh":17246,"tobago":17247,"cites":17248,"goin":17249,"##yte":17250,"satirical":17251,"##gies":17252,"characterised":17253,"correspond":17254,"bubbles":17255,"lure":17256,"participates":17257,"##vid":17258,"eruption":17259,"skate":17260,"therapeutic":17261,"1785":17262,"canals":17263,"wholesale":17264,"defaulted":17265,"sac":17266,"460":17267,"petit":17268,"##zzled":17269,"virgil":17270,"leak":17271,"ravens":17272,"256":17273,"portraying":17274,"##yx":17275,"ghetto":17276,"creators":17277,"dams":17278,"portray":17279,"vicente":17280,"##rington":17281,"fae":17282,"namesake":17283,"bounty":17284,"##arium":17285,"joachim":17286,"##ota":17287,"##iser":17288,"aforementioned":17289,"axle":17290,"snout":17291,"depended":17292,"dismantled":17293,"reuben":17294,"480":17295,"##ibly":17296,"gallagher":17297,"##lau":17298,"##pd":17299,"earnest":17300,"##ieu":17301,"##iary":17302,"inflicted":17303,"objections":17304,"##llar":17305,"asa":17306,"gritted":17307,"##athy":17308,"jericho":17309,"##sea":17310,"##was":17311,"flick":17312,"underside":17313,"ceramics":17314,"undead":17315,"substituted":17316,"195":17317,"eastward":17318,"undoubtedly":17319,"wheeled":17320,"chimney":17321,"##iche":17322,"guinness":17323,"cb":17324,"##ager":17325,"siding":17326,"##bell":17327,"traitor":17328,"baptiste":17329,"disguised":17330,"inauguration":17331,"149":17332,"tipperary":17333,"choreographer":17334,"perched":17335,"warmed":17336,"stationary":17337,"eco":17338,"##ike":17339,"##ntes":17340,"bacterial":17341,"##aurus":17342,"flores":17343,"phosphate":17344,"##core":17345,"attacker":17346,"invaders":17347,"alvin":17348,"intersects":17349,"a1":17350,"indirectly":17351,"immigrated":17352,"businessmen":17353,"cornelius":17354,"valves":17355,"narrated":17356,"pill":17357,"sober":17358,"ul":17359,"nationale":17360,"monastic":17361,"applicants":17362,"scenery":17363,"##jack":17364,"161":17365,"motifs":17366,"constitutes":17367,"cpu":17368,"##osh":17369,"jurisdictions":17370,"sd":17371,"tuning":17372,"irritation":17373,"woven":17374,"##uddin":17375,"fertility":17376,"gao":17377,"##erie":17378,"antagonist":17379,"impatient":17380,"glacial":17381,"hides":17382,"boarded":17383,"denominations":17384,"interception":17385,"##jas":17386,"cookie":17387,"nicola":17388,"##tee":17389,"algebraic":17390,"marquess":17391,"bahn":17392,"parole":17393,"buyers":17394,"bait":17395,"turbines":17396,"paperwork":17397,"bestowed":17398,"natasha":17399,"renee":17400,"oceans":17401,"purchases":17402,"157":17403,"vaccine":17404,"215":17405,"##tock":17406,"fixtures":17407,"playhouse":17408,"integrate":17409,"jai":17410,"oswald":17411,"intellectuals":17412,"##cky":17413,"booked":17414,"nests":17415,"mortimer":17416,"##isi":17417,"obsession":17418,"sept":17419,"##gler":17420,"##sum":17421,"440":17422,"scrutiny":17423,"simultaneous":17424,"squinted":17425,"##shin":17426,"collects":17427,"oven":17428,"shankar":17429,"penned":17430,"remarkably":17431,"##я":17432,"slips":17433,"luggage":17434,"spectral":17435,"1786":17436,"collaborations":17437,"louie":17438,"consolidation":17439,"##ailed":17440,"##ivating":17441,"420":17442,"hoover":17443,"blackpool":17444,"harness":17445,"ignition":17446,"vest":17447,"tails":17448,"belmont":17449,"mongol":17450,"skinner":17451,"##nae":17452,"visually":17453,"mage":17454,"derry":17455,"##tism":17456,"##unce":17457,"stevie":17458,"transitional":17459,"##rdy":17460,"redskins":17461,"drying":17462,"prep":17463,"prospective":17464,"##21":17465,"annoyance":17466,"oversee":17467,"##loaded":17468,"fills":17469,"##books":17470,"##iki":17471,"announces":17472,"fda":17473,"scowled":17474,"respects":17475,"prasad":17476,"mystic":17477,"tucson":17478,"##vale":17479,"revue":17480,"springer":17481,"bankrupt":17482,"1772":17483,"aristotle":17484,"salvatore":17485,"habsburg":17486,"##geny":17487,"dal":17488,"natal":17489,"nut":17490,"pod":17491,"chewing":17492,"darts":17493,"moroccan":17494,"walkover":17495,"rosario":17496,"lenin":17497,"punjabi":17498,"##ße":17499,"grossed":17500,"scattering":17501,"wired":17502,"invasive":17503,"hui":17504,"polynomial":17505,"corridors":17506,"wakes":17507,"gina":17508,"portrays":17509,"##cratic":17510,"arid":17511,"retreating":17512,"erich":17513,"irwin":17514,"sniper":17515,"##dha":17516,"linen":17517,"lindsey":17518,"maneuver":17519,"butch":17520,"shutting":17521,"socio":17522,"bounce":17523,"commemorative":17524,"postseason":17525,"jeremiah":17526,"pines":17527,"275":17528,"mystical":17529,"beads":17530,"bp":17531,"abbas":17532,"furnace":17533,"bidding":17534,"consulted":17535,"assaulted":17536,"empirical":17537,"rubble":17538,"enclosure":17539,"sob":17540,"weakly":17541,"cancel":17542,"polly":17543,"yielded":17544,"##emann":17545,"curly":17546,"prediction":17547,"battered":17548,"70s":17549,"vhs":17550,"jacqueline":17551,"render":17552,"sails":17553,"barked":17554,"detailing":17555,"grayson":17556,"riga":17557,"sloane":17558,"raging":17559,"##yah":17560,"herbs":17561,"bravo":17562,"##athlon":17563,"alloy":17564,"giggle":17565,"imminent":17566,"suffers":17567,"assumptions":17568,"waltz":17569,"##itate":17570,"accomplishments":17571,"##ited":17572,"bathing":17573,"remixed":17574,"deception":17575,"prefix":17576,"##emia":17577,"deepest":17578,"##tier":17579,"##eis":17580,"balkan":17581,"frogs":17582,"##rong":17583,"slab":17584,"##pate":17585,"philosophers":17586,"peterborough":17587,"grains":17588,"imports":17589,"dickinson":17590,"rwanda":17591,"##atics":17592,"1774":17593,"dirk":17594,"lan":17595,"tablets":17596,"##rove":17597,"clone":17598,"##rice":17599,"caretaker":17600,"hostilities":17601,"mclean":17602,"##gre":17603,"regimental":17604,"treasures":17605,"norms":17606,"impose":17607,"tsar":17608,"tango":17609,"diplomacy":17610,"variously":17611,"complain":17612,"192":17613,"recognise":17614,"arrests":17615,"1779":17616,"celestial":17617,"pulitzer":17618,"##dus":17619,"bing":17620,"libretto":17621,"##moor":17622,"adele":17623,"splash":17624,"##rite":17625,"expectation":17626,"lds":17627,"confronts":17628,"##izer":17629,"spontaneous":17630,"harmful":17631,"wedge":17632,"entrepreneurs":17633,"buyer":17634,"##ope":17635,"bilingual":17636,"translate":17637,"rugged":17638,"conner":17639,"circulated":17640,"uae":17641,"eaton":17642,"##gra":17643,"##zzle":17644,"lingered":17645,"lockheed":17646,"vishnu":17647,"reelection":17648,"alonso":17649,"##oom":17650,"joints":17651,"yankee":17652,"headline":17653,"cooperate":17654,"heinz":17655,"laureate":17656,"invading":17657,"##sford":17658,"echoes":17659,"scandinavian":17660,"##dham":17661,"hugging":17662,"vitamin":17663,"salute":17664,"micah":17665,"hind":17666,"trader":17667,"##sper":17668,"radioactive":17669,"##ndra":17670,"militants":17671,"poisoned":17672,"ratified":17673,"remark":17674,"campeonato":17675,"deprived":17676,"wander":17677,"prop":17678,"##dong":17679,"outlook":17680,"##tani":17681,"##rix":17682,"##eye":17683,"chiang":17684,"darcy":17685,"##oping":17686,"mandolin":17687,"spice":17688,"statesman":17689,"babylon":17690,"182":17691,"walled":17692,"forgetting":17693,"afro":17694,"##cap":17695,"158":17696,"giorgio":17697,"buffer":17698,"##polis":17699,"planetary":17700,"##gis":17701,"overlap":17702,"terminals":17703,"kinda":17704,"centenary":17705,"##bir":17706,"arising":17707,"manipulate":17708,"elm":17709,"ke":17710,"1770":17711,"ak":17712,"##tad":17713,"chrysler":17714,"mapped":17715,"moose":17716,"pomeranian":17717,"quad":17718,"macarthur":17719,"assemblies":17720,"shoreline":17721,"recalls":17722,"stratford":17723,"##rted":17724,"noticeable":17725,"##evic":17726,"imp":17727,"##rita":17728,"##sque":17729,"accustomed":17730,"supplying":17731,"tents":17732,"disgusted":17733,"vogue":17734,"sipped":17735,"filters":17736,"khz":17737,"reno":17738,"selecting":17739,"luftwaffe":17740,"mcmahon":17741,"tyne":17742,"masterpiece":17743,"carriages":17744,"collided":17745,"dunes":17746,"exercised":17747,"flare":17748,"remembers":17749,"muzzle":17750,"##mobile":17751,"heck":17752,"##rson":17753,"burgess":17754,"lunged":17755,"middleton":17756,"boycott":17757,"bilateral":17758,"##sity":17759,"hazardous":17760,"lumpur":17761,"multiplayer":17762,"spotlight":17763,"jackets":17764,"goldman":17765,"liege":17766,"porcelain":17767,"rag":17768,"waterford":17769,"benz":17770,"attracts":17771,"hopeful":17772,"battling":17773,"ottomans":17774,"kensington":17775,"baked":17776,"hymns":17777,"cheyenne":17778,"lattice":17779,"levine":17780,"borrow":17781,"polymer":17782,"clashes":17783,"michaels":17784,"monitored":17785,"commitments":17786,"denounced":17787,"##25":17788,"##von":17789,"cavity":17790,"##oney":17791,"hobby":17792,"akin":17793,"##holders":17794,"futures":17795,"intricate":17796,"cornish":17797,"patty":17798,"##oned":17799,"illegally":17800,"dolphin":17801,"##lag":17802,"barlow":17803,"yellowish":17804,"maddie":17805,"apologized":17806,"luton":17807,"plagued":17808,"##puram":17809,"nana":17810,"##rds":17811,"sway":17812,"fanny":17813,"łodz":17814,"##rino":17815,"psi":17816,"suspicions":17817,"hanged":17818,"##eding":17819,"initiate":17820,"charlton":17821,"##por":17822,"nak":17823,"competent":17824,"235":17825,"analytical":17826,"annex":17827,"wardrobe":17828,"reservations":17829,"##rma":17830,"sect":17831,"162":17832,"fairfax":17833,"hedge":17834,"piled":17835,"buckingham":17836,"uneven":17837,"bauer":17838,"simplicity":17839,"snyder":17840,"interpret":17841,"accountability":17842,"donors":17843,"moderately":17844,"byrd":17845,"continents":17846,"##cite":17847,"##max":17848,"disciple":17849,"hr":17850,"jamaican":17851,"ping":17852,"nominees":17853,"##uss":17854,"mongolian":17855,"diver":17856,"attackers":17857,"eagerly":17858,"ideological":17859,"pillows":17860,"miracles":17861,"apartheid":17862,"revolver":17863,"sulfur":17864,"clinics":17865,"moran":17866,"163":17867,"##enko":17868,"ile":17869,"katy":17870,"rhetoric":17871,"##icated":17872,"chronology":17873,"recycling":17874,"##hrer":17875,"elongated":17876,"mughal":17877,"pascal":17878,"profiles":17879,"vibration":17880,"databases":17881,"domination":17882,"##fare":17883,"##rant":17884,"matthias":17885,"digest":17886,"rehearsal":17887,"polling":17888,"weiss":17889,"initiation":17890,"reeves":17891,"clinging":17892,"flourished":17893,"impress":17894,"ngo":17895,"##hoff":17896,"##ume":17897,"buckley":17898,"symposium":17899,"rhythms":17900,"weed":17901,"emphasize":17902,"transforming":17903,"##taking":17904,"##gence":17905,"##yman":17906,"accountant":17907,"analyze":17908,"flicker":17909,"foil":17910,"priesthood":17911,"voluntarily":17912,"decreases":17913,"##80":17914,"##hya":17915,"slater":17916,"sv":17917,"charting":17918,"mcgill":17919,"##lde":17920,"moreno":17921,"##iu":17922,"besieged":17923,"zur":17924,"robes":17925,"##phic":17926,"admitting":17927,"api":17928,"deported":17929,"turmoil":17930,"peyton":17931,"earthquakes":17932,"##ares":17933,"nationalists":17934,"beau":17935,"clair":17936,"brethren":17937,"interrupt":17938,"welch":17939,"curated":17940,"galerie":17941,"requesting":17942,"164":17943,"##ested":17944,"impending":17945,"steward":17946,"viper":17947,"##vina":17948,"complaining":17949,"beautifully":17950,"brandy":17951,"foam":17952,"nl":17953,"1660":17954,"##cake":17955,"alessandro":17956,"punches":17957,"laced":17958,"explanations":17959,"##lim":17960,"attribute":17961,"clit":17962,"reggie":17963,"discomfort":17964,"##cards":17965,"smoothed":17966,"whales":17967,"##cene":17968,"adler":17969,"countered":17970,"duffy":17971,"disciplinary":17972,"widening":17973,"recipe":17974,"reliance":17975,"conducts":17976,"goats":17977,"gradient":17978,"preaching":17979,"##shaw":17980,"matilda":17981,"quasi":17982,"striped":17983,"meridian":17984,"cannabis":17985,"cordoba":17986,"certificates":17987,"##agh":17988,"##tering":17989,"graffiti":17990,"hangs":17991,"pilgrims":17992,"repeats":17993,"##ych":17994,"revive":17995,"urine":17996,"etat":17997,"##hawk":17998,"fueled":17999,"belts":18000,"fuzzy":18001,"susceptible":18002,"##hang":18003,"mauritius":18004,"salle":18005,"sincere":18006,"beers":18007,"hooks":18008,"##cki":18009,"arbitration":18010,"entrusted":18011,"advise":18012,"sniffed":18013,"seminar":18014,"junk":18015,"donnell":18016,"processors":18017,"principality":18018,"strapped":18019,"celia":18020,"mendoza":18021,"everton":18022,"fortunes":18023,"prejudice":18024,"starving":18025,"reassigned":18026,"steamer":18027,"##lund":18028,"tuck":18029,"evenly":18030,"foreman":18031,"##ffen":18032,"dans":18033,"375":18034,"envisioned":18035,"slit":18036,"##xy":18037,"baseman":18038,"liberia":18039,"rosemary":18040,"##weed":18041,"electrified":18042,"periodically":18043,"potassium":18044,"stride":18045,"contexts":18046,"sperm":18047,"slade":18048,"mariners":18049,"influx":18050,"bianca":18051,"subcommittee":18052,"##rane":18053,"spilling":18054,"icao":18055,"estuary":18056,"##nock":18057,"delivers":18058,"iphone":18059,"##ulata":18060,"isa":18061,"mira":18062,"bohemian":18063,"dessert":18064,"##sbury":18065,"welcoming":18066,"proudly":18067,"slowing":18068,"##chs":18069,"musee":18070,"ascension":18071,"russ":18072,"##vian":18073,"waits":18074,"##psy":18075,"africans":18076,"exploit":18077,"##morphic":18078,"gov":18079,"eccentric":18080,"crab":18081,"peck":18082,"##ull":18083,"entrances":18084,"formidable":18085,"marketplace":18086,"groom":18087,"bolted":18088,"metabolism":18089,"patton":18090,"robbins":18091,"courier":18092,"payload":18093,"endure":18094,"##ifier":18095,"andes":18096,"refrigerator":18097,"##pr":18098,"ornate":18099,"##uca":18100,"ruthless":18101,"illegitimate":18102,"masonry":18103,"strasbourg":18104,"bikes":18105,"adobe":18106,"##³":18107,"apples":18108,"quintet":18109,"willingly":18110,"niche":18111,"bakery":18112,"corpses":18113,"energetic":18114,"##cliffe":18115,"##sser":18116,"##ards":18117,"177":18118,"centimeters":18119,"centro":18120,"fuscous":18121,"cretaceous":18122,"rancho":18123,"##yde":18124,"andrei":18125,"telecom":18126,"tottenham":18127,"oasis":18128,"ordination":18129,"vulnerability":18130,"presiding":18131,"corey":18132,"cp":18133,"penguins":18134,"sims":18135,"##pis":18136,"malawi":18137,"piss":18138,"##48":18139,"correction":18140,"##cked":18141,"##ffle":18142,"##ryn":18143,"countdown":18144,"detectives":18145,"psychiatrist":18146,"psychedelic":18147,"dinosaurs":18148,"blouse":18149,"##get":18150,"choi":18151,"vowed":18152,"##oz":18153,"randomly":18154,"##pol":18155,"49ers":18156,"scrub":18157,"blanche":18158,"bruins":18159,"dusseldorf":18160,"##using":18161,"unwanted":18162,"##ums":18163,"212":18164,"dominique":18165,"elevations":18166,"headlights":18167,"om":18168,"laguna":18169,"##oga":18170,"1750":18171,"famously":18172,"ignorance":18173,"shrewsbury":18174,"##aine":18175,"ajax":18176,"breuning":18177,"che":18178,"confederacy":18179,"greco":18180,"overhaul":18181,"##screen":18182,"paz":18183,"skirts":18184,"disagreement":18185,"cruelty":18186,"jagged":18187,"phoebe":18188,"shifter":18189,"hovered":18190,"viruses":18191,"##wes":18192,"mandy":18193,"##lined":18194,"##gc":18195,"landlord":18196,"squirrel":18197,"dashed":18198,"##ι":18199,"ornamental":18200,"gag":18201,"wally":18202,"grange":18203,"literal":18204,"spurs":18205,"undisclosed":18206,"proceeding":18207,"yin":18208,"##text":18209,"billie":18210,"orphan":18211,"spanned":18212,"humidity":18213,"indy":18214,"weighted":18215,"presentations":18216,"explosions":18217,"lucian":18218,"##tary":18219,"vaughn":18220,"hindus":18221,"##anga":18222,"##hell":18223,"psycho":18224,"171":18225,"daytona":18226,"protects":18227,"efficiently":18228,"rematch":18229,"sly":18230,"tandem":18231,"##oya":18232,"rebranded":18233,"impaired":18234,"hee":18235,"metropolis":18236,"peach":18237,"godfrey":18238,"diaspora":18239,"ethnicity":18240,"prosperous":18241,"gleaming":18242,"dar":18243,"grossing":18244,"playback":18245,"##rden":18246,"stripe":18247,"pistols":18248,"##tain":18249,"births":18250,"labelled":18251,"##cating":18252,"172":18253,"rudy":18254,"alba":18255,"##onne":18256,"aquarium":18257,"hostility":18258,"##gb":18259,"##tase":18260,"shudder":18261,"sumatra":18262,"hardest":18263,"lakers":18264,"consonant":18265,"creeping":18266,"demos":18267,"homicide":18268,"capsule":18269,"zeke":18270,"liberties":18271,"expulsion":18272,"pueblo":18273,"##comb":18274,"trait":18275,"transporting":18276,"##ddin":18277,"##neck":18278,"##yna":18279,"depart":18280,"gregg":18281,"mold":18282,"ledge":18283,"hangar":18284,"oldham":18285,"playboy":18286,"termination":18287,"analysts":18288,"gmbh":18289,"romero":18290,"##itic":18291,"insist":18292,"cradle":18293,"filthy":18294,"brightness":18295,"slash":18296,"shootout":18297,"deposed":18298,"bordering":18299,"##truct":18300,"isis":18301,"microwave":18302,"tumbled":18303,"sheltered":18304,"cathy":18305,"werewolves":18306,"messy":18307,"andersen":18308,"convex":18309,"clapped":18310,"clinched":18311,"satire":18312,"wasting":18313,"edo":18314,"vc":18315,"rufus":18316,"##jak":18317,"mont":18318,"##etti":18319,"poznan":18320,"##keeping":18321,"restructuring":18322,"transverse":18323,"##rland":18324,"azerbaijani":18325,"slovene":18326,"gestures":18327,"roommate":18328,"choking":18329,"shear":18330,"##quist":18331,"vanguard":18332,"oblivious":18333,"##hiro":18334,"disagreed":18335,"baptism":18336,"##lich":18337,"coliseum":18338,"##aceae":18339,"salvage":18340,"societe":18341,"cory":18342,"locke":18343,"relocation":18344,"relying":18345,"versailles":18346,"ahl":18347,"swelling":18348,"##elo":18349,"cheerful":18350,"##word":18351,"##edes":18352,"gin":18353,"sarajevo":18354,"obstacle":18355,"diverted":18356,"##nac":18357,"messed":18358,"thoroughbred":18359,"fluttered":18360,"utrecht":18361,"chewed":18362,"acquaintance":18363,"assassins":18364,"dispatch":18365,"mirza":18366,"##wart":18367,"nike":18368,"salzburg":18369,"swell":18370,"yen":18371,"##gee":18372,"idle":18373,"ligue":18374,"samson":18375,"##nds":18376,"##igh":18377,"playful":18378,"spawned":18379,"##cise":18380,"tease":18381,"##case":18382,"burgundy":18383,"##bot":18384,"stirring":18385,"skeptical":18386,"interceptions":18387,"marathi":18388,"##dies":18389,"bedrooms":18390,"aroused":18391,"pinch":18392,"##lik":18393,"preferences":18394,"tattoos":18395,"buster":18396,"digitally":18397,"projecting":18398,"rust":18399,"##ital":18400,"kitten":18401,"priorities":18402,"addison":18403,"pseudo":18404,"##guard":18405,"dusk":18406,"icons":18407,"sermon":18408,"##psis":18409,"##iba":18410,"bt":18411,"##lift":18412,"##xt":18413,"ju":18414,"truce":18415,"rink":18416,"##dah":18417,"##wy":18418,"defects":18419,"psychiatry":18420,"offences":18421,"calculate":18422,"glucose":18423,"##iful":18424,"##rized":18425,"##unda":18426,"francaise":18427,"##hari":18428,"richest":18429,"warwickshire":18430,"carly":18431,"1763":18432,"purity":18433,"redemption":18434,"lending":18435,"##cious":18436,"muse":18437,"bruises":18438,"cerebral":18439,"aero":18440,"carving":18441,"##name":18442,"preface":18443,"terminology":18444,"invade":18445,"monty":18446,"##int":18447,"anarchist":18448,"blurred":18449,"##iled":18450,"rossi":18451,"treats":18452,"guts":18453,"shu":18454,"foothills":18455,"ballads":18456,"undertaking":18457,"premise":18458,"cecilia":18459,"affiliates":18460,"blasted":18461,"conditional":18462,"wilder":18463,"minors":18464,"drone":18465,"rudolph":18466,"buffy":18467,"swallowing":18468,"horton":18469,"attested":18470,"##hop":18471,"rutherford":18472,"howell":18473,"primetime":18474,"livery":18475,"penal":18476,"##bis":18477,"minimize":18478,"hydro":18479,"wrecked":18480,"wrought":18481,"palazzo":18482,"##gling":18483,"cans":18484,"vernacular":18485,"friedman":18486,"nobleman":18487,"shale":18488,"walnut":18489,"danielle":18490,"##ection":18491,"##tley":18492,"sears":18493,"##kumar":18494,"chords":18495,"lend":18496,"flipping":18497,"streamed":18498,"por":18499,"dracula":18500,"gallons":18501,"sacrifices":18502,"gamble":18503,"orphanage":18504,"##iman":18505,"mckenzie":18506,"##gible":18507,"boxers":18508,"daly":18509,"##balls":18510,"##ان":18511,"208":18512,"##ific":18513,"##rative":18514,"##iq":18515,"exploited":18516,"slated":18517,"##uity":18518,"circling":18519,"hillary":18520,"pinched":18521,"goldberg":18522,"provost":18523,"campaigning":18524,"lim":18525,"piles":18526,"ironically":18527,"jong":18528,"mohan":18529,"successors":18530,"usaf":18531,"##tem":18532,"##ught":18533,"autobiographical":18534,"haute":18535,"preserves":18536,"##ending":18537,"acquitted":18538,"comparisons":18539,"203":18540,"hydroelectric":18541,"gangs":18542,"cypriot":18543,"torpedoes":18544,"rushes":18545,"chrome":18546,"derive":18547,"bumps":18548,"instability":18549,"fiat":18550,"pets":18551,"##mbe":18552,"silas":18553,"dye":18554,"reckless":18555,"settler":18556,"##itation":18557,"info":18558,"heats":18559,"##writing":18560,"176":18561,"canonical":18562,"maltese":18563,"fins":18564,"mushroom":18565,"stacy":18566,"aspen":18567,"avid":18568,"##kur":18569,"##loading":18570,"vickers":18571,"gaston":18572,"hillside":18573,"statutes":18574,"wilde":18575,"gail":18576,"kung":18577,"sabine":18578,"comfortably":18579,"motorcycles":18580,"##rgo":18581,"169":18582,"pneumonia":18583,"fetch":18584,"##sonic":18585,"axel":18586,"faintly":18587,"parallels":18588,"##oop":18589,"mclaren":18590,"spouse":18591,"compton":18592,"interdisciplinary":18593,"miner":18594,"##eni":18595,"181":18596,"clamped":18597,"##chal":18598,"##llah":18599,"separates":18600,"versa":18601,"##mler":18602,"scarborough":18603,"labrador":18604,"##lity":18605,"##osing":18606,"rutgers":18607,"hurdles":18608,"como":18609,"166":18610,"burt":18611,"divers":18612,"##100":18613,"wichita":18614,"cade":18615,"coincided":18616,"##erson":18617,"bruised":18618,"mla":18619,"##pper":18620,"vineyard":18621,"##ili":18622,"##brush":18623,"notch":18624,"mentioning":18625,"jase":18626,"hearted":18627,"kits":18628,"doe":18629,"##acle":18630,"pomerania":18631,"##ady":18632,"ronan":18633,"seizure":18634,"pavel":18635,"problematic":18636,"##zaki":18637,"domenico":18638,"##ulin":18639,"catering":18640,"penelope":18641,"dependence":18642,"parental":18643,"emilio":18644,"ministerial":18645,"atkinson":18646,"##bolic":18647,"clarkson":18648,"chargers":18649,"colby":18650,"grill":18651,"peeked":18652,"arises":18653,"summon":18654,"##aged":18655,"fools":18656,"##grapher":18657,"faculties":18658,"qaeda":18659,"##vial":18660,"garner":18661,"refurbished":18662,"##hwa":18663,"geelong":18664,"disasters":18665,"nudged":18666,"bs":18667,"shareholder":18668,"lori":18669,"algae":18670,"reinstated":18671,"rot":18672,"##ades":18673,"##nous":18674,"invites":18675,"stainless":18676,"183":18677,"inclusive":18678,"##itude":18679,"diocesan":18680,"til":18681,"##icz":18682,"denomination":18683,"##xa":18684,"benton":18685,"floral":18686,"registers":18687,"##ider":18688,"##erman":18689,"##kell":18690,"absurd":18691,"brunei":18692,"guangzhou":18693,"hitter":18694,"retaliation":18695,"##uled":18696,"##eve":18697,"blanc":18698,"nh":18699,"consistency":18700,"contamination":18701,"##eres":18702,"##rner":18703,"dire":18704,"palermo":18705,"broadcasters":18706,"diaries":18707,"inspire":18708,"vols":18709,"brewer":18710,"tightening":18711,"ky":18712,"mixtape":18713,"hormone":18714,"##tok":18715,"stokes":18716,"##color":18717,"##dly":18718,"##ssi":18719,"pg":18720,"##ometer":18721,"##lington":18722,"sanitation":18723,"##tility":18724,"intercontinental":18725,"apps":18726,"##adt":18727,"¹⁄₂":18728,"cylinders":18729,"economies":18730,"favourable":18731,"unison":18732,"croix":18733,"gertrude":18734,"odyssey":18735,"vanity":18736,"dangling":18737,"##logists":18738,"upgrades":18739,"dice":18740,"middleweight":18741,"practitioner":18742,"##ight":18743,"206":18744,"henrik":18745,"parlor":18746,"orion":18747,"angered":18748,"lac":18749,"python":18750,"blurted":18751,"##rri":18752,"sensual":18753,"intends":18754,"swings":18755,"angled":18756,"##phs":18757,"husky":18758,"attain":18759,"peerage":18760,"precinct":18761,"textiles":18762,"cheltenham":18763,"shuffled":18764,"dai":18765,"confess":18766,"tasting":18767,"bhutan":18768,"##riation":18769,"tyrone":18770,"segregation":18771,"abrupt":18772,"ruiz":18773,"##rish":18774,"smirked":18775,"blackwell":18776,"confidential":18777,"browning":18778,"amounted":18779,"##put":18780,"vase":18781,"scarce":18782,"fabulous":18783,"raided":18784,"staple":18785,"guyana":18786,"unemployed":18787,"glider":18788,"shay":18789,"##tow":18790,"carmine":18791,"troll":18792,"intervene":18793,"squash":18794,"superstar":18795,"##uce":18796,"cylindrical":18797,"len":18798,"roadway":18799,"researched":18800,"handy":18801,"##rium":18802,"##jana":18803,"meta":18804,"lao":18805,"declares":18806,"##rring":18807,"##tadt":18808,"##elin":18809,"##kova":18810,"willem":18811,"shrubs":18812,"napoleonic":18813,"realms":18814,"skater":18815,"qi":18816,"volkswagen":18817,"##ł":18818,"tad":18819,"hara":18820,"archaeologist":18821,"awkwardly":18822,"eerie":18823,"##kind":18824,"wiley":18825,"##heimer":18826,"##24":18827,"titus":18828,"organizers":18829,"cfl":18830,"crusaders":18831,"lama":18832,"usb":18833,"vent":18834,"enraged":18835,"thankful":18836,"occupants":18837,"maximilian":18838,"##gaard":18839,"possessing":18840,"textbooks":18841,"##oran":18842,"collaborator":18843,"quaker":18844,"##ulo":18845,"avalanche":18846,"mono":18847,"silky":18848,"straits":18849,"isaiah":18850,"mustang":18851,"surged":18852,"resolutions":18853,"potomac":18854,"descend":18855,"cl":18856,"kilograms":18857,"plato":18858,"strains":18859,"saturdays":18860,"##olin":18861,"bernstein":18862,"##ype":18863,"holstein":18864,"ponytail":18865,"##watch":18866,"belize":18867,"conversely":18868,"heroine":18869,"perpetual":18870,"##ylus":18871,"charcoal":18872,"piedmont":18873,"glee":18874,"negotiating":18875,"backdrop":18876,"prologue":18877,"##jah":18878,"##mmy":18879,"pasadena":18880,"climbs":18881,"ramos":18882,"sunni":18883,"##holm":18884,"##tner":18885,"##tri":18886,"anand":18887,"deficiency":18888,"hertfordshire":18889,"stout":18890,"##avi":18891,"aperture":18892,"orioles":18893,"##irs":18894,"doncaster":18895,"intrigued":18896,"bombed":18897,"coating":18898,"otis":18899,"##mat":18900,"cocktail":18901,"##jit":18902,"##eto":18903,"amir":18904,"arousal":18905,"sar":18906,"##proof":18907,"##act":18908,"##ories":18909,"dixie":18910,"pots":18911,"##bow":18912,"whereabouts":18913,"159":18914,"##fted":18915,"drains":18916,"bullying":18917,"cottages":18918,"scripture":18919,"coherent":18920,"fore":18921,"poe":18922,"appetite":18923,"##uration":18924,"sampled":18925,"##ators":18926,"##dp":18927,"derrick":18928,"rotor":18929,"jays":18930,"peacock":18931,"installment":18932,"##rro":18933,"advisors":18934,"##coming":18935,"rodeo":18936,"scotch":18937,"##mot":18938,"##db":18939,"##fen":18940,"##vant":18941,"ensued":18942,"rodrigo":18943,"dictatorship":18944,"martyrs":18945,"twenties":18946,"##н":18947,"towed":18948,"incidence":18949,"marta":18950,"rainforest":18951,"sai":18952,"scaled":18953,"##cles":18954,"oceanic":18955,"qualifiers":18956,"symphonic":18957,"mcbride":18958,"dislike":18959,"generalized":18960,"aubrey":18961,"colonization":18962,"##iation":18963,"##lion":18964,"##ssing":18965,"disliked":18966,"lublin":18967,"salesman":18968,"##ulates":18969,"spherical":18970,"whatsoever":18971,"sweating":18972,"avalon":18973,"contention":18974,"punt":18975,"severity":18976,"alderman":18977,"atari":18978,"##dina":18979,"##grant":18980,"##rop":18981,"scarf":18982,"seville":18983,"vertices":18984,"annexation":18985,"fairfield":18986,"fascination":18987,"inspiring":18988,"launches":18989,"palatinate":18990,"regretted":18991,"##rca":18992,"feral":18993,"##iom":18994,"elk":18995,"nap":18996,"olsen":18997,"reddy":18998,"yong":18999,"##leader":19000,"##iae":19001,"garment":19002,"transports":19003,"feng":19004,"gracie":19005,"outrage":19006,"viceroy":19007,"insides":19008,"##esis":19009,"breakup":19010,"grady":19011,"organizer":19012,"softer":19013,"grimaced":19014,"222":19015,"murals":19016,"galicia":19017,"arranging":19018,"vectors":19019,"##rsten":19020,"bas":19021,"##sb":19022,"##cens":19023,"sloan":19024,"##eka":19025,"bitten":19026,"ara":19027,"fender":19028,"nausea":19029,"bumped":19030,"kris":19031,"banquet":19032,"comrades":19033,"detector":19034,"persisted":19035,"##llan":19036,"adjustment":19037,"endowed":19038,"cinemas":19039,"##shot":19040,"sellers":19041,"##uman":19042,"peek":19043,"epa":19044,"kindly":19045,"neglect":19046,"simpsons":19047,"talon":19048,"mausoleum":19049,"runaway":19050,"hangul":19051,"lookout":19052,"##cic":19053,"rewards":19054,"coughed":19055,"acquainted":19056,"chloride":19057,"##ald":19058,"quicker":19059,"accordion":19060,"neolithic":19061,"##qa":19062,"artemis":19063,"coefficient":19064,"lenny":19065,"pandora":19066,"tx":19067,"##xed":19068,"ecstasy":19069,"litter":19070,"segunda":19071,"chairperson":19072,"gemma":19073,"hiss":19074,"rumor":19075,"vow":19076,"nasal":19077,"antioch":19078,"compensate":19079,"patiently":19080,"transformers":19081,"##eded":19082,"judo":19083,"morrow":19084,"penis":19085,"posthumous":19086,"philips":19087,"bandits":19088,"husbands":19089,"denote":19090,"flaming":19091,"##any":19092,"##phones":19093,"langley":19094,"yorker":19095,"1760":19096,"walters":19097,"##uo":19098,"##kle":19099,"gubernatorial":19100,"fatty":19101,"samsung":19102,"leroy":19103,"outlaw":19104,"##nine":19105,"unpublished":19106,"poole":19107,"jakob":19108,"##ᵢ":19109,"##ₙ":19110,"crete":19111,"distorted":19112,"superiority":19113,"##dhi":19114,"intercept":19115,"crust":19116,"mig":19117,"claus":19118,"crashes":19119,"positioning":19120,"188":19121,"stallion":19122,"301":19123,"frontal":19124,"armistice":19125,"##estinal":19126,"elton":19127,"aj":19128,"encompassing":19129,"camel":19130,"commemorated":19131,"malaria":19132,"woodward":19133,"calf":19134,"cigar":19135,"penetrate":19136,"##oso":19137,"willard":19138,"##rno":19139,"##uche":19140,"illustrate":19141,"amusing":19142,"convergence":19143,"noteworthy":19144,"##lma":19145,"##rva":19146,"journeys":19147,"realise":19148,"manfred":19149,"##sable":19150,"410":19151,"##vocation":19152,"hearings":19153,"fiance":19154,"##posed":19155,"educators":19156,"provoked":19157,"adjusting":19158,"##cturing":19159,"modular":19160,"stockton":19161,"paterson":19162,"vlad":19163,"rejects":19164,"electors":19165,"selena":19166,"maureen":19167,"##tres":19168,"uber":19169,"##rce":19170,"swirled":19171,"##num":19172,"proportions":19173,"nanny":19174,"pawn":19175,"naturalist":19176,"parma":19177,"apostles":19178,"awoke":19179,"ethel":19180,"wen":19181,"##bey":19182,"monsoon":19183,"overview":19184,"##inating":19185,"mccain":19186,"rendition":19187,"risky":19188,"adorned":19189,"##ih":19190,"equestrian":19191,"germain":19192,"nj":19193,"conspicuous":19194,"confirming":19195,"##yoshi":19196,"shivering":19197,"##imeter":19198,"milestone":19199,"rumours":19200,"flinched":19201,"bounds":19202,"smacked":19203,"token":19204,"##bei":19205,"lectured":19206,"automobiles":19207,"##shore":19208,"impacted":19209,"##iable":19210,"nouns":19211,"nero":19212,"##leaf":19213,"ismail":19214,"prostitute":19215,"trams":19216,"##lace":19217,"bridget":19218,"sud":19219,"stimulus":19220,"impressions":19221,"reins":19222,"revolves":19223,"##oud":19224,"##gned":19225,"giro":19226,"honeymoon":19227,"##swell":19228,"criterion":19229,"##sms":19230,"##uil":19231,"libyan":19232,"prefers":19233,"##osition":19234,"211":19235,"preview":19236,"sucks":19237,"accusation":19238,"bursts":19239,"metaphor":19240,"diffusion":19241,"tolerate":19242,"faye":19243,"betting":19244,"cinematographer":19245,"liturgical":19246,"specials":19247,"bitterly":19248,"humboldt":19249,"##ckle":19250,"flux":19251,"rattled":19252,"##itzer":19253,"archaeologists":19254,"odor":19255,"authorised":19256,"marshes":19257,"discretion":19258,"##ов":19259,"alarmed":19260,"archaic":19261,"inverse":19262,"##leton":19263,"explorers":19264,"##pine":19265,"drummond":19266,"tsunami":19267,"woodlands":19268,"##minate":19269,"##tland":19270,"booklet":19271,"insanity":19272,"owning":19273,"insert":19274,"crafted":19275,"calculus":19276,"##tore":19277,"receivers":19278,"##bt":19279,"stung":19280,"##eca":19281,"##nched":19282,"prevailing":19283,"travellers":19284,"eyeing":19285,"lila":19286,"graphs":19287,"##borne":19288,"178":19289,"julien":19290,"##won":19291,"morale":19292,"adaptive":19293,"therapist":19294,"erica":19295,"cw":19296,"libertarian":19297,"bowman":19298,"pitches":19299,"vita":19300,"##ional":19301,"crook":19302,"##ads":19303,"##entation":19304,"caledonia":19305,"mutiny":19306,"##sible":19307,"1840s":19308,"automation":19309,"##ß":19310,"flock":19311,"##pia":19312,"ironic":19313,"pathology":19314,"##imus":19315,"remarried":19316,"##22":19317,"joker":19318,"withstand":19319,"energies":19320,"##att":19321,"shropshire":19322,"hostages":19323,"madeleine":19324,"tentatively":19325,"conflicting":19326,"mateo":19327,"recipes":19328,"euros":19329,"ol":19330,"mercenaries":19331,"nico":19332,"##ndon":19333,"albuquerque":19334,"augmented":19335,"mythical":19336,"bel":19337,"freud":19338,"##child":19339,"cough":19340,"##lica":19341,"365":19342,"freddy":19343,"lillian":19344,"genetically":19345,"nuremberg":19346,"calder":19347,"209":19348,"bonn":19349,"outdoors":19350,"paste":19351,"suns":19352,"urgency":19353,"vin":19354,"restraint":19355,"tyson":19356,"##cera":19357,"##selle":19358,"barrage":19359,"bethlehem":19360,"kahn":19361,"##par":19362,"mounts":19363,"nippon":19364,"barony":19365,"happier":19366,"ryu":19367,"makeshift":19368,"sheldon":19369,"blushed":19370,"castillo":19371,"barking":19372,"listener":19373,"taped":19374,"bethel":19375,"fluent":19376,"headlines":19377,"pornography":19378,"rum":19379,"disclosure":19380,"sighing":19381,"mace":19382,"doubling":19383,"gunther":19384,"manly":19385,"##plex":19386,"rt":19387,"interventions":19388,"physiological":19389,"forwards":19390,"emerges":19391,"##tooth":19392,"##gny":19393,"compliment":19394,"rib":19395,"recession":19396,"visibly":19397,"barge":19398,"faults":19399,"connector":19400,"exquisite":19401,"prefect":19402,"##rlin":19403,"patio":19404,"##cured":19405,"elevators":19406,"brandt":19407,"italics":19408,"pena":19409,"173":19410,"wasp":19411,"satin":19412,"ea":19413,"botswana":19414,"graceful":19415,"respectable":19416,"##jima":19417,"##rter":19418,"##oic":19419,"franciscan":19420,"generates":19421,"##dl":19422,"alfredo":19423,"disgusting":19424,"##olate":19425,"##iously":19426,"sherwood":19427,"warns":19428,"cod":19429,"promo":19430,"cheryl":19431,"sino":19432,"##ة":19433,"##escu":19434,"twitch":19435,"##zhi":19436,"brownish":19437,"thom":19438,"ortiz":19439,"##dron":19440,"densely":19441,"##beat":19442,"carmel":19443,"reinforce":19444,"##bana":19445,"187":19446,"anastasia":19447,"downhill":19448,"vertex":19449,"contaminated":19450,"remembrance":19451,"harmonic":19452,"homework":19453,"##sol":19454,"fiancee":19455,"gears":19456,"olds":19457,"angelica":19458,"loft":19459,"ramsay":19460,"quiz":19461,"colliery":19462,"sevens":19463,"##cape":19464,"autism":19465,"##hil":19466,"walkway":19467,"##boats":19468,"ruben":19469,"abnormal":19470,"ounce":19471,"khmer":19472,"##bbe":19473,"zachary":19474,"bedside":19475,"morphology":19476,"punching":19477,"##olar":19478,"sparrow":19479,"convinces":19480,"##35":19481,"hewitt":19482,"queer":19483,"remastered":19484,"rods":19485,"mabel":19486,"solemn":19487,"notified":19488,"lyricist":19489,"symmetric":19490,"##xide":19491,"174":19492,"encore":19493,"passports":19494,"wildcats":19495,"##uni":19496,"baja":19497,"##pac":19498,"mildly":19499,"##ease":19500,"bleed":19501,"commodity":19502,"mounds":19503,"glossy":19504,"orchestras":19505,"##omo":19506,"damian":19507,"prelude":19508,"ambitions":19509,"##vet":19510,"awhile":19511,"remotely":19512,"##aud":19513,"asserts":19514,"imply":19515,"##iques":19516,"distinctly":19517,"modelling":19518,"remedy":19519,"##dded":19520,"windshield":19521,"dani":19522,"xiao":19523,"##endra":19524,"audible":19525,"powerplant":19526,"1300":19527,"invalid":19528,"elemental":19529,"acquisitions":19530,"##hala":19531,"immaculate":19532,"libby":19533,"plata":19534,"smuggling":19535,"ventilation":19536,"denoted":19537,"minh":19538,"##morphism":19539,"430":19540,"differed":19541,"dion":19542,"kelley":19543,"lore":19544,"mocking":19545,"sabbath":19546,"spikes":19547,"hygiene":19548,"drown":19549,"runoff":19550,"stylized":19551,"tally":19552,"liberated":19553,"aux":19554,"interpreter":19555,"righteous":19556,"aba":19557,"siren":19558,"reaper":19559,"pearce":19560,"millie":19561,"##cier":19562,"##yra":19563,"gaius":19564,"##iso":19565,"captures":19566,"##ttering":19567,"dorm":19568,"claudio":19569,"##sic":19570,"benches":19571,"knighted":19572,"blackness":19573,"##ored":19574,"discount":19575,"fumble":19576,"oxidation":19577,"routed":19578,"##ς":19579,"novak":19580,"perpendicular":19581,"spoiled":19582,"fracture":19583,"splits":19584,"##urt":19585,"pads":19586,"topology":19587,"##cats":19588,"axes":19589,"fortunate":19590,"offenders":19591,"protestants":19592,"esteem":19593,"221":19594,"broadband":19595,"convened":19596,"frankly":19597,"hound":19598,"prototypes":19599,"isil":19600,"facilitated":19601,"keel":19602,"##sher":19603,"sahara":19604,"awaited":19605,"bubba":19606,"orb":19607,"prosecutors":19608,"186":19609,"hem":19610,"520":19611,"##xing":19612,"relaxing":19613,"remnant":19614,"romney":19615,"sorted":19616,"slalom":19617,"stefano":19618,"ulrich":19619,"##active":19620,"exemption":19621,"folder":19622,"pauses":19623,"foliage":19624,"hitchcock":19625,"epithet":19626,"204":19627,"criticisms":19628,"##aca":19629,"ballistic":19630,"brody":19631,"hinduism":19632,"chaotic":19633,"youths":19634,"equals":19635,"##pala":19636,"pts":19637,"thicker":19638,"analogous":19639,"capitalist":19640,"improvised":19641,"overseeing":19642,"sinatra":19643,"ascended":19644,"beverage":19645,"##tl":19646,"straightforward":19647,"##kon":19648,"curran":19649,"##west":19650,"bois":19651,"325":19652,"induce":19653,"surveying":19654,"emperors":19655,"sax":19656,"unpopular":19657,"##kk":19658,"cartoonist":19659,"fused":19660,"##mble":19661,"unto":19662,"##yuki":19663,"localities":19664,"##cko":19665,"##ln":19666,"darlington":19667,"slain":19668,"academie":19669,"lobbying":19670,"sediment":19671,"puzzles":19672,"##grass":19673,"defiance":19674,"dickens":19675,"manifest":19676,"tongues":19677,"alumnus":19678,"arbor":19679,"coincide":19680,"184":19681,"appalachian":19682,"mustafa":19683,"examiner":19684,"cabaret":19685,"traumatic":19686,"yves":19687,"bracelet":19688,"draining":19689,"heroin":19690,"magnum":19691,"baths":19692,"odessa":19693,"consonants":19694,"mitsubishi":19695,"##gua":19696,"kellan":19697,"vaudeville":19698,"##fr":19699,"joked":19700,"null":19701,"straps":19702,"probation":19703,"##ław":19704,"ceded":19705,"interfaces":19706,"##pas":19707,"##zawa":19708,"blinding":19709,"viet":19710,"224":19711,"rothschild":19712,"museo":19713,"640":19714,"huddersfield":19715,"##vr":19716,"tactic":19717,"##storm":19718,"brackets":19719,"dazed":19720,"incorrectly":19721,"##vu":19722,"reg":19723,"glazed":19724,"fearful":19725,"manifold":19726,"benefited":19727,"irony":19728,"##sun":19729,"stumbling":19730,"##rte":19731,"willingness":19732,"balkans":19733,"mei":19734,"wraps":19735,"##aba":19736,"injected":19737,"##lea":19738,"gu":19739,"syed":19740,"harmless":19741,"##hammer":19742,"bray":19743,"takeoff":19744,"poppy":19745,"timor":19746,"cardboard":19747,"astronaut":19748,"purdue":19749,"weeping":19750,"southbound":19751,"cursing":19752,"stalls":19753,"diagonal":19754,"##neer":19755,"lamar":19756,"bryce":19757,"comte":19758,"weekdays":19759,"harrington":19760,"##uba":19761,"negatively":19762,"##see":19763,"lays":19764,"grouping":19765,"##cken":19766,"##henko":19767,"affirmed":19768,"halle":19769,"modernist":19770,"##lai":19771,"hodges":19772,"smelling":19773,"aristocratic":19774,"baptized":19775,"dismiss":19776,"justification":19777,"oilers":19778,"##now":19779,"coupling":19780,"qin":19781,"snack":19782,"healer":19783,"##qing":19784,"gardener":19785,"layla":19786,"battled":19787,"formulated":19788,"stephenson":19789,"gravitational":19790,"##gill":19791,"##jun":19792,"1768":19793,"granny":19794,"coordinating":19795,"suites":19796,"##cd":19797,"##ioned":19798,"monarchs":19799,"##cote":19800,"##hips":19801,"sep":19802,"blended":19803,"apr":19804,"barrister":19805,"deposition":19806,"fia":19807,"mina":19808,"policemen":19809,"paranoid":19810,"##pressed":19811,"churchyard":19812,"covert":19813,"crumpled":19814,"creep":19815,"abandoning":19816,"tr":19817,"transmit":19818,"conceal":19819,"barr":19820,"understands":19821,"readiness":19822,"spire":19823,"##cology":19824,"##enia":19825,"##erry":19826,"610":19827,"startling":19828,"unlock":19829,"vida":19830,"bowled":19831,"slots":19832,"##nat":19833,"##islav":19834,"spaced":19835,"trusting":19836,"admire":19837,"rig":19838,"##ink":19839,"slack":19840,"##70":19841,"mv":19842,"207":19843,"casualty":19844,"##wei":19845,"classmates":19846,"##odes":19847,"##rar":19848,"##rked":19849,"amherst":19850,"furnished":19851,"evolve":19852,"foundry":19853,"menace":19854,"mead":19855,"##lein":19856,"flu":19857,"wesleyan":19858,"##kled":19859,"monterey":19860,"webber":19861,"##vos":19862,"wil":19863,"##mith":19864,"##на":19865,"bartholomew":19866,"justices":19867,"restrained":19868,"##cke":19869,"amenities":19870,"191":19871,"mediated":19872,"sewage":19873,"trenches":19874,"ml":19875,"mainz":19876,"##thus":19877,"1800s":19878,"##cula":19879,"##inski":19880,"caine":19881,"bonding":19882,"213":19883,"converts":19884,"spheres":19885,"superseded":19886,"marianne":19887,"crypt":19888,"sweaty":19889,"ensign":19890,"historia":19891,"##br":19892,"spruce":19893,"##post":19894,"##ask":19895,"forks":19896,"thoughtfully":19897,"yukon":19898,"pamphlet":19899,"ames":19900,"##uter":19901,"karma":19902,"##yya":19903,"bryn":19904,"negotiation":19905,"sighs":19906,"incapable":19907,"##mbre":19908,"##ntial":19909,"actresses":19910,"taft":19911,"##mill":19912,"luce":19913,"prevailed":19914,"##amine":19915,"1773":19916,"motionless":19917,"envoy":19918,"testify":19919,"investing":19920,"sculpted":19921,"instructors":19922,"provence":19923,"kali":19924,"cullen":19925,"horseback":19926,"##while":19927,"goodwin":19928,"##jos":19929,"gaa":19930,"norte":19931,"##ldon":19932,"modify":19933,"wavelength":19934,"abd":19935,"214":19936,"skinned":19937,"sprinter":19938,"forecast":19939,"scheduling":19940,"marries":19941,"squared":19942,"tentative":19943,"##chman":19944,"boer":19945,"##isch":19946,"bolts":19947,"swap":19948,"fisherman":19949,"assyrian":19950,"impatiently":19951,"guthrie":19952,"martins":19953,"murdoch":19954,"194":19955,"tanya":19956,"nicely":19957,"dolly":19958,"lacy":19959,"med":19960,"##45":19961,"syn":19962,"decks":19963,"fashionable":19964,"millionaire":19965,"##ust":19966,"surfing":19967,"##ml":19968,"##ision":19969,"heaved":19970,"tammy":19971,"consulate":19972,"attendees":19973,"routinely":19974,"197":19975,"fuse":19976,"saxophonist":19977,"backseat":19978,"malaya":19979,"##lord":19980,"scowl":19981,"tau":19982,"##ishly":19983,"193":19984,"sighted":19985,"steaming":19986,"##rks":19987,"303":19988,"911":19989,"##holes":19990,"##hong":19991,"ching":19992,"##wife":19993,"bless":19994,"conserved":19995,"jurassic":19996,"stacey":19997,"unix":19998,"zion":19999,"chunk":20000,"rigorous":20001,"blaine":20002,"198":20003,"peabody":20004,"slayer":20005,"dismay":20006,"brewers":20007,"nz":20008,"##jer":20009,"det":20010,"##glia":20011,"glover":20012,"postwar":20013,"int":20014,"penetration":20015,"sylvester":20016,"imitation":20017,"vertically":20018,"airlift":20019,"heiress":20020,"knoxville":20021,"viva":20022,"##uin":20023,"390":20024,"macon":20025,"##rim":20026,"##fighter":20027,"##gonal":20028,"janice":20029,"##orescence":20030,"##wari":20031,"marius":20032,"belongings":20033,"leicestershire":20034,"196":20035,"blanco":20036,"inverted":20037,"preseason":20038,"sanity":20039,"sobbing":20040,"##due":20041,"##elt":20042,"##dled":20043,"collingwood":20044,"regeneration":20045,"flickering":20046,"shortest":20047,"##mount":20048,"##osi":20049,"feminism":20050,"##lat":20051,"sherlock":20052,"cabinets":20053,"fumbled":20054,"northbound":20055,"precedent":20056,"snaps":20057,"##mme":20058,"researching":20059,"##akes":20060,"guillaume":20061,"insights":20062,"manipulated":20063,"vapor":20064,"neighbour":20065,"sap":20066,"gangster":20067,"frey":20068,"f1":20069,"stalking":20070,"scarcely":20071,"callie":20072,"barnett":20073,"tendencies":20074,"audi":20075,"doomed":20076,"assessing":20077,"slung":20078,"panchayat":20079,"ambiguous":20080,"bartlett":20081,"##etto":20082,"distributing":20083,"violating":20084,"wolverhampton":20085,"##hetic":20086,"swami":20087,"histoire":20088,"##urus":20089,"liable":20090,"pounder":20091,"groin":20092,"hussain":20093,"larsen":20094,"popping":20095,"surprises":20096,"##atter":20097,"vie":20098,"curt":20099,"##station":20100,"mute":20101,"relocate":20102,"musicals":20103,"authorization":20104,"richter":20105,"##sef":20106,"immortality":20107,"tna":20108,"bombings":20109,"##press":20110,"deteriorated":20111,"yiddish":20112,"##acious":20113,"robbed":20114,"colchester":20115,"cs":20116,"pmid":20117,"ao":20118,"verified":20119,"balancing":20120,"apostle":20121,"swayed":20122,"recognizable":20123,"oxfordshire":20124,"retention":20125,"nottinghamshire":20126,"contender":20127,"judd":20128,"invitational":20129,"shrimp":20130,"uhf":20131,"##icient":20132,"cleaner":20133,"longitudinal":20134,"tanker":20135,"##mur":20136,"acronym":20137,"broker":20138,"koppen":20139,"sundance":20140,"suppliers":20141,"##gil":20142,"4000":20143,"clipped":20144,"fuels":20145,"petite":20146,"##anne":20147,"landslide":20148,"helene":20149,"diversion":20150,"populous":20151,"landowners":20152,"auspices":20153,"melville":20154,"quantitative":20155,"##xes":20156,"ferries":20157,"nicky":20158,"##llus":20159,"doo":20160,"haunting":20161,"roche":20162,"carver":20163,"downed":20164,"unavailable":20165,"##pathy":20166,"approximation":20167,"hiroshima":20168,"##hue":20169,"garfield":20170,"valle":20171,"comparatively":20172,"keyboardist":20173,"traveler":20174,"##eit":20175,"congestion":20176,"calculating":20177,"subsidiaries":20178,"##bate":20179,"serb":20180,"modernization":20181,"fairies":20182,"deepened":20183,"ville":20184,"averages":20185,"##lore":20186,"inflammatory":20187,"tonga":20188,"##itch":20189,"co₂":20190,"squads":20191,"##hea":20192,"gigantic":20193,"serum":20194,"enjoyment":20195,"retailer":20196,"verona":20197,"35th":20198,"cis":20199,"##phobic":20200,"magna":20201,"technicians":20202,"##vati":20203,"arithmetic":20204,"##sport":20205,"levin":20206,"##dation":20207,"amtrak":20208,"chow":20209,"sienna":20210,"##eyer":20211,"backstage":20212,"entrepreneurship":20213,"##otic":20214,"learnt":20215,"tao":20216,"##udy":20217,"worcestershire":20218,"formulation":20219,"baggage":20220,"hesitant":20221,"bali":20222,"sabotage":20223,"##kari":20224,"barren":20225,"enhancing":20226,"murmur":20227,"pl":20228,"freshly":20229,"putnam":20230,"syntax":20231,"aces":20232,"medicines":20233,"resentment":20234,"bandwidth":20235,"##sier":20236,"grins":20237,"chili":20238,"guido":20239,"##sei":20240,"framing":20241,"implying":20242,"gareth":20243,"lissa":20244,"genevieve":20245,"pertaining":20246,"admissions":20247,"geo":20248,"thorpe":20249,"proliferation":20250,"sato":20251,"bela":20252,"analyzing":20253,"parting":20254,"##gor":20255,"awakened":20256,"##isman":20257,"huddled":20258,"secrecy":20259,"##kling":20260,"hush":20261,"gentry":20262,"540":20263,"dungeons":20264,"##ego":20265,"coasts":20266,"##utz":20267,"sacrificed":20268,"##chule":20269,"landowner":20270,"mutually":20271,"prevalence":20272,"programmer":20273,"adolescent":20274,"disrupted":20275,"seaside":20276,"gee":20277,"trusts":20278,"vamp":20279,"georgie":20280,"##nesian":20281,"##iol":20282,"schedules":20283,"sindh":20284,"##market":20285,"etched":20286,"hm":20287,"sparse":20288,"bey":20289,"beaux":20290,"scratching":20291,"gliding":20292,"unidentified":20293,"216":20294,"collaborating":20295,"gems":20296,"jesuits":20297,"oro":20298,"accumulation":20299,"shaping":20300,"mbe":20301,"anal":20302,"##xin":20303,"231":20304,"enthusiasts":20305,"newscast":20306,"##egan":20307,"janata":20308,"dewey":20309,"parkinson":20310,"179":20311,"ankara":20312,"biennial":20313,"towering":20314,"dd":20315,"inconsistent":20316,"950":20317,"##chet":20318,"thriving":20319,"terminate":20320,"cabins":20321,"furiously":20322,"eats":20323,"advocating":20324,"donkey":20325,"marley":20326,"muster":20327,"phyllis":20328,"leiden":20329,"##user":20330,"grassland":20331,"glittering":20332,"iucn":20333,"loneliness":20334,"217":20335,"memorandum":20336,"armenians":20337,"##ddle":20338,"popularized":20339,"rhodesia":20340,"60s":20341,"lame":20342,"##illon":20343,"sans":20344,"bikini":20345,"header":20346,"orbits":20347,"##xx":20348,"##finger":20349,"##ulator":20350,"sharif":20351,"spines":20352,"biotechnology":20353,"strolled":20354,"naughty":20355,"yates":20356,"##wire":20357,"fremantle":20358,"milo":20359,"##mour":20360,"abducted":20361,"removes":20362,"##atin":20363,"humming":20364,"wonderland":20365,"##chrome":20366,"##ester":20367,"hume":20368,"pivotal":20369,"##rates":20370,"armand":20371,"grams":20372,"believers":20373,"elector":20374,"rte":20375,"apron":20376,"bis":20377,"scraped":20378,"##yria":20379,"endorsement":20380,"initials":20381,"##llation":20382,"eps":20383,"dotted":20384,"hints":20385,"buzzing":20386,"emigration":20387,"nearer":20388,"##tom":20389,"indicators":20390,"##ulu":20391,"coarse":20392,"neutron":20393,"protectorate":20394,"##uze":20395,"directional":20396,"exploits":20397,"pains":20398,"loire":20399,"1830s":20400,"proponents":20401,"guggenheim":20402,"rabbits":20403,"ritchie":20404,"305":20405,"hectare":20406,"inputs":20407,"hutton":20408,"##raz":20409,"verify":20410,"##ako":20411,"boilers":20412,"longitude":20413,"##lev":20414,"skeletal":20415,"yer":20416,"emilia":20417,"citrus":20418,"compromised":20419,"##gau":20420,"pokemon":20421,"prescription":20422,"paragraph":20423,"eduard":20424,"cadillac":20425,"attire":20426,"categorized":20427,"kenyan":20428,"weddings":20429,"charley":20430,"##bourg":20431,"entertain":20432,"monmouth":20433,"##lles":20434,"nutrients":20435,"davey":20436,"mesh":20437,"incentive":20438,"practised":20439,"ecosystems":20440,"kemp":20441,"subdued":20442,"overheard":20443,"##rya":20444,"bodily":20445,"maxim":20446,"##nius":20447,"apprenticeship":20448,"ursula":20449,"##fight":20450,"lodged":20451,"rug":20452,"silesian":20453,"unconstitutional":20454,"patel":20455,"inspected":20456,"coyote":20457,"unbeaten":20458,"##hak":20459,"34th":20460,"disruption":20461,"convict":20462,"parcel":20463,"##cl":20464,"##nham":20465,"collier":20466,"implicated":20467,"mallory":20468,"##iac":20469,"##lab":20470,"susannah":20471,"winkler":20472,"##rber":20473,"shia":20474,"phelps":20475,"sediments":20476,"graphical":20477,"robotic":20478,"##sner":20479,"adulthood":20480,"mart":20481,"smoked":20482,"##isto":20483,"kathryn":20484,"clarified":20485,"##aran":20486,"divides":20487,"convictions":20488,"oppression":20489,"pausing":20490,"burying":20491,"##mt":20492,"federico":20493,"mathias":20494,"eileen":20495,"##tana":20496,"kite":20497,"hunched":20498,"##acies":20499,"189":20500,"##atz":20501,"disadvantage":20502,"liza":20503,"kinetic":20504,"greedy":20505,"paradox":20506,"yokohama":20507,"dowager":20508,"trunks":20509,"ventured":20510,"##gement":20511,"gupta":20512,"vilnius":20513,"olaf":20514,"##thest":20515,"crimean":20516,"hopper":20517,"##ej":20518,"progressively":20519,"arturo":20520,"mouthed":20521,"arrondissement":20522,"##fusion":20523,"rubin":20524,"simulcast":20525,"oceania":20526,"##orum":20527,"##stra":20528,"##rred":20529,"busiest":20530,"intensely":20531,"navigator":20532,"cary":20533,"##vine":20534,"##hini":20535,"##bies":20536,"fife":20537,"rowe":20538,"rowland":20539,"posing":20540,"insurgents":20541,"shafts":20542,"lawsuits":20543,"activate":20544,"conor":20545,"inward":20546,"culturally":20547,"garlic":20548,"265":20549,"##eering":20550,"eclectic":20551,"##hui":20552,"##kee":20553,"##nl":20554,"furrowed":20555,"vargas":20556,"meteorological":20557,"rendezvous":20558,"##aus":20559,"culinary":20560,"commencement":20561,"##dition":20562,"quota":20563,"##notes":20564,"mommy":20565,"salaries":20566,"overlapping":20567,"mule":20568,"##iology":20569,"##mology":20570,"sums":20571,"wentworth":20572,"##isk":20573,"##zione":20574,"mainline":20575,"subgroup":20576,"##illy":20577,"hack":20578,"plaintiff":20579,"verdi":20580,"bulb":20581,"differentiation":20582,"engagements":20583,"multinational":20584,"supplemented":20585,"bertrand":20586,"caller":20587,"regis":20588,"##naire":20589,"##sler":20590,"##arts":20591,"##imated":20592,"blossom":20593,"propagation":20594,"kilometer":20595,"viaduct":20596,"vineyards":20597,"##uate":20598,"beckett":20599,"optimization":20600,"golfer":20601,"songwriters":20602,"seminal":20603,"semitic":20604,"thud":20605,"volatile":20606,"evolving":20607,"ridley":20608,"##wley":20609,"trivial":20610,"distributions":20611,"scandinavia":20612,"jiang":20613,"##ject":20614,"wrestled":20615,"insistence":20616,"##dio":20617,"emphasizes":20618,"napkin":20619,"##ods":20620,"adjunct":20621,"rhyme":20622,"##ricted":20623,"##eti":20624,"hopeless":20625,"surrounds":20626,"tremble":20627,"32nd":20628,"smoky":20629,"##ntly":20630,"oils":20631,"medicinal":20632,"padded":20633,"steer":20634,"wilkes":20635,"219":20636,"255":20637,"concessions":20638,"hue":20639,"uniquely":20640,"blinded":20641,"landon":20642,"yahoo":20643,"##lane":20644,"hendrix":20645,"commemorating":20646,"dex":20647,"specify":20648,"chicks":20649,"##ggio":20650,"intercity":20651,"1400":20652,"morley":20653,"##torm":20654,"highlighting":20655,"##oting":20656,"pang":20657,"oblique":20658,"stalled":20659,"##liner":20660,"flirting":20661,"newborn":20662,"1769":20663,"bishopric":20664,"shaved":20665,"232":20666,"currie":20667,"##ush":20668,"dharma":20669,"spartan":20670,"##ooped":20671,"favorites":20672,"smug":20673,"novella":20674,"sirens":20675,"abusive":20676,"creations":20677,"espana":20678,"##lage":20679,"paradigm":20680,"semiconductor":20681,"sheen":20682,"##rdo":20683,"##yen":20684,"##zak":20685,"nrl":20686,"renew":20687,"##pose":20688,"##tur":20689,"adjutant":20690,"marches":20691,"norma":20692,"##enity":20693,"ineffective":20694,"weimar":20695,"grunt":20696,"##gat":20697,"lordship":20698,"plotting":20699,"expenditure":20700,"infringement":20701,"lbs":20702,"refrain":20703,"av":20704,"mimi":20705,"mistakenly":20706,"postmaster":20707,"1771":20708,"##bara":20709,"ras":20710,"motorsports":20711,"tito":20712,"199":20713,"subjective":20714,"##zza":20715,"bully":20716,"stew":20717,"##kaya":20718,"prescott":20719,"1a":20720,"##raphic":20721,"##zam":20722,"bids":20723,"styling":20724,"paranormal":20725,"reeve":20726,"sneaking":20727,"exploding":20728,"katz":20729,"akbar":20730,"migrant":20731,"syllables":20732,"indefinitely":20733,"##ogical":20734,"destroys":20735,"replaces":20736,"applause":20737,"##phine":20738,"pest":20739,"##fide":20740,"218":20741,"articulated":20742,"bertie":20743,"##thing":20744,"##cars":20745,"##ptic":20746,"courtroom":20747,"crowley":20748,"aesthetics":20749,"cummings":20750,"tehsil":20751,"hormones":20752,"titanic":20753,"dangerously":20754,"##ibe":20755,"stadion":20756,"jaenelle":20757,"auguste":20758,"ciudad":20759,"##chu":20760,"mysore":20761,"partisans":20762,"##sio":20763,"lucan":20764,"philipp":20765,"##aly":20766,"debating":20767,"henley":20768,"interiors":20769,"##rano":20770,"##tious":20771,"homecoming":20772,"beyonce":20773,"usher":20774,"henrietta":20775,"prepares":20776,"weeds":20777,"##oman":20778,"ely":20779,"plucked":20780,"##pire":20781,"##dable":20782,"luxurious":20783,"##aq":20784,"artifact":20785,"password":20786,"pasture":20787,"juno":20788,"maddy":20789,"minsk":20790,"##dder":20791,"##ologies":20792,"##rone":20793,"assessments":20794,"martian":20795,"royalist":20796,"1765":20797,"examines":20798,"##mani":20799,"##rge":20800,"nino":20801,"223":20802,"parry":20803,"scooped":20804,"relativity":20805,"##eli":20806,"##uting":20807,"##cao":20808,"congregational":20809,"noisy":20810,"traverse":20811,"##agawa":20812,"strikeouts":20813,"nickelodeon":20814,"obituary":20815,"transylvania":20816,"binds":20817,"depictions":20818,"polk":20819,"trolley":20820,"##yed":20821,"##lard":20822,"breeders":20823,"##under":20824,"dryly":20825,"hokkaido":20826,"1762":20827,"strengths":20828,"stacks":20829,"bonaparte":20830,"connectivity":20831,"neared":20832,"prostitutes":20833,"stamped":20834,"anaheim":20835,"gutierrez":20836,"sinai":20837,"##zzling":20838,"bram":20839,"fresno":20840,"madhya":20841,"##86":20842,"proton":20843,"##lena":20844,"##llum":20845,"##phon":20846,"reelected":20847,"wanda":20848,"##anus":20849,"##lb":20850,"ample":20851,"distinguishing":20852,"##yler":20853,"grasping":20854,"sermons":20855,"tomato":20856,"bland":20857,"stimulation":20858,"avenues":20859,"##eux":20860,"spreads":20861,"scarlett":20862,"fern":20863,"pentagon":20864,"assert":20865,"baird":20866,"chesapeake":20867,"ir":20868,"calmed":20869,"distortion":20870,"fatalities":20871,"##olis":20872,"correctional":20873,"pricing":20874,"##astic":20875,"##gina":20876,"prom":20877,"dammit":20878,"ying":20879,"collaborate":20880,"##chia":20881,"welterweight":20882,"33rd":20883,"pointer":20884,"substitution":20885,"bonded":20886,"umpire":20887,"communicating":20888,"multitude":20889,"paddle":20890,"##obe":20891,"federally":20892,"intimacy":20893,"##insky":20894,"betray":20895,"ssr":20896,"##lett":20897,"##lean":20898,"##lves":20899,"##therapy":20900,"airbus":20901,"##tery":20902,"functioned":20903,"ud":20904,"bearer":20905,"biomedical":20906,"netflix":20907,"##hire":20908,"##nca":20909,"condom":20910,"brink":20911,"ik":20912,"##nical":20913,"macy":20914,"##bet":20915,"flap":20916,"gma":20917,"experimented":20918,"jelly":20919,"lavender":20920,"##icles":20921,"##ulia":20922,"munro":20923,"##mian":20924,"##tial":20925,"rye":20926,"##rle":20927,"60th":20928,"gigs":20929,"hottest":20930,"rotated":20931,"predictions":20932,"fuji":20933,"bu":20934,"##erence":20935,"##omi":20936,"barangay":20937,"##fulness":20938,"##sas":20939,"clocks":20940,"##rwood":20941,"##liness":20942,"cereal":20943,"roe":20944,"wight":20945,"decker":20946,"uttered":20947,"babu":20948,"onion":20949,"xml":20950,"forcibly":20951,"##df":20952,"petra":20953,"sarcasm":20954,"hartley":20955,"peeled":20956,"storytelling":20957,"##42":20958,"##xley":20959,"##ysis":20960,"##ffa":20961,"fibre":20962,"kiel":20963,"auditor":20964,"fig":20965,"harald":20966,"greenville":20967,"##berries":20968,"geographically":20969,"nell":20970,"quartz":20971,"##athic":20972,"cemeteries":20973,"##lr":20974,"crossings":20975,"nah":20976,"holloway":20977,"reptiles":20978,"chun":20979,"sichuan":20980,"snowy":20981,"660":20982,"corrections":20983,"##ivo":20984,"zheng":20985,"ambassadors":20986,"blacksmith":20987,"fielded":20988,"fluids":20989,"hardcover":20990,"turnover":20991,"medications":20992,"melvin":20993,"academies":20994,"##erton":20995,"ro":20996,"roach":20997,"absorbing":20998,"spaniards":20999,"colton":21000,"##founded":21001,"outsider":21002,"espionage":21003,"kelsey":21004,"245":21005,"edible":21006,"##ulf":21007,"dora":21008,"establishes":21009,"##sham":21010,"##tries":21011,"contracting":21012,"##tania":21013,"cinematic":21014,"costello":21015,"nesting":21016,"##uron":21017,"connolly":21018,"duff":21019,"##nology":21020,"mma":21021,"##mata":21022,"fergus":21023,"sexes":21024,"gi":21025,"optics":21026,"spectator":21027,"woodstock":21028,"banning":21029,"##hee":21030,"##fle":21031,"differentiate":21032,"outfielder":21033,"refinery":21034,"226":21035,"312":21036,"gerhard":21037,"horde":21038,"lair":21039,"drastically":21040,"##udi":21041,"landfall":21042,"##cheng":21043,"motorsport":21044,"odi":21045,"##achi":21046,"predominant":21047,"quay":21048,"skins":21049,"##ental":21050,"edna":21051,"harshly":21052,"complementary":21053,"murdering":21054,"##aves":21055,"wreckage":21056,"##90":21057,"ono":21058,"outstretched":21059,"lennox":21060,"munitions":21061,"galen":21062,"reconcile":21063,"470":21064,"scalp":21065,"bicycles":21066,"gillespie":21067,"questionable":21068,"rosenberg":21069,"guillermo":21070,"hostel":21071,"jarvis":21072,"kabul":21073,"volvo":21074,"opium":21075,"yd":21076,"##twined":21077,"abuses":21078,"decca":21079,"outpost":21080,"##cino":21081,"sensible":21082,"neutrality":21083,"##64":21084,"ponce":21085,"anchorage":21086,"atkins":21087,"turrets":21088,"inadvertently":21089,"disagree":21090,"libre":21091,"vodka":21092,"reassuring":21093,"weighs":21094,"##yal":21095,"glide":21096,"jumper":21097,"ceilings":21098,"repertory":21099,"outs":21100,"stain":21101,"##bial":21102,"envy":21103,"##ucible":21104,"smashing":21105,"heightened":21106,"policing":21107,"hyun":21108,"mixes":21109,"lai":21110,"prima":21111,"##ples":21112,"celeste":21113,"##bina":21114,"lucrative":21115,"intervened":21116,"kc":21117,"manually":21118,"##rned":21119,"stature":21120,"staffed":21121,"bun":21122,"bastards":21123,"nairobi":21124,"priced":21125,"##auer":21126,"thatcher":21127,"##kia":21128,"tripped":21129,"comune":21130,"##ogan":21131,"##pled":21132,"brasil":21133,"incentives":21134,"emanuel":21135,"hereford":21136,"musica":21137,"##kim":21138,"benedictine":21139,"biennale":21140,"##lani":21141,"eureka":21142,"gardiner":21143,"rb":21144,"knocks":21145,"sha":21146,"##ael":21147,"##elled":21148,"##onate":21149,"efficacy":21150,"ventura":21151,"masonic":21152,"sanford":21153,"maize":21154,"leverage":21155,"##feit":21156,"capacities":21157,"santana":21158,"##aur":21159,"novelty":21160,"vanilla":21161,"##cter":21162,"##tour":21163,"benin":21164,"##oir":21165,"##rain":21166,"neptune":21167,"drafting":21168,"tallinn":21169,"##cable":21170,"humiliation":21171,"##boarding":21172,"schleswig":21173,"fabian":21174,"bernardo":21175,"liturgy":21176,"spectacle":21177,"sweeney":21178,"pont":21179,"routledge":21180,"##tment":21181,"cosmos":21182,"ut":21183,"hilt":21184,"sleek":21185,"universally":21186,"##eville":21187,"##gawa":21188,"typed":21189,"##dry":21190,"favors":21191,"allegheny":21192,"glaciers":21193,"##rly":21194,"recalling":21195,"aziz":21196,"##log":21197,"parasite":21198,"requiem":21199,"auf":21200,"##berto":21201,"##llin":21202,"illumination":21203,"##breaker":21204,"##issa":21205,"festivities":21206,"bows":21207,"govern":21208,"vibe":21209,"vp":21210,"333":21211,"sprawled":21212,"larson":21213,"pilgrim":21214,"bwf":21215,"leaping":21216,"##rts":21217,"##ssel":21218,"alexei":21219,"greyhound":21220,"hoarse":21221,"##dler":21222,"##oration":21223,"seneca":21224,"##cule":21225,"gaping":21226,"##ulously":21227,"##pura":21228,"cinnamon":21229,"##gens":21230,"##rricular":21231,"craven":21232,"fantasies":21233,"houghton":21234,"engined":21235,"reigned":21236,"dictator":21237,"supervising":21238,"##oris":21239,"bogota":21240,"commentaries":21241,"unnatural":21242,"fingernails":21243,"spirituality":21244,"tighten":21245,"##tm":21246,"canadiens":21247,"protesting":21248,"intentional":21249,"cheers":21250,"sparta":21251,"##ytic":21252,"##iere":21253,"##zine":21254,"widen":21255,"belgarath":21256,"controllers":21257,"dodd":21258,"iaaf":21259,"navarre":21260,"##ication":21261,"defect":21262,"squire":21263,"steiner":21264,"whisky":21265,"##mins":21266,"560":21267,"inevitably":21268,"tome":21269,"##gold":21270,"chew":21271,"##uid":21272,"##lid":21273,"elastic":21274,"##aby":21275,"streaked":21276,"alliances":21277,"jailed":21278,"regal":21279,"##ined":21280,"##phy":21281,"czechoslovak":21282,"narration":21283,"absently":21284,"##uld":21285,"bluegrass":21286,"guangdong":21287,"quran":21288,"criticizing":21289,"hose":21290,"hari":21291,"##liest":21292,"##owa":21293,"skier":21294,"streaks":21295,"deploy":21296,"##lom":21297,"raft":21298,"bose":21299,"dialed":21300,"huff":21301,"##eira":21302,"haifa":21303,"simplest":21304,"bursting":21305,"endings":21306,"ib":21307,"sultanate":21308,"##titled":21309,"franks":21310,"whitman":21311,"ensures":21312,"sven":21313,"##ggs":21314,"collaborators":21315,"forster":21316,"organising":21317,"ui":21318,"banished":21319,"napier":21320,"injustice":21321,"teller":21322,"layered":21323,"thump":21324,"##otti":21325,"roc":21326,"battleships":21327,"evidenced":21328,"fugitive":21329,"sadie":21330,"robotics":21331,"##roud":21332,"equatorial":21333,"geologist":21334,"##iza":21335,"yielding":21336,"##bron":21337,"##sr":21338,"internationale":21339,"mecca":21340,"##diment":21341,"sbs":21342,"skyline":21343,"toad":21344,"uploaded":21345,"reflective":21346,"undrafted":21347,"lal":21348,"leafs":21349,"bayern":21350,"##dai":21351,"lakshmi":21352,"shortlisted":21353,"##stick":21354,"##wicz":21355,"camouflage":21356,"donate":21357,"af":21358,"christi":21359,"lau":21360,"##acio":21361,"disclosed":21362,"nemesis":21363,"1761":21364,"assemble":21365,"straining":21366,"northamptonshire":21367,"tal":21368,"##asi":21369,"bernardino":21370,"premature":21371,"heidi":21372,"42nd":21373,"coefficients":21374,"galactic":21375,"reproduce":21376,"buzzed":21377,"sensations":21378,"zionist":21379,"monsieur":21380,"myrtle":21381,"##eme":21382,"archery":21383,"strangled":21384,"musically":21385,"viewpoint":21386,"antiquities":21387,"bei":21388,"trailers":21389,"seahawks":21390,"cured":21391,"pee":21392,"preferring":21393,"tasmanian":21394,"lange":21395,"sul":21396,"##mail":21397,"##working":21398,"colder":21399,"overland":21400,"lucivar":21401,"massey":21402,"gatherings":21403,"haitian":21404,"##smith":21405,"disapproval":21406,"flaws":21407,"##cco":21408,"##enbach":21409,"1766":21410,"npr":21411,"##icular":21412,"boroughs":21413,"creole":21414,"forums":21415,"techno":21416,"1755":21417,"dent":21418,"abdominal":21419,"streetcar":21420,"##eson":21421,"##stream":21422,"procurement":21423,"gemini":21424,"predictable":21425,"##tya":21426,"acheron":21427,"christoph":21428,"feeder":21429,"fronts":21430,"vendor":21431,"bernhard":21432,"jammu":21433,"tumors":21434,"slang":21435,"##uber":21436,"goaltender":21437,"twists":21438,"curving":21439,"manson":21440,"vuelta":21441,"mer":21442,"peanut":21443,"confessions":21444,"pouch":21445,"unpredictable":21446,"allowance":21447,"theodor":21448,"vascular":21449,"##factory":21450,"bala":21451,"authenticity":21452,"metabolic":21453,"coughing":21454,"nanjing":21455,"##cea":21456,"pembroke":21457,"##bard":21458,"splendid":21459,"36th":21460,"ff":21461,"hourly":21462,"##ahu":21463,"elmer":21464,"handel":21465,"##ivate":21466,"awarding":21467,"thrusting":21468,"dl":21469,"experimentation":21470,"##hesion":21471,"##46":21472,"caressed":21473,"entertained":21474,"steak":21475,"##rangle":21476,"biologist":21477,"orphans":21478,"baroness":21479,"oyster":21480,"stepfather":21481,"##dridge":21482,"mirage":21483,"reefs":21484,"speeding":21485,"##31":21486,"barons":21487,"1764":21488,"227":21489,"inhabit":21490,"preached":21491,"repealed":21492,"##tral":21493,"honoring":21494,"boogie":21495,"captives":21496,"administer":21497,"johanna":21498,"##imate":21499,"gel":21500,"suspiciously":21501,"1767":21502,"sobs":21503,"##dington":21504,"backbone":21505,"hayward":21506,"garry":21507,"##folding":21508,"##nesia":21509,"maxi":21510,"##oof":21511,"##ppe":21512,"ellison":21513,"galileo":21514,"##stand":21515,"crimea":21516,"frenzy":21517,"amour":21518,"bumper":21519,"matrices":21520,"natalia":21521,"baking":21522,"garth":21523,"palestinians":21524,"##grove":21525,"smack":21526,"conveyed":21527,"ensembles":21528,"gardening":21529,"##manship":21530,"##rup":21531,"##stituting":21532,"1640":21533,"harvesting":21534,"topography":21535,"jing":21536,"shifters":21537,"dormitory":21538,"##carriage":21539,"##lston":21540,"ist":21541,"skulls":21542,"##stadt":21543,"dolores":21544,"jewellery":21545,"sarawak":21546,"##wai":21547,"##zier":21548,"fences":21549,"christy":21550,"confinement":21551,"tumbling":21552,"credibility":21553,"fir":21554,"stench":21555,"##bria":21556,"##plication":21557,"##nged":21558,"##sam":21559,"virtues":21560,"##belt":21561,"marjorie":21562,"pba":21563,"##eem":21564,"##made":21565,"celebrates":21566,"schooner":21567,"agitated":21568,"barley":21569,"fulfilling":21570,"anthropologist":21571,"##pro":21572,"restrict":21573,"novi":21574,"regulating":21575,"##nent":21576,"padres":21577,"##rani":21578,"##hesive":21579,"loyola":21580,"tabitha":21581,"milky":21582,"olson":21583,"proprietor":21584,"crambidae":21585,"guarantees":21586,"intercollegiate":21587,"ljubljana":21588,"hilda":21589,"##sko":21590,"ignorant":21591,"hooded":21592,"##lts":21593,"sardinia":21594,"##lidae":21595,"##vation":21596,"frontman":21597,"privileged":21598,"witchcraft":21599,"##gp":21600,"jammed":21601,"laude":21602,"poking":21603,"##than":21604,"bracket":21605,"amazement":21606,"yunnan":21607,"##erus":21608,"maharaja":21609,"linnaeus":21610,"264":21611,"commissioning":21612,"milano":21613,"peacefully":21614,"##logies":21615,"akira":21616,"rani":21617,"regulator":21618,"##36":21619,"grasses":21620,"##rance":21621,"luzon":21622,"crows":21623,"compiler":21624,"gretchen":21625,"seaman":21626,"edouard":21627,"tab":21628,"buccaneers":21629,"ellington":21630,"hamlets":21631,"whig":21632,"socialists":21633,"##anto":21634,"directorial":21635,"easton":21636,"mythological":21637,"##kr":21638,"##vary":21639,"rhineland":21640,"semantic":21641,"taut":21642,"dune":21643,"inventions":21644,"succeeds":21645,"##iter":21646,"replication":21647,"branched":21648,"##pired":21649,"jul":21650,"prosecuted":21651,"kangaroo":21652,"penetrated":21653,"##avian":21654,"middlesbrough":21655,"doses":21656,"bleak":21657,"madam":21658,"predatory":21659,"relentless":21660,"##vili":21661,"reluctance":21662,"##vir":21663,"hailey":21664,"crore":21665,"silvery":21666,"1759":21667,"monstrous":21668,"swimmers":21669,"transmissions":21670,"hawthorn":21671,"informing":21672,"##eral":21673,"toilets":21674,"caracas":21675,"crouch":21676,"kb":21677,"##sett":21678,"295":21679,"cartel":21680,"hadley":21681,"##aling":21682,"alexia":21683,"yvonne":21684,"##biology":21685,"cinderella":21686,"eton":21687,"superb":21688,"blizzard":21689,"stabbing":21690,"industrialist":21691,"maximus":21692,"##gm":21693,"##orus":21694,"groves":21695,"maud":21696,"clade":21697,"oversized":21698,"comedic":21699,"##bella":21700,"rosen":21701,"nomadic":21702,"fulham":21703,"montane":21704,"beverages":21705,"galaxies":21706,"redundant":21707,"swarm":21708,"##rot":21709,"##folia":21710,"##llis":21711,"buckinghamshire":21712,"fen":21713,"bearings":21714,"bahadur":21715,"##rom":21716,"gilles":21717,"phased":21718,"dynamite":21719,"faber":21720,"benoit":21721,"vip":21722,"##ount":21723,"##wd":21724,"booking":21725,"fractured":21726,"tailored":21727,"anya":21728,"spices":21729,"westwood":21730,"cairns":21731,"auditions":21732,"inflammation":21733,"steamed":21734,"##rocity":21735,"##acion":21736,"##urne":21737,"skyla":21738,"thereof":21739,"watford":21740,"torment":21741,"archdeacon":21742,"transforms":21743,"lulu":21744,"demeanor":21745,"fucked":21746,"serge":21747,"##sor":21748,"mckenna":21749,"minas":21750,"entertainer":21751,"##icide":21752,"caress":21753,"originate":21754,"residue":21755,"##sty":21756,"1740":21757,"##ilised":21758,"##org":21759,"beech":21760,"##wana":21761,"subsidies":21762,"##ghton":21763,"emptied":21764,"gladstone":21765,"ru":21766,"firefighters":21767,"voodoo":21768,"##rcle":21769,"het":21770,"nightingale":21771,"tamara":21772,"edmond":21773,"ingredient":21774,"weaknesses":21775,"silhouette":21776,"285":21777,"compatibility":21778,"withdrawing":21779,"hampson":21780,"##mona":21781,"anguish":21782,"giggling":21783,"##mber":21784,"bookstore":21785,"##jiang":21786,"southernmost":21787,"tilting":21788,"##vance":21789,"bai":21790,"economical":21791,"rf":21792,"briefcase":21793,"dreadful":21794,"hinted":21795,"projections":21796,"shattering":21797,"totaling":21798,"##rogate":21799,"analogue":21800,"indicted":21801,"periodical":21802,"fullback":21803,"##dman":21804,"haynes":21805,"##tenberg":21806,"##ffs":21807,"##ishment":21808,"1745":21809,"thirst":21810,"stumble":21811,"penang":21812,"vigorous":21813,"##ddling":21814,"##kor":21815,"##lium":21816,"octave":21817,"##ove":21818,"##enstein":21819,"##inen":21820,"##ones":21821,"siberian":21822,"##uti":21823,"cbn":21824,"repeal":21825,"swaying":21826,"##vington":21827,"khalid":21828,"tanaka":21829,"unicorn":21830,"otago":21831,"plastered":21832,"lobe":21833,"riddle":21834,"##rella":21835,"perch":21836,"##ishing":21837,"croydon":21838,"filtered":21839,"graeme":21840,"tripoli":21841,"##ossa":21842,"crocodile":21843,"##chers":21844,"sufi":21845,"mined":21846,"##tung":21847,"inferno":21848,"lsu":21849,"##phi":21850,"swelled":21851,"utilizes":21852,"£2":21853,"cale":21854,"periodicals":21855,"styx":21856,"hike":21857,"informally":21858,"coop":21859,"lund":21860,"##tidae":21861,"ala":21862,"hen":21863,"qui":21864,"transformations":21865,"disposed":21866,"sheath":21867,"chickens":21868,"##cade":21869,"fitzroy":21870,"sas":21871,"silesia":21872,"unacceptable":21873,"odisha":21874,"1650":21875,"sabrina":21876,"pe":21877,"spokane":21878,"ratios":21879,"athena":21880,"massage":21881,"shen":21882,"dilemma":21883,"##drum":21884,"##riz":21885,"##hul":21886,"corona":21887,"doubtful":21888,"niall":21889,"##pha":21890,"##bino":21891,"fines":21892,"cite":21893,"acknowledging":21894,"bangor":21895,"ballard":21896,"bathurst":21897,"##resh":21898,"huron":21899,"mustered":21900,"alzheimer":21901,"garments":21902,"kinase":21903,"tyre":21904,"warship":21905,"##cp":21906,"flashback":21907,"pulmonary":21908,"braun":21909,"cheat":21910,"kamal":21911,"cyclists":21912,"constructions":21913,"grenades":21914,"ndp":21915,"traveller":21916,"excuses":21917,"stomped":21918,"signalling":21919,"trimmed":21920,"futsal":21921,"mosques":21922,"relevance":21923,"##wine":21924,"wta":21925,"##23":21926,"##vah":21927,"##lter":21928,"hoc":21929,"##riding":21930,"optimistic":21931,"##´s":21932,"deco":21933,"sim":21934,"interacting":21935,"rejecting":21936,"moniker":21937,"waterways":21938,"##ieri":21939,"##oku":21940,"mayors":21941,"gdansk":21942,"outnumbered":21943,"pearls":21944,"##ended":21945,"##hampton":21946,"fairs":21947,"totals":21948,"dominating":21949,"262":21950,"notions":21951,"stairway":21952,"compiling":21953,"pursed":21954,"commodities":21955,"grease":21956,"yeast":21957,"##jong":21958,"carthage":21959,"griffiths":21960,"residual":21961,"amc":21962,"contraction":21963,"laird":21964,"sapphire":21965,"##marine":21966,"##ivated":21967,"amalgamation":21968,"dissolve":21969,"inclination":21970,"lyle":21971,"packaged":21972,"altitudes":21973,"suez":21974,"canons":21975,"graded":21976,"lurched":21977,"narrowing":21978,"boasts":21979,"guise":21980,"wed":21981,"enrico":21982,"##ovsky":21983,"rower":21984,"scarred":21985,"bree":21986,"cub":21987,"iberian":21988,"protagonists":21989,"bargaining":21990,"proposing":21991,"trainers":21992,"voyages":21993,"vans":21994,"fishes":21995,"##aea":21996,"##ivist":21997,"##verance":21998,"encryption":21999,"artworks":22000,"kazan":22001,"sabre":22002,"cleopatra":22003,"hepburn":22004,"rotting":22005,"supremacy":22006,"mecklenburg":22007,"##brate":22008,"burrows":22009,"hazards":22010,"outgoing":22011,"flair":22012,"organizes":22013,"##ctions":22014,"scorpion":22015,"##usions":22016,"boo":22017,"234":22018,"chevalier":22019,"dunedin":22020,"slapping":22021,"##34":22022,"ineligible":22023,"pensions":22024,"##38":22025,"##omic":22026,"manufactures":22027,"emails":22028,"bismarck":22029,"238":22030,"weakening":22031,"blackish":22032,"ding":22033,"mcgee":22034,"quo":22035,"##rling":22036,"northernmost":22037,"xx":22038,"manpower":22039,"greed":22040,"sampson":22041,"clicking":22042,"##ange":22043,"##horpe":22044,"##inations":22045,"##roving":22046,"torre":22047,"##eptive":22048,"##moral":22049,"symbolism":22050,"38th":22051,"asshole":22052,"meritorious":22053,"outfits":22054,"splashed":22055,"biographies":22056,"sprung":22057,"astros":22058,"##tale":22059,"302":22060,"737":22061,"filly":22062,"raoul":22063,"nw":22064,"tokugawa":22065,"linden":22066,"clubhouse":22067,"##apa":22068,"tracts":22069,"romano":22070,"##pio":22071,"putin":22072,"tags":22073,"##note":22074,"chained":22075,"dickson":22076,"gunshot":22077,"moe":22078,"gunn":22079,"rashid":22080,"##tails":22081,"zipper":22082,"##bas":22083,"##nea":22084,"contrasted":22085,"##ply":22086,"##udes":22087,"plum":22088,"pharaoh":22089,"##pile":22090,"aw":22091,"comedies":22092,"ingrid":22093,"sandwiches":22094,"subdivisions":22095,"1100":22096,"mariana":22097,"nokia":22098,"kamen":22099,"hz":22100,"delaney":22101,"veto":22102,"herring":22103,"##words":22104,"possessive":22105,"outlines":22106,"##roup":22107,"siemens":22108,"stairwell":22109,"rc":22110,"gallantry":22111,"messiah":22112,"palais":22113,"yells":22114,"233":22115,"zeppelin":22116,"##dm":22117,"bolivar":22118,"##cede":22119,"smackdown":22120,"mckinley":22121,"##mora":22122,"##yt":22123,"muted":22124,"geologic":22125,"finely":22126,"unitary":22127,"avatar":22128,"hamas":22129,"maynard":22130,"rees":22131,"bog":22132,"contrasting":22133,"##rut":22134,"liv":22135,"chico":22136,"disposition":22137,"pixel":22138,"##erate":22139,"becca":22140,"dmitry":22141,"yeshiva":22142,"narratives":22143,"##lva":22144,"##ulton":22145,"mercenary":22146,"sharpe":22147,"tempered":22148,"navigate":22149,"stealth":22150,"amassed":22151,"keynes":22152,"##lini":22153,"untouched":22154,"##rrie":22155,"havoc":22156,"lithium":22157,"##fighting":22158,"abyss":22159,"graf":22160,"southward":22161,"wolverine":22162,"balloons":22163,"implements":22164,"ngos":22165,"transitions":22166,"##icum":22167,"ambushed":22168,"concacaf":22169,"dormant":22170,"economists":22171,"##dim":22172,"costing":22173,"csi":22174,"rana":22175,"universite":22176,"boulders":22177,"verity":22178,"##llon":22179,"collin":22180,"mellon":22181,"misses":22182,"cypress":22183,"fluorescent":22184,"lifeless":22185,"spence":22186,"##ulla":22187,"crewe":22188,"shepard":22189,"pak":22190,"revelations":22191,"##م":22192,"jolly":22193,"gibbons":22194,"paw":22195,"##dro":22196,"##quel":22197,"freeing":22198,"##test":22199,"shack":22200,"fries":22201,"palatine":22202,"##51":22203,"##hiko":22204,"accompaniment":22205,"cruising":22206,"recycled":22207,"##aver":22208,"erwin":22209,"sorting":22210,"synthesizers":22211,"dyke":22212,"realities":22213,"sg":22214,"strides":22215,"enslaved":22216,"wetland":22217,"##ghan":22218,"competence":22219,"gunpowder":22220,"grassy":22221,"maroon":22222,"reactors":22223,"objection":22224,"##oms":22225,"carlson":22226,"gearbox":22227,"macintosh":22228,"radios":22229,"shelton":22230,"##sho":22231,"clergyman":22232,"prakash":22233,"254":22234,"mongols":22235,"trophies":22236,"oricon":22237,"228":22238,"stimuli":22239,"twenty20":22240,"cantonese":22241,"cortes":22242,"mirrored":22243,"##saurus":22244,"bhp":22245,"cristina":22246,"melancholy":22247,"##lating":22248,"enjoyable":22249,"nuevo":22250,"##wny":22251,"downfall":22252,"schumacher":22253,"##ind":22254,"banging":22255,"lausanne":22256,"rumbled":22257,"paramilitary":22258,"reflex":22259,"ax":22260,"amplitude":22261,"migratory":22262,"##gall":22263,"##ups":22264,"midi":22265,"barnard":22266,"lastly":22267,"sherry":22268,"##hp":22269,"##nall":22270,"keystone":22271,"##kra":22272,"carleton":22273,"slippery":22274,"##53":22275,"coloring":22276,"foe":22277,"socket":22278,"otter":22279,"##rgos":22280,"mats":22281,"##tose":22282,"consultants":22283,"bafta":22284,"bison":22285,"topping":22286,"##km":22287,"490":22288,"primal":22289,"abandonment":22290,"transplant":22291,"atoll":22292,"hideous":22293,"mort":22294,"pained":22295,"reproduced":22296,"tae":22297,"howling":22298,"##turn":22299,"unlawful":22300,"billionaire":22301,"hotter":22302,"poised":22303,"lansing":22304,"##chang":22305,"dinamo":22306,"retro":22307,"messing":22308,"nfc":22309,"domesday":22310,"##mina":22311,"blitz":22312,"timed":22313,"##athing":22314,"##kley":22315,"ascending":22316,"gesturing":22317,"##izations":22318,"signaled":22319,"tis":22320,"chinatown":22321,"mermaid":22322,"savanna":22323,"jameson":22324,"##aint":22325,"catalina":22326,"##pet":22327,"##hers":22328,"cochrane":22329,"cy":22330,"chatting":22331,"##kus":22332,"alerted":22333,"computation":22334,"mused":22335,"noelle":22336,"majestic":22337,"mohawk":22338,"campo":22339,"octagonal":22340,"##sant":22341,"##hend":22342,"241":22343,"aspiring":22344,"##mart":22345,"comprehend":22346,"iona":22347,"paralyzed":22348,"shimmering":22349,"swindon":22350,"rhone":22351,"##eley":22352,"reputed":22353,"configurations":22354,"pitchfork":22355,"agitation":22356,"francais":22357,"gillian":22358,"lipstick":22359,"##ilo":22360,"outsiders":22361,"pontifical":22362,"resisting":22363,"bitterness":22364,"sewer":22365,"rockies":22366,"##edd":22367,"##ucher":22368,"misleading":22369,"1756":22370,"exiting":22371,"galloway":22372,"##nging":22373,"risked":22374,"##heart":22375,"246":22376,"commemoration":22377,"schultz":22378,"##rka":22379,"integrating":22380,"##rsa":22381,"poses":22382,"shrieked":22383,"##weiler":22384,"guineas":22385,"gladys":22386,"jerking":22387,"owls":22388,"goldsmith":22389,"nightly":22390,"penetrating":22391,"##unced":22392,"lia":22393,"##33":22394,"ignited":22395,"betsy":22396,"##aring":22397,"##thorpe":22398,"follower":22399,"vigorously":22400,"##rave":22401,"coded":22402,"kiran":22403,"knit":22404,"zoology":22405,"tbilisi":22406,"##28":22407,"##bered":22408,"repository":22409,"govt":22410,"deciduous":22411,"dino":22412,"growling":22413,"##bba":22414,"enhancement":22415,"unleashed":22416,"chanting":22417,"pussy":22418,"biochemistry":22419,"##eric":22420,"kettle":22421,"repression":22422,"toxicity":22423,"nrhp":22424,"##arth":22425,"##kko":22426,"##bush":22427,"ernesto":22428,"commended":22429,"outspoken":22430,"242":22431,"mca":22432,"parchment":22433,"sms":22434,"kristen":22435,"##aton":22436,"bisexual":22437,"raked":22438,"glamour":22439,"navajo":22440,"a2":22441,"conditioned":22442,"showcased":22443,"##hma":22444,"spacious":22445,"youthful":22446,"##esa":22447,"usl":22448,"appliances":22449,"junta":22450,"brest":22451,"layne":22452,"conglomerate":22453,"enchanted":22454,"chao":22455,"loosened":22456,"picasso":22457,"circulating":22458,"inspect":22459,"montevideo":22460,"##centric":22461,"##kti":22462,"piazza":22463,"spurred":22464,"##aith":22465,"bari":22466,"freedoms":22467,"poultry":22468,"stamford":22469,"lieu":22470,"##ect":22471,"indigo":22472,"sarcastic":22473,"bahia":22474,"stump":22475,"attach":22476,"dvds":22477,"frankenstein":22478,"lille":22479,"approx":22480,"scriptures":22481,"pollen":22482,"##script":22483,"nmi":22484,"overseen":22485,"##ivism":22486,"tides":22487,"proponent":22488,"newmarket":22489,"inherit":22490,"milling":22491,"##erland":22492,"centralized":22493,"##rou":22494,"distributors":22495,"credentials":22496,"drawers":22497,"abbreviation":22498,"##lco":22499,"##xon":22500,"downing":22501,"uncomfortably":22502,"ripe":22503,"##oes":22504,"erase":22505,"franchises":22506,"##ever":22507,"populace":22508,"##bery":22509,"##khar":22510,"decomposition":22511,"pleas":22512,"##tet":22513,"daryl":22514,"sabah":22515,"##stle":22516,"##wide":22517,"fearless":22518,"genie":22519,"lesions":22520,"annette":22521,"##ogist":22522,"oboe":22523,"appendix":22524,"nair":22525,"dripped":22526,"petitioned":22527,"maclean":22528,"mosquito":22529,"parrot":22530,"rpg":22531,"hampered":22532,"1648":22533,"operatic":22534,"reservoirs":22535,"##tham":22536,"irrelevant":22537,"jolt":22538,"summarized":22539,"##fp":22540,"medallion":22541,"##taff":22542,"##−":22543,"clawed":22544,"harlow":22545,"narrower":22546,"goddard":22547,"marcia":22548,"bodied":22549,"fremont":22550,"suarez":22551,"altering":22552,"tempest":22553,"mussolini":22554,"porn":22555,"##isms":22556,"sweetly":22557,"oversees":22558,"walkers":22559,"solitude":22560,"grimly":22561,"shrines":22562,"hk":22563,"ich":22564,"supervisors":22565,"hostess":22566,"dietrich":22567,"legitimacy":22568,"brushes":22569,"expressive":22570,"##yp":22571,"dissipated":22572,"##rse":22573,"localized":22574,"systemic":22575,"##nikov":22576,"gettysburg":22577,"##js":22578,"##uaries":22579,"dialogues":22580,"muttering":22581,"251":22582,"housekeeper":22583,"sicilian":22584,"discouraged":22585,"##frey":22586,"beamed":22587,"kaladin":22588,"halftime":22589,"kidnap":22590,"##amo":22591,"##llet":22592,"1754":22593,"synonymous":22594,"depleted":22595,"instituto":22596,"insulin":22597,"reprised":22598,"##opsis":22599,"clashed":22600,"##ctric":22601,"interrupting":22602,"radcliffe":22603,"insisting":22604,"medici":22605,"1715":22606,"ejected":22607,"playfully":22608,"turbulent":22609,"##47":22610,"starvation":22611,"##rini":22612,"shipment":22613,"rebellious":22614,"petersen":22615,"verification":22616,"merits":22617,"##rified":22618,"cakes":22619,"##charged":22620,"1757":22621,"milford":22622,"shortages":22623,"spying":22624,"fidelity":22625,"##aker":22626,"emitted":22627,"storylines":22628,"harvested":22629,"seismic":22630,"##iform":22631,"cheung":22632,"kilda":22633,"theoretically":22634,"barbie":22635,"lynx":22636,"##rgy":22637,"##tius":22638,"goblin":22639,"mata":22640,"poisonous":22641,"##nburg":22642,"reactive":22643,"residues":22644,"obedience":22645,"##евич":22646,"conjecture":22647,"##rac":22648,"401":22649,"hating":22650,"sixties":22651,"kicker":22652,"moaning":22653,"motown":22654,"##bha":22655,"emancipation":22656,"neoclassical":22657,"##hering":22658,"consoles":22659,"ebert":22660,"professorship":22661,"##tures":22662,"sustaining":22663,"assaults":22664,"obeyed":22665,"affluent":22666,"incurred":22667,"tornadoes":22668,"##eber":22669,"##zow":22670,"emphasizing":22671,"highlanders":22672,"cheated":22673,"helmets":22674,"##ctus":22675,"internship":22676,"terence":22677,"bony":22678,"executions":22679,"legislators":22680,"berries":22681,"peninsular":22682,"tinged":22683,"##aco":22684,"1689":22685,"amplifier":22686,"corvette":22687,"ribbons":22688,"lavish":22689,"pennant":22690,"##lander":22691,"worthless":22692,"##chfield":22693,"##forms":22694,"mariano":22695,"pyrenees":22696,"expenditures":22697,"##icides":22698,"chesterfield":22699,"mandir":22700,"tailor":22701,"39th":22702,"sergey":22703,"nestled":22704,"willed":22705,"aristocracy":22706,"devotees":22707,"goodnight":22708,"raaf":22709,"rumored":22710,"weaponry":22711,"remy":22712,"appropriations":22713,"harcourt":22714,"burr":22715,"riaa":22716,"##lence":22717,"limitation":22718,"unnoticed":22719,"guo":22720,"soaking":22721,"swamps":22722,"##tica":22723,"collapsing":22724,"tatiana":22725,"descriptive":22726,"brigham":22727,"psalm":22728,"##chment":22729,"maddox":22730,"##lization":22731,"patti":22732,"caliph":22733,"##aja":22734,"akron":22735,"injuring":22736,"serra":22737,"##ganj":22738,"basins":22739,"##sari":22740,"astonished":22741,"launcher":22742,"##church":22743,"hilary":22744,"wilkins":22745,"sewing":22746,"##sf":22747,"stinging":22748,"##fia":22749,"##ncia":22750,"underwood":22751,"startup":22752,"##ition":22753,"compilations":22754,"vibrations":22755,"embankment":22756,"jurist":22757,"##nity":22758,"bard":22759,"juventus":22760,"groundwater":22761,"kern":22762,"palaces":22763,"helium":22764,"boca":22765,"cramped":22766,"marissa":22767,"soto":22768,"##worm":22769,"jae":22770,"princely":22771,"##ggy":22772,"faso":22773,"bazaar":22774,"warmly":22775,"##voking":22776,"229":22777,"pairing":22778,"##lite":22779,"##grate":22780,"##nets":22781,"wien":22782,"freaked":22783,"ulysses":22784,"rebirth":22785,"##alia":22786,"##rent":22787,"mummy":22788,"guzman":22789,"jimenez":22790,"stilled":22791,"##nitz":22792,"trajectory":22793,"tha":22794,"woken":22795,"archival":22796,"professions":22797,"##pts":22798,"##pta":22799,"hilly":22800,"shadowy":22801,"shrink":22802,"##bolt":22803,"norwood":22804,"glued":22805,"migrate":22806,"stereotypes":22807,"devoid":22808,"##pheus":22809,"625":22810,"evacuate":22811,"horrors":22812,"infancy":22813,"gotham":22814,"knowles":22815,"optic":22816,"downloaded":22817,"sachs":22818,"kingsley":22819,"parramatta":22820,"darryl":22821,"mor":22822,"##onale":22823,"shady":22824,"commence":22825,"confesses":22826,"kan":22827,"##meter":22828,"##placed":22829,"marlborough":22830,"roundabout":22831,"regents":22832,"frigates":22833,"io":22834,"##imating":22835,"gothenburg":22836,"revoked":22837,"carvings":22838,"clockwise":22839,"convertible":22840,"intruder":22841,"##sche":22842,"banged":22843,"##ogo":22844,"vicky":22845,"bourgeois":22846,"##mony":22847,"dupont":22848,"footing":22849,"##gum":22850,"pd":22851,"##real":22852,"buckle":22853,"yun":22854,"penthouse":22855,"sane":22856,"720":22857,"serviced":22858,"stakeholders":22859,"neumann":22860,"bb":22861,"##eers":22862,"comb":22863,"##gam":22864,"catchment":22865,"pinning":22866,"rallies":22867,"typing":22868,"##elles":22869,"forefront":22870,"freiburg":22871,"sweetie":22872,"giacomo":22873,"widowed":22874,"goodwill":22875,"worshipped":22876,"aspirations":22877,"midday":22878,"##vat":22879,"fishery":22880,"##trick":22881,"bournemouth":22882,"turk":22883,"243":22884,"hearth":22885,"ethanol":22886,"guadalajara":22887,"murmurs":22888,"sl":22889,"##uge":22890,"afforded":22891,"scripted":22892,"##hta":22893,"wah":22894,"##jn":22895,"coroner":22896,"translucent":22897,"252":22898,"memorials":22899,"puck":22900,"progresses":22901,"clumsy":22902,"##race":22903,"315":22904,"candace":22905,"recounted":22906,"##27":22907,"##slin":22908,"##uve":22909,"filtering":22910,"##mac":22911,"howl":22912,"strata":22913,"heron":22914,"leveled":22915,"##ays":22916,"dubious":22917,"##oja":22918,"##т":22919,"##wheel":22920,"citations":22921,"exhibiting":22922,"##laya":22923,"##mics":22924,"##pods":22925,"turkic":22926,"##lberg":22927,"injunction":22928,"##ennial":22929,"##mit":22930,"antibodies":22931,"##44":22932,"organise":22933,"##rigues":22934,"cardiovascular":22935,"cushion":22936,"inverness":22937,"##zquez":22938,"dia":22939,"cocoa":22940,"sibling":22941,"##tman":22942,"##roid":22943,"expanse":22944,"feasible":22945,"tunisian":22946,"algiers":22947,"##relli":22948,"rus":22949,"bloomberg":22950,"dso":22951,"westphalia":22952,"bro":22953,"tacoma":22954,"281":22955,"downloads":22956,"##ours":22957,"konrad":22958,"duran":22959,"##hdi":22960,"continuum":22961,"jett":22962,"compares":22963,"legislator":22964,"secession":22965,"##nable":22966,"##gues":22967,"##zuka":22968,"translating":22969,"reacher":22970,"##gley":22971,"##ła":22972,"aleppo":22973,"##agi":22974,"tc":22975,"orchards":22976,"trapping":22977,"linguist":22978,"versatile":22979,"drumming":22980,"postage":22981,"calhoun":22982,"superiors":22983,"##mx":22984,"barefoot":22985,"leary":22986,"##cis":22987,"ignacio":22988,"alfa":22989,"kaplan":22990,"##rogen":22991,"bratislava":22992,"mori":22993,"##vot":22994,"disturb":22995,"haas":22996,"313":22997,"cartridges":22998,"gilmore":22999,"radiated":23000,"salford":23001,"tunic":23002,"hades":23003,"##ulsive":23004,"archeological":23005,"delilah":23006,"magistrates":23007,"auditioned":23008,"brewster":23009,"charters":23010,"empowerment":23011,"blogs":23012,"cappella":23013,"dynasties":23014,"iroquois":23015,"whipping":23016,"##krishna":23017,"raceway":23018,"truths":23019,"myra":23020,"weaken":23021,"judah":23022,"mcgregor":23023,"##horse":23024,"mic":23025,"refueling":23026,"37th":23027,"burnley":23028,"bosses":23029,"markus":23030,"premio":23031,"query":23032,"##gga":23033,"dunbar":23034,"##economic":23035,"darkest":23036,"lyndon":23037,"sealing":23038,"commendation":23039,"reappeared":23040,"##mun":23041,"addicted":23042,"ezio":23043,"slaughtered":23044,"satisfactory":23045,"shuffle":23046,"##eves":23047,"##thic":23048,"##uj":23049,"fortification":23050,"warrington":23051,"##otto":23052,"resurrected":23053,"fargo":23054,"mane":23055,"##utable":23056,"##lei":23057,"##space":23058,"foreword":23059,"ox":23060,"##aris":23061,"##vern":23062,"abrams":23063,"hua":23064,"##mento":23065,"sakura":23066,"##alo":23067,"uv":23068,"sentimental":23069,"##skaya":23070,"midfield":23071,"##eses":23072,"sturdy":23073,"scrolls":23074,"macleod":23075,"##kyu":23076,"entropy":23077,"##lance":23078,"mitochondrial":23079,"cicero":23080,"excelled":23081,"thinner":23082,"convoys":23083,"perceive":23084,"##oslav":23085,"##urable":23086,"systematically":23087,"grind":23088,"burkina":23089,"287":23090,"##tagram":23091,"ops":23092,"##aman":23093,"guantanamo":23094,"##cloth":23095,"##tite":23096,"forcefully":23097,"wavy":23098,"##jou":23099,"pointless":23100,"##linger":23101,"##tze":23102,"layton":23103,"portico":23104,"superficial":23105,"clerical":23106,"outlaws":23107,"##hism":23108,"burials":23109,"muir":23110,"##inn":23111,"creditors":23112,"hauling":23113,"rattle":23114,"##leg":23115,"calais":23116,"monde":23117,"archers":23118,"reclaimed":23119,"dwell":23120,"wexford":23121,"hellenic":23122,"falsely":23123,"remorse":23124,"##tek":23125,"dough":23126,"furnishings":23127,"##uttered":23128,"gabon":23129,"neurological":23130,"novice":23131,"##igraphy":23132,"contemplated":23133,"pulpit":23134,"nightstand":23135,"saratoga":23136,"##istan":23137,"documenting":23138,"pulsing":23139,"taluk":23140,"##firmed":23141,"busted":23142,"marital":23143,"##rien":23144,"disagreements":23145,"wasps":23146,"##yes":23147,"hodge":23148,"mcdonnell":23149,"mimic":23150,"fran":23151,"pendant":23152,"dhabi":23153,"musa":23154,"##nington":23155,"congratulations":23156,"argent":23157,"darrell":23158,"concussion":23159,"losers":23160,"regrets":23161,"thessaloniki":23162,"reversal":23163,"donaldson":23164,"hardwood":23165,"thence":23166,"achilles":23167,"ritter":23168,"##eran":23169,"demonic":23170,"jurgen":23171,"prophets":23172,"goethe":23173,"eki":23174,"classmate":23175,"buff":23176,"##cking":23177,"yank":23178,"irrational":23179,"##inging":23180,"perished":23181,"seductive":23182,"qur":23183,"sourced":23184,"##crat":23185,"##typic":23186,"mustard":23187,"ravine":23188,"barre":23189,"horizontally":23190,"characterization":23191,"phylogenetic":23192,"boise":23193,"##dit":23194,"##runner":23195,"##tower":23196,"brutally":23197,"intercourse":23198,"seduce":23199,"##bbing":23200,"fay":23201,"ferris":23202,"ogden":23203,"amar":23204,"nik":23205,"unarmed":23206,"##inator":23207,"evaluating":23208,"kyrgyzstan":23209,"sweetness":23210,"##lford":23211,"##oki":23212,"mccormick":23213,"meiji":23214,"notoriety":23215,"stimulate":23216,"disrupt":23217,"figuring":23218,"instructional":23219,"mcgrath":23220,"##zoo":23221,"groundbreaking":23222,"##lto":23223,"flinch":23224,"khorasan":23225,"agrarian":23226,"bengals":23227,"mixer":23228,"radiating":23229,"##sov":23230,"ingram":23231,"pitchers":23232,"nad":23233,"tariff":23234,"##cript":23235,"tata":23236,"##codes":23237,"##emi":23238,"##ungen":23239,"appellate":23240,"lehigh":23241,"##bled":23242,"##giri":23243,"brawl":23244,"duct":23245,"texans":23246,"##ciation":23247,"##ropolis":23248,"skipper":23249,"speculative":23250,"vomit":23251,"doctrines":23252,"stresses":23253,"253":23254,"davy":23255,"graders":23256,"whitehead":23257,"jozef":23258,"timely":23259,"cumulative":23260,"haryana":23261,"paints":23262,"appropriately":23263,"boon":23264,"cactus":23265,"##ales":23266,"##pid":23267,"dow":23268,"legions":23269,"##pit":23270,"perceptions":23271,"1730":23272,"picturesque":23273,"##yse":23274,"periphery":23275,"rune":23276,"wr":23277,"##aha":23278,"celtics":23279,"sentencing":23280,"whoa":23281,"##erin":23282,"confirms":23283,"variance":23284,"425":23285,"moines":23286,"mathews":23287,"spade":23288,"rave":23289,"m1":23290,"fronted":23291,"fx":23292,"blending":23293,"alleging":23294,"reared":23295,"##gl":23296,"237":23297,"##paper":23298,"grassroots":23299,"eroded":23300,"##free":23301,"##physical":23302,"directs":23303,"ordeal":23304,"##sław":23305,"accelerate":23306,"hacker":23307,"rooftop":23308,"##inia":23309,"lev":23310,"buys":23311,"cebu":23312,"devote":23313,"##lce":23314,"specialising":23315,"##ulsion":23316,"choreographed":23317,"repetition":23318,"warehouses":23319,"##ryl":23320,"paisley":23321,"tuscany":23322,"analogy":23323,"sorcerer":23324,"hash":23325,"huts":23326,"shards":23327,"descends":23328,"exclude":23329,"nix":23330,"chaplin":23331,"gaga":23332,"ito":23333,"vane":23334,"##drich":23335,"causeway":23336,"misconduct":23337,"limo":23338,"orchestrated":23339,"glands":23340,"jana":23341,"##kot":23342,"u2":23343,"##mple":23344,"##sons":23345,"branching":23346,"contrasts":23347,"scoop":23348,"longed":23349,"##virus":23350,"chattanooga":23351,"##75":23352,"syrup":23353,"cornerstone":23354,"##tized":23355,"##mind":23356,"##iaceae":23357,"careless":23358,"precedence":23359,"frescoes":23360,"##uet":23361,"chilled":23362,"consult":23363,"modelled":23364,"snatch":23365,"peat":23366,"##thermal":23367,"caucasian":23368,"humane":23369,"relaxation":23370,"spins":23371,"temperance":23372,"##lbert":23373,"occupations":23374,"lambda":23375,"hybrids":23376,"moons":23377,"mp3":23378,"##oese":23379,"247":23380,"rolf":23381,"societal":23382,"yerevan":23383,"ness":23384,"##ssler":23385,"befriended":23386,"mechanized":23387,"nominate":23388,"trough":23389,"boasted":23390,"cues":23391,"seater":23392,"##hom":23393,"bends":23394,"##tangle":23395,"conductors":23396,"emptiness":23397,"##lmer":23398,"eurasian":23399,"adriatic":23400,"tian":23401,"##cie":23402,"anxiously":23403,"lark":23404,"propellers":23405,"chichester":23406,"jock":23407,"ev":23408,"2a":23409,"##holding":23410,"credible":23411,"recounts":23412,"tori":23413,"loyalist":23414,"abduction":23415,"##hoot":23416,"##redo":23417,"nepali":23418,"##mite":23419,"ventral":23420,"tempting":23421,"##ango":23422,"##crats":23423,"steered":23424,"##wice":23425,"javelin":23426,"dipping":23427,"laborers":23428,"prentice":23429,"looming":23430,"titanium":23431,"##ː":23432,"badges":23433,"emir":23434,"tensor":23435,"##ntation":23436,"egyptians":23437,"rash":23438,"denies":23439,"hawthorne":23440,"lombard":23441,"showers":23442,"wehrmacht":23443,"dietary":23444,"trojan":23445,"##reus":23446,"welles":23447,"executing":23448,"horseshoe":23449,"lifeboat":23450,"##lak":23451,"elsa":23452,"infirmary":23453,"nearing":23454,"roberta":23455,"boyer":23456,"mutter":23457,"trillion":23458,"joanne":23459,"##fine":23460,"##oked":23461,"sinks":23462,"vortex":23463,"uruguayan":23464,"clasp":23465,"sirius":23466,"##block":23467,"accelerator":23468,"prohibit":23469,"sunken":23470,"byu":23471,"chronological":23472,"diplomats":23473,"ochreous":23474,"510":23475,"symmetrical":23476,"1644":23477,"maia":23478,"##tology":23479,"salts":23480,"reigns":23481,"atrocities":23482,"##ия":23483,"hess":23484,"bared":23485,"issn":23486,"##vyn":23487,"cater":23488,"saturated":23489,"##cycle":23490,"##isse":23491,"sable":23492,"voyager":23493,"dyer":23494,"yusuf":23495,"##inge":23496,"fountains":23497,"wolff":23498,"##39":23499,"##nni":23500,"engraving":23501,"rollins":23502,"atheist":23503,"ominous":23504,"##ault":23505,"herr":23506,"chariot":23507,"martina":23508,"strung":23509,"##fell":23510,"##farlane":23511,"horrific":23512,"sahib":23513,"gazes":23514,"saetan":23515,"erased":23516,"ptolemy":23517,"##olic":23518,"flushing":23519,"lauderdale":23520,"analytic":23521,"##ices":23522,"530":23523,"navarro":23524,"beak":23525,"gorilla":23526,"herrera":23527,"broom":23528,"guadalupe":23529,"raiding":23530,"sykes":23531,"311":23532,"bsc":23533,"deliveries":23534,"1720":23535,"invasions":23536,"carmichael":23537,"tajikistan":23538,"thematic":23539,"ecumenical":23540,"sentiments":23541,"onstage":23542,"##rians":23543,"##brand":23544,"##sume":23545,"catastrophic":23546,"flanks":23547,"molten":23548,"##arns":23549,"waller":23550,"aimee":23551,"terminating":23552,"##icing":23553,"alternately":23554,"##oche":23555,"nehru":23556,"printers":23557,"outraged":23558,"##eving":23559,"empires":23560,"template":23561,"banners":23562,"repetitive":23563,"za":23564,"##oise":23565,"vegetarian":23566,"##tell":23567,"guiana":23568,"opt":23569,"cavendish":23570,"lucknow":23571,"synthesized":23572,"##hani":23573,"##mada":23574,"finalized":23575,"##ctable":23576,"fictitious":23577,"mayoral":23578,"unreliable":23579,"##enham":23580,"embracing":23581,"peppers":23582,"rbis":23583,"##chio":23584,"##neo":23585,"inhibition":23586,"slashed":23587,"togo":23588,"orderly":23589,"embroidered":23590,"safari":23591,"salty":23592,"236":23593,"barron":23594,"benito":23595,"totaled":23596,"##dak":23597,"pubs":23598,"simulated":23599,"caden":23600,"devin":23601,"tolkien":23602,"momma":23603,"welding":23604,"sesame":23605,"##ept":23606,"gottingen":23607,"hardness":23608,"630":23609,"shaman":23610,"temeraire":23611,"620":23612,"adequately":23613,"pediatric":23614,"##kit":23615,"ck":23616,"assertion":23617,"radicals":23618,"composure":23619,"cadence":23620,"seafood":23621,"beaufort":23622,"lazarus":23623,"mani":23624,"warily":23625,"cunning":23626,"kurdistan":23627,"249":23628,"cantata":23629,"##kir":23630,"ares":23631,"##41":23632,"##clusive":23633,"nape":23634,"townland":23635,"geared":23636,"insulted":23637,"flutter":23638,"boating":23639,"violate":23640,"draper":23641,"dumping":23642,"malmo":23643,"##hh":23644,"##romatic":23645,"firearm":23646,"alta":23647,"bono":23648,"obscured":23649,"##clave":23650,"exceeds":23651,"panorama":23652,"unbelievable":23653,"##train":23654,"preschool":23655,"##essed":23656,"disconnected":23657,"installing":23658,"rescuing":23659,"secretaries":23660,"accessibility":23661,"##castle":23662,"##drive":23663,"##ifice":23664,"##film":23665,"bouts":23666,"slug":23667,"waterway":23668,"mindanao":23669,"##buro":23670,"##ratic":23671,"halves":23672,"##ل":23673,"calming":23674,"liter":23675,"maternity":23676,"adorable":23677,"bragg":23678,"electrification":23679,"mcc":23680,"##dote":23681,"roxy":23682,"schizophrenia":23683,"##body":23684,"munoz":23685,"kaye":23686,"whaling":23687,"239":23688,"mil":23689,"tingling":23690,"tolerant":23691,"##ago":23692,"unconventional":23693,"volcanoes":23694,"##finder":23695,"deportivo":23696,"##llie":23697,"robson":23698,"kaufman":23699,"neuroscience":23700,"wai":23701,"deportation":23702,"masovian":23703,"scraping":23704,"converse":23705,"##bh":23706,"hacking":23707,"bulge":23708,"##oun":23709,"administratively":23710,"yao":23711,"580":23712,"amp":23713,"mammoth":23714,"booster":23715,"claremont":23716,"hooper":23717,"nomenclature":23718,"pursuits":23719,"mclaughlin":23720,"melinda":23721,"##sul":23722,"catfish":23723,"barclay":23724,"substrates":23725,"taxa":23726,"zee":23727,"originals":23728,"kimberly":23729,"packets":23730,"padma":23731,"##ality":23732,"borrowing":23733,"ostensibly":23734,"solvent":23735,"##bri":23736,"##genesis":23737,"##mist":23738,"lukas":23739,"shreveport":23740,"veracruz":23741,"##ь":23742,"##lou":23743,"##wives":23744,"cheney":23745,"tt":23746,"anatolia":23747,"hobbs":23748,"##zyn":23749,"cyclic":23750,"radiant":23751,"alistair":23752,"greenish":23753,"siena":23754,"dat":23755,"independents":23756,"##bation":23757,"conform":23758,"pieter":23759,"hyper":23760,"applicant":23761,"bradshaw":23762,"spores":23763,"telangana":23764,"vinci":23765,"inexpensive":23766,"nuclei":23767,"322":23768,"jang":23769,"nme":23770,"soho":23771,"spd":23772,"##ign":23773,"cradled":23774,"receptionist":23775,"pow":23776,"##43":23777,"##rika":23778,"fascism":23779,"##ifer":23780,"experimenting":23781,"##ading":23782,"##iec":23783,"##region":23784,"345":23785,"jocelyn":23786,"maris":23787,"stair":23788,"nocturnal":23789,"toro":23790,"constabulary":23791,"elgin":23792,"##kker":23793,"msc":23794,"##giving":23795,"##schen":23796,"##rase":23797,"doherty":23798,"doping":23799,"sarcastically":23800,"batter":23801,"maneuvers":23802,"##cano":23803,"##apple":23804,"##gai":23805,"##git":23806,"intrinsic":23807,"##nst":23808,"##stor":23809,"1753":23810,"showtime":23811,"cafes":23812,"gasps":23813,"lviv":23814,"ushered":23815,"##thed":23816,"fours":23817,"restart":23818,"astonishment":23819,"transmitting":23820,"flyer":23821,"shrugs":23822,"##sau":23823,"intriguing":23824,"cones":23825,"dictated":23826,"mushrooms":23827,"medial":23828,"##kovsky":23829,"##elman":23830,"escorting":23831,"gaped":23832,"##26":23833,"godfather":23834,"##door":23835,"##sell":23836,"djs":23837,"recaptured":23838,"timetable":23839,"vila":23840,"1710":23841,"3a":23842,"aerodrome":23843,"mortals":23844,"scientology":23845,"##orne":23846,"angelina":23847,"mag":23848,"convection":23849,"unpaid":23850,"insertion":23851,"intermittent":23852,"lego":23853,"##nated":23854,"endeavor":23855,"kota":23856,"pereira":23857,"##lz":23858,"304":23859,"bwv":23860,"glamorgan":23861,"insults":23862,"agatha":23863,"fey":23864,"##cend":23865,"fleetwood":23866,"mahogany":23867,"protruding":23868,"steamship":23869,"zeta":23870,"##arty":23871,"mcguire":23872,"suspense":23873,"##sphere":23874,"advising":23875,"urges":23876,"##wala":23877,"hurriedly":23878,"meteor":23879,"gilded":23880,"inline":23881,"arroyo":23882,"stalker":23883,"##oge":23884,"excitedly":23885,"revered":23886,"##cure":23887,"earle":23888,"introductory":23889,"##break":23890,"##ilde":23891,"mutants":23892,"puff":23893,"pulses":23894,"reinforcement":23895,"##haling":23896,"curses":23897,"lizards":23898,"stalk":23899,"correlated":23900,"##fixed":23901,"fallout":23902,"macquarie":23903,"##unas":23904,"bearded":23905,"denton":23906,"heaving":23907,"802":23908,"##ocation":23909,"winery":23910,"assign":23911,"dortmund":23912,"##lkirk":23913,"everest":23914,"invariant":23915,"charismatic":23916,"susie":23917,"##elling":23918,"bled":23919,"lesley":23920,"telegram":23921,"sumner":23922,"bk":23923,"##ogen":23924,"##к":23925,"wilcox":23926,"needy":23927,"colbert":23928,"duval":23929,"##iferous":23930,"##mbled":23931,"allotted":23932,"attends":23933,"imperative":23934,"##hita":23935,"replacements":23936,"hawker":23937,"##inda":23938,"insurgency":23939,"##zee":23940,"##eke":23941,"casts":23942,"##yla":23943,"680":23944,"ives":23945,"transitioned":23946,"##pack":23947,"##powering":23948,"authoritative":23949,"baylor":23950,"flex":23951,"cringed":23952,"plaintiffs":23953,"woodrow":23954,"##skie":23955,"drastic":23956,"ape":23957,"aroma":23958,"unfolded":23959,"commotion":23960,"nt":23961,"preoccupied":23962,"theta":23963,"routines":23964,"lasers":23965,"privatization":23966,"wand":23967,"domino":23968,"ek":23969,"clenching":23970,"nsa":23971,"strategically":23972,"showered":23973,"bile":23974,"handkerchief":23975,"pere":23976,"storing":23977,"christophe":23978,"insulting":23979,"316":23980,"nakamura":23981,"romani":23982,"asiatic":23983,"magdalena":23984,"palma":23985,"cruises":23986,"stripping":23987,"405":23988,"konstantin":23989,"soaring":23990,"##berman":23991,"colloquially":23992,"forerunner":23993,"havilland":23994,"incarcerated":23995,"parasites":23996,"sincerity":23997,"##utus":23998,"disks":23999,"plank":24000,"saigon":24001,"##ining":24002,"corbin":24003,"homo":24004,"ornaments":24005,"powerhouse":24006,"##tlement":24007,"chong":24008,"fastened":24009,"feasibility":24010,"idf":24011,"morphological":24012,"usable":24013,"##nish":24014,"##zuki":24015,"aqueduct":24016,"jaguars":24017,"keepers":24018,"##flies":24019,"aleksandr":24020,"faust":24021,"assigns":24022,"ewing":24023,"bacterium":24024,"hurled":24025,"tricky":24026,"hungarians":24027,"integers":24028,"wallis":24029,"321":24030,"yamaha":24031,"##isha":24032,"hushed":24033,"oblivion":24034,"aviator":24035,"evangelist":24036,"friars":24037,"##eller":24038,"monograph":24039,"ode":24040,"##nary":24041,"airplanes":24042,"labourers":24043,"charms":24044,"##nee":24045,"1661":24046,"hagen":24047,"tnt":24048,"rudder":24049,"fiesta":24050,"transcript":24051,"dorothea":24052,"ska":24053,"inhibitor":24054,"maccabi":24055,"retorted":24056,"raining":24057,"encompassed":24058,"clauses":24059,"menacing":24060,"1642":24061,"lineman":24062,"##gist":24063,"vamps":24064,"##ape":24065,"##dick":24066,"gloom":24067,"##rera":24068,"dealings":24069,"easing":24070,"seekers":24071,"##nut":24072,"##pment":24073,"helens":24074,"unmanned":24075,"##anu":24076,"##isson":24077,"basics":24078,"##amy":24079,"##ckman":24080,"adjustments":24081,"1688":24082,"brutality":24083,"horne":24084,"##zell":24085,"sui":24086,"##55":24087,"##mable":24088,"aggregator":24089,"##thal":24090,"rhino":24091,"##drick":24092,"##vira":24093,"counters":24094,"zoom":24095,"##01":24096,"##rting":24097,"mn":24098,"montenegrin":24099,"packard":24100,"##unciation":24101,"##♭":24102,"##kki":24103,"reclaim":24104,"scholastic":24105,"thugs":24106,"pulsed":24107,"##icia":24108,"syriac":24109,"quan":24110,"saddam":24111,"banda":24112,"kobe":24113,"blaming":24114,"buddies":24115,"dissent":24116,"##lusion":24117,"##usia":24118,"corbett":24119,"jaya":24120,"delle":24121,"erratic":24122,"lexie":24123,"##hesis":24124,"435":24125,"amiga":24126,"hermes":24127,"##pressing":24128,"##leen":24129,"chapels":24130,"gospels":24131,"jamal":24132,"##uating":24133,"compute":24134,"revolving":24135,"warp":24136,"##sso":24137,"##thes":24138,"armory":24139,"##eras":24140,"##gol":24141,"antrim":24142,"loki":24143,"##kow":24144,"##asian":24145,"##good":24146,"##zano":24147,"braid":24148,"handwriting":24149,"subdistrict":24150,"funky":24151,"pantheon":24152,"##iculate":24153,"concurrency":24154,"estimation":24155,"improper":24156,"juliana":24157,"##his":24158,"newcomers":24159,"johnstone":24160,"staten":24161,"communicated":24162,"##oco":24163,"##alle":24164,"sausage":24165,"stormy":24166,"##stered":24167,"##tters":24168,"superfamily":24169,"##grade":24170,"acidic":24171,"collateral":24172,"tabloid":24173,"##oped":24174,"##rza":24175,"bladder":24176,"austen":24177,"##ellant":24178,"mcgraw":24179,"##hay":24180,"hannibal":24181,"mein":24182,"aquino":24183,"lucifer":24184,"wo":24185,"badger":24186,"boar":24187,"cher":24188,"christensen":24189,"greenberg":24190,"interruption":24191,"##kken":24192,"jem":24193,"244":24194,"mocked":24195,"bottoms":24196,"cambridgeshire":24197,"##lide":24198,"sprawling":24199,"##bbly":24200,"eastwood":24201,"ghent":24202,"synth":24203,"##buck":24204,"advisers":24205,"##bah":24206,"nominally":24207,"hapoel":24208,"qu":24209,"daggers":24210,"estranged":24211,"fabricated":24212,"towels":24213,"vinnie":24214,"wcw":24215,"misunderstanding":24216,"anglia":24217,"nothin":24218,"unmistakable":24219,"##dust":24220,"##lova":24221,"chilly":24222,"marquette":24223,"truss":24224,"##edge":24225,"##erine":24226,"reece":24227,"##lty":24228,"##chemist":24229,"##connected":24230,"272":24231,"308":24232,"41st":24233,"bash":24234,"raion":24235,"waterfalls":24236,"##ump":24237,"##main":24238,"labyrinth":24239,"queue":24240,"theorist":24241,"##istle":24242,"bharatiya":24243,"flexed":24244,"soundtracks":24245,"rooney":24246,"leftist":24247,"patrolling":24248,"wharton":24249,"plainly":24250,"alleviate":24251,"eastman":24252,"schuster":24253,"topographic":24254,"engages":24255,"immensely":24256,"unbearable":24257,"fairchild":24258,"1620":24259,"dona":24260,"lurking":24261,"parisian":24262,"oliveira":24263,"ia":24264,"indictment":24265,"hahn":24266,"bangladeshi":24267,"##aster":24268,"vivo":24269,"##uming":24270,"##ential":24271,"antonia":24272,"expects":24273,"indoors":24274,"kildare":24275,"harlan":24276,"##logue":24277,"##ogenic":24278,"##sities":24279,"forgiven":24280,"##wat":24281,"childish":24282,"tavi":24283,"##mide":24284,"##orra":24285,"plausible":24286,"grimm":24287,"successively":24288,"scooted":24289,"##bola":24290,"##dget":24291,"##rith":24292,"spartans":24293,"emery":24294,"flatly":24295,"azure":24296,"epilogue":24297,"##wark":24298,"flourish":24299,"##iny":24300,"##tracted":24301,"##overs":24302,"##oshi":24303,"bestseller":24304,"distressed":24305,"receipt":24306,"spitting":24307,"hermit":24308,"topological":24309,"##cot":24310,"drilled":24311,"subunit":24312,"francs":24313,"##layer":24314,"eel":24315,"##fk":24316,"##itas":24317,"octopus":24318,"footprint":24319,"petitions":24320,"ufo":24321,"##say":24322,"##foil":24323,"interfering":24324,"leaking":24325,"palo":24326,"##metry":24327,"thistle":24328,"valiant":24329,"##pic":24330,"narayan":24331,"mcpherson":24332,"##fast":24333,"gonzales":24334,"##ym":24335,"##enne":24336,"dustin":24337,"novgorod":24338,"solos":24339,"##zman":24340,"doin":24341,"##raph":24342,"##patient":24343,"##meyer":24344,"soluble":24345,"ashland":24346,"cuffs":24347,"carole":24348,"pendleton":24349,"whistling":24350,"vassal":24351,"##river":24352,"deviation":24353,"revisited":24354,"constituents":24355,"rallied":24356,"rotate":24357,"loomed":24358,"##eil":24359,"##nting":24360,"amateurs":24361,"augsburg":24362,"auschwitz":24363,"crowns":24364,"skeletons":24365,"##cona":24366,"bonnet":24367,"257":24368,"dummy":24369,"globalization":24370,"simeon":24371,"sleeper":24372,"mandal":24373,"differentiated":24374,"##crow":24375,"##mare":24376,"milne":24377,"bundled":24378,"exasperated":24379,"talmud":24380,"owes":24381,"segregated":24382,"##feng":24383,"##uary":24384,"dentist":24385,"piracy":24386,"props":24387,"##rang":24388,"devlin":24389,"##torium":24390,"malicious":24391,"paws":24392,"##laid":24393,"dependency":24394,"##ergy":24395,"##fers":24396,"##enna":24397,"258":24398,"pistons":24399,"rourke":24400,"jed":24401,"grammatical":24402,"tres":24403,"maha":24404,"wig":24405,"512":24406,"ghostly":24407,"jayne":24408,"##achal":24409,"##creen":24410,"##ilis":24411,"##lins":24412,"##rence":24413,"designate":24414,"##with":24415,"arrogance":24416,"cambodian":24417,"clones":24418,"showdown":24419,"throttle":24420,"twain":24421,"##ception":24422,"lobes":24423,"metz":24424,"nagoya":24425,"335":24426,"braking":24427,"##furt":24428,"385":24429,"roaming":24430,"##minster":24431,"amin":24432,"crippled":24433,"##37":24434,"##llary":24435,"indifferent":24436,"hoffmann":24437,"idols":24438,"intimidating":24439,"1751":24440,"261":24441,"influenza":24442,"memo":24443,"onions":24444,"1748":24445,"bandage":24446,"consciously":24447,"##landa":24448,"##rage":24449,"clandestine":24450,"observes":24451,"swiped":24452,"tangle":24453,"##ener":24454,"##jected":24455,"##trum":24456,"##bill":24457,"##lta":24458,"hugs":24459,"congresses":24460,"josiah":24461,"spirited":24462,"##dek":24463,"humanist":24464,"managerial":24465,"filmmaking":24466,"inmate":24467,"rhymes":24468,"debuting":24469,"grimsby":24470,"ur":24471,"##laze":24472,"duplicate":24473,"vigor":24474,"##tf":24475,"republished":24476,"bolshevik":24477,"refurbishment":24478,"antibiotics":24479,"martini":24480,"methane":24481,"newscasts":24482,"royale":24483,"horizons":24484,"levant":24485,"iain":24486,"visas":24487,"##ischen":24488,"paler":24489,"##around":24490,"manifestation":24491,"snuck":24492,"alf":24493,"chop":24494,"futile":24495,"pedestal":24496,"rehab":24497,"##kat":24498,"bmg":24499,"kerman":24500,"res":24501,"fairbanks":24502,"jarrett":24503,"abstraction":24504,"saharan":24505,"##zek":24506,"1746":24507,"procedural":24508,"clearer":24509,"kincaid":24510,"sash":24511,"luciano":24512,"##ffey":24513,"crunch":24514,"helmut":24515,"##vara":24516,"revolutionaries":24517,"##tute":24518,"creamy":24519,"leach":24520,"##mmon":24521,"1747":24522,"permitting":24523,"nes":24524,"plight":24525,"wendell":24526,"##lese":24527,"contra":24528,"ts":24529,"clancy":24530,"ipa":24531,"mach":24532,"staples":24533,"autopsy":24534,"disturbances":24535,"nueva":24536,"karin":24537,"pontiac":24538,"##uding":24539,"proxy":24540,"venerable":24541,"haunt":24542,"leto":24543,"bergman":24544,"expands":24545,"##helm":24546,"wal":24547,"##pipe":24548,"canning":24549,"celine":24550,"cords":24551,"obesity":24552,"##enary":24553,"intrusion":24554,"planner":24555,"##phate":24556,"reasoned":24557,"sequencing":24558,"307":24559,"harrow":24560,"##chon":24561,"##dora":24562,"marred":24563,"mcintyre":24564,"repay":24565,"tarzan":24566,"darting":24567,"248":24568,"harrisburg":24569,"margarita":24570,"repulsed":24571,"##hur":24572,"##lding":24573,"belinda":24574,"hamburger":24575,"novo":24576,"compliant":24577,"runways":24578,"bingham":24579,"registrar":24580,"skyscraper":24581,"ic":24582,"cuthbert":24583,"improvisation":24584,"livelihood":24585,"##corp":24586,"##elial":24587,"admiring":24588,"##dened":24589,"sporadic":24590,"believer":24591,"casablanca":24592,"popcorn":24593,"##29":24594,"asha":24595,"shovel":24596,"##bek":24597,"##dice":24598,"coiled":24599,"tangible":24600,"##dez":24601,"casper":24602,"elsie":24603,"resin":24604,"tenderness":24605,"rectory":24606,"##ivision":24607,"avail":24608,"sonar":24609,"##mori":24610,"boutique":24611,"##dier":24612,"guerre":24613,"bathed":24614,"upbringing":24615,"vaulted":24616,"sandals":24617,"blessings":24618,"##naut":24619,"##utnant":24620,"1680":24621,"306":24622,"foxes":24623,"pia":24624,"corrosion":24625,"hesitantly":24626,"confederates":24627,"crystalline":24628,"footprints":24629,"shapiro":24630,"tirana":24631,"valentin":24632,"drones":24633,"45th":24634,"microscope":24635,"shipments":24636,"texted":24637,"inquisition":24638,"wry":24639,"guernsey":24640,"unauthorized":24641,"resigning":24642,"760":24643,"ripple":24644,"schubert":24645,"stu":24646,"reassure":24647,"felony":24648,"##ardo":24649,"brittle":24650,"koreans":24651,"##havan":24652,"##ives":24653,"dun":24654,"implicit":24655,"tyres":24656,"##aldi":24657,"##lth":24658,"magnolia":24659,"##ehan":24660,"##puri":24661,"##poulos":24662,"aggressively":24663,"fei":24664,"gr":24665,"familiarity":24666,"##poo":24667,"indicative":24668,"##trust":24669,"fundamentally":24670,"jimmie":24671,"overrun":24672,"395":24673,"anchors":24674,"moans":24675,"##opus":24676,"britannia":24677,"armagh":24678,"##ggle":24679,"purposely":24680,"seizing":24681,"##vao":24682,"bewildered":24683,"mundane":24684,"avoidance":24685,"cosmopolitan":24686,"geometridae":24687,"quartermaster":24688,"caf":24689,"415":24690,"chatter":24691,"engulfed":24692,"gleam":24693,"purge":24694,"##icate":24695,"juliette":24696,"jurisprudence":24697,"guerra":24698,"revisions":24699,"##bn":24700,"casimir":24701,"brew":24702,"##jm":24703,"1749":24704,"clapton":24705,"cloudy":24706,"conde":24707,"hermitage":24708,"278":24709,"simulations":24710,"torches":24711,"vincenzo":24712,"matteo":24713,"##rill":24714,"hidalgo":24715,"booming":24716,"westbound":24717,"accomplishment":24718,"tentacles":24719,"unaffected":24720,"##sius":24721,"annabelle":24722,"flopped":24723,"sloping":24724,"##litz":24725,"dreamer":24726,"interceptor":24727,"vu":24728,"##loh":24729,"consecration":24730,"copying":24731,"messaging":24732,"breaker":24733,"climates":24734,"hospitalized":24735,"1752":24736,"torino":24737,"afternoons":24738,"winfield":24739,"witnessing":24740,"##teacher":24741,"breakers":24742,"choirs":24743,"sawmill":24744,"coldly":24745,"##ege":24746,"sipping":24747,"haste":24748,"uninhabited":24749,"conical":24750,"bibliography":24751,"pamphlets":24752,"severn":24753,"edict":24754,"##oca":24755,"deux":24756,"illnesses":24757,"grips":24758,"##pl":24759,"rehearsals":24760,"sis":24761,"thinkers":24762,"tame":24763,"##keepers":24764,"1690":24765,"acacia":24766,"reformer":24767,"##osed":24768,"##rys":24769,"shuffling":24770,"##iring":24771,"##shima":24772,"eastbound":24773,"ionic":24774,"rhea":24775,"flees":24776,"littered":24777,"##oum":24778,"rocker":24779,"vomiting":24780,"groaning":24781,"champ":24782,"overwhelmingly":24783,"civilizations":24784,"paces":24785,"sloop":24786,"adoptive":24787,"##tish":24788,"skaters":24789,"##vres":24790,"aiding":24791,"mango":24792,"##joy":24793,"nikola":24794,"shriek":24795,"##ignon":24796,"pharmaceuticals":24797,"##mg":24798,"tuna":24799,"calvert":24800,"gustavo":24801,"stocked":24802,"yearbook":24803,"##urai":24804,"##mana":24805,"computed":24806,"subsp":24807,"riff":24808,"hanoi":24809,"kelvin":24810,"hamid":24811,"moors":24812,"pastures":24813,"summons":24814,"jihad":24815,"nectar":24816,"##ctors":24817,"bayou":24818,"untitled":24819,"pleasing":24820,"vastly":24821,"republics":24822,"intellect":24823,"##η":24824,"##ulio":24825,"##tou":24826,"crumbling":24827,"stylistic":24828,"sb":24829,"##ی":24830,"consolation":24831,"frequented":24832,"h₂o":24833,"walden":24834,"widows":24835,"##iens":24836,"404":24837,"##ignment":24838,"chunks":24839,"improves":24840,"288":24841,"grit":24842,"recited":24843,"##dev":24844,"snarl":24845,"sociological":24846,"##arte":24847,"##gul":24848,"inquired":24849,"##held":24850,"bruise":24851,"clube":24852,"consultancy":24853,"homogeneous":24854,"hornets":24855,"multiplication":24856,"pasta":24857,"prick":24858,"savior":24859,"##grin":24860,"##kou":24861,"##phile":24862,"yoon":24863,"##gara":24864,"grimes":24865,"vanishing":24866,"cheering":24867,"reacting":24868,"bn":24869,"distillery":24870,"##quisite":24871,"##vity":24872,"coe":24873,"dockyard":24874,"massif":24875,"##jord":24876,"escorts":24877,"voss":24878,"##valent":24879,"byte":24880,"chopped":24881,"hawke":24882,"illusions":24883,"workings":24884,"floats":24885,"##koto":24886,"##vac":24887,"kv":24888,"annapolis":24889,"madden":24890,"##onus":24891,"alvaro":24892,"noctuidae":24893,"##cum":24894,"##scopic":24895,"avenge":24896,"steamboat":24897,"forte":24898,"illustrates":24899,"erika":24900,"##trip":24901,"570":24902,"dew":24903,"nationalities":24904,"bran":24905,"manifested":24906,"thirsty":24907,"diversified":24908,"muscled":24909,"reborn":24910,"##standing":24911,"arson":24912,"##lessness":24913,"##dran":24914,"##logram":24915,"##boys":24916,"##kushima":24917,"##vious":24918,"willoughby":24919,"##phobia":24920,"286":24921,"alsace":24922,"dashboard":24923,"yuki":24924,"##chai":24925,"granville":24926,"myspace":24927,"publicized":24928,"tricked":24929,"##gang":24930,"adjective":24931,"##ater":24932,"relic":24933,"reorganisation":24934,"enthusiastically":24935,"indications":24936,"saxe":24937,"##lassified":24938,"consolidate":24939,"iec":24940,"padua":24941,"helplessly":24942,"ramps":24943,"renaming":24944,"regulars":24945,"pedestrians":24946,"accents":24947,"convicts":24948,"inaccurate":24949,"lowers":24950,"mana":24951,"##pati":24952,"barrie":24953,"bjp":24954,"outta":24955,"someplace":24956,"berwick":24957,"flanking":24958,"invoked":24959,"marrow":24960,"sparsely":24961,"excerpts":24962,"clothed":24963,"rei":24964,"##ginal":24965,"wept":24966,"##straße":24967,"##vish":24968,"alexa":24969,"excel":24970,"##ptive":24971,"membranes":24972,"aquitaine":24973,"creeks":24974,"cutler":24975,"sheppard":24976,"implementations":24977,"ns":24978,"##dur":24979,"fragrance":24980,"budge":24981,"concordia":24982,"magnesium":24983,"marcelo":24984,"##antes":24985,"gladly":24986,"vibrating":24987,"##rral":24988,"##ggles":24989,"montrose":24990,"##omba":24991,"lew":24992,"seamus":24993,"1630":24994,"cocky":24995,"##ament":24996,"##uen":24997,"bjorn":24998,"##rrick":24999,"fielder":25000,"fluttering":25001,"##lase":25002,"methyl":25003,"kimberley":25004,"mcdowell":25005,"reductions":25006,"barbed":25007,"##jic":25008,"##tonic":25009,"aeronautical":25010,"condensed":25011,"distracting":25012,"##promising":25013,"huffed":25014,"##cala":25015,"##sle":25016,"claudius":25017,"invincible":25018,"missy":25019,"pious":25020,"balthazar":25021,"ci":25022,"##lang":25023,"butte":25024,"combo":25025,"orson":25026,"##dication":25027,"myriad":25028,"1707":25029,"silenced":25030,"##fed":25031,"##rh":25032,"coco":25033,"netball":25034,"yourselves":25035,"##oza":25036,"clarify":25037,"heller":25038,"peg":25039,"durban":25040,"etudes":25041,"offender":25042,"roast":25043,"blackmail":25044,"curvature":25045,"##woods":25046,"vile":25047,"309":25048,"illicit":25049,"suriname":25050,"##linson":25051,"overture":25052,"1685":25053,"bubbling":25054,"gymnast":25055,"tucking":25056,"##mming":25057,"##ouin":25058,"maldives":25059,"##bala":25060,"gurney":25061,"##dda":25062,"##eased":25063,"##oides":25064,"backside":25065,"pinto":25066,"jars":25067,"racehorse":25068,"tending":25069,"##rdial":25070,"baronetcy":25071,"wiener":25072,"duly":25073,"##rke":25074,"barbarian":25075,"cupping":25076,"flawed":25077,"##thesis":25078,"bertha":25079,"pleistocene":25080,"puddle":25081,"swearing":25082,"##nob":25083,"##tically":25084,"fleeting":25085,"prostate":25086,"amulet":25087,"educating":25088,"##mined":25089,"##iti":25090,"##tler":25091,"75th":25092,"jens":25093,"respondents":25094,"analytics":25095,"cavaliers":25096,"papacy":25097,"raju":25098,"##iente":25099,"##ulum":25100,"##tip":25101,"funnel":25102,"271":25103,"disneyland":25104,"##lley":25105,"sociologist":25106,"##iam":25107,"2500":25108,"faulkner":25109,"louvre":25110,"menon":25111,"##dson":25112,"276":25113,"##ower":25114,"afterlife":25115,"mannheim":25116,"peptide":25117,"referees":25118,"comedians":25119,"meaningless":25120,"##anger":25121,"##laise":25122,"fabrics":25123,"hurley":25124,"renal":25125,"sleeps":25126,"##bour":25127,"##icle":25128,"breakout":25129,"kristin":25130,"roadside":25131,"animator":25132,"clover":25133,"disdain":25134,"unsafe":25135,"redesign":25136,"##urity":25137,"firth":25138,"barnsley":25139,"portage":25140,"reset":25141,"narrows":25142,"268":25143,"commandos":25144,"expansive":25145,"speechless":25146,"tubular":25147,"##lux":25148,"essendon":25149,"eyelashes":25150,"smashwords":25151,"##yad":25152,"##bang":25153,"##claim":25154,"craved":25155,"sprinted":25156,"chet":25157,"somme":25158,"astor":25159,"wrocław":25160,"orton":25161,"266":25162,"bane":25163,"##erving":25164,"##uing":25165,"mischief":25166,"##amps":25167,"##sund":25168,"scaling":25169,"terre":25170,"##xious":25171,"impairment":25172,"offenses":25173,"undermine":25174,"moi":25175,"soy":25176,"contiguous":25177,"arcadia":25178,"inuit":25179,"seam":25180,"##tops":25181,"macbeth":25182,"rebelled":25183,"##icative":25184,"##iot":25185,"590":25186,"elaborated":25187,"frs":25188,"uniformed":25189,"##dberg":25190,"259":25191,"powerless":25192,"priscilla":25193,"stimulated":25194,"980":25195,"qc":25196,"arboretum":25197,"frustrating":25198,"trieste":25199,"bullock":25200,"##nified":25201,"enriched":25202,"glistening":25203,"intern":25204,"##adia":25205,"locus":25206,"nouvelle":25207,"ollie":25208,"ike":25209,"lash":25210,"starboard":25211,"ee":25212,"tapestry":25213,"headlined":25214,"hove":25215,"rigged":25216,"##vite":25217,"pollock":25218,"##yme":25219,"thrive":25220,"clustered":25221,"cas":25222,"roi":25223,"gleamed":25224,"olympiad":25225,"##lino":25226,"pressured":25227,"regimes":25228,"##hosis":25229,"##lick":25230,"ripley":25231,"##ophone":25232,"kickoff":25233,"gallon":25234,"rockwell":25235,"##arable":25236,"crusader":25237,"glue":25238,"revolutions":25239,"scrambling":25240,"1714":25241,"grover":25242,"##jure":25243,"englishman":25244,"aztec":25245,"263":25246,"contemplating":25247,"coven":25248,"ipad":25249,"preach":25250,"triumphant":25251,"tufts":25252,"##esian":25253,"rotational":25254,"##phus":25255,"328":25256,"falkland":25257,"##brates":25258,"strewn":25259,"clarissa":25260,"rejoin":25261,"environmentally":25262,"glint":25263,"banded":25264,"drenched":25265,"moat":25266,"albanians":25267,"johor":25268,"rr":25269,"maestro":25270,"malley":25271,"nouveau":25272,"shaded":25273,"taxonomy":25274,"v6":25275,"adhere":25276,"bunk":25277,"airfields":25278,"##ritan":25279,"1741":25280,"encompass":25281,"remington":25282,"tran":25283,"##erative":25284,"amelie":25285,"mazda":25286,"friar":25287,"morals":25288,"passions":25289,"##zai":25290,"breadth":25291,"vis":25292,"##hae":25293,"argus":25294,"burnham":25295,"caressing":25296,"insider":25297,"rudd":25298,"##imov":25299,"##mini":25300,"##rso":25301,"italianate":25302,"murderous":25303,"textual":25304,"wainwright":25305,"armada":25306,"bam":25307,"weave":25308,"timer":25309,"##taken":25310,"##nh":25311,"fra":25312,"##crest":25313,"ardent":25314,"salazar":25315,"taps":25316,"tunis":25317,"##ntino":25318,"allegro":25319,"gland":25320,"philanthropic":25321,"##chester":25322,"implication":25323,"##optera":25324,"esq":25325,"judas":25326,"noticeably":25327,"wynn":25328,"##dara":25329,"inched":25330,"indexed":25331,"crises":25332,"villiers":25333,"bandit":25334,"royalties":25335,"patterned":25336,"cupboard":25337,"interspersed":25338,"accessory":25339,"isla":25340,"kendrick":25341,"entourage":25342,"stitches":25343,"##esthesia":25344,"headwaters":25345,"##ior":25346,"interlude":25347,"distraught":25348,"draught":25349,"1727":25350,"##basket":25351,"biased":25352,"sy":25353,"transient":25354,"triad":25355,"subgenus":25356,"adapting":25357,"kidd":25358,"shortstop":25359,"##umatic":25360,"dimly":25361,"spiked":25362,"mcleod":25363,"reprint":25364,"nellie":25365,"pretoria":25366,"windmill":25367,"##cek":25368,"singled":25369,"##mps":25370,"273":25371,"reunite":25372,"##orous":25373,"747":25374,"bankers":25375,"outlying":25376,"##omp":25377,"##ports":25378,"##tream":25379,"apologies":25380,"cosmetics":25381,"patsy":25382,"##deh":25383,"##ocks":25384,"##yson":25385,"bender":25386,"nantes":25387,"serene":25388,"##nad":25389,"lucha":25390,"mmm":25391,"323":25392,"##cius":25393,"##gli":25394,"cmll":25395,"coinage":25396,"nestor":25397,"juarez":25398,"##rook":25399,"smeared":25400,"sprayed":25401,"twitching":25402,"sterile":25403,"irina":25404,"embodied":25405,"juveniles":25406,"enveloped":25407,"miscellaneous":25408,"cancers":25409,"dq":25410,"gulped":25411,"luisa":25412,"crested":25413,"swat":25414,"donegal":25415,"ref":25416,"##anov":25417,"##acker":25418,"hearst":25419,"mercantile":25420,"##lika":25421,"doorbell":25422,"ua":25423,"vicki":25424,"##alla":25425,"##som":25426,"bilbao":25427,"psychologists":25428,"stryker":25429,"sw":25430,"horsemen":25431,"turkmenistan":25432,"wits":25433,"##national":25434,"anson":25435,"mathew":25436,"screenings":25437,"##umb":25438,"rihanna":25439,"##agne":25440,"##nessy":25441,"aisles":25442,"##iani":25443,"##osphere":25444,"hines":25445,"kenton":25446,"saskatoon":25447,"tasha":25448,"truncated":25449,"##champ":25450,"##itan":25451,"mildred":25452,"advises":25453,"fredrik":25454,"interpreting":25455,"inhibitors":25456,"##athi":25457,"spectroscopy":25458,"##hab":25459,"##kong":25460,"karim":25461,"panda":25462,"##oia":25463,"##nail":25464,"##vc":25465,"conqueror":25466,"kgb":25467,"leukemia":25468,"##dity":25469,"arrivals":25470,"cheered":25471,"pisa":25472,"phosphorus":25473,"shielded":25474,"##riated":25475,"mammal":25476,"unitarian":25477,"urgently":25478,"chopin":25479,"sanitary":25480,"##mission":25481,"spicy":25482,"drugged":25483,"hinges":25484,"##tort":25485,"tipping":25486,"trier":25487,"impoverished":25488,"westchester":25489,"##caster":25490,"267":25491,"epoch":25492,"nonstop":25493,"##gman":25494,"##khov":25495,"aromatic":25496,"centrally":25497,"cerro":25498,"##tively":25499,"##vio":25500,"billions":25501,"modulation":25502,"sedimentary":25503,"283":25504,"facilitating":25505,"outrageous":25506,"goldstein":25507,"##eak":25508,"##kt":25509,"ld":25510,"maitland":25511,"penultimate":25512,"pollard":25513,"##dance":25514,"fleets":25515,"spaceship":25516,"vertebrae":25517,"##nig":25518,"alcoholism":25519,"als":25520,"recital":25521,"##bham":25522,"##ference":25523,"##omics":25524,"m2":25525,"##bm":25526,"trois":25527,"##tropical":25528,"##в":25529,"commemorates":25530,"##meric":25531,"marge":25532,"##raction":25533,"1643":25534,"670":25535,"cosmetic":25536,"ravaged":25537,"##ige":25538,"catastrophe":25539,"eng":25540,"##shida":25541,"albrecht":25542,"arterial":25543,"bellamy":25544,"decor":25545,"harmon":25546,"##rde":25547,"bulbs":25548,"synchronized":25549,"vito":25550,"easiest":25551,"shetland":25552,"shielding":25553,"wnba":25554,"##glers":25555,"##ssar":25556,"##riam":25557,"brianna":25558,"cumbria":25559,"##aceous":25560,"##rard":25561,"cores":25562,"thayer":25563,"##nsk":25564,"brood":25565,"hilltop":25566,"luminous":25567,"carts":25568,"keynote":25569,"larkin":25570,"logos":25571,"##cta":25572,"##ا":25573,"##mund":25574,"##quay":25575,"lilith":25576,"tinted":25577,"277":25578,"wrestle":25579,"mobilization":25580,"##uses":25581,"sequential":25582,"siam":25583,"bloomfield":25584,"takahashi":25585,"274":25586,"##ieving":25587,"presenters":25588,"ringo":25589,"blazed":25590,"witty":25591,"##oven":25592,"##ignant":25593,"devastation":25594,"haydn":25595,"harmed":25596,"newt":25597,"therese":25598,"##peed":25599,"gershwin":25600,"molina":25601,"rabbis":25602,"sudanese":25603,"001":25604,"innate":25605,"restarted":25606,"##sack":25607,"##fus":25608,"slices":25609,"wb":25610,"##shah":25611,"enroll":25612,"hypothetical":25613,"hysterical":25614,"1743":25615,"fabio":25616,"indefinite":25617,"warped":25618,"##hg":25619,"exchanging":25620,"525":25621,"unsuitable":25622,"##sboro":25623,"gallo":25624,"1603":25625,"bret":25626,"cobalt":25627,"homemade":25628,"##hunter":25629,"mx":25630,"operatives":25631,"##dhar":25632,"terraces":25633,"durable":25634,"latch":25635,"pens":25636,"whorls":25637,"##ctuated":25638,"##eaux":25639,"billing":25640,"ligament":25641,"succumbed":25642,"##gly":25643,"regulators":25644,"spawn":25645,"##brick":25646,"##stead":25647,"filmfare":25648,"rochelle":25649,"##nzo":25650,"1725":25651,"circumstance":25652,"saber":25653,"supplements":25654,"##nsky":25655,"##tson":25656,"crowe":25657,"wellesley":25658,"carrot":25659,"##9th":25660,"##movable":25661,"primate":25662,"drury":25663,"sincerely":25664,"topical":25665,"##mad":25666,"##rao":25667,"callahan":25668,"kyiv":25669,"smarter":25670,"tits":25671,"undo":25672,"##yeh":25673,"announcements":25674,"anthologies":25675,"barrio":25676,"nebula":25677,"##islaus":25678,"##shaft":25679,"##tyn":25680,"bodyguards":25681,"2021":25682,"assassinate":25683,"barns":25684,"emmett":25685,"scully":25686,"##mah":25687,"##yd":25688,"##eland":25689,"##tino":25690,"##itarian":25691,"demoted":25692,"gorman":25693,"lashed":25694,"prized":25695,"adventist":25696,"writ":25697,"##gui":25698,"alla":25699,"invertebrates":25700,"##ausen":25701,"1641":25702,"amman":25703,"1742":25704,"align":25705,"healy":25706,"redistribution":25707,"##gf":25708,"##rize":25709,"insulation":25710,"##drop":25711,"adherents":25712,"hezbollah":25713,"vitro":25714,"ferns":25715,"yanking":25716,"269":25717,"php":25718,"registering":25719,"uppsala":25720,"cheerleading":25721,"confines":25722,"mischievous":25723,"tully":25724,"##ross":25725,"49th":25726,"docked":25727,"roam":25728,"stipulated":25729,"pumpkin":25730,"##bry":25731,"prompt":25732,"##ezer":25733,"blindly":25734,"shuddering":25735,"craftsmen":25736,"frail":25737,"scented":25738,"katharine":25739,"scramble":25740,"shaggy":25741,"sponge":25742,"helix":25743,"zaragoza":25744,"279":25745,"##52":25746,"43rd":25747,"backlash":25748,"fontaine":25749,"seizures":25750,"posse":25751,"cowan":25752,"nonfiction":25753,"telenovela":25754,"wwii":25755,"hammered":25756,"undone":25757,"##gpur":25758,"encircled":25759,"irs":25760,"##ivation":25761,"artefacts":25762,"oneself":25763,"searing":25764,"smallpox":25765,"##belle":25766,"##osaurus":25767,"shandong":25768,"breached":25769,"upland":25770,"blushing":25771,"rankin":25772,"infinitely":25773,"psyche":25774,"tolerated":25775,"docking":25776,"evicted":25777,"##col":25778,"unmarked":25779,"##lving":25780,"gnome":25781,"lettering":25782,"litres":25783,"musique":25784,"##oint":25785,"benevolent":25786,"##jal":25787,"blackened":25788,"##anna":25789,"mccall":25790,"racers":25791,"tingle":25792,"##ocene":25793,"##orestation":25794,"introductions":25795,"radically":25796,"292":25797,"##hiff":25798,"##باد":25799,"1610":25800,"1739":25801,"munchen":25802,"plead":25803,"##nka":25804,"condo":25805,"scissors":25806,"##sight":25807,"##tens":25808,"apprehension":25809,"##cey":25810,"##yin":25811,"hallmark":25812,"watering":25813,"formulas":25814,"sequels":25815,"##llas":25816,"aggravated":25817,"bae":25818,"commencing":25819,"##building":25820,"enfield":25821,"prohibits":25822,"marne":25823,"vedic":25824,"civilized":25825,"euclidean":25826,"jagger":25827,"beforehand":25828,"blasts":25829,"dumont":25830,"##arney":25831,"##nem":25832,"740":25833,"conversions":25834,"hierarchical":25835,"rios":25836,"simulator":25837,"##dya":25838,"##lellan":25839,"hedges":25840,"oleg":25841,"thrusts":25842,"shadowed":25843,"darby":25844,"maximize":25845,"1744":25846,"gregorian":25847,"##nded":25848,"##routed":25849,"sham":25850,"unspecified":25851,"##hog":25852,"emory":25853,"factual":25854,"##smo":25855,"##tp":25856,"fooled":25857,"##rger":25858,"ortega":25859,"wellness":25860,"marlon":25861,"##oton":25862,"##urance":25863,"casket":25864,"keating":25865,"ley":25866,"enclave":25867,"##ayan":25868,"char":25869,"influencing":25870,"jia":25871,"##chenko":25872,"412":25873,"ammonia":25874,"erebidae":25875,"incompatible":25876,"violins":25877,"cornered":25878,"##arat":25879,"grooves":25880,"astronauts":25881,"columbian":25882,"rampant":25883,"fabrication":25884,"kyushu":25885,"mahmud":25886,"vanish":25887,"##dern":25888,"mesopotamia":25889,"##lete":25890,"ict":25891,"##rgen":25892,"caspian":25893,"kenji":25894,"pitted":25895,"##vered":25896,"999":25897,"grimace":25898,"roanoke":25899,"tchaikovsky":25900,"twinned":25901,"##analysis":25902,"##awan":25903,"xinjiang":25904,"arias":25905,"clemson":25906,"kazakh":25907,"sizable":25908,"1662":25909,"##khand":25910,"##vard":25911,"plunge":25912,"tatum":25913,"vittorio":25914,"##nden":25915,"cholera":25916,"##dana":25917,"##oper":25918,"bracing":25919,"indifference":25920,"projectile":25921,"superliga":25922,"##chee":25923,"realises":25924,"upgrading":25925,"299":25926,"porte":25927,"retribution":25928,"##vies":25929,"nk":25930,"stil":25931,"##resses":25932,"ama":25933,"bureaucracy":25934,"blackberry":25935,"bosch":25936,"testosterone":25937,"collapses":25938,"greer":25939,"##pathic":25940,"ioc":25941,"fifties":25942,"malls":25943,"##erved":25944,"bao":25945,"baskets":25946,"adolescents":25947,"siegfried":25948,"##osity":25949,"##tosis":25950,"mantra":25951,"detecting":25952,"existent":25953,"fledgling":25954,"##cchi":25955,"dissatisfied":25956,"gan":25957,"telecommunication":25958,"mingled":25959,"sobbed":25960,"6000":25961,"controversies":25962,"outdated":25963,"taxis":25964,"##raus":25965,"fright":25966,"slams":25967,"##lham":25968,"##fect":25969,"##tten":25970,"detectors":25971,"fetal":25972,"tanned":25973,"##uw":25974,"fray":25975,"goth":25976,"olympian":25977,"skipping":25978,"mandates":25979,"scratches":25980,"sheng":25981,"unspoken":25982,"hyundai":25983,"tracey":25984,"hotspur":25985,"restrictive":25986,"##buch":25987,"americana":25988,"mundo":25989,"##bari":25990,"burroughs":25991,"diva":25992,"vulcan":25993,"##6th":25994,"distinctions":25995,"thumping":25996,"##ngen":25997,"mikey":25998,"sheds":25999,"fide":26000,"rescues":26001,"springsteen":26002,"vested":26003,"valuation":26004,"##ece":26005,"##ely":26006,"pinnacle":26007,"rake":26008,"sylvie":26009,"##edo":26010,"almond":26011,"quivering":26012,"##irus":26013,"alteration":26014,"faltered":26015,"##wad":26016,"51st":26017,"hydra":26018,"ticked":26019,"##kato":26020,"recommends":26021,"##dicated":26022,"antigua":26023,"arjun":26024,"stagecoach":26025,"wilfred":26026,"trickle":26027,"pronouns":26028,"##pon":26029,"aryan":26030,"nighttime":26031,"##anian":26032,"gall":26033,"pea":26034,"stitch":26035,"##hei":26036,"leung":26037,"milos":26038,"##dini":26039,"eritrea":26040,"nexus":26041,"starved":26042,"snowfall":26043,"kant":26044,"parasitic":26045,"cot":26046,"discus":26047,"hana":26048,"strikers":26049,"appleton":26050,"kitchens":26051,"##erina":26052,"##partisan":26053,"##itha":26054,"##vius":26055,"disclose":26056,"metis":26057,"##channel":26058,"1701":26059,"tesla":26060,"##vera":26061,"fitch":26062,"1735":26063,"blooded":26064,"##tila":26065,"decimal":26066,"##tang":26067,"##bai":26068,"cyclones":26069,"eun":26070,"bottled":26071,"peas":26072,"pensacola":26073,"basha":26074,"bolivian":26075,"crabs":26076,"boil":26077,"lanterns":26078,"partridge":26079,"roofed":26080,"1645":26081,"necks":26082,"##phila":26083,"opined":26084,"patting":26085,"##kla":26086,"##lland":26087,"chuckles":26088,"volta":26089,"whereupon":26090,"##nche":26091,"devout":26092,"euroleague":26093,"suicidal":26094,"##dee":26095,"inherently":26096,"involuntary":26097,"knitting":26098,"nasser":26099,"##hide":26100,"puppets":26101,"colourful":26102,"courageous":26103,"southend":26104,"stills":26105,"miraculous":26106,"hodgson":26107,"richer":26108,"rochdale":26109,"ethernet":26110,"greta":26111,"uniting":26112,"prism":26113,"umm":26114,"##haya":26115,"##itical":26116,"##utation":26117,"deterioration":26118,"pointe":26119,"prowess":26120,"##ropriation":26121,"lids":26122,"scranton":26123,"billings":26124,"subcontinent":26125,"##koff":26126,"##scope":26127,"brute":26128,"kellogg":26129,"psalms":26130,"degraded":26131,"##vez":26132,"stanisław":26133,"##ructured":26134,"ferreira":26135,"pun":26136,"astonishing":26137,"gunnar":26138,"##yat":26139,"arya":26140,"prc":26141,"gottfried":26142,"##tight":26143,"excursion":26144,"##ographer":26145,"dina":26146,"##quil":26147,"##nare":26148,"huffington":26149,"illustrious":26150,"wilbur":26151,"gundam":26152,"verandah":26153,"##zard":26154,"naacp":26155,"##odle":26156,"constructive":26157,"fjord":26158,"kade":26159,"##naud":26160,"generosity":26161,"thrilling":26162,"baseline":26163,"cayman":26164,"frankish":26165,"plastics":26166,"accommodations":26167,"zoological":26168,"##fting":26169,"cedric":26170,"qb":26171,"motorized":26172,"##dome":26173,"##otted":26174,"squealed":26175,"tackled":26176,"canucks":26177,"budgets":26178,"situ":26179,"asthma":26180,"dail":26181,"gabled":26182,"grasslands":26183,"whimpered":26184,"writhing":26185,"judgments":26186,"##65":26187,"minnie":26188,"pv":26189,"##carbon":26190,"bananas":26191,"grille":26192,"domes":26193,"monique":26194,"odin":26195,"maguire":26196,"markham":26197,"tierney":26198,"##estra":26199,"##chua":26200,"libel":26201,"poke":26202,"speedy":26203,"atrium":26204,"laval":26205,"notwithstanding":26206,"##edly":26207,"fai":26208,"kala":26209,"##sur":26210,"robb":26211,"##sma":26212,"listings":26213,"luz":26214,"supplementary":26215,"tianjin":26216,"##acing":26217,"enzo":26218,"jd":26219,"ric":26220,"scanner":26221,"croats":26222,"transcribed":26223,"##49":26224,"arden":26225,"cv":26226,"##hair":26227,"##raphy":26228,"##lver":26229,"##uy":26230,"357":26231,"seventies":26232,"staggering":26233,"alam":26234,"horticultural":26235,"hs":26236,"regression":26237,"timbers":26238,"blasting":26239,"##ounded":26240,"montagu":26241,"manipulating":26242,"##cit":26243,"catalytic":26244,"1550":26245,"troopers":26246,"##meo":26247,"condemnation":26248,"fitzpatrick":26249,"##oire":26250,"##roved":26251,"inexperienced":26252,"1670":26253,"castes":26254,"##lative":26255,"outing":26256,"314":26257,"dubois":26258,"flicking":26259,"quarrel":26260,"ste":26261,"learners":26262,"1625":26263,"iq":26264,"whistled":26265,"##class":26266,"282":26267,"classify":26268,"tariffs":26269,"temperament":26270,"355":26271,"folly":26272,"liszt":26273,"##yles":26274,"immersed":26275,"jordanian":26276,"ceasefire":26277,"apparel":26278,"extras":26279,"maru":26280,"fished":26281,"##bio":26282,"harta":26283,"stockport":26284,"assortment":26285,"craftsman":26286,"paralysis":26287,"transmitters":26288,"##cola":26289,"blindness":26290,"##wk":26291,"fatally":26292,"proficiency":26293,"solemnly":26294,"##orno":26295,"repairing":26296,"amore":26297,"groceries":26298,"ultraviolet":26299,"##chase":26300,"schoolhouse":26301,"##tua":26302,"resurgence":26303,"nailed":26304,"##otype":26305,"##×":26306,"ruse":26307,"saliva":26308,"diagrams":26309,"##tructing":26310,"albans":26311,"rann":26312,"thirties":26313,"1b":26314,"antennas":26315,"hilarious":26316,"cougars":26317,"paddington":26318,"stats":26319,"##eger":26320,"breakaway":26321,"ipod":26322,"reza":26323,"authorship":26324,"prohibiting":26325,"scoffed":26326,"##etz":26327,"##ttle":26328,"conscription":26329,"defected":26330,"trondheim":26331,"##fires":26332,"ivanov":26333,"keenan":26334,"##adan":26335,"##ciful":26336,"##fb":26337,"##slow":26338,"locating":26339,"##ials":26340,"##tford":26341,"cadiz":26342,"basalt":26343,"blankly":26344,"interned":26345,"rags":26346,"rattling":26347,"##tick":26348,"carpathian":26349,"reassured":26350,"sync":26351,"bum":26352,"guildford":26353,"iss":26354,"staunch":26355,"##onga":26356,"astronomers":26357,"sera":26358,"sofie":26359,"emergencies":26360,"susquehanna":26361,"##heard":26362,"duc":26363,"mastery":26364,"vh1":26365,"williamsburg":26366,"bayer":26367,"buckled":26368,"craving":26369,"##khan":26370,"##rdes":26371,"bloomington":26372,"##write":26373,"alton":26374,"barbecue":26375,"##bians":26376,"justine":26377,"##hri":26378,"##ndt":26379,"delightful":26380,"smartphone":26381,"newtown":26382,"photon":26383,"retrieval":26384,"peugeot":26385,"hissing":26386,"##monium":26387,"##orough":26388,"flavors":26389,"lighted":26390,"relaunched":26391,"tainted":26392,"##games":26393,"##lysis":26394,"anarchy":26395,"microscopic":26396,"hopping":26397,"adept":26398,"evade":26399,"evie":26400,"##beau":26401,"inhibit":26402,"sinn":26403,"adjustable":26404,"hurst":26405,"intuition":26406,"wilton":26407,"cisco":26408,"44th":26409,"lawful":26410,"lowlands":26411,"stockings":26412,"thierry":26413,"##dalen":26414,"##hila":26415,"##nai":26416,"fates":26417,"prank":26418,"tb":26419,"maison":26420,"lobbied":26421,"provocative":26422,"1724":26423,"4a":26424,"utopia":26425,"##qual":26426,"carbonate":26427,"gujarati":26428,"purcell":26429,"##rford":26430,"curtiss":26431,"##mei":26432,"overgrown":26433,"arenas":26434,"mediation":26435,"swallows":26436,"##rnik":26437,"respectful":26438,"turnbull":26439,"##hedron":26440,"##hope":26441,"alyssa":26442,"ozone":26443,"##ʻi":26444,"ami":26445,"gestapo":26446,"johansson":26447,"snooker":26448,"canteen":26449,"cuff":26450,"declines":26451,"empathy":26452,"stigma":26453,"##ags":26454,"##iner":26455,"##raine":26456,"taxpayers":26457,"gui":26458,"volga":26459,"##wright":26460,"##copic":26461,"lifespan":26462,"overcame":26463,"tattooed":26464,"enactment":26465,"giggles":26466,"##ador":26467,"##camp":26468,"barrington":26469,"bribe":26470,"obligatory":26471,"orbiting":26472,"peng":26473,"##enas":26474,"elusive":26475,"sucker":26476,"##vating":26477,"cong":26478,"hardship":26479,"empowered":26480,"anticipating":26481,"estrada":26482,"cryptic":26483,"greasy":26484,"detainees":26485,"planck":26486,"sudbury":26487,"plaid":26488,"dod":26489,"marriott":26490,"kayla":26491,"##ears":26492,"##vb":26493,"##zd":26494,"mortally":26495,"##hein":26496,"cognition":26497,"radha":26498,"319":26499,"liechtenstein":26500,"meade":26501,"richly":26502,"argyle":26503,"harpsichord":26504,"liberalism":26505,"trumpets":26506,"lauded":26507,"tyrant":26508,"salsa":26509,"tiled":26510,"lear":26511,"promoters":26512,"reused":26513,"slicing":26514,"trident":26515,"##chuk":26516,"##gami":26517,"##lka":26518,"cantor":26519,"checkpoint":26520,"##points":26521,"gaul":26522,"leger":26523,"mammalian":26524,"##tov":26525,"##aar":26526,"##schaft":26527,"doha":26528,"frenchman":26529,"nirvana":26530,"##vino":26531,"delgado":26532,"headlining":26533,"##eron":26534,"##iography":26535,"jug":26536,"tko":26537,"1649":26538,"naga":26539,"intersections":26540,"##jia":26541,"benfica":26542,"nawab":26543,"##suka":26544,"ashford":26545,"gulp":26546,"##deck":26547,"##vill":26548,"##rug":26549,"brentford":26550,"frazier":26551,"pleasures":26552,"dunne":26553,"potsdam":26554,"shenzhen":26555,"dentistry":26556,"##tec":26557,"flanagan":26558,"##dorff":26559,"##hear":26560,"chorale":26561,"dinah":26562,"prem":26563,"quezon":26564,"##rogated":26565,"relinquished":26566,"sutra":26567,"terri":26568,"##pani":26569,"flaps":26570,"##rissa":26571,"poly":26572,"##rnet":26573,"homme":26574,"aback":26575,"##eki":26576,"linger":26577,"womb":26578,"##kson":26579,"##lewood":26580,"doorstep":26581,"orthodoxy":26582,"threaded":26583,"westfield":26584,"##rval":26585,"dioceses":26586,"fridays":26587,"subsided":26588,"##gata":26589,"loyalists":26590,"##biotic":26591,"##ettes":26592,"letterman":26593,"lunatic":26594,"prelate":26595,"tenderly":26596,"invariably":26597,"souza":26598,"thug":26599,"winslow":26600,"##otide":26601,"furlongs":26602,"gogh":26603,"jeopardy":26604,"##runa":26605,"pegasus":26606,"##umble":26607,"humiliated":26608,"standalone":26609,"tagged":26610,"##roller":26611,"freshmen":26612,"klan":26613,"##bright":26614,"attaining":26615,"initiating":26616,"transatlantic":26617,"logged":26618,"viz":26619,"##uance":26620,"1723":26621,"combatants":26622,"intervening":26623,"stephane":26624,"chieftain":26625,"despised":26626,"grazed":26627,"317":26628,"cdc":26629,"galveston":26630,"godzilla":26631,"macro":26632,"simulate":26633,"##planes":26634,"parades":26635,"##esses":26636,"960":26637,"##ductive":26638,"##unes":26639,"equator":26640,"overdose":26641,"##cans":26642,"##hosh":26643,"##lifting":26644,"joshi":26645,"epstein":26646,"sonora":26647,"treacherous":26648,"aquatics":26649,"manchu":26650,"responsive":26651,"##sation":26652,"supervisory":26653,"##christ":26654,"##llins":26655,"##ibar":26656,"##balance":26657,"##uso":26658,"kimball":26659,"karlsruhe":26660,"mab":26661,"##emy":26662,"ignores":26663,"phonetic":26664,"reuters":26665,"spaghetti":26666,"820":26667,"almighty":26668,"danzig":26669,"rumbling":26670,"tombstone":26671,"designations":26672,"lured":26673,"outset":26674,"##felt":26675,"supermarkets":26676,"##wt":26677,"grupo":26678,"kei":26679,"kraft":26680,"susanna":26681,"##blood":26682,"comprehension":26683,"genealogy":26684,"##aghan":26685,"##verted":26686,"redding":26687,"##ythe":26688,"1722":26689,"bowing":26690,"##pore":26691,"##roi":26692,"lest":26693,"sharpened":26694,"fulbright":26695,"valkyrie":26696,"sikhs":26697,"##unds":26698,"swans":26699,"bouquet":26700,"merritt":26701,"##tage":26702,"##venting":26703,"commuted":26704,"redhead":26705,"clerks":26706,"leasing":26707,"cesare":26708,"dea":26709,"hazy":26710,"##vances":26711,"fledged":26712,"greenfield":26713,"servicemen":26714,"##gical":26715,"armando":26716,"blackout":26717,"dt":26718,"sagged":26719,"downloadable":26720,"intra":26721,"potion":26722,"pods":26723,"##4th":26724,"##mism":26725,"xp":26726,"attendants":26727,"gambia":26728,"stale":26729,"##ntine":26730,"plump":26731,"asteroids":26732,"rediscovered":26733,"buds":26734,"flea":26735,"hive":26736,"##neas":26737,"1737":26738,"classifications":26739,"debuts":26740,"##eles":26741,"olympus":26742,"scala":26743,"##eurs":26744,"##gno":26745,"##mute":26746,"hummed":26747,"sigismund":26748,"visuals":26749,"wiggled":26750,"await":26751,"pilasters":26752,"clench":26753,"sulfate":26754,"##ances":26755,"bellevue":26756,"enigma":26757,"trainee":26758,"snort":26759,"##sw":26760,"clouded":26761,"denim":26762,"##rank":26763,"##rder":26764,"churning":26765,"hartman":26766,"lodges":26767,"riches":26768,"sima":26769,"##missible":26770,"accountable":26771,"socrates":26772,"regulates":26773,"mueller":26774,"##cr":26775,"1702":26776,"avoids":26777,"solids":26778,"himalayas":26779,"nutrient":26780,"pup":26781,"##jevic":26782,"squat":26783,"fades":26784,"nec":26785,"##lates":26786,"##pina":26787,"##rona":26788,"##ου":26789,"privateer":26790,"tequila":26791,"##gative":26792,"##mpton":26793,"apt":26794,"hornet":26795,"immortals":26796,"##dou":26797,"asturias":26798,"cleansing":26799,"dario":26800,"##rries":26801,"##anta":26802,"etymology":26803,"servicing":26804,"zhejiang":26805,"##venor":26806,"##nx":26807,"horned":26808,"erasmus":26809,"rayon":26810,"relocating":26811,"£10":26812,"##bags":26813,"escalated":26814,"promenade":26815,"stubble":26816,"2010s":26817,"artisans":26818,"axial":26819,"liquids":26820,"mora":26821,"sho":26822,"yoo":26823,"##tsky":26824,"bundles":26825,"oldies":26826,"##nally":26827,"notification":26828,"bastion":26829,"##ths":26830,"sparkle":26831,"##lved":26832,"1728":26833,"leash":26834,"pathogen":26835,"highs":26836,"##hmi":26837,"immature":26838,"880":26839,"gonzaga":26840,"ignatius":26841,"mansions":26842,"monterrey":26843,"sweets":26844,"bryson":26845,"##loe":26846,"polled":26847,"regatta":26848,"brightest":26849,"pei":26850,"rosy":26851,"squid":26852,"hatfield":26853,"payroll":26854,"addict":26855,"meath":26856,"cornerback":26857,"heaviest":26858,"lodging":26859,"##mage":26860,"capcom":26861,"rippled":26862,"##sily":26863,"barnet":26864,"mayhem":26865,"ymca":26866,"snuggled":26867,"rousseau":26868,"##cute":26869,"blanchard":26870,"284":26871,"fragmented":26872,"leighton":26873,"chromosomes":26874,"risking":26875,"##md":26876,"##strel":26877,"##utter":26878,"corinne":26879,"coyotes":26880,"cynical":26881,"hiroshi":26882,"yeomanry":26883,"##ractive":26884,"ebook":26885,"grading":26886,"mandela":26887,"plume":26888,"agustin":26889,"magdalene":26890,"##rkin":26891,"bea":26892,"femme":26893,"trafford":26894,"##coll":26895,"##lun":26896,"##tance":26897,"52nd":26898,"fourier":26899,"upton":26900,"##mental":26901,"camilla":26902,"gust":26903,"iihf":26904,"islamabad":26905,"longevity":26906,"##kala":26907,"feldman":26908,"netting":26909,"##rization":26910,"endeavour":26911,"foraging":26912,"mfa":26913,"orr":26914,"##open":26915,"greyish":26916,"contradiction":26917,"graz":26918,"##ruff":26919,"handicapped":26920,"marlene":26921,"tweed":26922,"oaxaca":26923,"spp":26924,"campos":26925,"miocene":26926,"pri":26927,"configured":26928,"cooks":26929,"pluto":26930,"cozy":26931,"pornographic":26932,"##entes":26933,"70th":26934,"fairness":26935,"glided":26936,"jonny":26937,"lynne":26938,"rounding":26939,"sired":26940,"##emon":26941,"##nist":26942,"remade":26943,"uncover":26944,"##mack":26945,"complied":26946,"lei":26947,"newsweek":26948,"##jured":26949,"##parts":26950,"##enting":26951,"##pg":26952,"293":26953,"finer":26954,"guerrillas":26955,"athenian":26956,"deng":26957,"disused":26958,"stepmother":26959,"accuse":26960,"gingerly":26961,"seduction":26962,"521":26963,"confronting":26964,"##walker":26965,"##going":26966,"gora":26967,"nostalgia":26968,"sabres":26969,"virginity":26970,"wrenched":26971,"##minated":26972,"syndication":26973,"wielding":26974,"eyre":26975,"##56":26976,"##gnon":26977,"##igny":26978,"behaved":26979,"taxpayer":26980,"sweeps":26981,"##growth":26982,"childless":26983,"gallant":26984,"##ywood":26985,"amplified":26986,"geraldine":26987,"scrape":26988,"##ffi":26989,"babylonian":26990,"fresco":26991,"##rdan":26992,"##kney":26993,"##position":26994,"1718":26995,"restricting":26996,"tack":26997,"fukuoka":26998,"osborn":26999,"selector":27000,"partnering":27001,"##dlow":27002,"318":27003,"gnu":27004,"kia":27005,"tak":27006,"whitley":27007,"gables":27008,"##54":27009,"##mania":27010,"mri":27011,"softness":27012,"immersion":27013,"##bots":27014,"##evsky":27015,"1713":27016,"chilling":27017,"insignificant":27018,"pcs":27019,"##uis":27020,"elites":27021,"lina":27022,"purported":27023,"supplemental":27024,"teaming":27025,"##americana":27026,"##dding":27027,"##inton":27028,"proficient":27029,"rouen":27030,"##nage":27031,"##rret":27032,"niccolo":27033,"selects":27034,"##bread":27035,"fluffy":27036,"1621":27037,"gruff":27038,"knotted":27039,"mukherjee":27040,"polgara":27041,"thrash":27042,"nicholls":27043,"secluded":27044,"smoothing":27045,"thru":27046,"corsica":27047,"loaf":27048,"whitaker":27049,"inquiries":27050,"##rrier":27051,"##kam":27052,"indochina":27053,"289":27054,"marlins":27055,"myles":27056,"peking":27057,"##tea":27058,"extracts":27059,"pastry":27060,"superhuman":27061,"connacht":27062,"vogel":27063,"##ditional":27064,"##het":27065,"##udged":27066,"##lash":27067,"gloss":27068,"quarries":27069,"refit":27070,"teaser":27071,"##alic":27072,"##gaon":27073,"20s":27074,"materialized":27075,"sling":27076,"camped":27077,"pickering":27078,"tung":27079,"tracker":27080,"pursuant":27081,"##cide":27082,"cranes":27083,"soc":27084,"##cini":27085,"##typical":27086,"##viere":27087,"anhalt":27088,"overboard":27089,"workout":27090,"chores":27091,"fares":27092,"orphaned":27093,"stains":27094,"##logie":27095,"fenton":27096,"surpassing":27097,"joyah":27098,"triggers":27099,"##itte":27100,"grandmaster":27101,"##lass":27102,"##lists":27103,"clapping":27104,"fraudulent":27105,"ledger":27106,"nagasaki":27107,"##cor":27108,"##nosis":27109,"##tsa":27110,"eucalyptus":27111,"tun":27112,"##icio":27113,"##rney":27114,"##tara":27115,"dax":27116,"heroism":27117,"ina":27118,"wrexham":27119,"onboard":27120,"unsigned":27121,"##dates":27122,"moshe":27123,"galley":27124,"winnie":27125,"droplets":27126,"exiles":27127,"praises":27128,"watered":27129,"noodles":27130,"##aia":27131,"fein":27132,"adi":27133,"leland":27134,"multicultural":27135,"stink":27136,"bingo":27137,"comets":27138,"erskine":27139,"modernized":27140,"canned":27141,"constraint":27142,"domestically":27143,"chemotherapy":27144,"featherweight":27145,"stifled":27146,"##mum":27147,"darkly":27148,"irresistible":27149,"refreshing":27150,"hasty":27151,"isolate":27152,"##oys":27153,"kitchener":27154,"planners":27155,"##wehr":27156,"cages":27157,"yarn":27158,"implant":27159,"toulon":27160,"elects":27161,"childbirth":27162,"yue":27163,"##lind":27164,"##lone":27165,"cn":27166,"rightful":27167,"sportsman":27168,"junctions":27169,"remodeled":27170,"specifies":27171,"##rgh":27172,"291":27173,"##oons":27174,"complimented":27175,"##urgent":27176,"lister":27177,"ot":27178,"##logic":27179,"bequeathed":27180,"cheekbones":27181,"fontana":27182,"gabby":27183,"##dial":27184,"amadeus":27185,"corrugated":27186,"maverick":27187,"resented":27188,"triangles":27189,"##hered":27190,"##usly":27191,"nazareth":27192,"tyrol":27193,"1675":27194,"assent":27195,"poorer":27196,"sectional":27197,"aegean":27198,"##cous":27199,"296":27200,"nylon":27201,"ghanaian":27202,"##egorical":27203,"##weig":27204,"cushions":27205,"forbid":27206,"fusiliers":27207,"obstruction":27208,"somerville":27209,"##scia":27210,"dime":27211,"earrings":27212,"elliptical":27213,"leyte":27214,"oder":27215,"polymers":27216,"timmy":27217,"atm":27218,"midtown":27219,"piloted":27220,"settles":27221,"continual":27222,"externally":27223,"mayfield":27224,"##uh":27225,"enrichment":27226,"henson":27227,"keane":27228,"persians":27229,"1733":27230,"benji":27231,"braden":27232,"pep":27233,"324":27234,"##efe":27235,"contenders":27236,"pepsi":27237,"valet":27238,"##isches":27239,"298":27240,"##asse":27241,"##earing":27242,"goofy":27243,"stroll":27244,"##amen":27245,"authoritarian":27246,"occurrences":27247,"adversary":27248,"ahmedabad":27249,"tangent":27250,"toppled":27251,"dorchester":27252,"1672":27253,"modernism":27254,"marxism":27255,"islamist":27256,"charlemagne":27257,"exponential":27258,"racks":27259,"unicode":27260,"brunette":27261,"mbc":27262,"pic":27263,"skirmish":27264,"##bund":27265,"##lad":27266,"##powered":27267,"##yst":27268,"hoisted":27269,"messina":27270,"shatter":27271,"##ctum":27272,"jedi":27273,"vantage":27274,"##music":27275,"##neil":27276,"clemens":27277,"mahmoud":27278,"corrupted":27279,"authentication":27280,"lowry":27281,"nils":27282,"##washed":27283,"omnibus":27284,"wounding":27285,"jillian":27286,"##itors":27287,"##opped":27288,"serialized":27289,"narcotics":27290,"handheld":27291,"##arm":27292,"##plicity":27293,"intersecting":27294,"stimulating":27295,"##onis":27296,"crate":27297,"fellowships":27298,"hemingway":27299,"casinos":27300,"climatic":27301,"fordham":27302,"copeland":27303,"drip":27304,"beatty":27305,"leaflets":27306,"robber":27307,"brothel":27308,"madeira":27309,"##hedral":27310,"sphinx":27311,"ultrasound":27312,"##vana":27313,"valor":27314,"forbade":27315,"leonid":27316,"villas":27317,"##aldo":27318,"duane":27319,"marquez":27320,"##cytes":27321,"disadvantaged":27322,"forearms":27323,"kawasaki":27324,"reacts":27325,"consular":27326,"lax":27327,"uncles":27328,"uphold":27329,"##hopper":27330,"concepcion":27331,"dorsey":27332,"lass":27333,"##izan":27334,"arching":27335,"passageway":27336,"1708":27337,"researches":27338,"tia":27339,"internationals":27340,"##graphs":27341,"##opers":27342,"distinguishes":27343,"javanese":27344,"divert":27345,"##uven":27346,"plotted":27347,"##listic":27348,"##rwin":27349,"##erik":27350,"##tify":27351,"affirmative":27352,"signifies":27353,"validation":27354,"##bson":27355,"kari":27356,"felicity":27357,"georgina":27358,"zulu":27359,"##eros":27360,"##rained":27361,"##rath":27362,"overcoming":27363,"##dot":27364,"argyll":27365,"##rbin":27366,"1734":27367,"chiba":27368,"ratification":27369,"windy":27370,"earls":27371,"parapet":27372,"##marks":27373,"hunan":27374,"pristine":27375,"astrid":27376,"punta":27377,"##gart":27378,"brodie":27379,"##kota":27380,"##oder":27381,"malaga":27382,"minerva":27383,"rouse":27384,"##phonic":27385,"bellowed":27386,"pagoda":27387,"portals":27388,"reclamation":27389,"##gur":27390,"##odies":27391,"##⁄₄":27392,"parentheses":27393,"quoting":27394,"allergic":27395,"palette":27396,"showcases":27397,"benefactor":27398,"heartland":27399,"nonlinear":27400,"##tness":27401,"bladed":27402,"cheerfully":27403,"scans":27404,"##ety":27405,"##hone":27406,"1666":27407,"girlfriends":27408,"pedersen":27409,"hiram":27410,"sous":27411,"##liche":27412,"##nator":27413,"1683":27414,"##nery":27415,"##orio":27416,"##umen":27417,"bobo":27418,"primaries":27419,"smiley":27420,"##cb":27421,"unearthed":27422,"uniformly":27423,"fis":27424,"metadata":27425,"1635":27426,"ind":27427,"##oted":27428,"recoil":27429,"##titles":27430,"##tura":27431,"##ια":27432,"406":27433,"hilbert":27434,"jamestown":27435,"mcmillan":27436,"tulane":27437,"seychelles":27438,"##frid":27439,"antics":27440,"coli":27441,"fated":27442,"stucco":27443,"##grants":27444,"1654":27445,"bulky":27446,"accolades":27447,"arrays":27448,"caledonian":27449,"carnage":27450,"optimism":27451,"puebla":27452,"##tative":27453,"##cave":27454,"enforcing":27455,"rotherham":27456,"seo":27457,"dunlop":27458,"aeronautics":27459,"chimed":27460,"incline":27461,"zoning":27462,"archduke":27463,"hellenistic":27464,"##oses":27465,"##sions":27466,"candi":27467,"thong":27468,"##ople":27469,"magnate":27470,"rustic":27471,"##rsk":27472,"projective":27473,"slant":27474,"##offs":27475,"danes":27476,"hollis":27477,"vocalists":27478,"##ammed":27479,"congenital":27480,"contend":27481,"gesellschaft":27482,"##ocating":27483,"##pressive":27484,"douglass":27485,"quieter":27486,"##cm":27487,"##kshi":27488,"howled":27489,"salim":27490,"spontaneously":27491,"townsville":27492,"buena":27493,"southport":27494,"##bold":27495,"kato":27496,"1638":27497,"faerie":27498,"stiffly":27499,"##vus":27500,"##rled":27501,"297":27502,"flawless":27503,"realising":27504,"taboo":27505,"##7th":27506,"bytes":27507,"straightening":27508,"356":27509,"jena":27510,"##hid":27511,"##rmin":27512,"cartwright":27513,"berber":27514,"bertram":27515,"soloists":27516,"411":27517,"noses":27518,"417":27519,"coping":27520,"fission":27521,"hardin":27522,"inca":27523,"##cen":27524,"1717":27525,"mobilized":27526,"vhf":27527,"##raf":27528,"biscuits":27529,"curate":27530,"##85":27531,"##anial":27532,"331":27533,"gaunt":27534,"neighbourhoods":27535,"1540":27536,"##abas":27537,"blanca":27538,"bypassed":27539,"sockets":27540,"behold":27541,"coincidentally":27542,"##bane":27543,"nara":27544,"shave":27545,"splinter":27546,"terrific":27547,"##arion":27548,"##erian":27549,"commonplace":27550,"juris":27551,"redwood":27552,"waistband":27553,"boxed":27554,"caitlin":27555,"fingerprints":27556,"jennie":27557,"naturalized":27558,"##ired":27559,"balfour":27560,"craters":27561,"jody":27562,"bungalow":27563,"hugely":27564,"quilt":27565,"glitter":27566,"pigeons":27567,"undertaker":27568,"bulging":27569,"constrained":27570,"goo":27571,"##sil":27572,"##akh":27573,"assimilation":27574,"reworked":27575,"##person":27576,"persuasion":27577,"##pants":27578,"felicia":27579,"##cliff":27580,"##ulent":27581,"1732":27582,"explodes":27583,"##dun":27584,"##inium":27585,"##zic":27586,"lyman":27587,"vulture":27588,"hog":27589,"overlook":27590,"begs":27591,"northwards":27592,"ow":27593,"spoil":27594,"##urer":27595,"fatima":27596,"favorably":27597,"accumulate":27598,"sargent":27599,"sorority":27600,"corresponded":27601,"dispersal":27602,"kochi":27603,"toned":27604,"##imi":27605,"##lita":27606,"internacional":27607,"newfound":27608,"##agger":27609,"##lynn":27610,"##rigue":27611,"booths":27612,"peanuts":27613,"##eborg":27614,"medicare":27615,"muriel":27616,"nur":27617,"##uram":27618,"crates":27619,"millennia":27620,"pajamas":27621,"worsened":27622,"##breakers":27623,"jimi":27624,"vanuatu":27625,"yawned":27626,"##udeau":27627,"carousel":27628,"##hony":27629,"hurdle":27630,"##ccus":27631,"##mounted":27632,"##pod":27633,"rv":27634,"##eche":27635,"airship":27636,"ambiguity":27637,"compulsion":27638,"recapture":27639,"##claiming":27640,"arthritis":27641,"##osomal":27642,"1667":27643,"asserting":27644,"ngc":27645,"sniffing":27646,"dade":27647,"discontent":27648,"glendale":27649,"ported":27650,"##amina":27651,"defamation":27652,"rammed":27653,"##scent":27654,"fling":27655,"livingstone":27656,"##fleet":27657,"875":27658,"##ppy":27659,"apocalyptic":27660,"comrade":27661,"lcd":27662,"##lowe":27663,"cessna":27664,"eine":27665,"persecuted":27666,"subsistence":27667,"demi":27668,"hoop":27669,"reliefs":27670,"710":27671,"coptic":27672,"progressing":27673,"stemmed":27674,"perpetrators":27675,"1665":27676,"priestess":27677,"##nio":27678,"dobson":27679,"ebony":27680,"rooster":27681,"itf":27682,"tortricidae":27683,"##bbon":27684,"##jian":27685,"cleanup":27686,"##jean":27687,"##øy":27688,"1721":27689,"eighties":27690,"taxonomic":27691,"holiness":27692,"##hearted":27693,"##spar":27694,"antilles":27695,"showcasing":27696,"stabilized":27697,"##nb":27698,"gia":27699,"mascara":27700,"michelangelo":27701,"dawned":27702,"##uria":27703,"##vinsky":27704,"extinguished":27705,"fitz":27706,"grotesque":27707,"£100":27708,"##fera":27709,"##loid":27710,"##mous":27711,"barges":27712,"neue":27713,"throbbed":27714,"cipher":27715,"johnnie":27716,"##a1":27717,"##mpt":27718,"outburst":27719,"##swick":27720,"spearheaded":27721,"administrations":27722,"c1":27723,"heartbreak":27724,"pixels":27725,"pleasantly":27726,"##enay":27727,"lombardy":27728,"plush":27729,"##nsed":27730,"bobbie":27731,"##hly":27732,"reapers":27733,"tremor":27734,"xiang":27735,"minogue":27736,"substantive":27737,"hitch":27738,"barak":27739,"##wyl":27740,"kwan":27741,"##encia":27742,"910":27743,"obscene":27744,"elegance":27745,"indus":27746,"surfer":27747,"bribery":27748,"conserve":27749,"##hyllum":27750,"##masters":27751,"horatio":27752,"##fat":27753,"apes":27754,"rebound":27755,"psychotic":27756,"##pour":27757,"iteration":27758,"##mium":27759,"##vani":27760,"botanic":27761,"horribly":27762,"antiques":27763,"dispose":27764,"paxton":27765,"##hli":27766,"##wg":27767,"timeless":27768,"1704":27769,"disregard":27770,"engraver":27771,"hounds":27772,"##bau":27773,"##version":27774,"looted":27775,"uno":27776,"facilitates":27777,"groans":27778,"masjid":27779,"rutland":27780,"antibody":27781,"disqualification":27782,"decatur":27783,"footballers":27784,"quake":27785,"slacks":27786,"48th":27787,"rein":27788,"scribe":27789,"stabilize":27790,"commits":27791,"exemplary":27792,"tho":27793,"##hort":27794,"##chison":27795,"pantry":27796,"traversed":27797,"##hiti":27798,"disrepair":27799,"identifiable":27800,"vibrated":27801,"baccalaureate":27802,"##nnis":27803,"csa":27804,"interviewing":27805,"##iensis":27806,"##raße":27807,"greaves":27808,"wealthiest":27809,"343":27810,"classed":27811,"jogged":27812,"£5":27813,"##58":27814,"##atal":27815,"illuminating":27816,"knicks":27817,"respecting":27818,"##uno":27819,"scrubbed":27820,"##iji":27821,"##dles":27822,"kruger":27823,"moods":27824,"growls":27825,"raider":27826,"silvia":27827,"chefs":27828,"kam":27829,"vr":27830,"cree":27831,"percival":27832,"##terol":27833,"gunter":27834,"counterattack":27835,"defiant":27836,"henan":27837,"ze":27838,"##rasia":27839,"##riety":27840,"equivalence":27841,"submissions":27842,"##fra":27843,"##thor":27844,"bautista":27845,"mechanically":27846,"##heater":27847,"cornice":27848,"herbal":27849,"templar":27850,"##mering":27851,"outputs":27852,"ruining":27853,"ligand":27854,"renumbered":27855,"extravagant":27856,"mika":27857,"blockbuster":27858,"eta":27859,"insurrection":27860,"##ilia":27861,"darkening":27862,"ferocious":27863,"pianos":27864,"strife":27865,"kinship":27866,"##aer":27867,"melee":27868,"##anor":27869,"##iste":27870,"##may":27871,"##oue":27872,"decidedly":27873,"weep":27874,"##jad":27875,"##missive":27876,"##ppel":27877,"354":27878,"puget":27879,"unease":27880,"##gnant":27881,"1629":27882,"hammering":27883,"kassel":27884,"ob":27885,"wessex":27886,"##lga":27887,"bromwich":27888,"egan":27889,"paranoia":27890,"utilization":27891,"##atable":27892,"##idad":27893,"contradictory":27894,"provoke":27895,"##ols":27896,"##ouring":27897,"##tangled":27898,"knesset":27899,"##very":27900,"##lette":27901,"plumbing":27902,"##sden":27903,"##¹":27904,"greensboro":27905,"occult":27906,"sniff":27907,"338":27908,"zev":27909,"beaming":27910,"gamer":27911,"haggard":27912,"mahal":27913,"##olt":27914,"##pins":27915,"mendes":27916,"utmost":27917,"briefing":27918,"gunnery":27919,"##gut":27920,"##pher":27921,"##zh":27922,"##rok":27923,"1679":27924,"khalifa":27925,"sonya":27926,"##boot":27927,"principals":27928,"urbana":27929,"wiring":27930,"##liffe":27931,"##minating":27932,"##rrado":27933,"dahl":27934,"nyu":27935,"skepticism":27936,"np":27937,"townspeople":27938,"ithaca":27939,"lobster":27940,"somethin":27941,"##fur":27942,"##arina":27943,"##−1":27944,"freighter":27945,"zimmerman":27946,"biceps":27947,"contractual":27948,"##herton":27949,"amend":27950,"hurrying":27951,"subconscious":27952,"##anal":27953,"336":27954,"meng":27955,"clermont":27956,"spawning":27957,"##eia":27958,"##lub":27959,"dignitaries":27960,"impetus":27961,"snacks":27962,"spotting":27963,"twigs":27964,"##bilis":27965,"##cz":27966,"##ouk":27967,"libertadores":27968,"nic":27969,"skylar":27970,"##aina":27971,"##firm":27972,"gustave":27973,"asean":27974,"##anum":27975,"dieter":27976,"legislatures":27977,"flirt":27978,"bromley":27979,"trolls":27980,"umar":27981,"##bbies":27982,"##tyle":27983,"blah":27984,"parc":27985,"bridgeport":27986,"crank":27987,"negligence":27988,"##nction":27989,"46th":27990,"constantin":27991,"molded":27992,"bandages":27993,"seriousness":27994,"00pm":27995,"siegel":27996,"carpets":27997,"compartments":27998,"upbeat":27999,"statehood":28000,"##dner":28001,"##edging":28002,"marko":28003,"730":28004,"platt":28005,"##hane":28006,"paving":28007,"##iy":28008,"1738":28009,"abbess":28010,"impatience":28011,"limousine":28012,"nbl":28013,"##talk":28014,"441":28015,"lucille":28016,"mojo":28017,"nightfall":28018,"robbers":28019,"##nais":28020,"karel":28021,"brisk":28022,"calves":28023,"replicate":28024,"ascribed":28025,"telescopes":28026,"##olf":28027,"intimidated":28028,"##reen":28029,"ballast":28030,"specialization":28031,"##sit":28032,"aerodynamic":28033,"caliphate":28034,"rainer":28035,"visionary":28036,"##arded":28037,"epsilon":28038,"##aday":28039,"##onte":28040,"aggregation":28041,"auditory":28042,"boosted":28043,"reunification":28044,"kathmandu":28045,"loco":28046,"robyn":28047,"402":28048,"acknowledges":28049,"appointing":28050,"humanoid":28051,"newell":28052,"redeveloped":28053,"restraints":28054,"##tained":28055,"barbarians":28056,"chopper":28057,"1609":28058,"italiana":28059,"##lez":28060,"##lho":28061,"investigates":28062,"wrestlemania":28063,"##anies":28064,"##bib":28065,"690":28066,"##falls":28067,"creaked":28068,"dragoons":28069,"gravely":28070,"minions":28071,"stupidity":28072,"volley":28073,"##harat":28074,"##week":28075,"musik":28076,"##eries":28077,"##uously":28078,"fungal":28079,"massimo":28080,"semantics":28081,"malvern":28082,"##ahl":28083,"##pee":28084,"discourage":28085,"embryo":28086,"imperialism":28087,"1910s":28088,"profoundly":28089,"##ddled":28090,"jiangsu":28091,"sparkled":28092,"stat":28093,"##holz":28094,"sweatshirt":28095,"tobin":28096,"##iction":28097,"sneered":28098,"##cheon":28099,"##oit":28100,"brit":28101,"causal":28102,"smyth":28103,"##neuve":28104,"diffuse":28105,"perrin":28106,"silvio":28107,"##ipes":28108,"##recht":28109,"detonated":28110,"iqbal":28111,"selma":28112,"##nism":28113,"##zumi":28114,"roasted":28115,"##riders":28116,"tay":28117,"##ados":28118,"##mament":28119,"##mut":28120,"##rud":28121,"840":28122,"completes":28123,"nipples":28124,"cfa":28125,"flavour":28126,"hirsch":28127,"##laus":28128,"calderon":28129,"sneakers":28130,"moravian":28131,"##ksha":28132,"1622":28133,"rq":28134,"294":28135,"##imeters":28136,"bodo":28137,"##isance":28138,"##pre":28139,"##ronia":28140,"anatomical":28141,"excerpt":28142,"##lke":28143,"dh":28144,"kunst":28145,"##tablished":28146,"##scoe":28147,"biomass":28148,"panted":28149,"unharmed":28150,"gael":28151,"housemates":28152,"montpellier":28153,"##59":28154,"coa":28155,"rodents":28156,"tonic":28157,"hickory":28158,"singleton":28159,"##taro":28160,"451":28161,"1719":28162,"aldo":28163,"breaststroke":28164,"dempsey":28165,"och":28166,"rocco":28167,"##cuit":28168,"merton":28169,"dissemination":28170,"midsummer":28171,"serials":28172,"##idi":28173,"haji":28174,"polynomials":28175,"##rdon":28176,"gs":28177,"enoch":28178,"prematurely":28179,"shutter":28180,"taunton":28181,"£3":28182,"##grating":28183,"##inates":28184,"archangel":28185,"harassed":28186,"##asco":28187,"326":28188,"archway":28189,"dazzling":28190,"##ecin":28191,"1736":28192,"sumo":28193,"wat":28194,"##kovich":28195,"1086":28196,"honneur":28197,"##ently":28198,"##nostic":28199,"##ttal":28200,"##idon":28201,"1605":28202,"403":28203,"1716":28204,"blogger":28205,"rents":28206,"##gnan":28207,"hires":28208,"##ikh":28209,"##dant":28210,"howie":28211,"##rons":28212,"handler":28213,"retracted":28214,"shocks":28215,"1632":28216,"arun":28217,"duluth":28218,"kepler":28219,"trumpeter":28220,"##lary":28221,"peeking":28222,"seasoned":28223,"trooper":28224,"##mara":28225,"laszlo":28226,"##iciencies":28227,"##rti":28228,"heterosexual":28229,"##inatory":28230,"##ssion":28231,"indira":28232,"jogging":28233,"##inga":28234,"##lism":28235,"beit":28236,"dissatisfaction":28237,"malice":28238,"##ately":28239,"nedra":28240,"peeling":28241,"##rgeon":28242,"47th":28243,"stadiums":28244,"475":28245,"vertigo":28246,"##ains":28247,"iced":28248,"restroom":28249,"##plify":28250,"##tub":28251,"illustrating":28252,"pear":28253,"##chner":28254,"##sibility":28255,"inorganic":28256,"rappers":28257,"receipts":28258,"watery":28259,"##kura":28260,"lucinda":28261,"##oulos":28262,"reintroduced":28263,"##8th":28264,"##tched":28265,"gracefully":28266,"saxons":28267,"nutritional":28268,"wastewater":28269,"rained":28270,"favourites":28271,"bedrock":28272,"fisted":28273,"hallways":28274,"likeness":28275,"upscale":28276,"##lateral":28277,"1580":28278,"blinds":28279,"prequel":28280,"##pps":28281,"##tama":28282,"deter":28283,"humiliating":28284,"restraining":28285,"tn":28286,"vents":28287,"1659":28288,"laundering":28289,"recess":28290,"rosary":28291,"tractors":28292,"coulter":28293,"federer":28294,"##ifiers":28295,"##plin":28296,"persistence":28297,"##quitable":28298,"geschichte":28299,"pendulum":28300,"quakers":28301,"##beam":28302,"bassett":28303,"pictorial":28304,"buffet":28305,"koln":28306,"##sitor":28307,"drills":28308,"reciprocal":28309,"shooters":28310,"##57":28311,"##cton":28312,"##tees":28313,"converge":28314,"pip":28315,"dmitri":28316,"donnelly":28317,"yamamoto":28318,"aqua":28319,"azores":28320,"demographics":28321,"hypnotic":28322,"spitfire":28323,"suspend":28324,"wryly":28325,"roderick":28326,"##rran":28327,"sebastien":28328,"##asurable":28329,"mavericks":28330,"##fles":28331,"##200":28332,"himalayan":28333,"prodigy":28334,"##iance":28335,"transvaal":28336,"demonstrators":28337,"handcuffs":28338,"dodged":28339,"mcnamara":28340,"sublime":28341,"1726":28342,"crazed":28343,"##efined":28344,"##till":28345,"ivo":28346,"pondered":28347,"reconciled":28348,"shrill":28349,"sava":28350,"##duk":28351,"bal":28352,"cad":28353,"heresy":28354,"jaipur":28355,"goran":28356,"##nished":28357,"341":28358,"lux":28359,"shelly":28360,"whitehall":28361,"##hre":28362,"israelis":28363,"peacekeeping":28364,"##wled":28365,"1703":28366,"demetrius":28367,"ousted":28368,"##arians":28369,"##zos":28370,"beale":28371,"anwar":28372,"backstroke":28373,"raged":28374,"shrinking":28375,"cremated":28376,"##yck":28377,"benign":28378,"towing":28379,"wadi":28380,"darmstadt":28381,"landfill":28382,"parana":28383,"soothe":28384,"colleen":28385,"sidewalks":28386,"mayfair":28387,"tumble":28388,"hepatitis":28389,"ferrer":28390,"superstructure":28391,"##gingly":28392,"##urse":28393,"##wee":28394,"anthropological":28395,"translators":28396,"##mies":28397,"closeness":28398,"hooves":28399,"##pw":28400,"mondays":28401,"##roll":28402,"##vita":28403,"landscaping":28404,"##urized":28405,"purification":28406,"sock":28407,"thorns":28408,"thwarted":28409,"jalan":28410,"tiberius":28411,"##taka":28412,"saline":28413,"##rito":28414,"confidently":28415,"khyber":28416,"sculptors":28417,"##ij":28418,"brahms":28419,"hammersmith":28420,"inspectors":28421,"battista":28422,"fivb":28423,"fragmentation":28424,"hackney":28425,"##uls":28426,"arresting":28427,"exercising":28428,"antoinette":28429,"bedfordshire":28430,"##zily":28431,"dyed":28432,"##hema":28433,"1656":28434,"racetrack":28435,"variability":28436,"##tique":28437,"1655":28438,"austrians":28439,"deteriorating":28440,"madman":28441,"theorists":28442,"aix":28443,"lehman":28444,"weathered":28445,"1731":28446,"decreed":28447,"eruptions":28448,"1729":28449,"flaw":28450,"quinlan":28451,"sorbonne":28452,"flutes":28453,"nunez":28454,"1711":28455,"adored":28456,"downwards":28457,"fable":28458,"rasped":28459,"1712":28460,"moritz":28461,"mouthful":28462,"renegade":28463,"shivers":28464,"stunts":28465,"dysfunction":28466,"restrain":28467,"translit":28468,"327":28469,"pancakes":28470,"##avio":28471,"##cision":28472,"##tray":28473,"351":28474,"vial":28475,"##lden":28476,"bain":28477,"##maid":28478,"##oxide":28479,"chihuahua":28480,"malacca":28481,"vimes":28482,"##rba":28483,"##rnier":28484,"1664":28485,"donnie":28486,"plaques":28487,"##ually":28488,"337":28489,"bangs":28490,"floppy":28491,"huntsville":28492,"loretta":28493,"nikolay":28494,"##otte":28495,"eater":28496,"handgun":28497,"ubiquitous":28498,"##hett":28499,"eras":28500,"zodiac":28501,"1634":28502,"##omorphic":28503,"1820s":28504,"##zog":28505,"cochran":28506,"##bula":28507,"##lithic":28508,"warring":28509,"##rada":28510,"dalai":28511,"excused":28512,"blazers":28513,"mcconnell":28514,"reeling":28515,"bot":28516,"este":28517,"##abi":28518,"geese":28519,"hoax":28520,"taxon":28521,"##bla":28522,"guitarists":28523,"##icon":28524,"condemning":28525,"hunts":28526,"inversion":28527,"moffat":28528,"taekwondo":28529,"##lvis":28530,"1624":28531,"stammered":28532,"##rest":28533,"##rzy":28534,"sousa":28535,"fundraiser":28536,"marylebone":28537,"navigable":28538,"uptown":28539,"cabbage":28540,"daniela":28541,"salman":28542,"shitty":28543,"whimper":28544,"##kian":28545,"##utive":28546,"programmers":28547,"protections":28548,"rm":28549,"##rmi":28550,"##rued":28551,"forceful":28552,"##enes":28553,"fuss":28554,"##tao":28555,"##wash":28556,"brat":28557,"oppressive":28558,"reykjavik":28559,"spartak":28560,"ticking":28561,"##inkles":28562,"##kiewicz":28563,"adolph":28564,"horst":28565,"maui":28566,"protege":28567,"straighten":28568,"cpc":28569,"landau":28570,"concourse":28571,"clements":28572,"resultant":28573,"##ando":28574,"imaginative":28575,"joo":28576,"reactivated":28577,"##rem":28578,"##ffled":28579,"##uising":28580,"consultative":28581,"##guide":28582,"flop":28583,"kaitlyn":28584,"mergers":28585,"parenting":28586,"somber":28587,"##vron":28588,"supervise":28589,"vidhan":28590,"##imum":28591,"courtship":28592,"exemplified":28593,"harmonies":28594,"medallist":28595,"refining":28596,"##rrow":28597,"##ка":28598,"amara":28599,"##hum":28600,"780":28601,"goalscorer":28602,"sited":28603,"overshadowed":28604,"rohan":28605,"displeasure":28606,"secretive":28607,"multiplied":28608,"osman":28609,"##orth":28610,"engravings":28611,"padre":28612,"##kali":28613,"##veda":28614,"miniatures":28615,"mis":28616,"##yala":28617,"clap":28618,"pali":28619,"rook":28620,"##cana":28621,"1692":28622,"57th":28623,"antennae":28624,"astro":28625,"oskar":28626,"1628":28627,"bulldog":28628,"crotch":28629,"hackett":28630,"yucatan":28631,"##sure":28632,"amplifiers":28633,"brno":28634,"ferrara":28635,"migrating":28636,"##gree":28637,"thanking":28638,"turing":28639,"##eza":28640,"mccann":28641,"ting":28642,"andersson":28643,"onslaught":28644,"gaines":28645,"ganga":28646,"incense":28647,"standardization":28648,"##mation":28649,"sentai":28650,"scuba":28651,"stuffing":28652,"turquoise":28653,"waivers":28654,"alloys":28655,"##vitt":28656,"regaining":28657,"vaults":28658,"##clops":28659,"##gizing":28660,"digger":28661,"furry":28662,"memorabilia":28663,"probing":28664,"##iad":28665,"payton":28666,"rec":28667,"deutschland":28668,"filippo":28669,"opaque":28670,"seamen":28671,"zenith":28672,"afrikaans":28673,"##filtration":28674,"disciplined":28675,"inspirational":28676,"##merie":28677,"banco":28678,"confuse":28679,"grafton":28680,"tod":28681,"##dgets":28682,"championed":28683,"simi":28684,"anomaly":28685,"biplane":28686,"##ceptive":28687,"electrode":28688,"##para":28689,"1697":28690,"cleavage":28691,"crossbow":28692,"swirl":28693,"informant":28694,"##lars":28695,"##osta":28696,"afi":28697,"bonfire":28698,"spec":28699,"##oux":28700,"lakeside":28701,"slump":28702,"##culus":28703,"##lais":28704,"##qvist":28705,"##rrigan":28706,"1016":28707,"facades":28708,"borg":28709,"inwardly":28710,"cervical":28711,"xl":28712,"pointedly":28713,"050":28714,"stabilization":28715,"##odon":28716,"chests":28717,"1699":28718,"hacked":28719,"ctv":28720,"orthogonal":28721,"suzy":28722,"##lastic":28723,"gaulle":28724,"jacobite":28725,"rearview":28726,"##cam":28727,"##erted":28728,"ashby":28729,"##drik":28730,"##igate":28731,"##mise":28732,"##zbek":28733,"affectionately":28734,"canine":28735,"disperse":28736,"latham":28737,"##istles":28738,"##ivar":28739,"spielberg":28740,"##orin":28741,"##idium":28742,"ezekiel":28743,"cid":28744,"##sg":28745,"durga":28746,"middletown":28747,"##cina":28748,"customized":28749,"frontiers":28750,"harden":28751,"##etano":28752,"##zzy":28753,"1604":28754,"bolsheviks":28755,"##66":28756,"coloration":28757,"yoko":28758,"##bedo":28759,"briefs":28760,"slabs":28761,"debra":28762,"liquidation":28763,"plumage":28764,"##oin":28765,"blossoms":28766,"dementia":28767,"subsidy":28768,"1611":28769,"proctor":28770,"relational":28771,"jerseys":28772,"parochial":28773,"ter":28774,"##ici":28775,"esa":28776,"peshawar":28777,"cavalier":28778,"loren":28779,"cpi":28780,"idiots":28781,"shamrock":28782,"1646":28783,"dutton":28784,"malabar":28785,"mustache":28786,"##endez":28787,"##ocytes":28788,"referencing":28789,"terminates":28790,"marche":28791,"yarmouth":28792,"##sop":28793,"acton":28794,"mated":28795,"seton":28796,"subtly":28797,"baptised":28798,"beige":28799,"extremes":28800,"jolted":28801,"kristina":28802,"telecast":28803,"##actic":28804,"safeguard":28805,"waldo":28806,"##baldi":28807,"##bular":28808,"endeavors":28809,"sloppy":28810,"subterranean":28811,"##ensburg":28812,"##itung":28813,"delicately":28814,"pigment":28815,"tq":28816,"##scu":28817,"1626":28818,"##ound":28819,"collisions":28820,"coveted":28821,"herds":28822,"##personal":28823,"##meister":28824,"##nberger":28825,"chopra":28826,"##ricting":28827,"abnormalities":28828,"defective":28829,"galician":28830,"lucie":28831,"##dilly":28832,"alligator":28833,"likened":28834,"##genase":28835,"burundi":28836,"clears":28837,"complexion":28838,"derelict":28839,"deafening":28840,"diablo":28841,"fingered":28842,"champaign":28843,"dogg":28844,"enlist":28845,"isotope":28846,"labeling":28847,"mrna":28848,"##erre":28849,"brilliance":28850,"marvelous":28851,"##ayo":28852,"1652":28853,"crawley":28854,"ether":28855,"footed":28856,"dwellers":28857,"deserts":28858,"hamish":28859,"rubs":28860,"warlock":28861,"skimmed":28862,"##lizer":28863,"870":28864,"buick":28865,"embark":28866,"heraldic":28867,"irregularities":28868,"##ajan":28869,"kiara":28870,"##kulam":28871,"##ieg":28872,"antigen":28873,"kowalski":28874,"##lge":28875,"oakley":28876,"visitation":28877,"##mbit":28878,"vt":28879,"##suit":28880,"1570":28881,"murderers":28882,"##miento":28883,"##rites":28884,"chimneys":28885,"##sling":28886,"condemn":28887,"custer":28888,"exchequer":28889,"havre":28890,"##ghi":28891,"fluctuations":28892,"##rations":28893,"dfb":28894,"hendricks":28895,"vaccines":28896,"##tarian":28897,"nietzsche":28898,"biking":28899,"juicy":28900,"##duced":28901,"brooding":28902,"scrolling":28903,"selangor":28904,"##ragan":28905,"352":28906,"annum":28907,"boomed":28908,"seminole":28909,"sugarcane":28910,"##dna":28911,"departmental":28912,"dismissing":28913,"innsbruck":28914,"arteries":28915,"ashok":28916,"batavia":28917,"daze":28918,"kun":28919,"overtook":28920,"##rga":28921,"##tlan":28922,"beheaded":28923,"gaddafi":28924,"holm":28925,"electronically":28926,"faulty":28927,"galilee":28928,"fractures":28929,"kobayashi":28930,"##lized":28931,"gunmen":28932,"magma":28933,"aramaic":28934,"mala":28935,"eastenders":28936,"inference":28937,"messengers":28938,"bf":28939,"##qu":28940,"407":28941,"bathrooms":28942,"##vere":28943,"1658":28944,"flashbacks":28945,"ideally":28946,"misunderstood":28947,"##jali":28948,"##weather":28949,"mendez":28950,"##grounds":28951,"505":28952,"uncanny":28953,"##iii":28954,"1709":28955,"friendships":28956,"##nbc":28957,"sacrament":28958,"accommodated":28959,"reiterated":28960,"logistical":28961,"pebbles":28962,"thumped":28963,"##escence":28964,"administering":28965,"decrees":28966,"drafts":28967,"##flight":28968,"##cased":28969,"##tula":28970,"futuristic":28971,"picket":28972,"intimidation":28973,"winthrop":28974,"##fahan":28975,"interfered":28976,"339":28977,"afar":28978,"francoise":28979,"morally":28980,"uta":28981,"cochin":28982,"croft":28983,"dwarfs":28984,"##bruck":28985,"##dents":28986,"##nami":28987,"biker":28988,"##hner":28989,"##meral":28990,"nano":28991,"##isen":28992,"##ometric":28993,"##pres":28994,"##ан":28995,"brightened":28996,"meek":28997,"parcels":28998,"securely":28999,"gunners":29000,"##jhl":29001,"##zko":29002,"agile":29003,"hysteria":29004,"##lten":29005,"##rcus":29006,"bukit":29007,"champs":29008,"chevy":29009,"cuckoo":29010,"leith":29011,"sadler":29012,"theologians":29013,"welded":29014,"##section":29015,"1663":29016,"jj":29017,"plurality":29018,"xander":29019,"##rooms":29020,"##formed":29021,"shredded":29022,"temps":29023,"intimately":29024,"pau":29025,"tormented":29026,"##lok":29027,"##stellar":29028,"1618":29029,"charred":29030,"ems":29031,"essen":29032,"##mmel":29033,"alarms":29034,"spraying":29035,"ascot":29036,"blooms":29037,"twinkle":29038,"##abia":29039,"##apes":29040,"internment":29041,"obsidian":29042,"##chaft":29043,"snoop":29044,"##dav":29045,"##ooping":29046,"malibu":29047,"##tension":29048,"quiver":29049,"##itia":29050,"hays":29051,"mcintosh":29052,"travers":29053,"walsall":29054,"##ffie":29055,"1623":29056,"beverley":29057,"schwarz":29058,"plunging":29059,"structurally":29060,"m3":29061,"rosenthal":29062,"vikram":29063,"##tsk":29064,"770":29065,"ghz":29066,"##onda":29067,"##tiv":29068,"chalmers":29069,"groningen":29070,"pew":29071,"reckon":29072,"unicef":29073,"##rvis":29074,"55th":29075,"##gni":29076,"1651":29077,"sulawesi":29078,"avila":29079,"cai":29080,"metaphysical":29081,"screwing":29082,"turbulence":29083,"##mberg":29084,"augusto":29085,"samba":29086,"56th":29087,"baffled":29088,"momentary":29089,"toxin":29090,"##urian":29091,"##wani":29092,"aachen":29093,"condoms":29094,"dali":29095,"steppe":29096,"##3d":29097,"##app":29098,"##oed":29099,"##year":29100,"adolescence":29101,"dauphin":29102,"electrically":29103,"inaccessible":29104,"microscopy":29105,"nikita":29106,"##ega":29107,"atv":29108,"##cel":29109,"##enter":29110,"##oles":29111,"##oteric":29112,"##ы":29113,"accountants":29114,"punishments":29115,"wrongly":29116,"bribes":29117,"adventurous":29118,"clinch":29119,"flinders":29120,"southland":29121,"##hem":29122,"##kata":29123,"gough":29124,"##ciency":29125,"lads":29126,"soared":29127,"##ה":29128,"undergoes":29129,"deformation":29130,"outlawed":29131,"rubbish":29132,"##arus":29133,"##mussen":29134,"##nidae":29135,"##rzburg":29136,"arcs":29137,"##ingdon":29138,"##tituted":29139,"1695":29140,"wheelbase":29141,"wheeling":29142,"bombardier":29143,"campground":29144,"zebra":29145,"##lices":29146,"##oj":29147,"##bain":29148,"lullaby":29149,"##ecure":29150,"donetsk":29151,"wylie":29152,"grenada":29153,"##arding":29154,"##ης":29155,"squinting":29156,"eireann":29157,"opposes":29158,"##andra":29159,"maximal":29160,"runes":29161,"##broken":29162,"##cuting":29163,"##iface":29164,"##ror":29165,"##rosis":29166,"additive":29167,"britney":29168,"adultery":29169,"triggering":29170,"##drome":29171,"detrimental":29172,"aarhus":29173,"containment":29174,"jc":29175,"swapped":29176,"vichy":29177,"##ioms":29178,"madly":29179,"##oric":29180,"##rag":29181,"brant":29182,"##ckey":29183,"##trix":29184,"1560":29185,"1612":29186,"broughton":29187,"rustling":29188,"##stems":29189,"##uder":29190,"asbestos":29191,"mentoring":29192,"##nivorous":29193,"finley":29194,"leaps":29195,"##isan":29196,"apical":29197,"pry":29198,"slits":29199,"substitutes":29200,"##dict":29201,"intuitive":29202,"fantasia":29203,"insistent":29204,"unreasonable":29205,"##igen":29206,"##vna":29207,"domed":29208,"hannover":29209,"margot":29210,"ponder":29211,"##zziness":29212,"impromptu":29213,"jian":29214,"lc":29215,"rampage":29216,"stemming":29217,"##eft":29218,"andrey":29219,"gerais":29220,"whichever":29221,"amnesia":29222,"appropriated":29223,"anzac":29224,"clicks":29225,"modifying":29226,"ultimatum":29227,"cambrian":29228,"maids":29229,"verve":29230,"yellowstone":29231,"##mbs":29232,"conservatoire":29233,"##scribe":29234,"adherence":29235,"dinners":29236,"spectra":29237,"imperfect":29238,"mysteriously":29239,"sidekick":29240,"tatar":29241,"tuba":29242,"##aks":29243,"##ifolia":29244,"distrust":29245,"##athan":29246,"##zle":29247,"c2":29248,"ronin":29249,"zac":29250,"##pse":29251,"celaena":29252,"instrumentalist":29253,"scents":29254,"skopje":29255,"##mbling":29256,"comical":29257,"compensated":29258,"vidal":29259,"condor":29260,"intersect":29261,"jingle":29262,"wavelengths":29263,"##urrent":29264,"mcqueen":29265,"##izzly":29266,"carp":29267,"weasel":29268,"422":29269,"kanye":29270,"militias":29271,"postdoctoral":29272,"eugen":29273,"gunslinger":29274,"##ɛ":29275,"faux":29276,"hospice":29277,"##for":29278,"appalled":29279,"derivation":29280,"dwarves":29281,"##elis":29282,"dilapidated":29283,"##folk":29284,"astoria":29285,"philology":29286,"##lwyn":29287,"##otho":29288,"##saka":29289,"inducing":29290,"philanthropy":29291,"##bf":29292,"##itative":29293,"geek":29294,"markedly":29295,"sql":29296,"##yce":29297,"bessie":29298,"indices":29299,"rn":29300,"##flict":29301,"495":29302,"frowns":29303,"resolving":29304,"weightlifting":29305,"tugs":29306,"cleric":29307,"contentious":29308,"1653":29309,"mania":29310,"rms":29311,"##miya":29312,"##reate":29313,"##ruck":29314,"##tucket":29315,"bien":29316,"eels":29317,"marek":29318,"##ayton":29319,"##cence":29320,"discreet":29321,"unofficially":29322,"##ife":29323,"leaks":29324,"##bber":29325,"1705":29326,"332":29327,"dung":29328,"compressor":29329,"hillsborough":29330,"pandit":29331,"shillings":29332,"distal":29333,"##skin":29334,"381":29335,"##tat":29336,"##you":29337,"nosed":29338,"##nir":29339,"mangrove":29340,"undeveloped":29341,"##idia":29342,"textures":29343,"##inho":29344,"##500":29345,"##rise":29346,"ae":29347,"irritating":29348,"nay":29349,"amazingly":29350,"bancroft":29351,"apologetic":29352,"compassionate":29353,"kata":29354,"symphonies":29355,"##lovic":29356,"airspace":29357,"##lch":29358,"930":29359,"gifford":29360,"precautions":29361,"fulfillment":29362,"sevilla":29363,"vulgar":29364,"martinique":29365,"##urities":29366,"looting":29367,"piccolo":29368,"tidy":29369,"##dermott":29370,"quadrant":29371,"armchair":29372,"incomes":29373,"mathematicians":29374,"stampede":29375,"nilsson":29376,"##inking":29377,"##scan":29378,"foo":29379,"quarterfinal":29380,"##ostal":29381,"shang":29382,"shouldered":29383,"squirrels":29384,"##owe":29385,"344":29386,"vinegar":29387,"##bner":29388,"##rchy":29389,"##systems":29390,"delaying":29391,"##trics":29392,"ars":29393,"dwyer":29394,"rhapsody":29395,"sponsoring":29396,"##gration":29397,"bipolar":29398,"cinder":29399,"starters":29400,"##olio":29401,"##urst":29402,"421":29403,"signage":29404,"##nty":29405,"aground":29406,"figurative":29407,"mons":29408,"acquaintances":29409,"duets":29410,"erroneously":29411,"soyuz":29412,"elliptic":29413,"recreated":29414,"##cultural":29415,"##quette":29416,"##ssed":29417,"##tma":29418,"##zcz":29419,"moderator":29420,"scares":29421,"##itaire":29422,"##stones":29423,"##udence":29424,"juniper":29425,"sighting":29426,"##just":29427,"##nsen":29428,"britten":29429,"calabria":29430,"ry":29431,"bop":29432,"cramer":29433,"forsyth":29434,"stillness":29435,"##л":29436,"airmen":29437,"gathers":29438,"unfit":29439,"##umber":29440,"##upt":29441,"taunting":29442,"##rip":29443,"seeker":29444,"streamlined":29445,"##bution":29446,"holster":29447,"schumann":29448,"tread":29449,"vox":29450,"##gano":29451,"##onzo":29452,"strive":29453,"dil":29454,"reforming":29455,"covent":29456,"newbury":29457,"predicting":29458,"##orro":29459,"decorate":29460,"tre":29461,"##puted":29462,"andover":29463,"ie":29464,"asahi":29465,"dept":29466,"dunkirk":29467,"gills":29468,"##tori":29469,"buren":29470,"huskies":29471,"##stis":29472,"##stov":29473,"abstracts":29474,"bets":29475,"loosen":29476,"##opa":29477,"1682":29478,"yearning":29479,"##glio":29480,"##sir":29481,"berman":29482,"effortlessly":29483,"enamel":29484,"napoli":29485,"persist":29486,"##peration":29487,"##uez":29488,"attache":29489,"elisa":29490,"b1":29491,"invitations":29492,"##kic":29493,"accelerating":29494,"reindeer":29495,"boardwalk":29496,"clutches":29497,"nelly":29498,"polka":29499,"starbucks":29500,"##kei":29501,"adamant":29502,"huey":29503,"lough":29504,"unbroken":29505,"adventurer":29506,"embroidery":29507,"inspecting":29508,"stanza":29509,"##ducted":29510,"naia":29511,"taluka":29512,"##pone":29513,"##roids":29514,"chases":29515,"deprivation":29516,"florian":29517,"##jing":29518,"##ppet":29519,"earthly":29520,"##lib":29521,"##ssee":29522,"colossal":29523,"foreigner":29524,"vet":29525,"freaks":29526,"patrice":29527,"rosewood":29528,"triassic":29529,"upstate":29530,"##pkins":29531,"dominates":29532,"ata":29533,"chants":29534,"ks":29535,"vo":29536,"##400":29537,"##bley":29538,"##raya":29539,"##rmed":29540,"555":29541,"agra":29542,"infiltrate":29543,"##ailing":29544,"##ilation":29545,"##tzer":29546,"##uppe":29547,"##werk":29548,"binoculars":29549,"enthusiast":29550,"fujian":29551,"squeak":29552,"##avs":29553,"abolitionist":29554,"almeida":29555,"boredom":29556,"hampstead":29557,"marsden":29558,"rations":29559,"##ands":29560,"inflated":29561,"334":29562,"bonuses":29563,"rosalie":29564,"patna":29565,"##rco":29566,"329":29567,"detachments":29568,"penitentiary":29569,"54th":29570,"flourishing":29571,"woolf":29572,"##dion":29573,"##etched":29574,"papyrus":29575,"##lster":29576,"##nsor":29577,"##toy":29578,"bobbed":29579,"dismounted":29580,"endelle":29581,"inhuman":29582,"motorola":29583,"tbs":29584,"wince":29585,"wreath":29586,"##ticus":29587,"hideout":29588,"inspections":29589,"sanjay":29590,"disgrace":29591,"infused":29592,"pudding":29593,"stalks":29594,"##urbed":29595,"arsenic":29596,"leases":29597,"##hyl":29598,"##rrard":29599,"collarbone":29600,"##waite":29601,"##wil":29602,"dowry":29603,"##bant":29604,"##edance":29605,"genealogical":29606,"nitrate":29607,"salamanca":29608,"scandals":29609,"thyroid":29610,"necessitated":29611,"##!":29612,"##\"":29613,"###":29614,"##$":29615,"##%":29616,"##&":29617,"##'":29618,"##(":29619,"##)":29620,"##*":29621,"##+":29622,"##,":29623,"##-":29624,"##.":29625,"##/":29626,"##:":29627,"##;":29628,"##<":29629,"##=":29630,"##>":29631,"##?":29632,"##@":29633,"##[":29634,"##\\":29635,"##]":29636,"##^":29637,"##_":29638,"##`":29639,"##{":29640,"##|":29641,"##}":29642,"##~":29643,"##¡":29644,"##¢":29645,"##£":29646,"##¤":29647,"##¥":29648,"##¦":29649,"##§":29650,"##¨":29651,"##©":29652,"##ª":29653,"##«":29654,"##¬":29655,"##®":29656,"##±":29657,"##´":29658,"##µ":29659,"##¶":29660,"##·":29661,"##º":29662,"##»":29663,"##¼":29664,"##¾":29665,"##¿":29666,"##æ":29667,"##ð":29668,"##÷":29669,"##þ":29670,"##đ":29671,"##ħ":29672,"##ŋ":29673,"##œ":29674,"##ƒ":29675,"##ɐ":29676,"##ɑ":29677,"##ɒ":29678,"##ɔ":29679,"##ɕ":29680,"##ə":29681,"##ɡ":29682,"##ɣ":29683,"##ɨ":29684,"##ɪ":29685,"##ɫ":29686,"##ɬ":29687,"##ɯ":29688,"##ɲ":29689,"##ɴ":29690,"##ɹ":29691,"##ɾ":29692,"##ʀ":29693,"##ʁ":29694,"##ʂ":29695,"##ʃ":29696,"##ʉ":29697,"##ʊ":29698,"##ʋ":29699,"##ʌ":29700,"##ʎ":29701,"##ʐ":29702,"##ʑ":29703,"##ʒ":29704,"##ʔ":29705,"##ʰ":29706,"##ʲ":29707,"##ʳ":29708,"##ʷ":29709,"##ʸ":29710,"##ʻ":29711,"##ʼ":29712,"##ʾ":29713,"##ʿ":29714,"##ˈ":29715,"##ˡ":29716,"##ˢ":29717,"##ˣ":29718,"##ˤ":29719,"##β":29720,"##γ":29721,"##δ":29722,"##ε":29723,"##ζ":29724,"##θ":29725,"##κ":29726,"##λ":29727,"##μ":29728,"##ξ":29729,"##ο":29730,"##π":29731,"##ρ":29732,"##σ":29733,"##τ":29734,"##υ":29735,"##φ":29736,"##χ":29737,"##ψ":29738,"##ω":29739,"##б":29740,"##г":29741,"##д":29742,"##ж":29743,"##з":29744,"##м":29745,"##п":29746,"##с":29747,"##у":29748,"##ф":29749,"##х":29750,"##ц":29751,"##ч":29752,"##ш":29753,"##щ":29754,"##ъ":29755,"##э":29756,"##ю":29757,"##ђ":29758,"##є":29759,"##і":29760,"##ј":29761,"##љ":29762,"##њ":29763,"##ћ":29764,"##ӏ":29765,"##ա":29766,"##բ":29767,"##գ":29768,"##դ":29769,"##ե":29770,"##թ":29771,"##ի":29772,"##լ":29773,"##կ":29774,"##հ":29775,"##մ":29776,"##յ":29777,"##ն":29778,"##ո":29779,"##պ":29780,"##ս":29781,"##վ":29782,"##տ":29783,"##ր":29784,"##ւ":29785,"##ք":29786,"##־":29787,"##א":29788,"##ב":29789,"##ג":29790,"##ד":29791,"##ו":29792,"##ז":29793,"##ח":29794,"##ט":29795,"##י":29796,"##ך":29797,"##כ":29798,"##ל":29799,"##ם":29800,"##מ":29801,"##ן":29802,"##נ":29803,"##ס":29804,"##ע":29805,"##ף":29806,"##פ":29807,"##ץ":29808,"##צ":29809,"##ק":29810,"##ר":29811,"##ש":29812,"##ת":29813,"##،":29814,"##ء":29815,"##ب":29816,"##ت":29817,"##ث":29818,"##ج":29819,"##ح":29820,"##خ":29821,"##ذ":29822,"##ز":29823,"##س":29824,"##ش":29825,"##ص":29826,"##ض":29827,"##ط":29828,"##ظ":29829,"##ع":29830,"##غ":29831,"##ـ":29832,"##ف":29833,"##ق":29834,"##ك":29835,"##و":29836,"##ى":29837,"##ٹ":29838,"##پ":29839,"##چ":29840,"##ک":29841,"##گ":29842,"##ں":29843,"##ھ":29844,"##ہ":29845,"##ے":29846,"##अ":29847,"##आ":29848,"##उ":29849,"##ए":29850,"##क":29851,"##ख":29852,"##ग":29853,"##च":29854,"##ज":29855,"##ट":29856,"##ड":29857,"##ण":29858,"##त":29859,"##थ":29860,"##द":29861,"##ध":29862,"##न":29863,"##प":29864,"##ब":29865,"##भ":29866,"##म":29867,"##य":29868,"##र":29869,"##ल":29870,"##व":29871,"##श":29872,"##ष":29873,"##स":29874,"##ह":29875,"##ा":29876,"##ि":29877,"##ी":29878,"##ो":29879,"##।":29880,"##॥":29881,"##ং":29882,"##অ":29883,"##আ":29884,"##ই":29885,"##উ":29886,"##এ":29887,"##ও":29888,"##ক":29889,"##খ":29890,"##গ":29891,"##চ":29892,"##ছ":29893,"##জ":29894,"##ট":29895,"##ড":29896,"##ণ":29897,"##ত":29898,"##থ":29899,"##দ":29900,"##ধ":29901,"##ন":29902,"##প":29903,"##ব":29904,"##ভ":29905,"##ম":29906,"##য":29907,"##র":29908,"##ল":29909,"##শ":29910,"##ষ":29911,"##স":29912,"##হ":29913,"##া":29914,"##ি":29915,"##ী":29916,"##ে":29917,"##க":29918,"##ச":29919,"##ட":29920,"##த":29921,"##ந":29922,"##ன":29923,"##ப":29924,"##ம":29925,"##ய":29926,"##ர":29927,"##ல":29928,"##ள":29929,"##வ":29930,"##ா":29931,"##ி":29932,"##ு":29933,"##ே":29934,"##ை":29935,"##ನ":29936,"##ರ":29937,"##ಾ":29938,"##ක":29939,"##ය":29940,"##ර":29941,"##ල":29942,"##ව":29943,"##ා":29944,"##ก":29945,"##ง":29946,"##ต":29947,"##ท":29948,"##น":29949,"##พ":29950,"##ม":29951,"##ย":29952,"##ร":29953,"##ล":29954,"##ว":29955,"##ส":29956,"##อ":29957,"##า":29958,"##เ":29959,"##་":29960,"##།":29961,"##ག":29962,"##ང":29963,"##ད":29964,"##ན":29965,"##པ":29966,"##བ":29967,"##མ":29968,"##འ":29969,"##ར":29970,"##ལ":29971,"##ས":29972,"##မ":29973,"##ა":29974,"##ბ":29975,"##გ":29976,"##დ":29977,"##ე":29978,"##ვ":29979,"##თ":29980,"##ი":29981,"##კ":29982,"##ლ":29983,"##მ":29984,"##ნ":29985,"##ო":29986,"##რ":29987,"##ს":29988,"##ტ":29989,"##უ":29990,"##ᄀ":29991,"##ᄂ":29992,"##ᄃ":29993,"##ᄅ":29994,"##ᄆ":29995,"##ᄇ":29996,"##ᄉ":29997,"##ᄊ":29998,"##ᄋ":29999,"##ᄌ":30000,"##ᄎ":30001,"##ᄏ":30002,"##ᄐ":30003,"##ᄑ":30004,"##ᄒ":30005,"##ᅡ":30006,"##ᅢ":30007,"##ᅥ":30008,"##ᅦ":30009,"##ᅧ":30010,"##ᅩ":30011,"##ᅪ":30012,"##ᅭ":30013,"##ᅮ":30014,"##ᅯ":30015,"##ᅲ":30016,"##ᅳ":30017,"##ᅴ":30018,"##ᅵ":30019,"##ᆨ":30020,"##ᆫ":30021,"##ᆯ":30022,"##ᆷ":30023,"##ᆸ":30024,"##ᆼ":30025,"##ᴬ":30026,"##ᴮ":30027,"##ᴰ":30028,"##ᴵ":30029,"##ᴺ":30030,"##ᵀ":30031,"##ᵃ":30032,"##ᵇ":30033,"##ᵈ":30034,"##ᵉ":30035,"##ᵍ":30036,"##ᵏ":30037,"##ᵐ":30038,"##ᵒ":30039,"##ᵖ":30040,"##ᵗ":30041,"##ᵘ":30042,"##ᵣ":30043,"##ᵤ":30044,"##ᵥ":30045,"##ᶜ":30046,"##ᶠ":30047,"##‐":30048,"##‑":30049,"##‒":30050,"##–":30051,"##—":30052,"##―":30053,"##‖":30054,"##‘":30055,"##’":30056,"##‚":30057,"##“":30058,"##”":30059,"##„":30060,"##†":30061,"##‡":30062,"##•":30063,"##…":30064,"##‰":30065,"##′":30066,"##″":30067,"##›":30068,"##‿":30069,"##⁄":30070,"##⁰":30071,"##ⁱ":30072,"##⁴":30073,"##⁵":30074,"##⁶":30075,"##⁷":30076,"##⁸":30077,"##⁹":30078,"##⁻":30079,"##ⁿ":30080,"##₅":30081,"##₆":30082,"##₇":30083,"##₈":30084,"##₉":30085,"##₊":30086,"##₍":30087,"##₎":30088,"##ₐ":30089,"##ₑ":30090,"##ₒ":30091,"##ₓ":30092,"##ₕ":30093,"##ₖ":30094,"##ₗ":30095,"##ₘ":30096,"##ₚ":30097,"##ₛ":30098,"##ₜ":30099,"##₤":30100,"##₩":30101,"##€":30102,"##₱":30103,"##₹":30104,"##ℓ":30105,"##№":30106,"##ℝ":30107,"##™":30108,"##⅓":30109,"##⅔":30110,"##←":30111,"##↑":30112,"##→":30113,"##↓":30114,"##↔":30115,"##↦":30116,"##⇄":30117,"##⇌":30118,"##⇒":30119,"##∂":30120,"##∅":30121,"##∆":30122,"##∇":30123,"##∈":30124,"##∗":30125,"##∘":30126,"##√":30127,"##∞":30128,"##∧":30129,"##∨":30130,"##∩":30131,"##∪":30132,"##≈":30133,"##≡":30134,"##≤":30135,"##≥":30136,"##⊂":30137,"##⊆":30138,"##⊕":30139,"##⊗":30140,"##⋅":30141,"##─":30142,"##│":30143,"##■":30144,"##▪":30145,"##●":30146,"##★":30147,"##☆":30148,"##☉":30149,"##♠":30150,"##♣":30151,"##♥":30152,"##♦":30153,"##♯":30154,"##⟨":30155,"##⟩":30156,"##ⱼ":30157,"##⺩":30158,"##⺼":30159,"##⽥":30160,"##、":30161,"##。":30162,"##〈":30163,"##〉":30164,"##《":30165,"##》":30166,"##「":30167,"##」":30168,"##『":30169,"##』":30170,"##〜":30171,"##あ":30172,"##い":30173,"##う":30174,"##え":30175,"##お":30176,"##か":30177,"##き":30178,"##く":30179,"##け":30180,"##こ":30181,"##さ":30182,"##し":30183,"##す":30184,"##せ":30185,"##そ":30186,"##た":30187,"##ち":30188,"##っ":30189,"##つ":30190,"##て":30191,"##と":30192,"##な":30193,"##に":30194,"##ぬ":30195,"##ね":30196,"##の":30197,"##は":30198,"##ひ":30199,"##ふ":30200,"##へ":30201,"##ほ":30202,"##ま":30203,"##み":30204,"##む":30205,"##め":30206,"##も":30207,"##や":30208,"##ゆ":30209,"##よ":30210,"##ら":30211,"##り":30212,"##る":30213,"##れ":30214,"##ろ":30215,"##を":30216,"##ん":30217,"##ァ":30218,"##ア":30219,"##ィ":30220,"##イ":30221,"##ウ":30222,"##ェ":30223,"##エ":30224,"##オ":30225,"##カ":30226,"##キ":30227,"##ク":30228,"##ケ":30229,"##コ":30230,"##サ":30231,"##シ":30232,"##ス":30233,"##セ":30234,"##タ":30235,"##チ":30236,"##ッ":30237,"##ツ":30238,"##テ":30239,"##ト":30240,"##ナ":30241,"##ニ":30242,"##ノ":30243,"##ハ":30244,"##ヒ":30245,"##フ":30246,"##ヘ":30247,"##ホ":30248,"##マ":30249,"##ミ":30250,"##ム":30251,"##メ":30252,"##モ":30253,"##ャ":30254,"##ュ":30255,"##ョ":30256,"##ラ":30257,"##リ":30258,"##ル":30259,"##レ":30260,"##ロ":30261,"##ワ":30262,"##ン":30263,"##・":30264,"##ー":30265,"##一":30266,"##三":30267,"##上":30268,"##下":30269,"##不":30270,"##世":30271,"##中":30272,"##主":30273,"##久":30274,"##之":30275,"##也":30276,"##事":30277,"##二":30278,"##五":30279,"##井":30280,"##京":30281,"##人":30282,"##亻":30283,"##仁":30284,"##介":30285,"##代":30286,"##仮":30287,"##伊":30288,"##会":30289,"##佐":30290,"##侍":30291,"##保":30292,"##信":30293,"##健":30294,"##元":30295,"##光":30296,"##八":30297,"##公":30298,"##内":30299,"##出":30300,"##分":30301,"##前":30302,"##劉":30303,"##力":30304,"##加":30305,"##勝":30306,"##北":30307,"##区":30308,"##十":30309,"##千":30310,"##南":30311,"##博":30312,"##原":30313,"##口":30314,"##古":30315,"##史":30316,"##司":30317,"##合":30318,"##吉":30319,"##同":30320,"##名":30321,"##和":30322,"##囗":30323,"##四":30324,"##国":30325,"##國":30326,"##土":30327,"##地":30328,"##坂":30329,"##城":30330,"##堂":30331,"##場":30332,"##士":30333,"##夏":30334,"##外":30335,"##大":30336,"##天":30337,"##太":30338,"##夫":30339,"##奈":30340,"##女":30341,"##子":30342,"##学":30343,"##宀":30344,"##宇":30345,"##安":30346,"##宗":30347,"##定":30348,"##宣":30349,"##宮":30350,"##家":30351,"##宿":30352,"##寺":30353,"##將":30354,"##小":30355,"##尚":30356,"##山":30357,"##岡":30358,"##島":30359,"##崎":30360,"##川":30361,"##州":30362,"##巿":30363,"##帝":30364,"##平":30365,"##年":30366,"##幸":30367,"##广":30368,"##弘":30369,"##張":30370,"##彳":30371,"##後":30372,"##御":30373,"##德":30374,"##心":30375,"##忄":30376,"##志":30377,"##忠":30378,"##愛":30379,"##成":30380,"##我":30381,"##戦":30382,"##戸":30383,"##手":30384,"##扌":30385,"##政":30386,"##文":30387,"##新":30388,"##方":30389,"##日":30390,"##明":30391,"##星":30392,"##春":30393,"##昭":30394,"##智":30395,"##曲":30396,"##書":30397,"##月":30398,"##有":30399,"##朝":30400,"##木":30401,"##本":30402,"##李":30403,"##村":30404,"##東":30405,"##松":30406,"##林":30407,"##森":30408,"##楊":30409,"##樹":30410,"##橋":30411,"##歌":30412,"##止":30413,"##正":30414,"##武":30415,"##比":30416,"##氏":30417,"##民":30418,"##水":30419,"##氵":30420,"##氷":30421,"##永":30422,"##江":30423,"##沢":30424,"##河":30425,"##治":30426,"##法":30427,"##海":30428,"##清":30429,"##漢":30430,"##瀬":30431,"##火":30432,"##版":30433,"##犬":30434,"##王":30435,"##生":30436,"##田":30437,"##男":30438,"##疒":30439,"##発":30440,"##白":30441,"##的":30442,"##皇":30443,"##目":30444,"##相":30445,"##省":30446,"##真":30447,"##石":30448,"##示":30449,"##社":30450,"##神":30451,"##福":30452,"##禾":30453,"##秀":30454,"##秋":30455,"##空":30456,"##立":30457,"##章":30458,"##竹":30459,"##糹":30460,"##美":30461,"##義":30462,"##耳":30463,"##良":30464,"##艹":30465,"##花":30466,"##英":30467,"##華":30468,"##葉":30469,"##藤":30470,"##行":30471,"##街":30472,"##西":30473,"##見":30474,"##訁":30475,"##語":30476,"##谷":30477,"##貝":30478,"##貴":30479,"##車":30480,"##軍":30481,"##辶":30482,"##道":30483,"##郎":30484,"##郡":30485,"##部":30486,"##都":30487,"##里":30488,"##野":30489,"##金":30490,"##鈴":30491,"##镇":30492,"##長":30493,"##門":30494,"##間":30495,"##阝":30496,"##阿":30497,"##陳":30498,"##陽":30499,"##雄":30500,"##青":30501,"##面":30502,"##風":30503,"##食":30504,"##香":30505,"##馬":30506,"##高":30507,"##龍":30508,"##龸":30509,"##ﬁ":30510,"##ﬂ":30511,"##！":30512,"##（":30513,"##）":30514,"##，":30515,"##－":30516,"##．":30517,"##／":30518,"##：":30519,"##？":30520,"##～":30521}}}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/5fd10429389515d3e5cccdeda08cae5fea1ae82e b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/5fd10429389515d3e5cccdeda08cae5fea1ae82e
new file mode 100644
index 0000000..5fd1042
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/5fd10429389515d3e5cccdeda08cae5fea1ae82e
@@ -0,0 +1,4 @@
+{
+  "max_seq_length": 128,
+  "do_lower_case": false
+}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/7410db66f06de178beeadfdd11b1fc241b04f683 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/7410db66f06de178beeadfdd11b1fc241b04f683
new file mode 100644
index 0000000..7410db6
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/7410db66f06de178beeadfdd11b1fc241b04f683
@@ -0,0 +1 @@
+{"do_lower_case": true, "unk_token": "[UNK]", "sep_token": "[SEP]", "pad_token": "[PAD]", "cls_token": "[CLS]", "mask_token": "[MASK]", "tokenize_chinese_chars": true, "strip_accents": null, "name_or_path": "nreimers/MiniLM-L6-H384-uncased", "do_basic_tokenize": true, "never_split": null, "model_max_length": 512}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/b974b349cb2d419ada11181750a733ff82f291ad b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/b974b349cb2d419ada11181750a733ff82f291ad
new file mode 100644
index 0000000..b974b34
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/b974b349cb2d419ada11181750a733ff82f291ad
@@ -0,0 +1,7 @@
+{
+  "__version__": {
+    "sentence_transformers": "2.0.0",
+    "transformers": "4.7.0",
+    "pytorch": "1.9.0+cu102"
+  }
+}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03
new file mode 100644
index 0000000..d1514c3
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03
@@ -0,0 +1,7 @@
+{
+  "word_embedding_dimension": 384,
+  "pooling_mode_cls_token": false,
+  "pooling_mode_mean_tokens": true,
+  "pooling_mode_max_tokens": false,
+  "pooling_mode_mean_sqrt_len_tokens": false
+}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69
new file mode 100644
index 0000000..d931afc
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69
@@ -0,0 +1,24 @@
+{
+  "_name_or_path": "old_models/paraphrase-MiniLM-L6-v2/0_Transformer",
+  "architectures": [
+    "BertModel"
+  ],
+  "attention_probs_dropout_prob": 0.1,
+  "gradient_checkpointing": false,
+  "hidden_act": "gelu",
+  "hidden_dropout_prob": 0.1,
+  "hidden_size": 384,
+  "initializer_range": 0.02,
+  "intermediate_size": 1536,
+  "layer_norm_eps": 1e-12,
+  "max_position_embeddings": 512,
+  "model_type": "bert",
+  "num_attention_heads": 12,
+  "num_hidden_layers": 6,
+  "pad_token_id": 0,
+  "position_embedding_type": "absolute",
+  "transformers_version": "4.7.0",
+  "type_vocab_size": 2,
+  "use_cache": true,
+  "vocab_size": 30522
+}
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5
new file mode 100644
index 0000000..e7b0375
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5
@@ -0,0 +1 @@
+{"unk_token": "[UNK]", "sep_token": "[SEP]", "pad_token": "[PAD]", "cls_token": "[CLS]", "mask_token": "[MASK]"}
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/f7640f94e81bb7f4f04daf1668850b38763a13d9 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/f7640f94e81bb7f4f04daf1668850b38763a13d9
new file mode 100644
index 0000000..f7640f9
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/f7640f94e81bb7f4f04daf1668850b38763a13d9
@@ -0,0 +1,14 @@
+[
+  {
+    "idx": 0,
+    "name": "0",
+    "path": "",
+    "type": "sentence_transformers.models.Transformer"
+  },
+  {
+    "idx": 1,
+    "name": "1",
+    "path": "1_Pooling",
+    "type": "sentence_transformers.models.Pooling"
+  }
+]
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938 b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938
new file mode 100644
index 0000000..fb14027
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938
@@ -0,0 +1,30522 @@
+[PAD]
+[unused0]
+[unused1]
+[unused2]
+[unused3]
+[unused4]
+[unused5]
+[unused6]
+[unused7]
+[unused8]
+[unused9]
+[unused10]
+[unused11]
+[unused12]
+[unused13]
+[unused14]
+[unused15]
+[unused16]
+[unused17]
+[unused18]
+[unused19]
+[unused20]
+[unused21]
+[unused22]
+[unused23]
+[unused24]
+[unused25]
+[unused26]
+[unused27]
+[unused28]
+[unused29]
+[unused30]
+[unused31]
+[unused32]
+[unused33]
+[unused34]
+[unused35]
+[unused36]
+[unused37]
+[unused38]
+[unused39]
+[unused40]
+[unused41]
+[unused42]
+[unused43]
+[unused44]
+[unused45]
+[unused46]
+[unused47]
+[unused48]
+[unused49]
+[unused50]
+[unused51]
+[unused52]
+[unused53]
+[unused54]
+[unused55]
+[unused56]
+[unused57]
+[unused58]
+[unused59]
+[unused60]
+[unused61]
+[unused62]
+[unused63]
+[unused64]
+[unused65]
+[unused66]
+[unused67]
+[unused68]
+[unused69]
+[unused70]
+[unused71]
+[unused72]
+[unused73]
+[unused74]
+[unused75]
+[unused76]
+[unused77]
+[unused78]
+[unused79]
+[unused80]
+[unused81]
+[unused82]
+[unused83]
+[unused84]
+[unused85]
+[unused86]
+[unused87]
+[unused88]
+[unused89]
+[unused90]
+[unused91]
+[unused92]
+[unused93]
+[unused94]
+[unused95]
+[unused96]
+[unused97]
+[unused98]
+[UNK]
+[CLS]
+[SEP]
+[MASK]
+[unused99]
+[unused100]
+[unused101]
+[unused102]
+[unused103]
+[unused104]
+[unused105]
+[unused106]
+[unused107]
+[unused108]
+[unused109]
+[unused110]
+[unused111]
+[unused112]
+[unused113]
+[unused114]
+[unused115]
+[unused116]
+[unused117]
+[unused118]
+[unused119]
+[unused120]
+[unused121]
+[unused122]
+[unused123]
+[unused124]
+[unused125]
+[unused126]
+[unused127]
+[unused128]
+[unused129]
+[unused130]
+[unused131]
+[unused132]
+[unused133]
+[unused134]
+[unused135]
+[unused136]
+[unused137]
+[unused138]
+[unused139]
+[unused140]
+[unused141]
+[unused142]
+[unused143]
+[unused144]
+[unused145]
+[unused146]
+[unused147]
+[unused148]
+[unused149]
+[unused150]
+[unused151]
+[unused152]
+[unused153]
+[unused154]
+[unused155]
+[unused156]
+[unused157]
+[unused158]
+[unused159]
+[unused160]
+[unused161]
+[unused162]
+[unused163]
+[unused164]
+[unused165]
+[unused166]
+[unused167]
+[unused168]
+[unused169]
+[unused170]
+[unused171]
+[unused172]
+[unused173]
+[unused174]
+[unused175]
+[unused176]
+[unused177]
+[unused178]
+[unused179]
+[unused180]
+[unused181]
+[unused182]
+[unused183]
+[unused184]
+[unused185]
+[unused186]
+[unused187]
+[unused188]
+[unused189]
+[unused190]
+[unused191]
+[unused192]
+[unused193]
+[unused194]
+[unused195]
+[unused196]
+[unused197]
+[unused198]
+[unused199]
+[unused200]
+[unused201]
+[unused202]
+[unused203]
+[unused204]
+[unused205]
+[unused206]
+[unused207]
+[unused208]
+[unused209]
+[unused210]
+[unused211]
+[unused212]
+[unused213]
+[unused214]
+[unused215]
+[unused216]
+[unused217]
+[unused218]
+[unused219]
+[unused220]
+[unused221]
+[unused222]
+[unused223]
+[unused224]
+[unused225]
+[unused226]
+[unused227]
+[unused228]
+[unused229]
+[unused230]
+[unused231]
+[unused232]
+[unused233]
+[unused234]
+[unused235]
+[unused236]
+[unused237]
+[unused238]
+[unused239]
+[unused240]
+[unused241]
+[unused242]
+[unused243]
+[unused244]
+[unused245]
+[unused246]
+[unused247]
+[unused248]
+[unused249]
+[unused250]
+[unused251]
+[unused252]
+[unused253]
+[unused254]
+[unused255]
+[unused256]
+[unused257]
+[unused258]
+[unused259]
+[unused260]
+[unused261]
+[unused262]
+[unused263]
+[unused264]
+[unused265]
+[unused266]
+[unused267]
+[unused268]
+[unused269]
+[unused270]
+[unused271]
+[unused272]
+[unused273]
+[unused274]
+[unused275]
+[unused276]
+[unused277]
+[unused278]
+[unused279]
+[unused280]
+[unused281]
+[unused282]
+[unused283]
+[unused284]
+[unused285]
+[unused286]
+[unused287]
+[unused288]
+[unused289]
+[unused290]
+[unused291]
+[unused292]
+[unused293]
+[unused294]
+[unused295]
+[unused296]
+[unused297]
+[unused298]
+[unused299]
+[unused300]
+[unused301]
+[unused302]
+[unused303]
+[unused304]
+[unused305]
+[unused306]
+[unused307]
+[unused308]
+[unused309]
+[unused310]
+[unused311]
+[unused312]
+[unused313]
+[unused314]
+[unused315]
+[unused316]
+[unused317]
+[unused318]
+[unused319]
+[unused320]
+[unused321]
+[unused322]
+[unused323]
+[unused324]
+[unused325]
+[unused326]
+[unused327]
+[unused328]
+[unused329]
+[unused330]
+[unused331]
+[unused332]
+[unused333]
+[unused334]
+[unused335]
+[unused336]
+[unused337]
+[unused338]
+[unused339]
+[unused340]
+[unused341]
+[unused342]
+[unused343]
+[unused344]
+[unused345]
+[unused346]
+[unused347]
+[unused348]
+[unused349]
+[unused350]
+[unused351]
+[unused352]
+[unused353]
+[unused354]
+[unused355]
+[unused356]
+[unused357]
+[unused358]
+[unused359]
+[unused360]
+[unused361]
+[unused362]
+[unused363]
+[unused364]
+[unused365]
+[unused366]
+[unused367]
+[unused368]
+[unused369]
+[unused370]
+[unused371]
+[unused372]
+[unused373]
+[unused374]
+[unused375]
+[unused376]
+[unused377]
+[unused378]
+[unused379]
+[unused380]
+[unused381]
+[unused382]
+[unused383]
+[unused384]
+[unused385]
+[unused386]
+[unused387]
+[unused388]
+[unused389]
+[unused390]
+[unused391]
+[unused392]
+[unused393]
+[unused394]
+[unused395]
+[unused396]
+[unused397]
+[unused398]
+[unused399]
+[unused400]
+[unused401]
+[unused402]
+[unused403]
+[unused404]
+[unused405]
+[unused406]
+[unused407]
+[unused408]
+[unused409]
+[unused410]
+[unused411]
+[unused412]
+[unused413]
+[unused414]
+[unused415]
+[unused416]
+[unused417]
+[unused418]
+[unused419]
+[unused420]
+[unused421]
+[unused422]
+[unused423]
+[unused424]
+[unused425]
+[unused426]
+[unused427]
+[unused428]
+[unused429]
+[unused430]
+[unused431]
+[unused432]
+[unused433]
+[unused434]
+[unused435]
+[unused436]
+[unused437]
+[unused438]
+[unused439]
+[unused440]
+[unused441]
+[unused442]
+[unused443]
+[unused444]
+[unused445]
+[unused446]
+[unused447]
+[unused448]
+[unused449]
+[unused450]
+[unused451]
+[unused452]
+[unused453]
+[unused454]
+[unused455]
+[unused456]
+[unused457]
+[unused458]
+[unused459]
+[unused460]
+[unused461]
+[unused462]
+[unused463]
+[unused464]
+[unused465]
+[unused466]
+[unused467]
+[unused468]
+[unused469]
+[unused470]
+[unused471]
+[unused472]
+[unused473]
+[unused474]
+[unused475]
+[unused476]
+[unused477]
+[unused478]
+[unused479]
+[unused480]
+[unused481]
+[unused482]
+[unused483]
+[unused484]
+[unused485]
+[unused486]
+[unused487]
+[unused488]
+[unused489]
+[unused490]
+[unused491]
+[unused492]
+[unused493]
+[unused494]
+[unused495]
+[unused496]
+[unused497]
+[unused498]
+[unused499]
+[unused500]
+[unused501]
+[unused502]
+[unused503]
+[unused504]
+[unused505]
+[unused506]
+[unused507]
+[unused508]
+[unused509]
+[unused510]
+[unused511]
+[unused512]
+[unused513]
+[unused514]
+[unused515]
+[unused516]
+[unused517]
+[unused518]
+[unused519]
+[unused520]
+[unused521]
+[unused522]
+[unused523]
+[unused524]
+[unused525]
+[unused526]
+[unused527]
+[unused528]
+[unused529]
+[unused530]
+[unused531]
+[unused532]
+[unused533]
+[unused534]
+[unused535]
+[unused536]
+[unused537]
+[unused538]
+[unused539]
+[unused540]
+[unused541]
+[unused542]
+[unused543]
+[unused544]
+[unused545]
+[unused546]
+[unused547]
+[unused548]
+[unused549]
+[unused550]
+[unused551]
+[unused552]
+[unused553]
+[unused554]
+[unused555]
+[unused556]
+[unused557]
+[unused558]
+[unused559]
+[unused560]
+[unused561]
+[unused562]
+[unused563]
+[unused564]
+[unused565]
+[unused566]
+[unused567]
+[unused568]
+[unused569]
+[unused570]
+[unused571]
+[unused572]
+[unused573]
+[unused574]
+[unused575]
+[unused576]
+[unused577]
+[unused578]
+[unused579]
+[unused580]
+[unused581]
+[unused582]
+[unused583]
+[unused584]
+[unused585]
+[unused586]
+[unused587]
+[unused588]
+[unused589]
+[unused590]
+[unused591]
+[unused592]
+[unused593]
+[unused594]
+[unused595]
+[unused596]
+[unused597]
+[unused598]
+[unused599]
+[unused600]
+[unused601]
+[unused602]
+[unused603]
+[unused604]
+[unused605]
+[unused606]
+[unused607]
+[unused608]
+[unused609]
+[unused610]
+[unused611]
+[unused612]
+[unused613]
+[unused614]
+[unused615]
+[unused616]
+[unused617]
+[unused618]
+[unused619]
+[unused620]
+[unused621]
+[unused622]
+[unused623]
+[unused624]
+[unused625]
+[unused626]
+[unused627]
+[unused628]
+[unused629]
+[unused630]
+[unused631]
+[unused632]
+[unused633]
+[unused634]
+[unused635]
+[unused636]
+[unused637]
+[unused638]
+[unused639]
+[unused640]
+[unused641]
+[unused642]
+[unused643]
+[unused644]
+[unused645]
+[unused646]
+[unused647]
+[unused648]
+[unused649]
+[unused650]
+[unused651]
+[unused652]
+[unused653]
+[unused654]
+[unused655]
+[unused656]
+[unused657]
+[unused658]
+[unused659]
+[unused660]
+[unused661]
+[unused662]
+[unused663]
+[unused664]
+[unused665]
+[unused666]
+[unused667]
+[unused668]
+[unused669]
+[unused670]
+[unused671]
+[unused672]
+[unused673]
+[unused674]
+[unused675]
+[unused676]
+[unused677]
+[unused678]
+[unused679]
+[unused680]
+[unused681]
+[unused682]
+[unused683]
+[unused684]
+[unused685]
+[unused686]
+[unused687]
+[unused688]
+[unused689]
+[unused690]
+[unused691]
+[unused692]
+[unused693]
+[unused694]
+[unused695]
+[unused696]
+[unused697]
+[unused698]
+[unused699]
+[unused700]
+[unused701]
+[unused702]
+[unused703]
+[unused704]
+[unused705]
+[unused706]
+[unused707]
+[unused708]
+[unused709]
+[unused710]
+[unused711]
+[unused712]
+[unused713]
+[unused714]
+[unused715]
+[unused716]
+[unused717]
+[unused718]
+[unused719]
+[unused720]
+[unused721]
+[unused722]
+[unused723]
+[unused724]
+[unused725]
+[unused726]
+[unused727]
+[unused728]
+[unused729]
+[unused730]
+[unused731]
+[unused732]
+[unused733]
+[unused734]
+[unused735]
+[unused736]
+[unused737]
+[unused738]
+[unused739]
+[unused740]
+[unused741]
+[unused742]
+[unused743]
+[unused744]
+[unused745]
+[unused746]
+[unused747]
+[unused748]
+[unused749]
+[unused750]
+[unused751]
+[unused752]
+[unused753]
+[unused754]
+[unused755]
+[unused756]
+[unused757]
+[unused758]
+[unused759]
+[unused760]
+[unused761]
+[unused762]
+[unused763]
+[unused764]
+[unused765]
+[unused766]
+[unused767]
+[unused768]
+[unused769]
+[unused770]
+[unused771]
+[unused772]
+[unused773]
+[unused774]
+[unused775]
+[unused776]
+[unused777]
+[unused778]
+[unused779]
+[unused780]
+[unused781]
+[unused782]
+[unused783]
+[unused784]
+[unused785]
+[unused786]
+[unused787]
+[unused788]
+[unused789]
+[unused790]
+[unused791]
+[unused792]
+[unused793]
+[unused794]
+[unused795]
+[unused796]
+[unused797]
+[unused798]
+[unused799]
+[unused800]
+[unused801]
+[unused802]
+[unused803]
+[unused804]
+[unused805]
+[unused806]
+[unused807]
+[unused808]
+[unused809]
+[unused810]
+[unused811]
+[unused812]
+[unused813]
+[unused814]
+[unused815]
+[unused816]
+[unused817]
+[unused818]
+[unused819]
+[unused820]
+[unused821]
+[unused822]
+[unused823]
+[unused824]
+[unused825]
+[unused826]
+[unused827]
+[unused828]
+[unused829]
+[unused830]
+[unused831]
+[unused832]
+[unused833]
+[unused834]
+[unused835]
+[unused836]
+[unused837]
+[unused838]
+[unused839]
+[unused840]
+[unused841]
+[unused842]
+[unused843]
+[unused844]
+[unused845]
+[unused846]
+[unused847]
+[unused848]
+[unused849]
+[unused850]
+[unused851]
+[unused852]
+[unused853]
+[unused854]
+[unused855]
+[unused856]
+[unused857]
+[unused858]
+[unused859]
+[unused860]
+[unused861]
+[unused862]
+[unused863]
+[unused864]
+[unused865]
+[unused866]
+[unused867]
+[unused868]
+[unused869]
+[unused870]
+[unused871]
+[unused872]
+[unused873]
+[unused874]
+[unused875]
+[unused876]
+[unused877]
+[unused878]
+[unused879]
+[unused880]
+[unused881]
+[unused882]
+[unused883]
+[unused884]
+[unused885]
+[unused886]
+[unused887]
+[unused888]
+[unused889]
+[unused890]
+[unused891]
+[unused892]
+[unused893]
+[unused894]
+[unused895]
+[unused896]
+[unused897]
+[unused898]
+[unused899]
+[unused900]
+[unused901]
+[unused902]
+[unused903]
+[unused904]
+[unused905]
+[unused906]
+[unused907]
+[unused908]
+[unused909]
+[unused910]
+[unused911]
+[unused912]
+[unused913]
+[unused914]
+[unused915]
+[unused916]
+[unused917]
+[unused918]
+[unused919]
+[unused920]
+[unused921]
+[unused922]
+[unused923]
+[unused924]
+[unused925]
+[unused926]
+[unused927]
+[unused928]
+[unused929]
+[unused930]
+[unused931]
+[unused932]
+[unused933]
+[unused934]
+[unused935]
+[unused936]
+[unused937]
+[unused938]
+[unused939]
+[unused940]
+[unused941]
+[unused942]
+[unused943]
+[unused944]
+[unused945]
+[unused946]
+[unused947]
+[unused948]
+[unused949]
+[unused950]
+[unused951]
+[unused952]
+[unused953]
+[unused954]
+[unused955]
+[unused956]
+[unused957]
+[unused958]
+[unused959]
+[unused960]
+[unused961]
+[unused962]
+[unused963]
+[unused964]
+[unused965]
+[unused966]
+[unused967]
+[unused968]
+[unused969]
+[unused970]
+[unused971]
+[unused972]
+[unused973]
+[unused974]
+[unused975]
+[unused976]
+[unused977]
+[unused978]
+[unused979]
+[unused980]
+[unused981]
+[unused982]
+[unused983]
+[unused984]
+[unused985]
+[unused986]
+[unused987]
+[unused988]
+[unused989]
+[unused990]
+[unused991]
+[unused992]
+[unused993]
+!
+"
+#
+$
+%
+&
+'
+(
+)
+*
++
+,
+-
+.
+/
+0
+1
+2
+3
+4
+5
+6
+7
+8
+9
+:
+;
+<
+=
+>
+?
+@
+[
+\
+]
+^
+_
+`
+a
+b
+c
+d
+e
+f
+g
+h
+i
+j
+k
+l
+m
+n
+o
+p
+q
+r
+s
+t
+u
+v
+w
+x
+y
+z
+{
+|
+}
+~
+¡
+¢
+£
+¤
+¥
+¦
+§
+¨
+©
+ª
+«
+¬
+®
+°
+±
+²
+³
+´
+µ
+¶
+·
+¹
+º
+»
+¼
+½
+¾
+¿
+×
+ß
+æ
+ð
+÷
+ø
+þ
+đ
+ħ
+ı
+ł
+ŋ
+œ
+ƒ
+ɐ
+ɑ
+ɒ
+ɔ
+ɕ
+ə
+ɛ
+ɡ
+ɣ
+ɨ
+ɪ
+ɫ
+ɬ
+ɯ
+ɲ
+ɴ
+ɹ
+ɾ
+ʀ
+ʁ
+ʂ
+ʃ
+ʉ
+ʊ
+ʋ
+ʌ
+ʎ
+ʐ
+ʑ
+ʒ
+ʔ
+ʰ
+ʲ
+ʳ
+ʷ
+ʸ
+ʻ
+ʼ
+ʾ
+ʿ
+ˈ
+ː
+ˡ
+ˢ
+ˣ
+ˤ
+α
+β
+γ
+δ
+ε
+ζ
+η
+θ
+ι
+κ
+λ
+μ
+ν
+ξ
+ο
+π
+ρ
+ς
+σ
+τ
+υ
+φ
+χ
+ψ
+ω
+а
+б
+в
+г
+д
+е
+ж
+з
+и
+к
+л
+м
+н
+о
+п
+р
+с
+т
+у
+ф
+х
+ц
+ч
+ш
+щ
+ъ
+ы
+ь
+э
+ю
+я
+ђ
+є
+і
+ј
+љ
+њ
+ћ
+ӏ
+ա
+բ
+գ
+դ
+ե
+թ
+ի
+լ
+կ
+հ
+մ
+յ
+ն
+ո
+պ
+ս
+վ
+տ
+ր
+ւ
+ք
+־
+א
+ב
+ג
+ד
+ה
+ו
+ז
+ח
+ט
+י
+ך
+כ
+ל
+ם
+מ
+ן
+נ
+ס
+ע
+ף
+פ
+ץ
+צ
+ק
+ר
+ש
+ת
+،
+ء
+ا
+ب
+ة
+ت
+ث
+ج
+ح
+خ
+د
+ذ
+ر
+ز
+س
+ش
+ص
+ض
+ط
+ظ
+ع
+غ
+ـ
+ف
+ق
+ك
+ل
+م
+ن
+ه
+و
+ى
+ي
+ٹ
+پ
+چ
+ک
+گ
+ں
+ھ
+ہ
+ی
+ے
+अ
+आ
+उ
+ए
+क
+ख
+ग
+च
+ज
+ट
+ड
+ण
+त
+थ
+द
+ध
+न
+प
+ब
+भ
+म
+य
+र
+ल
+व
+श
+ष
+स
+ह
+ा
+ि
+ी
+ो
+।
+॥
+ং
+অ
+আ
+ই
+উ
+এ
+ও
+ক
+খ
+গ
+চ
+ছ
+জ
+ট
+ড
+ণ
+ত
+থ
+দ
+ধ
+ন
+প
+ব
+ভ
+ম
+য
+র
+ল
+শ
+ষ
+স
+হ
+া
+ি
+ী
+ে
+க
+ச
+ட
+த
+ந
+ன
+ப
+ம
+ய
+ர
+ல
+ள
+வ
+ா
+ி
+ு
+ே
+ை
+ನ
+ರ
+ಾ
+ක
+ය
+ර
+ල
+ව
+ා
+ก
+ง
+ต
+ท
+น
+พ
+ม
+ย
+ร
+ล
+ว
+ส
+อ
+า
+เ
+་
+།
+ག
+ང
+ད
+ན
+པ
+བ
+མ
+འ
+ར
+ལ
+ས
+မ
+ა
+ბ
+გ
+დ
+ე
+ვ
+თ
+ი
+კ
+ლ
+მ
+ნ
+ო
+რ
+ს
+ტ
+უ
+ᄀ
+ᄂ
+ᄃ
+ᄅ
+ᄆ
+ᄇ
+ᄉ
+ᄊ
+ᄋ
+ᄌ
+ᄎ
+ᄏ
+ᄐ
+ᄑ
+ᄒ
+ᅡ
+ᅢ
+ᅥ
+ᅦ
+ᅧ
+ᅩ
+ᅪ
+ᅭ
+ᅮ
+ᅯ
+ᅲ
+ᅳ
+ᅴ
+ᅵ
+ᆨ
+ᆫ
+ᆯ
+ᆷ
+ᆸ
+ᆼ
+ᴬ
+ᴮ
+ᴰ
+ᴵ
+ᴺ
+ᵀ
+ᵃ
+ᵇ
+ᵈ
+ᵉ
+ᵍ
+ᵏ
+ᵐ
+ᵒ
+ᵖ
+ᵗ
+ᵘ
+ᵢ
+ᵣ
+ᵤ
+ᵥ
+ᶜ
+ᶠ
+‐
+‑
+‒
+–
+—
+―
+‖
+‘
+’
+‚
+“
+”
+„
+†
+‡
+•
+…
+‰
+′
+″
+›
+‿
+⁄
+⁰
+ⁱ
+⁴
+⁵
+⁶
+⁷
+⁸
+⁹
+⁺
+⁻
+ⁿ
+₀
+₁
+₂
+₃
+₄
+₅
+₆
+₇
+₈
+₉
+₊
+₍
+₎
+ₐ
+ₑ
+ₒ
+ₓ
+ₕ
+ₖ
+ₗ
+ₘ
+ₙ
+ₚ
+ₛ
+ₜ
+₤
+₩
+€
+₱
+₹
+ℓ
+№
+ℝ
+™
+⅓
+⅔
+←
+↑
+→
+↓
+↔
+↦
+⇄
+⇌
+⇒
+∂
+∅
+∆
+∇
+∈
+−
+∗
+∘
+√
+∞
+∧
+∨
+∩
+∪
+≈
+≡
+≤
+≥
+⊂
+⊆
+⊕
+⊗
+⋅
+─
+│
+■
+▪
+●
+★
+☆
+☉
+♠
+♣
+♥
+♦
+♭
+♯
+⟨
+⟩
+ⱼ
+⺩
+⺼
+⽥
+、
+。
+〈
+〉
+《
+》
+「
+」
+『
+』
+〜
+あ
+い
+う
+え
+お
+か
+き
+く
+け
+こ
+さ
+し
+す
+せ
+そ
+た
+ち
+っ
+つ
+て
+と
+な
+に
+ぬ
+ね
+の
+は
+ひ
+ふ
+へ
+ほ
+ま
+み
+む
+め
+も
+や
+ゆ
+よ
+ら
+り
+る
+れ
+ろ
+を
+ん
+ァ
+ア
+ィ
+イ
+ウ
+ェ
+エ
+オ
+カ
+キ
+ク
+ケ
+コ
+サ
+シ
+ス
+セ
+タ
+チ
+ッ
+ツ
+テ
+ト
+ナ
+ニ
+ノ
+ハ
+ヒ
+フ
+ヘ
+ホ
+マ
+ミ
+ム
+メ
+モ
+ャ
+ュ
+ョ
+ラ
+リ
+ル
+レ
+ロ
+ワ
+ン
+・
+ー
+一
+三
+上
+下
+不
+世
+中
+主
+久
+之
+也
+事
+二
+五
+井
+京
+人
+亻
+仁
+介
+代
+仮
+伊
+会
+佐
+侍
+保
+信
+健
+元
+光
+八
+公
+内
+出
+分
+前
+劉
+力
+加
+勝
+北
+区
+十
+千
+南
+博
+原
+口
+古
+史
+司
+合
+吉
+同
+名
+和
+囗
+四
+国
+國
+土
+地
+坂
+城
+堂
+場
+士
+夏
+外
+大
+天
+太
+夫
+奈
+女
+子
+学
+宀
+宇
+安
+宗
+定
+宣
+宮
+家
+宿
+寺
+將
+小
+尚
+山
+岡
+島
+崎
+川
+州
+巿
+帝
+平
+年
+幸
+广
+弘
+張
+彳
+後
+御
+德
+心
+忄
+志
+忠
+愛
+成
+我
+戦
+戸
+手
+扌
+政
+文
+新
+方
+日
+明
+星
+春
+昭
+智
+曲
+書
+月
+有
+朝
+木
+本
+李
+村
+東
+松
+林
+森
+楊
+樹
+橋
+歌
+止
+正
+武
+比
+氏
+民
+水
+氵
+氷
+永
+江
+沢
+河
+治
+法
+海
+清
+漢
+瀬
+火
+版
+犬
+王
+生
+田
+男
+疒
+発
+白
+的
+皇
+目
+相
+省
+真
+石
+示
+社
+神
+福
+禾
+秀
+秋
+空
+立
+章
+竹
+糹
+美
+義
+耳
+良
+艹
+花
+英
+華
+葉
+藤
+行
+街
+西
+見
+訁
+語
+谷
+貝
+貴
+車
+軍
+辶
+道
+郎
+郡
+部
+都
+里
+野
+金
+鈴
+镇
+長
+門
+間
+阝
+阿
+陳
+陽
+雄
+青
+面
+風
+食
+香
+馬
+高
+龍
+龸
+ﬁ
+ﬂ
+！
+（
+）
+，
+－
+．
+／
+：
+？
+～
+the
+of
+and
+in
+to
+was
+he
+is
+as
+for
+on
+with
+that
+it
+his
+by
+at
+from
+her
+##s
+she
+you
+had
+an
+were
+but
+be
+this
+are
+not
+my
+they
+one
+which
+or
+have
+him
+me
+first
+all
+also
+their
+has
+up
+who
+out
+been
+when
+after
+there
+into
+new
+two
+its
+##a
+time
+would
+no
+what
+about
+said
+we
+over
+then
+other
+so
+more
+##e
+can
+if
+like
+back
+them
+only
+some
+could
+##i
+where
+just
+##ing
+during
+before
+##n
+do
+##o
+made
+school
+through
+than
+now
+years
+most
+world
+may
+between
+down
+well
+three
+##d
+year
+while
+will
+##ed
+##r
+##y
+later
+##t
+city
+under
+around
+did
+such
+being
+used
+state
+people
+part
+know
+against
+your
+many
+second
+university
+both
+national
+##er
+these
+don
+known
+off
+way
+until
+re
+how
+even
+get
+head
+...
+didn
+##ly
+team
+american
+because
+de
+##l
+born
+united
+film
+since
+still
+long
+work
+south
+us
+became
+any
+high
+again
+day
+family
+see
+right
+man
+eyes
+house
+season
+war
+states
+including
+took
+life
+north
+same
+each
+called
+name
+much
+place
+however
+go
+four
+group
+another
+found
+won
+area
+here
+going
+10
+away
+series
+left
+home
+music
+best
+make
+hand
+number
+company
+several
+never
+last
+john
+000
+very
+album
+take
+end
+good
+too
+following
+released
+game
+played
+little
+began
+district
+##m
+old
+want
+those
+side
+held
+own
+early
+county
+ll
+league
+use
+west
+##u
+face
+think
+##es
+2010
+government
+##h
+march
+came
+small
+general
+town
+june
+##on
+line
+based
+something
+##k
+september
+thought
+looked
+along
+international
+2011
+air
+july
+club
+went
+january
+october
+our
+august
+april
+york
+12
+few
+2012
+2008
+east
+show
+member
+college
+2009
+father
+public
+##us
+come
+men
+five
+set
+station
+church
+##c
+next
+former
+november
+room
+party
+located
+december
+2013
+age
+got
+2007
+##g
+system
+let
+love
+2006
+though
+every
+2014
+look
+song
+water
+century
+without
+body
+black
+night
+within
+great
+women
+single
+ve
+building
+large
+population
+river
+named
+band
+white
+started
+##an
+once
+15
+20
+should
+18
+2015
+service
+top
+built
+british
+open
+death
+king
+moved
+local
+times
+children
+february
+book
+why
+11
+door
+need
+president
+order
+final
+road
+wasn
+although
+due
+major
+died
+village
+third
+knew
+2016
+asked
+turned
+st
+wanted
+say
+##p
+together
+received
+main
+son
+served
+different
+##en
+behind
+himself
+felt
+members
+power
+football
+law
+voice
+play
+##in
+near
+park
+history
+30
+having
+2005
+16
+##man
+saw
+mother
+##al
+army
+point
+front
+help
+english
+street
+art
+late
+hands
+games
+award
+##ia
+young
+14
+put
+published
+country
+division
+across
+told
+13
+often
+ever
+french
+london
+center
+six
+red
+2017
+led
+days
+include
+light
+25
+find
+tell
+among
+species
+really
+according
+central
+half
+2004
+form
+original
+gave
+office
+making
+enough
+lost
+full
+opened
+must
+included
+live
+given
+german
+player
+run
+business
+woman
+community
+cup
+might
+million
+land
+2000
+court
+development
+17
+short
+round
+ii
+km
+seen
+class
+story
+always
+become
+sure
+research
+almost
+director
+council
+la
+##2
+career
+things
+using
+island
+##z
+couldn
+car
+##is
+24
+close
+force
+##1
+better
+free
+support
+control
+field
+students
+2003
+education
+married
+##b
+nothing
+worked
+others
+record
+big
+inside
+level
+anything
+continued
+give
+james
+##3
+military
+established
+non
+returned
+feel
+does
+title
+written
+thing
+feet
+william
+far
+co
+association
+hard
+already
+2002
+##ra
+championship
+human
+western
+100
+##na
+department
+hall
+role
+various
+production
+21
+19
+heart
+2001
+living
+fire
+version
+##ers
+##f
+television
+royal
+##4
+produced
+working
+act
+case
+society
+region
+present
+radio
+period
+looking
+least
+total
+keep
+england
+wife
+program
+per
+brother
+mind
+special
+22
+##le
+am
+works
+soon
+##6
+political
+george
+services
+taken
+created
+##7
+further
+able
+reached
+david
+union
+joined
+upon
+done
+important
+social
+information
+either
+##ic
+##x
+appeared
+position
+ground
+lead
+rock
+dark
+election
+23
+board
+france
+hair
+course
+arms
+site
+police
+girl
+instead
+real
+sound
+##v
+words
+moment
+##te
+someone
+##8
+summer
+project
+announced
+san
+less
+wrote
+past
+followed
+##5
+blue
+founded
+al
+finally
+india
+taking
+records
+america
+##ne
+1999
+design
+considered
+northern
+god
+stop
+battle
+toward
+european
+outside
+described
+track
+today
+playing
+language
+28
+call
+26
+heard
+professional
+low
+australia
+miles
+california
+win
+yet
+green
+##ie
+trying
+blood
+##ton
+southern
+science
+maybe
+everything
+match
+square
+27
+mouth
+video
+race
+recorded
+leave
+above
+##9
+daughter
+points
+space
+1998
+museum
+change
+middle
+common
+##0
+move
+tv
+post
+##ta
+lake
+seven
+tried
+elected
+closed
+ten
+paul
+minister
+##th
+months
+start
+chief
+return
+canada
+person
+sea
+release
+similar
+modern
+brought
+rest
+hit
+formed
+mr
+##la
+1997
+floor
+event
+doing
+thomas
+1996
+robert
+care
+killed
+training
+star
+week
+needed
+turn
+finished
+railway
+rather
+news
+health
+sent
+example
+ran
+term
+michael
+coming
+currently
+yes
+forces
+despite
+gold
+areas
+50
+stage
+fact
+29
+dead
+says
+popular
+2018
+originally
+germany
+probably
+developed
+result
+pulled
+friend
+stood
+money
+running
+mi
+signed
+word
+songs
+child
+eventually
+met
+tour
+average
+teams
+minutes
+festival
+current
+deep
+kind
+1995
+decided
+usually
+eastern
+seemed
+##ness
+episode
+bed
+added
+table
+indian
+private
+charles
+route
+available
+idea
+throughout
+centre
+addition
+appointed
+style
+1994
+books
+eight
+construction
+press
+mean
+wall
+friends
+remained
+schools
+study
+##ch
+##um
+institute
+oh
+chinese
+sometimes
+events
+possible
+1992
+australian
+type
+brown
+forward
+talk
+process
+food
+debut
+seat
+performance
+committee
+features
+character
+arts
+herself
+else
+lot
+strong
+russian
+range
+hours
+peter
+arm
+##da
+morning
+dr
+sold
+##ry
+quickly
+directed
+1993
+guitar
+china
+##w
+31
+list
+##ma
+performed
+media
+uk
+players
+smile
+##rs
+myself
+40
+placed
+coach
+province
+towards
+wouldn
+leading
+whole
+boy
+official
+designed
+grand
+census
+##el
+europe
+attack
+japanese
+henry
+1991
+##re
+##os
+cross
+getting
+alone
+action
+lower
+network
+wide
+washington
+japan
+1990
+hospital
+believe
+changed
+sister
+##ar
+hold
+gone
+sir
+hadn
+ship
+##ka
+studies
+academy
+shot
+rights
+below
+base
+bad
+involved
+kept
+largest
+##ist
+bank
+future
+especially
+beginning
+mark
+movement
+section
+female
+magazine
+plan
+professor
+lord
+longer
+##ian
+sat
+walked
+hill
+actually
+civil
+energy
+model
+families
+size
+thus
+aircraft
+completed
+includes
+data
+captain
+##or
+fight
+vocals
+featured
+richard
+bridge
+fourth
+1989
+officer
+stone
+hear
+##ism
+means
+medical
+groups
+management
+self
+lips
+competition
+entire
+lived
+technology
+leaving
+federal
+tournament
+bit
+passed
+hot
+independent
+awards
+kingdom
+mary
+spent
+fine
+doesn
+reported
+##ling
+jack
+fall
+raised
+itself
+stay
+true
+studio
+1988
+sports
+replaced
+paris
+systems
+saint
+leader
+theatre
+whose
+market
+capital
+parents
+spanish
+canadian
+earth
+##ity
+cut
+degree
+writing
+bay
+christian
+awarded
+natural
+higher
+bill
+##as
+coast
+provided
+previous
+senior
+ft
+valley
+organization
+stopped
+onto
+countries
+parts
+conference
+queen
+security
+interest
+saying
+allowed
+master
+earlier
+phone
+matter
+smith
+winning
+try
+happened
+moving
+campaign
+los
+##ley
+breath
+nearly
+mid
+1987
+certain
+girls
+date
+italian
+african
+standing
+fell
+artist
+##ted
+shows
+deal
+mine
+industry
+1986
+##ng
+everyone
+republic
+provide
+collection
+library
+student
+##ville
+primary
+owned
+older
+via
+heavy
+1st
+makes
+##able
+attention
+anyone
+africa
+##ri
+stated
+length
+ended
+fingers
+command
+staff
+skin
+foreign
+opening
+governor
+okay
+medal
+kill
+sun
+cover
+job
+1985
+introduced
+chest
+hell
+feeling
+##ies
+success
+meet
+reason
+standard
+meeting
+novel
+1984
+trade
+source
+buildings
+##land
+rose
+guy
+goal
+##ur
+chapter
+native
+husband
+previously
+unit
+limited
+entered
+weeks
+producer
+operations
+mountain
+takes
+covered
+forced
+related
+roman
+complete
+successful
+key
+texas
+cold
+##ya
+channel
+1980
+traditional
+films
+dance
+clear
+approximately
+500
+nine
+van
+prince
+question
+active
+tracks
+ireland
+regional
+silver
+author
+personal
+sense
+operation
+##ine
+economic
+1983
+holding
+twenty
+isbn
+additional
+speed
+hour
+edition
+regular
+historic
+places
+whom
+shook
+movie
+km²
+secretary
+prior
+report
+chicago
+read
+foundation
+view
+engine
+scored
+1982
+units
+ask
+airport
+property
+ready
+immediately
+lady
+month
+listed
+contract
+##de
+manager
+themselves
+lines
+##ki
+navy
+writer
+meant
+##ts
+runs
+##ro
+practice
+championships
+singer
+glass
+commission
+required
+forest
+starting
+culture
+generally
+giving
+access
+attended
+test
+couple
+stand
+catholic
+martin
+caught
+executive
+##less
+eye
+##ey
+thinking
+chair
+quite
+shoulder
+1979
+hope
+decision
+plays
+defeated
+municipality
+whether
+structure
+offered
+slowly
+pain
+ice
+direction
+##ion
+paper
+mission
+1981
+mostly
+200
+noted
+individual
+managed
+nature
+lives
+plant
+##ha
+helped
+except
+studied
+computer
+figure
+relationship
+issue
+significant
+loss
+die
+smiled
+gun
+ago
+highest
+1972
+##am
+male
+bring
+goals
+mexico
+problem
+distance
+commercial
+completely
+location
+annual
+famous
+drive
+1976
+neck
+1978
+surface
+caused
+italy
+understand
+greek
+highway
+wrong
+hotel
+comes
+appearance
+joseph
+double
+issues
+musical
+companies
+castle
+income
+review
+assembly
+bass
+initially
+parliament
+artists
+experience
+1974
+particular
+walk
+foot
+engineering
+talking
+window
+dropped
+##ter
+miss
+baby
+boys
+break
+1975
+stars
+edge
+remember
+policy
+carried
+train
+stadium
+bar
+sex
+angeles
+evidence
+##ge
+becoming
+assistant
+soviet
+1977
+upper
+step
+wing
+1970
+youth
+financial
+reach
+##ll
+actor
+numerous
+##se
+##st
+nodded
+arrived
+##ation
+minute
+##nt
+believed
+sorry
+complex
+beautiful
+victory
+associated
+temple
+1968
+1973
+chance
+perhaps
+metal
+##son
+1945
+bishop
+##et
+lee
+launched
+particularly
+tree
+le
+retired
+subject
+prize
+contains
+yeah
+theory
+empire
+##ce
+suddenly
+waiting
+trust
+recording
+##to
+happy
+terms
+camp
+champion
+1971
+religious
+pass
+zealand
+names
+2nd
+port
+ancient
+tom
+corner
+represented
+watch
+legal
+anti
+justice
+cause
+watched
+brothers
+45
+material
+changes
+simply
+response
+louis
+fast
+##ting
+answer
+60
+historical
+1969
+stories
+straight
+create
+feature
+increased
+rate
+administration
+virginia
+el
+activities
+cultural
+overall
+winner
+programs
+basketball
+legs
+guard
+beyond
+cast
+doctor
+mm
+flight
+results
+remains
+cost
+effect
+winter
+##ble
+larger
+islands
+problems
+chairman
+grew
+commander
+isn
+1967
+pay
+failed
+selected
+hurt
+fort
+box
+regiment
+majority
+journal
+35
+edward
+plans
+##ke
+##ni
+shown
+pretty
+irish
+characters
+directly
+scene
+likely
+operated
+allow
+spring
+##j
+junior
+matches
+looks
+mike
+houses
+fellow
+##tion
+beach
+marriage
+##ham
+##ive
+rules
+oil
+65
+florida
+expected
+nearby
+congress
+sam
+peace
+recent
+iii
+wait
+subsequently
+cell
+##do
+variety
+serving
+agreed
+please
+poor
+joe
+pacific
+attempt
+wood
+democratic
+piece
+prime
+##ca
+rural
+mile
+touch
+appears
+township
+1964
+1966
+soldiers
+##men
+##ized
+1965
+pennsylvania
+closer
+fighting
+claimed
+score
+jones
+physical
+editor
+##ous
+filled
+genus
+specific
+sitting
+super
+mom
+##va
+therefore
+supported
+status
+fear
+cases
+store
+meaning
+wales
+minor
+spain
+tower
+focus
+vice
+frank
+follow
+parish
+separate
+golden
+horse
+fifth
+remaining
+branch
+32
+presented
+stared
+##id
+uses
+secret
+forms
+##co
+baseball
+exactly
+##ck
+choice
+note
+discovered
+travel
+composed
+truth
+russia
+ball
+color
+kiss
+dad
+wind
+continue
+ring
+referred
+numbers
+digital
+greater
+##ns
+metres
+slightly
+direct
+increase
+1960
+responsible
+crew
+rule
+trees
+troops
+##no
+broke
+goes
+individuals
+hundred
+weight
+creek
+sleep
+memory
+defense
+provides
+ordered
+code
+value
+jewish
+windows
+1944
+safe
+judge
+whatever
+corps
+realized
+growing
+pre
+##ga
+cities
+alexander
+gaze
+lies
+spread
+scott
+letter
+showed
+situation
+mayor
+transport
+watching
+workers
+extended
+##li
+expression
+normal
+##ment
+chart
+multiple
+border
+##ba
+host
+##ner
+daily
+mrs
+walls
+piano
+##ko
+heat
+cannot
+##ate
+earned
+products
+drama
+era
+authority
+seasons
+join
+grade
+##io
+sign
+difficult
+machine
+1963
+territory
+mainly
+##wood
+stations
+squadron
+1962
+stepped
+iron
+19th
+##led
+serve
+appear
+sky
+speak
+broken
+charge
+knowledge
+kilometres
+removed
+ships
+article
+campus
+simple
+##ty
+pushed
+britain
+##ve
+leaves
+recently
+cd
+soft
+boston
+latter
+easy
+acquired
+poland
+##sa
+quality
+officers
+presence
+planned
+nations
+mass
+broadcast
+jean
+share
+image
+influence
+wild
+offer
+emperor
+electric
+reading
+headed
+ability
+promoted
+yellow
+ministry
+1942
+throat
+smaller
+politician
+##by
+latin
+spoke
+cars
+williams
+males
+lack
+pop
+80
+##ier
+acting
+seeing
+consists
+##ti
+estate
+1961
+pressure
+johnson
+newspaper
+jr
+chris
+olympics
+online
+conditions
+beat
+elements
+walking
+vote
+##field
+needs
+carolina
+text
+featuring
+global
+block
+shirt
+levels
+francisco
+purpose
+females
+et
+dutch
+duke
+ahead
+gas
+twice
+safety
+serious
+turning
+highly
+lieutenant
+firm
+maria
+amount
+mixed
+daniel
+proposed
+perfect
+agreement
+affairs
+3rd
+seconds
+contemporary
+paid
+1943
+prison
+save
+kitchen
+label
+administrative
+intended
+constructed
+academic
+nice
+teacher
+races
+1956
+formerly
+corporation
+ben
+nation
+issued
+shut
+1958
+drums
+housing
+victoria
+seems
+opera
+1959
+graduated
+function
+von
+mentioned
+picked
+build
+recognized
+shortly
+protection
+picture
+notable
+exchange
+elections
+1980s
+loved
+percent
+racing
+fish
+elizabeth
+garden
+volume
+hockey
+1941
+beside
+settled
+##ford
+1940
+competed
+replied
+drew
+1948
+actress
+marine
+scotland
+steel
+glanced
+farm
+steve
+1957
+risk
+tonight
+positive
+magic
+singles
+effects
+gray
+screen
+dog
+##ja
+residents
+bus
+sides
+none
+secondary
+literature
+polish
+destroyed
+flying
+founder
+households
+1939
+lay
+reserve
+usa
+gallery
+##ler
+1946
+industrial
+younger
+approach
+appearances
+urban
+ones
+1950
+finish
+avenue
+powerful
+fully
+growth
+page
+honor
+jersey
+projects
+advanced
+revealed
+basic
+90
+infantry
+pair
+equipment
+visit
+33
+evening
+search
+grant
+effort
+solo
+treatment
+buried
+republican
+primarily
+bottom
+owner
+1970s
+israel
+gives
+jim
+dream
+bob
+remain
+spot
+70
+notes
+produce
+champions
+contact
+ed
+soul
+accepted
+ways
+del
+##ally
+losing
+split
+price
+capacity
+basis
+trial
+questions
+##ina
+1955
+20th
+guess
+officially
+memorial
+naval
+initial
+##ization
+whispered
+median
+engineer
+##ful
+sydney
+##go
+columbia
+strength
+300
+1952
+tears
+senate
+00
+card
+asian
+agent
+1947
+software
+44
+draw
+warm
+supposed
+com
+pro
+##il
+transferred
+leaned
+##at
+candidate
+escape
+mountains
+asia
+potential
+activity
+entertainment
+seem
+traffic
+jackson
+murder
+36
+slow
+product
+orchestra
+haven
+agency
+bbc
+taught
+website
+comedy
+unable
+storm
+planning
+albums
+rugby
+environment
+scientific
+grabbed
+protect
+##hi
+boat
+typically
+1954
+1953
+damage
+principal
+divided
+dedicated
+mount
+ohio
+##berg
+pick
+fought
+driver
+##der
+empty
+shoulders
+sort
+thank
+berlin
+prominent
+account
+freedom
+necessary
+efforts
+alex
+headquarters
+follows
+alongside
+des
+simon
+andrew
+suggested
+operating
+learning
+steps
+1949
+sweet
+technical
+begin
+easily
+34
+teeth
+speaking
+settlement
+scale
+##sh
+renamed
+ray
+max
+enemy
+semi
+joint
+compared
+##rd
+scottish
+leadership
+analysis
+offers
+georgia
+pieces
+captured
+animal
+deputy
+guest
+organized
+##lin
+tony
+combined
+method
+challenge
+1960s
+huge
+wants
+battalion
+sons
+rise
+crime
+types
+facilities
+telling
+path
+1951
+platform
+sit
+1990s
+##lo
+tells
+assigned
+rich
+pull
+##ot
+commonly
+alive
+##za
+letters
+concept
+conducted
+wearing
+happen
+bought
+becomes
+holy
+gets
+ocean
+defeat
+languages
+purchased
+coffee
+occurred
+titled
+##q
+declared
+applied
+sciences
+concert
+sounds
+jazz
+brain
+##me
+painting
+fleet
+tax
+nick
+##ius
+michigan
+count
+animals
+leaders
+episodes
+##line
+content
+##den
+birth
+##it
+clubs
+64
+palace
+critical
+refused
+fair
+leg
+laughed
+returning
+surrounding
+participated
+formation
+lifted
+pointed
+connected
+rome
+medicine
+laid
+taylor
+santa
+powers
+adam
+tall
+shared
+focused
+knowing
+yards
+entrance
+falls
+##wa
+calling
+##ad
+sources
+chosen
+beneath
+resources
+yard
+##ite
+nominated
+silence
+zone
+defined
+##que
+gained
+thirty
+38
+bodies
+moon
+##ard
+adopted
+christmas
+widely
+register
+apart
+iran
+premier
+serves
+du
+unknown
+parties
+##les
+generation
+##ff
+continues
+quick
+fields
+brigade
+quiet
+teaching
+clothes
+impact
+weapons
+partner
+flat
+theater
+supreme
+1938
+37
+relations
+##tor
+plants
+suffered
+1936
+wilson
+kids
+begins
+##age
+1918
+seats
+armed
+internet
+models
+worth
+laws
+400
+communities
+classes
+background
+knows
+thanks
+quarter
+reaching
+humans
+carry
+killing
+format
+kong
+hong
+setting
+75
+architecture
+disease
+railroad
+inc
+possibly
+wish
+arthur
+thoughts
+harry
+doors
+density
+##di
+crowd
+illinois
+stomach
+tone
+unique
+reports
+anyway
+##ir
+liberal
+der
+vehicle
+thick
+dry
+drug
+faced
+largely
+facility
+theme
+holds
+creation
+strange
+colonel
+##mi
+revolution
+bell
+politics
+turns
+silent
+rail
+relief
+independence
+combat
+shape
+write
+determined
+sales
+learned
+4th
+finger
+oxford
+providing
+1937
+heritage
+fiction
+situated
+designated
+allowing
+distribution
+hosted
+##est
+sight
+interview
+estimated
+reduced
+##ria
+toronto
+footballer
+keeping
+guys
+damn
+claim
+motion
+sport
+sixth
+stayed
+##ze
+en
+rear
+receive
+handed
+twelve
+dress
+audience
+granted
+brazil
+##well
+spirit
+##ated
+noticed
+etc
+olympic
+representative
+eric
+tight
+trouble
+reviews
+drink
+vampire
+missing
+roles
+ranked
+newly
+household
+finals
+wave
+critics
+##ee
+phase
+massachusetts
+pilot
+unlike
+philadelphia
+bright
+guns
+crown
+organizations
+roof
+42
+respectively
+clearly
+tongue
+marked
+circle
+fox
+korea
+bronze
+brian
+expanded
+sexual
+supply
+yourself
+inspired
+labour
+fc
+##ah
+reference
+vision
+draft
+connection
+brand
+reasons
+1935
+classic
+driving
+trip
+jesus
+cells
+entry
+1920
+neither
+trail
+claims
+atlantic
+orders
+labor
+nose
+afraid
+identified
+intelligence
+calls
+cancer
+attacked
+passing
+stephen
+positions
+imperial
+grey
+jason
+39
+sunday
+48
+swedish
+avoid
+extra
+uncle
+message
+covers
+allows
+surprise
+materials
+fame
+hunter
+##ji
+1930
+citizens
+figures
+davis
+environmental
+confirmed
+shit
+titles
+di
+performing
+difference
+acts
+attacks
+##ov
+existing
+votes
+opportunity
+nor
+shop
+entirely
+trains
+opposite
+pakistan
+##pa
+develop
+resulted
+representatives
+actions
+reality
+pressed
+##ish
+barely
+wine
+conversation
+faculty
+northwest
+ends
+documentary
+nuclear
+stock
+grace
+sets
+eat
+alternative
+##ps
+bag
+resulting
+creating
+surprised
+cemetery
+1919
+drop
+finding
+sarah
+cricket
+streets
+tradition
+ride
+1933
+exhibition
+target
+ear
+explained
+rain
+composer
+injury
+apartment
+municipal
+educational
+occupied
+netherlands
+clean
+billion
+constitution
+learn
+1914
+maximum
+classical
+francis
+lose
+opposition
+jose
+ontario
+bear
+core
+hills
+rolled
+ending
+drawn
+permanent
+fun
+##tes
+##lla
+lewis
+sites
+chamber
+ryan
+##way
+scoring
+height
+1934
+##house
+lyrics
+staring
+55
+officials
+1917
+snow
+oldest
+##tic
+orange
+##ger
+qualified
+interior
+apparently
+succeeded
+thousand
+dinner
+lights
+existence
+fans
+heavily
+41
+greatest
+conservative
+send
+bowl
+plus
+enter
+catch
+##un
+economy
+duty
+1929
+speech
+authorities
+princess
+performances
+versions
+shall
+graduate
+pictures
+effective
+remembered
+poetry
+desk
+crossed
+starring
+starts
+passenger
+sharp
+##ant
+acres
+ass
+weather
+falling
+rank
+fund
+supporting
+check
+adult
+publishing
+heads
+cm
+southeast
+lane
+##burg
+application
+bc
+##ura
+les
+condition
+transfer
+prevent
+display
+ex
+regions
+earl
+federation
+cool
+relatively
+answered
+besides
+1928
+obtained
+portion
+##town
+mix
+##ding
+reaction
+liked
+dean
+express
+peak
+1932
+##tte
+counter
+religion
+chain
+rare
+miller
+convention
+aid
+lie
+vehicles
+mobile
+perform
+squad
+wonder
+lying
+crazy
+sword
+##ping
+attempted
+centuries
+weren
+philosophy
+category
+##ize
+anna
+interested
+47
+sweden
+wolf
+frequently
+abandoned
+kg
+literary
+alliance
+task
+entitled
+##ay
+threw
+promotion
+factory
+tiny
+soccer
+visited
+matt
+fm
+achieved
+52
+defence
+internal
+persian
+43
+methods
+##ging
+arrested
+otherwise
+cambridge
+programming
+villages
+elementary
+districts
+rooms
+criminal
+conflict
+worry
+trained
+1931
+attempts
+waited
+signal
+bird
+truck
+subsequent
+programme
+##ol
+ad
+49
+communist
+details
+faith
+sector
+patrick
+carrying
+laugh
+##ss
+controlled
+korean
+showing
+origin
+fuel
+evil
+1927
+##ent
+brief
+identity
+darkness
+address
+pool
+missed
+publication
+web
+planet
+ian
+anne
+wings
+invited
+##tt
+briefly
+standards
+kissed
+##be
+ideas
+climate
+causing
+walter
+worse
+albert
+articles
+winners
+desire
+aged
+northeast
+dangerous
+gate
+doubt
+1922
+wooden
+multi
+##ky
+poet
+rising
+funding
+46
+communications
+communication
+violence
+copies
+prepared
+ford
+investigation
+skills
+1924
+pulling
+electronic
+##ak
+##ial
+##han
+containing
+ultimately
+offices
+singing
+understanding
+restaurant
+tomorrow
+fashion
+christ
+ward
+da
+pope
+stands
+5th
+flow
+studios
+aired
+commissioned
+contained
+exist
+fresh
+americans
+##per
+wrestling
+approved
+kid
+employed
+respect
+suit
+1925
+angel
+asking
+increasing
+frame
+angry
+selling
+1950s
+thin
+finds
+##nd
+temperature
+statement
+ali
+explain
+inhabitants
+towns
+extensive
+narrow
+51
+jane
+flowers
+images
+promise
+somewhere
+object
+fly
+closely
+##ls
+1912
+bureau
+cape
+1926
+weekly
+presidential
+legislative
+1921
+##ai
+##au
+launch
+founding
+##ny
+978
+##ring
+artillery
+strike
+un
+institutions
+roll
+writers
+landing
+chose
+kevin
+anymore
+pp
+##ut
+attorney
+fit
+dan
+billboard
+receiving
+agricultural
+breaking
+sought
+dave
+admitted
+lands
+mexican
+##bury
+charlie
+specifically
+hole
+iv
+howard
+credit
+moscow
+roads
+accident
+1923
+proved
+wear
+struck
+hey
+guards
+stuff
+slid
+expansion
+1915
+cat
+anthony
+##kin
+melbourne
+opposed
+sub
+southwest
+architect
+failure
+plane
+1916
+##ron
+map
+camera
+tank
+listen
+regarding
+wet
+introduction
+metropolitan
+link
+ep
+fighter
+inch
+grown
+gene
+anger
+fixed
+buy
+dvd
+khan
+domestic
+worldwide
+chapel
+mill
+functions
+examples
+##head
+developing
+1910
+turkey
+hits
+pocket
+antonio
+papers
+grow
+unless
+circuit
+18th
+concerned
+attached
+journalist
+selection
+journey
+converted
+provincial
+painted
+hearing
+aren
+bands
+negative
+aside
+wondered
+knight
+lap
+survey
+ma
+##ow
+noise
+billy
+##ium
+shooting
+guide
+bedroom
+priest
+resistance
+motor
+homes
+sounded
+giant
+##mer
+150
+scenes
+equal
+comic
+patients
+hidden
+solid
+actual
+bringing
+afternoon
+touched
+funds
+wedding
+consisted
+marie
+canal
+sr
+kim
+treaty
+turkish
+recognition
+residence
+cathedral
+broad
+knees
+incident
+shaped
+fired
+norwegian
+handle
+cheek
+contest
+represent
+##pe
+representing
+beauty
+##sen
+birds
+advantage
+emergency
+wrapped
+drawing
+notice
+pink
+broadcasting
+##ong
+somehow
+bachelor
+seventh
+collected
+registered
+establishment
+alan
+assumed
+chemical
+personnel
+roger
+retirement
+jeff
+portuguese
+wore
+tied
+device
+threat
+progress
+advance
+##ised
+banks
+hired
+manchester
+nfl
+teachers
+structures
+forever
+##bo
+tennis
+helping
+saturday
+sale
+applications
+junction
+hip
+incorporated
+neighborhood
+dressed
+ceremony
+##ds
+influenced
+hers
+visual
+stairs
+decades
+inner
+kansas
+hung
+hoped
+gain
+scheduled
+downtown
+engaged
+austria
+clock
+norway
+certainly
+pale
+protected
+1913
+victor
+employees
+plate
+putting
+surrounded
+##ists
+finishing
+blues
+tropical
+##ries
+minnesota
+consider
+philippines
+accept
+54
+retrieved
+1900
+concern
+anderson
+properties
+institution
+gordon
+successfully
+vietnam
+##dy
+backing
+outstanding
+muslim
+crossing
+folk
+producing
+usual
+demand
+occurs
+observed
+lawyer
+educated
+##ana
+kelly
+string
+pleasure
+budget
+items
+quietly
+colorado
+philip
+typical
+##worth
+derived
+600
+survived
+asks
+mental
+##ide
+56
+jake
+jews
+distinguished
+ltd
+1911
+sri
+extremely
+53
+athletic
+loud
+thousands
+worried
+shadow
+transportation
+horses
+weapon
+arena
+importance
+users
+tim
+objects
+contributed
+dragon
+douglas
+aware
+senator
+johnny
+jordan
+sisters
+engines
+flag
+investment
+samuel
+shock
+capable
+clark
+row
+wheel
+refers
+session
+familiar
+biggest
+wins
+hate
+maintained
+drove
+hamilton
+request
+expressed
+injured
+underground
+churches
+walker
+wars
+tunnel
+passes
+stupid
+agriculture
+softly
+cabinet
+regarded
+joining
+indiana
+##ea
+##ms
+push
+dates
+spend
+behavior
+woods
+protein
+gently
+chase
+morgan
+mention
+burning
+wake
+combination
+occur
+mirror
+leads
+jimmy
+indeed
+impossible
+singapore
+paintings
+covering
+##nes
+soldier
+locations
+attendance
+sell
+historian
+wisconsin
+invasion
+argued
+painter
+diego
+changing
+egypt
+##don
+experienced
+inches
+##ku
+missouri
+vol
+grounds
+spoken
+switzerland
+##gan
+reform
+rolling
+ha
+forget
+massive
+resigned
+burned
+allen
+tennessee
+locked
+values
+improved
+##mo
+wounded
+universe
+sick
+dating
+facing
+pack
+purchase
+user
+##pur
+moments
+##ul
+merged
+anniversary
+1908
+coal
+brick
+understood
+causes
+dynasty
+queensland
+establish
+stores
+crisis
+promote
+hoping
+views
+cards
+referee
+extension
+##si
+raise
+arizona
+improve
+colonial
+formal
+charged
+##rt
+palm
+lucky
+hide
+rescue
+faces
+95
+feelings
+candidates
+juan
+##ell
+goods
+6th
+courses
+weekend
+59
+luke
+cash
+fallen
+##om
+delivered
+affected
+installed
+carefully
+tries
+swiss
+hollywood
+costs
+lincoln
+responsibility
+##he
+shore
+file
+proper
+normally
+maryland
+assistance
+jump
+constant
+offering
+friendly
+waters
+persons
+realize
+contain
+trophy
+800
+partnership
+factor
+58
+musicians
+cry
+bound
+oregon
+indicated
+hero
+houston
+medium
+##ure
+consisting
+somewhat
+##ara
+57
+cycle
+##che
+beer
+moore
+frederick
+gotten
+eleven
+worst
+weak
+approached
+arranged
+chin
+loan
+universal
+bond
+fifteen
+pattern
+disappeared
+##ney
+translated
+##zed
+lip
+arab
+capture
+interests
+insurance
+##chi
+shifted
+cave
+prix
+warning
+sections
+courts
+coat
+plot
+smell
+feed
+golf
+favorite
+maintain
+knife
+vs
+voted
+degrees
+finance
+quebec
+opinion
+translation
+manner
+ruled
+operate
+productions
+choose
+musician
+discovery
+confused
+tired
+separated
+stream
+techniques
+committed
+attend
+ranking
+kings
+throw
+passengers
+measure
+horror
+fan
+mining
+sand
+danger
+salt
+calm
+decade
+dam
+require
+runner
+##ik
+rush
+associate
+greece
+##ker
+rivers
+consecutive
+matthew
+##ski
+sighed
+sq
+documents
+steam
+edited
+closing
+tie
+accused
+1905
+##ini
+islamic
+distributed
+directors
+organisation
+bruce
+7th
+breathing
+mad
+lit
+arrival
+concrete
+taste
+08
+composition
+shaking
+faster
+amateur
+adjacent
+stating
+1906
+twin
+flew
+##ran
+tokyo
+publications
+##tone
+obviously
+ridge
+storage
+1907
+carl
+pages
+concluded
+desert
+driven
+universities
+ages
+terminal
+sequence
+borough
+250
+constituency
+creative
+cousin
+economics
+dreams
+margaret
+notably
+reduce
+montreal
+mode
+17th
+ears
+saved
+jan
+vocal
+##ica
+1909
+andy
+##jo
+riding
+roughly
+threatened
+##ise
+meters
+meanwhile
+landed
+compete
+repeated
+grass
+czech
+regularly
+charges
+tea
+sudden
+appeal
+##ung
+solution
+describes
+pierre
+classification
+glad
+parking
+##ning
+belt
+physics
+99
+rachel
+add
+hungarian
+participate
+expedition
+damaged
+gift
+childhood
+85
+fifty
+##red
+mathematics
+jumped
+letting
+defensive
+mph
+##ux
+##gh
+testing
+##hip
+hundreds
+shoot
+owners
+matters
+smoke
+israeli
+kentucky
+dancing
+mounted
+grandfather
+emma
+designs
+profit
+argentina
+##gs
+truly
+li
+lawrence
+cole
+begun
+detroit
+willing
+branches
+smiling
+decide
+miami
+enjoyed
+recordings
+##dale
+poverty
+ethnic
+gay
+##bi
+gary
+arabic
+09
+accompanied
+##one
+##ons
+fishing
+determine
+residential
+acid
+##ary
+alice
+returns
+starred
+mail
+##ang
+jonathan
+strategy
+##ue
+net
+forty
+cook
+businesses
+equivalent
+commonwealth
+distinct
+ill
+##cy
+seriously
+##ors
+##ped
+shift
+harris
+replace
+rio
+imagine
+formula
+ensure
+##ber
+additionally
+scheme
+conservation
+occasionally
+purposes
+feels
+favor
+##and
+##ore
+1930s
+contrast
+hanging
+hunt
+movies
+1904
+instruments
+victims
+danish
+christopher
+busy
+demon
+sugar
+earliest
+colony
+studying
+balance
+duties
+##ks
+belgium
+slipped
+carter
+05
+visible
+stages
+iraq
+fifa
+##im
+commune
+forming
+zero
+07
+continuing
+talked
+counties
+legend
+bathroom
+option
+tail
+clay
+daughters
+afterwards
+severe
+jaw
+visitors
+##ded
+devices
+aviation
+russell
+kate
+##vi
+entering
+subjects
+##ino
+temporary
+swimming
+forth
+smooth
+ghost
+audio
+bush
+operates
+rocks
+movements
+signs
+eddie
+##tz
+ann
+voices
+honorary
+06
+memories
+dallas
+pure
+measures
+racial
+promised
+66
+harvard
+ceo
+16th
+parliamentary
+indicate
+benefit
+flesh
+dublin
+louisiana
+1902
+1901
+patient
+sleeping
+1903
+membership
+coastal
+medieval
+wanting
+element
+scholars
+rice
+62
+limit
+survive
+makeup
+rating
+definitely
+collaboration
+obvious
+##tan
+boss
+ms
+baron
+birthday
+linked
+soil
+diocese
+##lan
+ncaa
+##mann
+offensive
+shell
+shouldn
+waist
+##tus
+plain
+ross
+organ
+resolution
+manufacturing
+adding
+relative
+kennedy
+98
+whilst
+moth
+marketing
+gardens
+crash
+72
+heading
+partners
+credited
+carlos
+moves
+cable
+##zi
+marshall
+##out
+depending
+bottle
+represents
+rejected
+responded
+existed
+04
+jobs
+denmark
+lock
+##ating
+treated
+graham
+routes
+talent
+commissioner
+drugs
+secure
+tests
+reign
+restored
+photography
+##gi
+contributions
+oklahoma
+designer
+disc
+grin
+seattle
+robin
+paused
+atlanta
+unusual
+##gate
+praised
+las
+laughing
+satellite
+hungary
+visiting
+##sky
+interesting
+factors
+deck
+poems
+norman
+##water
+stuck
+speaker
+rifle
+domain
+premiered
+##her
+dc
+comics
+actors
+01
+reputation
+eliminated
+8th
+ceiling
+prisoners
+script
+##nce
+leather
+austin
+mississippi
+rapidly
+admiral
+parallel
+charlotte
+guilty
+tools
+gender
+divisions
+fruit
+##bs
+laboratory
+nelson
+fantasy
+marry
+rapid
+aunt
+tribe
+requirements
+aspects
+suicide
+amongst
+adams
+bone
+ukraine
+abc
+kick
+sees
+edinburgh
+clothing
+column
+rough
+gods
+hunting
+broadway
+gathered
+concerns
+##ek
+spending
+ty
+12th
+snapped
+requires
+solar
+bones
+cavalry
+##tta
+iowa
+drinking
+waste
+index
+franklin
+charity
+thompson
+stewart
+tip
+flash
+landscape
+friday
+enjoy
+singh
+poem
+listening
+##back
+eighth
+fred
+differences
+adapted
+bomb
+ukrainian
+surgery
+corporate
+masters
+anywhere
+##more
+waves
+odd
+sean
+portugal
+orleans
+dick
+debate
+kent
+eating
+puerto
+cleared
+96
+expect
+cinema
+97
+guitarist
+blocks
+electrical
+agree
+involving
+depth
+dying
+panel
+struggle
+##ged
+peninsula
+adults
+novels
+emerged
+vienna
+metro
+debuted
+shoes
+tamil
+songwriter
+meets
+prove
+beating
+instance
+heaven
+scared
+sending
+marks
+artistic
+passage
+superior
+03
+significantly
+shopping
+##tive
+retained
+##izing
+malaysia
+technique
+cheeks
+##ola
+warren
+maintenance
+destroy
+extreme
+allied
+120
+appearing
+##yn
+fill
+advice
+alabama
+qualifying
+policies
+cleveland
+hat
+battery
+smart
+authors
+10th
+soundtrack
+acted
+dated
+lb
+glance
+equipped
+coalition
+funny
+outer
+ambassador
+roy
+possibility
+couples
+campbell
+dna
+loose
+ethan
+supplies
+1898
+gonna
+88
+monster
+##res
+shake
+agents
+frequency
+springs
+dogs
+practices
+61
+gang
+plastic
+easier
+suggests
+gulf
+blade
+exposed
+colors
+industries
+markets
+pan
+nervous
+electoral
+charts
+legislation
+ownership
+##idae
+mac
+appointment
+shield
+copy
+assault
+socialist
+abbey
+monument
+license
+throne
+employment
+jay
+93
+replacement
+charter
+cloud
+powered
+suffering
+accounts
+oak
+connecticut
+strongly
+wright
+colour
+crystal
+13th
+context
+welsh
+networks
+voiced
+gabriel
+jerry
+##cing
+forehead
+mp
+##ens
+manage
+schedule
+totally
+remix
+##ii
+forests
+occupation
+print
+nicholas
+brazilian
+strategic
+vampires
+engineers
+76
+roots
+seek
+correct
+instrumental
+und
+alfred
+backed
+hop
+##des
+stanley
+robinson
+traveled
+wayne
+welcome
+austrian
+achieve
+67
+exit
+rates
+1899
+strip
+whereas
+##cs
+sing
+deeply
+adventure
+bobby
+rick
+jamie
+careful
+components
+cap
+useful
+personality
+knee
+##shi
+pushing
+hosts
+02
+protest
+ca
+ottoman
+symphony
+##sis
+63
+boundary
+1890
+processes
+considering
+considerable
+tons
+##work
+##ft
+##nia
+cooper
+trading
+dear
+conduct
+91
+illegal
+apple
+revolutionary
+holiday
+definition
+harder
+##van
+jacob
+circumstances
+destruction
+##lle
+popularity
+grip
+classified
+liverpool
+donald
+baltimore
+flows
+seeking
+honour
+approval
+92
+mechanical
+till
+happening
+statue
+critic
+increasingly
+immediate
+describe
+commerce
+stare
+##ster
+indonesia
+meat
+rounds
+boats
+baker
+orthodox
+depression
+formally
+worn
+naked
+claire
+muttered
+sentence
+11th
+emily
+document
+77
+criticism
+wished
+vessel
+spiritual
+bent
+virgin
+parker
+minimum
+murray
+lunch
+danny
+printed
+compilation
+keyboards
+false
+blow
+belonged
+68
+raising
+78
+cutting
+##board
+pittsburgh
+##up
+9th
+shadows
+81
+hated
+indigenous
+jon
+15th
+barry
+scholar
+ah
+##zer
+oliver
+##gy
+stick
+susan
+meetings
+attracted
+spell
+romantic
+##ver
+ye
+1895
+photo
+demanded
+customers
+##ac
+1896
+logan
+revival
+keys
+modified
+commanded
+jeans
+##ious
+upset
+raw
+phil
+detective
+hiding
+resident
+vincent
+##bly
+experiences
+diamond
+defeating
+coverage
+lucas
+external
+parks
+franchise
+helen
+bible
+successor
+percussion
+celebrated
+il
+lift
+profile
+clan
+romania
+##ied
+mills
+##su
+nobody
+achievement
+shrugged
+fault
+1897
+rhythm
+initiative
+breakfast
+carbon
+700
+69
+lasted
+violent
+74
+wound
+ken
+killer
+gradually
+filmed
+°c
+dollars
+processing
+94
+remove
+criticized
+guests
+sang
+chemistry
+##vin
+legislature
+disney
+##bridge
+uniform
+escaped
+integrated
+proposal
+purple
+denied
+liquid
+karl
+influential
+morris
+nights
+stones
+intense
+experimental
+twisted
+71
+84
+##ld
+pace
+nazi
+mitchell
+ny
+blind
+reporter
+newspapers
+14th
+centers
+burn
+basin
+forgotten
+surviving
+filed
+collections
+monastery
+losses
+manual
+couch
+description
+appropriate
+merely
+tag
+missions
+sebastian
+restoration
+replacing
+triple
+73
+elder
+julia
+warriors
+benjamin
+julian
+convinced
+stronger
+amazing
+declined
+versus
+merchant
+happens
+output
+finland
+bare
+barbara
+absence
+ignored
+dawn
+injuries
+##port
+producers
+##ram
+82
+luis
+##ities
+kw
+admit
+expensive
+electricity
+nba
+exception
+symbol
+##ving
+ladies
+shower
+sheriff
+characteristics
+##je
+aimed
+button
+ratio
+effectively
+summit
+angle
+jury
+bears
+foster
+vessels
+pants
+executed
+evans
+dozen
+advertising
+kicked
+patrol
+1889
+competitions
+lifetime
+principles
+athletics
+##logy
+birmingham
+sponsored
+89
+rob
+nomination
+1893
+acoustic
+##sm
+creature
+longest
+##tra
+credits
+harbor
+dust
+josh
+##so
+territories
+milk
+infrastructure
+completion
+thailand
+indians
+leon
+archbishop
+##sy
+assist
+pitch
+blake
+arrangement
+girlfriend
+serbian
+operational
+hence
+sad
+scent
+fur
+dj
+sessions
+hp
+refer
+rarely
+##ora
+exists
+1892
+##ten
+scientists
+dirty
+penalty
+burst
+portrait
+seed
+79
+pole
+limits
+rival
+1894
+stable
+alpha
+grave
+constitutional
+alcohol
+arrest
+flower
+mystery
+devil
+architectural
+relationships
+greatly
+habitat
+##istic
+larry
+progressive
+remote
+cotton
+##ics
+##ok
+preserved
+reaches
+##ming
+cited
+86
+vast
+scholarship
+decisions
+cbs
+joy
+teach
+1885
+editions
+knocked
+eve
+searching
+partly
+participation
+gap
+animated
+fate
+excellent
+##ett
+na
+87
+alternate
+saints
+youngest
+##ily
+climbed
+##ita
+##tors
+suggest
+##ct
+discussion
+staying
+choir
+lakes
+jacket
+revenue
+nevertheless
+peaked
+instrument
+wondering
+annually
+managing
+neil
+1891
+signing
+terry
+##ice
+apply
+clinical
+brooklyn
+aim
+catherine
+fuck
+farmers
+figured
+ninth
+pride
+hugh
+evolution
+ordinary
+involvement
+comfortable
+shouted
+tech
+encouraged
+taiwan
+representation
+sharing
+##lia
+##em
+panic
+exact
+cargo
+competing
+fat
+cried
+83
+1920s
+occasions
+pa
+cabin
+borders
+utah
+marcus
+##isation
+badly
+muscles
+##ance
+victorian
+transition
+warner
+bet
+permission
+##rin
+slave
+terrible
+similarly
+shares
+seth
+uefa
+possession
+medals
+benefits
+colleges
+lowered
+perfectly
+mall
+transit
+##ye
+##kar
+publisher
+##ened
+harrison
+deaths
+elevation
+##ae
+asleep
+machines
+sigh
+ash
+hardly
+argument
+occasion
+parent
+leo
+decline
+1888
+contribution
+##ua
+concentration
+1000
+opportunities
+hispanic
+guardian
+extent
+emotions
+hips
+mason
+volumes
+bloody
+controversy
+diameter
+steady
+mistake
+phoenix
+identify
+violin
+##sk
+departure
+richmond
+spin
+funeral
+enemies
+1864
+gear
+literally
+connor
+random
+sergeant
+grab
+confusion
+1865
+transmission
+informed
+op
+leaning
+sacred
+suspended
+thinks
+gates
+portland
+luck
+agencies
+yours
+hull
+expert
+muscle
+layer
+practical
+sculpture
+jerusalem
+latest
+lloyd
+statistics
+deeper
+recommended
+warrior
+arkansas
+mess
+supports
+greg
+eagle
+1880
+recovered
+rated
+concerts
+rushed
+##ano
+stops
+eggs
+files
+premiere
+keith
+##vo
+delhi
+turner
+pit
+affair
+belief
+paint
+##zing
+mate
+##ach
+##ev
+victim
+##ology
+withdrew
+bonus
+styles
+fled
+##ud
+glasgow
+technologies
+funded
+nbc
+adaptation
+##ata
+portrayed
+cooperation
+supporters
+judges
+bernard
+justin
+hallway
+ralph
+##ick
+graduating
+controversial
+distant
+continental
+spider
+bite
+##ho
+recognize
+intention
+mixing
+##ese
+egyptian
+bow
+tourism
+suppose
+claiming
+tiger
+dominated
+participants
+vi
+##ru
+nurse
+partially
+tape
+##rum
+psychology
+##rn
+essential
+touring
+duo
+voting
+civilian
+emotional
+channels
+##king
+apparent
+hebrew
+1887
+tommy
+carrier
+intersection
+beast
+hudson
+##gar
+##zo
+lab
+nova
+bench
+discuss
+costa
+##ered
+detailed
+behalf
+drivers
+unfortunately
+obtain
+##lis
+rocky
+##dae
+siege
+friendship
+honey
+##rian
+1861
+amy
+hang
+posted
+governments
+collins
+respond
+wildlife
+preferred
+operator
+##po
+laura
+pregnant
+videos
+dennis
+suspected
+boots
+instantly
+weird
+automatic
+businessman
+alleged
+placing
+throwing
+ph
+mood
+1862
+perry
+venue
+jet
+remainder
+##lli
+##ci
+passion
+biological
+boyfriend
+1863
+dirt
+buffalo
+ron
+segment
+fa
+abuse
+##era
+genre
+thrown
+stroke
+colored
+stress
+exercise
+displayed
+##gen
+struggled
+##tti
+abroad
+dramatic
+wonderful
+thereafter
+madrid
+component
+widespread
+##sed
+tale
+citizen
+todd
+monday
+1886
+vancouver
+overseas
+forcing
+crying
+descent
+##ris
+discussed
+substantial
+ranks
+regime
+1870
+provinces
+switch
+drum
+zane
+ted
+tribes
+proof
+lp
+cream
+researchers
+volunteer
+manor
+silk
+milan
+donated
+allies
+venture
+principle
+delivery
+enterprise
+##ves
+##ans
+bars
+traditionally
+witch
+reminded
+copper
+##uk
+pete
+inter
+links
+colin
+grinned
+elsewhere
+competitive
+frequent
+##oy
+scream
+##hu
+tension
+texts
+submarine
+finnish
+defending
+defend
+pat
+detail
+1884
+affiliated
+stuart
+themes
+villa
+periods
+tool
+belgian
+ruling
+crimes
+answers
+folded
+licensed
+resort
+demolished
+hans
+lucy
+1881
+lion
+traded
+photographs
+writes
+craig
+##fa
+trials
+generated
+beth
+noble
+debt
+percentage
+yorkshire
+erected
+ss
+viewed
+grades
+confidence
+ceased
+islam
+telephone
+retail
+##ible
+chile
+m²
+roberts
+sixteen
+##ich
+commented
+hampshire
+innocent
+dual
+pounds
+checked
+regulations
+afghanistan
+sung
+rico
+liberty
+assets
+bigger
+options
+angels
+relegated
+tribute
+wells
+attending
+leaf
+##yan
+butler
+romanian
+forum
+monthly
+lisa
+patterns
+gmina
+##tory
+madison
+hurricane
+rev
+##ians
+bristol
+##ula
+elite
+valuable
+disaster
+democracy
+awareness
+germans
+freyja
+##ins
+loop
+absolutely
+paying
+populations
+maine
+sole
+prayer
+spencer
+releases
+doorway
+bull
+##ani
+lover
+midnight
+conclusion
+##sson
+thirteen
+lily
+mediterranean
+##lt
+nhl
+proud
+sample
+##hill
+drummer
+guinea
+##ova
+murphy
+climb
+##ston
+instant
+attributed
+horn
+ain
+railways
+steven
+##ao
+autumn
+ferry
+opponent
+root
+traveling
+secured
+corridor
+stretched
+tales
+sheet
+trinity
+cattle
+helps
+indicates
+manhattan
+murdered
+fitted
+1882
+gentle
+grandmother
+mines
+shocked
+vegas
+produces
+##light
+caribbean
+##ou
+belong
+continuous
+desperate
+drunk
+historically
+trio
+waved
+raf
+dealing
+nathan
+bat
+murmured
+interrupted
+residing
+scientist
+pioneer
+harold
+aaron
+##net
+delta
+attempting
+minority
+mini
+believes
+chorus
+tend
+lots
+eyed
+indoor
+load
+shots
+updated
+jail
+##llo
+concerning
+connecting
+wealth
+##ved
+slaves
+arrive
+rangers
+sufficient
+rebuilt
+##wick
+cardinal
+flood
+muhammad
+whenever
+relation
+runners
+moral
+repair
+viewers
+arriving
+revenge
+punk
+assisted
+bath
+fairly
+breathe
+lists
+innings
+illustrated
+whisper
+nearest
+voters
+clinton
+ties
+ultimate
+screamed
+beijing
+lions
+andre
+fictional
+gathering
+comfort
+radar
+suitable
+dismissed
+hms
+ban
+pine
+wrist
+atmosphere
+voivodeship
+bid
+timber
+##ned
+##nan
+giants
+##ane
+cameron
+recovery
+uss
+identical
+categories
+switched
+serbia
+laughter
+noah
+ensemble
+therapy
+peoples
+touching
+##off
+locally
+pearl
+platforms
+everywhere
+ballet
+tables
+lanka
+herbert
+outdoor
+toured
+derek
+1883
+spaces
+contested
+swept
+1878
+exclusive
+slight
+connections
+##dra
+winds
+prisoner
+collective
+bangladesh
+tube
+publicly
+wealthy
+thai
+##ys
+isolated
+select
+##ric
+insisted
+pen
+fortune
+ticket
+spotted
+reportedly
+animation
+enforcement
+tanks
+110
+decides
+wider
+lowest
+owen
+##time
+nod
+hitting
+##hn
+gregory
+furthermore
+magazines
+fighters
+solutions
+##ery
+pointing
+requested
+peru
+reed
+chancellor
+knights
+mask
+worker
+eldest
+flames
+reduction
+1860
+volunteers
+##tis
+reporting
+##hl
+wire
+advisory
+endemic
+origins
+settlers
+pursue
+knock
+consumer
+1876
+eu
+compound
+creatures
+mansion
+sentenced
+ivan
+deployed
+guitars
+frowned
+involves
+mechanism
+kilometers
+perspective
+shops
+maps
+terminus
+duncan
+alien
+fist
+bridges
+##pers
+heroes
+fed
+derby
+swallowed
+##ros
+patent
+sara
+illness
+characterized
+adventures
+slide
+hawaii
+jurisdiction
+##op
+organised
+##side
+adelaide
+walks
+biology
+se
+##ties
+rogers
+swing
+tightly
+boundaries
+##rie
+prepare
+implementation
+stolen
+##sha
+certified
+colombia
+edwards
+garage
+##mm
+recalled
+##ball
+rage
+harm
+nigeria
+breast
+##ren
+furniture
+pupils
+settle
+##lus
+cuba
+balls
+client
+alaska
+21st
+linear
+thrust
+celebration
+latino
+genetic
+terror
+##cia
+##ening
+lightning
+fee
+witness
+lodge
+establishing
+skull
+##ique
+earning
+hood
+##ei
+rebellion
+wang
+sporting
+warned
+missile
+devoted
+activist
+porch
+worship
+fourteen
+package
+1871
+decorated
+##shire
+housed
+##ock
+chess
+sailed
+doctors
+oscar
+joan
+treat
+garcia
+harbour
+jeremy
+##ire
+traditions
+dominant
+jacques
+##gon
+##wan
+relocated
+1879
+amendment
+sized
+companion
+simultaneously
+volleyball
+spun
+acre
+increases
+stopping
+loves
+belongs
+affect
+drafted
+tossed
+scout
+battles
+1875
+filming
+shoved
+munich
+tenure
+vertical
+romance
+pc
+##cher
+argue
+##ical
+craft
+ranging
+www
+opens
+honest
+tyler
+yesterday
+virtual
+##let
+muslims
+reveal
+snake
+immigrants
+radical
+screaming
+speakers
+firing
+saving
+belonging
+ease
+lighting
+prefecture
+blame
+farmer
+hungry
+grows
+rubbed
+beam
+sur
+subsidiary
+##cha
+armenian
+sao
+dropping
+conventional
+##fer
+microsoft
+reply
+qualify
+spots
+1867
+sweat
+festivals
+##ken
+immigration
+physician
+discover
+exposure
+sandy
+explanation
+isaac
+implemented
+##fish
+hart
+initiated
+connect
+stakes
+presents
+heights
+householder
+pleased
+tourist
+regardless
+slip
+closest
+##ction
+surely
+sultan
+brings
+riley
+preparation
+aboard
+slammed
+baptist
+experiment
+ongoing
+interstate
+organic
+playoffs
+##ika
+1877
+130
+##tar
+hindu
+error
+tours
+tier
+plenty
+arrangements
+talks
+trapped
+excited
+sank
+ho
+athens
+1872
+denver
+welfare
+suburb
+athletes
+trick
+diverse
+belly
+exclusively
+yelled
+1868
+##med
+conversion
+##ette
+1874
+internationally
+computers
+conductor
+abilities
+sensitive
+hello
+dispute
+measured
+globe
+rocket
+prices
+amsterdam
+flights
+tigers
+inn
+municipalities
+emotion
+references
+3d
+##mus
+explains
+airlines
+manufactured
+pm
+archaeological
+1873
+interpretation
+devon
+comment
+##ites
+settlements
+kissing
+absolute
+improvement
+suite
+impressed
+barcelona
+sullivan
+jefferson
+towers
+jesse
+julie
+##tin
+##lu
+grandson
+hi
+gauge
+regard
+rings
+interviews
+trace
+raymond
+thumb
+departments
+burns
+serial
+bulgarian
+scores
+demonstrated
+##ix
+1866
+kyle
+alberta
+underneath
+romanized
+##ward
+relieved
+acquisition
+phrase
+cliff
+reveals
+han
+cuts
+merger
+custom
+##dar
+nee
+gilbert
+graduation
+##nts
+assessment
+cafe
+difficulty
+demands
+swung
+democrat
+jennifer
+commons
+1940s
+grove
+##yo
+completing
+focuses
+sum
+substitute
+bearing
+stretch
+reception
+##py
+reflected
+essentially
+destination
+pairs
+##ched
+survival
+resource
+##bach
+promoting
+doubles
+messages
+tear
+##down
+##fully
+parade
+florence
+harvey
+incumbent
+partial
+framework
+900
+pedro
+frozen
+procedure
+olivia
+controls
+##mic
+shelter
+personally
+temperatures
+##od
+brisbane
+tested
+sits
+marble
+comprehensive
+oxygen
+leonard
+##kov
+inaugural
+iranian
+referring
+quarters
+attitude
+##ivity
+mainstream
+lined
+mars
+dakota
+norfolk
+unsuccessful
+##°
+explosion
+helicopter
+congressional
+##sing
+inspector
+bitch
+seal
+departed
+divine
+##ters
+coaching
+examination
+punishment
+manufacturer
+sink
+columns
+unincorporated
+signals
+nevada
+squeezed
+dylan
+dining
+photos
+martial
+manuel
+eighteen
+elevator
+brushed
+plates
+ministers
+ivy
+congregation
+##len
+slept
+specialized
+taxes
+curve
+restricted
+negotiations
+likes
+statistical
+arnold
+inspiration
+execution
+bold
+intermediate
+significance
+margin
+ruler
+wheels
+gothic
+intellectual
+dependent
+listened
+eligible
+buses
+widow
+syria
+earn
+cincinnati
+collapsed
+recipient
+secrets
+accessible
+philippine
+maritime
+goddess
+clerk
+surrender
+breaks
+playoff
+database
+##ified
+##lon
+ideal
+beetle
+aspect
+soap
+regulation
+strings
+expand
+anglo
+shorter
+crosses
+retreat
+tough
+coins
+wallace
+directions
+pressing
+##oon
+shipping
+locomotives
+comparison
+topics
+nephew
+##mes
+distinction
+honors
+travelled
+sierra
+ibn
+##over
+fortress
+sa
+recognised
+carved
+1869
+clients
+##dan
+intent
+##mar
+coaches
+describing
+bread
+##ington
+beaten
+northwestern
+##ona
+merit
+youtube
+collapse
+challenges
+em
+historians
+objective
+submitted
+virus
+attacking
+drake
+assume
+##ere
+diseases
+marc
+stem
+leeds
+##cus
+##ab
+farming
+glasses
+##lock
+visits
+nowhere
+fellowship
+relevant
+carries
+restaurants
+experiments
+101
+constantly
+bases
+targets
+shah
+tenth
+opponents
+verse
+territorial
+##ira
+writings
+corruption
+##hs
+instruction
+inherited
+reverse
+emphasis
+##vic
+employee
+arch
+keeps
+rabbi
+watson
+payment
+uh
+##ala
+nancy
+##tre
+venice
+fastest
+sexy
+banned
+adrian
+properly
+ruth
+touchdown
+dollar
+boards
+metre
+circles
+edges
+favour
+comments
+ok
+travels
+liberation
+scattered
+firmly
+##ular
+holland
+permitted
+diesel
+kenya
+den
+originated
+##ral
+demons
+resumed
+dragged
+rider
+##rus
+servant
+blinked
+extend
+torn
+##ias
+##sey
+input
+meal
+everybody
+cylinder
+kinds
+camps
+##fe
+bullet
+logic
+##wn
+croatian
+evolved
+healthy
+fool
+chocolate
+wise
+preserve
+pradesh
+##ess
+respective
+1850
+##ew
+chicken
+artificial
+gross
+corresponding
+convicted
+cage
+caroline
+dialogue
+##dor
+narrative
+stranger
+mario
+br
+christianity
+failing
+trent
+commanding
+buddhist
+1848
+maurice
+focusing
+yale
+bike
+altitude
+##ering
+mouse
+revised
+##sley
+veteran
+##ig
+pulls
+theology
+crashed
+campaigns
+legion
+##ability
+drag
+excellence
+customer
+cancelled
+intensity
+excuse
+##lar
+liga
+participating
+contributing
+printing
+##burn
+variable
+##rk
+curious
+bin
+legacy
+renaissance
+##my
+symptoms
+binding
+vocalist
+dancer
+##nie
+grammar
+gospel
+democrats
+ya
+enters
+sc
+diplomatic
+hitler
+##ser
+clouds
+mathematical
+quit
+defended
+oriented
+##heim
+fundamental
+hardware
+impressive
+equally
+convince
+confederate
+guilt
+chuck
+sliding
+##ware
+magnetic
+narrowed
+petersburg
+bulgaria
+otto
+phd
+skill
+##ama
+reader
+hopes
+pitcher
+reservoir
+hearts
+automatically
+expecting
+mysterious
+bennett
+extensively
+imagined
+seeds
+monitor
+fix
+##ative
+journalism
+struggling
+signature
+ranch
+encounter
+photographer
+observation
+protests
+##pin
+influences
+##hr
+calendar
+##all
+cruz
+croatia
+locomotive
+hughes
+naturally
+shakespeare
+basement
+hook
+uncredited
+faded
+theories
+approaches
+dare
+phillips
+filling
+fury
+obama
+##ain
+efficient
+arc
+deliver
+min
+raid
+breeding
+inducted
+leagues
+efficiency
+axis
+montana
+eagles
+##ked
+supplied
+instructions
+karen
+picking
+indicating
+trap
+anchor
+practically
+christians
+tomb
+vary
+occasional
+electronics
+lords
+readers
+newcastle
+faint
+innovation
+collect
+situations
+engagement
+160
+claude
+mixture
+##feld
+peer
+tissue
+logo
+lean
+##ration
+°f
+floors
+##ven
+architects
+reducing
+##our
+##ments
+rope
+1859
+ottawa
+##har
+samples
+banking
+declaration
+proteins
+resignation
+francois
+saudi
+advocate
+exhibited
+armor
+twins
+divorce
+##ras
+abraham
+reviewed
+jo
+temporarily
+matrix
+physically
+pulse
+curled
+##ena
+difficulties
+bengal
+usage
+##ban
+annie
+riders
+certificate
+##pi
+holes
+warsaw
+distinctive
+jessica
+##mon
+mutual
+1857
+customs
+circular
+eugene
+removal
+loaded
+mere
+vulnerable
+depicted
+generations
+dame
+heir
+enormous
+lightly
+climbing
+pitched
+lessons
+pilots
+nepal
+ram
+google
+preparing
+brad
+louise
+renowned
+##₂
+liam
+##ably
+plaza
+shaw
+sophie
+brilliant
+bills
+##bar
+##nik
+fucking
+mainland
+server
+pleasant
+seized
+veterans
+jerked
+fail
+beta
+brush
+radiation
+stored
+warmth
+southeastern
+nate
+sin
+raced
+berkeley
+joke
+athlete
+designation
+trunk
+##low
+roland
+qualification
+archives
+heels
+artwork
+receives
+judicial
+reserves
+##bed
+woke
+installation
+abu
+floating
+fake
+lesser
+excitement
+interface
+concentrated
+addressed
+characteristic
+amanda
+saxophone
+monk
+auto
+##bus
+releasing
+egg
+dies
+interaction
+defender
+ce
+outbreak
+glory
+loving
+##bert
+sequel
+consciousness
+http
+awake
+ski
+enrolled
+##ress
+handling
+rookie
+brow
+somebody
+biography
+warfare
+amounts
+contracts
+presentation
+fabric
+dissolved
+challenged
+meter
+psychological
+lt
+elevated
+rally
+accurate
+##tha
+hospitals
+undergraduate
+specialist
+venezuela
+exhibit
+shed
+nursing
+protestant
+fluid
+structural
+footage
+jared
+consistent
+prey
+##ska
+succession
+reflect
+exile
+lebanon
+wiped
+suspect
+shanghai
+resting
+integration
+preservation
+marvel
+variant
+pirates
+sheep
+rounded
+capita
+sailing
+colonies
+manuscript
+deemed
+variations
+clarke
+functional
+emerging
+boxing
+relaxed
+curse
+azerbaijan
+heavyweight
+nickname
+editorial
+rang
+grid
+tightened
+earthquake
+flashed
+miguel
+rushing
+##ches
+improvements
+boxes
+brooks
+180
+consumption
+molecular
+felix
+societies
+repeatedly
+variation
+aids
+civic
+graphics
+professionals
+realm
+autonomous
+receiver
+delayed
+workshop
+militia
+chairs
+trump
+canyon
+##point
+harsh
+extending
+lovely
+happiness
+##jan
+stake
+eyebrows
+embassy
+wellington
+hannah
+##ella
+sony
+corners
+bishops
+swear
+cloth
+contents
+xi
+namely
+commenced
+1854
+stanford
+nashville
+courage
+graphic
+commitment
+garrison
+##bin
+hamlet
+clearing
+rebels
+attraction
+literacy
+cooking
+ruins
+temples
+jenny
+humanity
+celebrate
+hasn
+freight
+sixty
+rebel
+bastard
+##art
+newton
+##ada
+deer
+##ges
+##ching
+smiles
+delaware
+singers
+##ets
+approaching
+assists
+flame
+##ph
+boulevard
+barrel
+planted
+##ome
+pursuit
+##sia
+consequences
+posts
+shallow
+invitation
+rode
+depot
+ernest
+kane
+rod
+concepts
+preston
+topic
+chambers
+striking
+blast
+arrives
+descendants
+montgomery
+ranges
+worlds
+##lay
+##ari
+span
+chaos
+praise
+##ag
+fewer
+1855
+sanctuary
+mud
+fbi
+##ions
+programmes
+maintaining
+unity
+harper
+bore
+handsome
+closure
+tournaments
+thunder
+nebraska
+linda
+facade
+puts
+satisfied
+argentine
+dale
+cork
+dome
+panama
+##yl
+1858
+tasks
+experts
+##ates
+feeding
+equation
+##las
+##ida
+##tu
+engage
+bryan
+##ax
+um
+quartet
+melody
+disbanded
+sheffield
+blocked
+gasped
+delay
+kisses
+maggie
+connects
+##non
+sts
+poured
+creator
+publishers
+##we
+guided
+ellis
+extinct
+hug
+gaining
+##ord
+complicated
+##bility
+poll
+clenched
+investigate
+##use
+thereby
+quantum
+spine
+cdp
+humor
+kills
+administered
+semifinals
+##du
+encountered
+ignore
+##bu
+commentary
+##maker
+bother
+roosevelt
+140
+plains
+halfway
+flowing
+cultures
+crack
+imprisoned
+neighboring
+airline
+##ses
+##view
+##mate
+##ec
+gather
+wolves
+marathon
+transformed
+##ill
+cruise
+organisations
+carol
+punch
+exhibitions
+numbered
+alarm
+ratings
+daddy
+silently
+##stein
+queens
+colours
+impression
+guidance
+liu
+tactical
+##rat
+marshal
+della
+arrow
+##ings
+rested
+feared
+tender
+owns
+bitter
+advisor
+escort
+##ides
+spare
+farms
+grants
+##ene
+dragons
+encourage
+colleagues
+cameras
+##und
+sucked
+pile
+spirits
+prague
+statements
+suspension
+landmark
+fence
+torture
+recreation
+bags
+permanently
+survivors
+pond
+spy
+predecessor
+bombing
+coup
+##og
+protecting
+transformation
+glow
+##lands
+##book
+dug
+priests
+andrea
+feat
+barn
+jumping
+##chen
+##ologist
+##con
+casualties
+stern
+auckland
+pipe
+serie
+revealing
+ba
+##bel
+trevor
+mercy
+spectrum
+yang
+consist
+governing
+collaborated
+possessed
+epic
+comprises
+blew
+shane
+##ack
+lopez
+honored
+magical
+sacrifice
+judgment
+perceived
+hammer
+mtv
+baronet
+tune
+das
+missionary
+sheets
+350
+neutral
+oral
+threatening
+attractive
+shade
+aims
+seminary
+##master
+estates
+1856
+michel
+wounds
+refugees
+manufacturers
+##nic
+mercury
+syndrome
+porter
+##iya
+##din
+hamburg
+identification
+upstairs
+purse
+widened
+pause
+cared
+breathed
+affiliate
+santiago
+prevented
+celtic
+fisher
+125
+recruited
+byzantine
+reconstruction
+farther
+##mp
+diet
+sake
+au
+spite
+sensation
+##ert
+blank
+separation
+105
+##hon
+vladimir
+armies
+anime
+##lie
+accommodate
+orbit
+cult
+sofia
+archive
+##ify
+##box
+founders
+sustained
+disorder
+honours
+northeastern
+mia
+crops
+violet
+threats
+blanket
+fires
+canton
+followers
+southwestern
+prototype
+voyage
+assignment
+altered
+moderate
+protocol
+pistol
+##eo
+questioned
+brass
+lifting
+1852
+math
+authored
+##ual
+doug
+dimensional
+dynamic
+##san
+1851
+pronounced
+grateful
+quest
+uncomfortable
+boom
+presidency
+stevens
+relating
+politicians
+chen
+barrier
+quinn
+diana
+mosque
+tribal
+cheese
+palmer
+portions
+sometime
+chester
+treasure
+wu
+bend
+download
+millions
+reforms
+registration
+##osa
+consequently
+monitoring
+ate
+preliminary
+brandon
+invented
+ps
+eaten
+exterior
+intervention
+ports
+documented
+log
+displays
+lecture
+sally
+favourite
+##itz
+vermont
+lo
+invisible
+isle
+breed
+##ator
+journalists
+relay
+speaks
+backward
+explore
+midfielder
+actively
+stefan
+procedures
+cannon
+blond
+kenneth
+centered
+servants
+chains
+libraries
+malcolm
+essex
+henri
+slavery
+##hal
+facts
+fairy
+coached
+cassie
+cats
+washed
+cop
+##fi
+announcement
+item
+2000s
+vinyl
+activated
+marco
+frontier
+growled
+curriculum
+##das
+loyal
+accomplished
+leslie
+ritual
+kenny
+##00
+vii
+napoleon
+hollow
+hybrid
+jungle
+stationed
+friedrich
+counted
+##ulated
+platinum
+theatrical
+seated
+col
+rubber
+glen
+1840
+diversity
+healing
+extends
+id
+provisions
+administrator
+columbus
+##oe
+tributary
+te
+assured
+org
+##uous
+prestigious
+examined
+lectures
+grammy
+ronald
+associations
+bailey
+allan
+essays
+flute
+believing
+consultant
+proceedings
+travelling
+1853
+kit
+kerala
+yugoslavia
+buddy
+methodist
+##ith
+burial
+centres
+batman
+##nda
+discontinued
+bo
+dock
+stockholm
+lungs
+severely
+##nk
+citing
+manga
+##ugh
+steal
+mumbai
+iraqi
+robot
+celebrity
+bride
+broadcasts
+abolished
+pot
+joel
+overhead
+franz
+packed
+reconnaissance
+johann
+acknowledged
+introduce
+handled
+doctorate
+developments
+drinks
+alley
+palestine
+##nis
+##aki
+proceeded
+recover
+bradley
+grain
+patch
+afford
+infection
+nationalist
+legendary
+##ath
+interchange
+virtually
+gen
+gravity
+exploration
+amber
+vital
+wishes
+powell
+doctrine
+elbow
+screenplay
+##bird
+contribute
+indonesian
+pet
+creates
+##com
+enzyme
+kylie
+discipline
+drops
+manila
+hunger
+##ien
+layers
+suffer
+fever
+bits
+monica
+keyboard
+manages
+##hood
+searched
+appeals
+##bad
+testament
+grande
+reid
+##war
+beliefs
+congo
+##ification
+##dia
+si
+requiring
+##via
+casey
+1849
+regret
+streak
+rape
+depends
+syrian
+sprint
+pound
+tourists
+upcoming
+pub
+##xi
+tense
+##els
+practiced
+echo
+nationwide
+guild
+motorcycle
+liz
+##zar
+chiefs
+desired
+elena
+bye
+precious
+absorbed
+relatives
+booth
+pianist
+##mal
+citizenship
+exhausted
+wilhelm
+##ceae
+##hed
+noting
+quarterback
+urge
+hectares
+##gue
+ace
+holly
+##tal
+blonde
+davies
+parked
+sustainable
+stepping
+twentieth
+airfield
+galaxy
+nest
+chip
+##nell
+tan
+shaft
+paulo
+requirement
+##zy
+paradise
+tobacco
+trans
+renewed
+vietnamese
+##cker
+##ju
+suggesting
+catching
+holmes
+enjoying
+md
+trips
+colt
+holder
+butterfly
+nerve
+reformed
+cherry
+bowling
+trailer
+carriage
+goodbye
+appreciate
+toy
+joshua
+interactive
+enabled
+involve
+##kan
+collar
+determination
+bunch
+facebook
+recall
+shorts
+superintendent
+episcopal
+frustration
+giovanni
+nineteenth
+laser
+privately
+array
+circulation
+##ovic
+armstrong
+deals
+painful
+permit
+discrimination
+##wi
+aires
+retiring
+cottage
+ni
+##sta
+horizon
+ellen
+jamaica
+ripped
+fernando
+chapters
+playstation
+patron
+lecturer
+navigation
+behaviour
+genes
+georgian
+export
+solomon
+rivals
+swift
+seventeen
+rodriguez
+princeton
+independently
+sox
+1847
+arguing
+entity
+casting
+hank
+criteria
+oakland
+geographic
+milwaukee
+reflection
+expanding
+conquest
+dubbed
+##tv
+halt
+brave
+brunswick
+doi
+arched
+curtis
+divorced
+predominantly
+somerset
+streams
+ugly
+zoo
+horrible
+curved
+buenos
+fierce
+dictionary
+vector
+theological
+unions
+handful
+stability
+chan
+punjab
+segments
+##lly
+altar
+ignoring
+gesture
+monsters
+pastor
+##stone
+thighs
+unexpected
+operators
+abruptly
+coin
+compiled
+associates
+improving
+migration
+pin
+##ose
+compact
+collegiate
+reserved
+##urs
+quarterfinals
+roster
+restore
+assembled
+hurry
+oval
+##cies
+1846
+flags
+martha
+##del
+victories
+sharply
+##rated
+argues
+deadly
+neo
+drawings
+symbols
+performer
+##iel
+griffin
+restrictions
+editing
+andrews
+java
+journals
+arabia
+compositions
+dee
+pierce
+removing
+hindi
+casino
+runway
+civilians
+minds
+nasa
+hotels
+##zation
+refuge
+rent
+retain
+potentially
+conferences
+suburban
+conducting
+##tto
+##tions
+##tle
+descended
+massacre
+##cal
+ammunition
+terrain
+fork
+souls
+counts
+chelsea
+durham
+drives
+cab
+##bank
+perth
+realizing
+palestinian
+finn
+simpson
+##dal
+betty
+##ule
+moreover
+particles
+cardinals
+tent
+evaluation
+extraordinary
+##oid
+inscription
+##works
+wednesday
+chloe
+maintains
+panels
+ashley
+trucks
+##nation
+cluster
+sunlight
+strikes
+zhang
+##wing
+dialect
+canon
+##ap
+tucked
+##ws
+collecting
+##mas
+##can
+##sville
+maker
+quoted
+evan
+franco
+aria
+buying
+cleaning
+eva
+closet
+provision
+apollo
+clinic
+rat
+##ez
+necessarily
+ac
+##gle
+##ising
+venues
+flipped
+cent
+spreading
+trustees
+checking
+authorized
+##sco
+disappointed
+##ado
+notion
+duration
+trumpet
+hesitated
+topped
+brussels
+rolls
+theoretical
+hint
+define
+aggressive
+repeat
+wash
+peaceful
+optical
+width
+allegedly
+mcdonald
+strict
+copyright
+##illa
+investors
+mar
+jam
+witnesses
+sounding
+miranda
+michelle
+privacy
+hugo
+harmony
+##pp
+valid
+lynn
+glared
+nina
+102
+headquartered
+diving
+boarding
+gibson
+##ncy
+albanian
+marsh
+routine
+dealt
+enhanced
+er
+intelligent
+substance
+targeted
+enlisted
+discovers
+spinning
+observations
+pissed
+smoking
+rebecca
+capitol
+visa
+varied
+costume
+seemingly
+indies
+compensation
+surgeon
+thursday
+arsenal
+westminster
+suburbs
+rid
+anglican
+##ridge
+knots
+foods
+alumni
+lighter
+fraser
+whoever
+portal
+scandal
+##ray
+gavin
+advised
+instructor
+flooding
+terrorist
+##ale
+teenage
+interim
+senses
+duck
+teen
+thesis
+abby
+eager
+overcome
+##ile
+newport
+glenn
+rises
+shame
+##cc
+prompted
+priority
+forgot
+bomber
+nicolas
+protective
+360
+cartoon
+katherine
+breeze
+lonely
+trusted
+henderson
+richardson
+relax
+banner
+candy
+palms
+remarkable
+##rio
+legends
+cricketer
+essay
+ordained
+edmund
+rifles
+trigger
+##uri
+##away
+sail
+alert
+1830
+audiences
+penn
+sussex
+siblings
+pursued
+indianapolis
+resist
+rosa
+consequence
+succeed
+avoided
+1845
+##ulation
+inland
+##tie
+##nna
+counsel
+profession
+chronicle
+hurried
+##una
+eyebrow
+eventual
+bleeding
+innovative
+cure
+##dom
+committees
+accounting
+con
+scope
+hardy
+heather
+tenor
+gut
+herald
+codes
+tore
+scales
+wagon
+##oo
+luxury
+tin
+prefer
+fountain
+triangle
+bonds
+darling
+convoy
+dried
+traced
+beings
+troy
+accidentally
+slam
+findings
+smelled
+joey
+lawyers
+outcome
+steep
+bosnia
+configuration
+shifting
+toll
+brook
+performers
+lobby
+philosophical
+construct
+shrine
+aggregate
+boot
+cox
+phenomenon
+savage
+insane
+solely
+reynolds
+lifestyle
+##ima
+nationally
+holdings
+consideration
+enable
+edgar
+mo
+mama
+##tein
+fights
+relegation
+chances
+atomic
+hub
+conjunction
+awkward
+reactions
+currency
+finale
+kumar
+underwent
+steering
+elaborate
+gifts
+comprising
+melissa
+veins
+reasonable
+sunshine
+chi
+solve
+trails
+inhabited
+elimination
+ethics
+huh
+ana
+molly
+consent
+apartments
+layout
+marines
+##ces
+hunters
+bulk
+##oma
+hometown
+##wall
+##mont
+cracked
+reads
+neighbouring
+withdrawn
+admission
+wingspan
+damned
+anthology
+lancashire
+brands
+batting
+forgive
+cuban
+awful
+##lyn
+104
+dimensions
+imagination
+##ade
+dante
+##ship
+tracking
+desperately
+goalkeeper
+##yne
+groaned
+workshops
+confident
+burton
+gerald
+milton
+circus
+uncertain
+slope
+copenhagen
+sophia
+fog
+philosopher
+portraits
+accent
+cycling
+varying
+gripped
+larvae
+garrett
+specified
+scotia
+mature
+luther
+kurt
+rap
+##kes
+aerial
+750
+ferdinand
+heated
+es
+transported
+##shan
+safely
+nonetheless
+##orn
+##gal
+motors
+demanding
+##sburg
+startled
+##brook
+ally
+generate
+caps
+ghana
+stained
+demo
+mentions
+beds
+ap
+afterward
+diary
+##bling
+utility
+##iro
+richards
+1837
+conspiracy
+conscious
+shining
+footsteps
+observer
+cyprus
+urged
+loyalty
+developer
+probability
+olive
+upgraded
+gym
+miracle
+insects
+graves
+1844
+ourselves
+hydrogen
+amazon
+katie
+tickets
+poets
+##pm
+planes
+##pan
+prevention
+witnessed
+dense
+jin
+randy
+tang
+warehouse
+monroe
+bang
+archived
+elderly
+investigations
+alec
+granite
+mineral
+conflicts
+controlling
+aboriginal
+carlo
+##zu
+mechanics
+stan
+stark
+rhode
+skirt
+est
+##berry
+bombs
+respected
+##horn
+imposed
+limestone
+deny
+nominee
+memphis
+grabbing
+disabled
+##als
+amusement
+aa
+frankfurt
+corn
+referendum
+varies
+slowed
+disk
+firms
+unconscious
+incredible
+clue
+sue
+##zhou
+twist
+##cio
+joins
+idaho
+chad
+developers
+computing
+destroyer
+103
+mortal
+tucker
+kingston
+choices
+yu
+carson
+1800
+os
+whitney
+geneva
+pretend
+dimension
+staged
+plateau
+maya
+##une
+freestyle
+##bc
+rovers
+hiv
+##ids
+tristan
+classroom
+prospect
+##hus
+honestly
+diploma
+lied
+thermal
+auxiliary
+feast
+unlikely
+iata
+##tel
+morocco
+pounding
+treasury
+lithuania
+considerably
+1841
+dish
+1812
+geological
+matching
+stumbled
+destroying
+marched
+brien
+advances
+cake
+nicole
+belle
+settling
+measuring
+directing
+##mie
+tuesday
+bassist
+capabilities
+stunned
+fraud
+torpedo
+##list
+##phone
+anton
+wisdom
+surveillance
+ruined
+##ulate
+lawsuit
+healthcare
+theorem
+halls
+trend
+aka
+horizontal
+dozens
+acquire
+lasting
+swim
+hawk
+gorgeous
+fees
+vicinity
+decrease
+adoption
+tactics
+##ography
+pakistani
+##ole
+draws
+##hall
+willie
+burke
+heath
+algorithm
+integral
+powder
+elliott
+brigadier
+jackie
+tate
+varieties
+darker
+##cho
+lately
+cigarette
+specimens
+adds
+##ree
+##ensis
+##inger
+exploded
+finalist
+cia
+murders
+wilderness
+arguments
+nicknamed
+acceptance
+onwards
+manufacture
+robertson
+jets
+tampa
+enterprises
+blog
+loudly
+composers
+nominations
+1838
+ai
+malta
+inquiry
+automobile
+hosting
+viii
+rays
+tilted
+grief
+museums
+strategies
+furious
+euro
+equality
+cohen
+poison
+surrey
+wireless
+governed
+ridiculous
+moses
+##esh
+##room
+vanished
+##ito
+barnes
+attract
+morrison
+istanbul
+##iness
+absent
+rotation
+petition
+janet
+##logical
+satisfaction
+custody
+deliberately
+observatory
+comedian
+surfaces
+pinyin
+novelist
+strictly
+canterbury
+oslo
+monks
+embrace
+ibm
+jealous
+photograph
+continent
+dorothy
+marina
+doc
+excess
+holden
+allegations
+explaining
+stack
+avoiding
+lance
+storyline
+majesty
+poorly
+spike
+dos
+bradford
+raven
+travis
+classics
+proven
+voltage
+pillow
+fists
+butt
+1842
+interpreted
+##car
+1839
+gage
+telegraph
+lens
+promising
+expelled
+casual
+collector
+zones
+##min
+silly
+nintendo
+##kh
+##bra
+downstairs
+chef
+suspicious
+afl
+flies
+vacant
+uganda
+pregnancy
+condemned
+lutheran
+estimates
+cheap
+decree
+saxon
+proximity
+stripped
+idiot
+deposits
+contrary
+presenter
+magnus
+glacier
+im
+offense
+edwin
+##ori
+upright
+##long
+bolt
+##ois
+toss
+geographical
+##izes
+environments
+delicate
+marking
+abstract
+xavier
+nails
+windsor
+plantation
+occurring
+equity
+saskatchewan
+fears
+drifted
+sequences
+vegetation
+revolt
+##stic
+1843
+sooner
+fusion
+opposing
+nato
+skating
+1836
+secretly
+ruin
+lease
+##oc
+edit
+##nne
+flora
+anxiety
+ruby
+##ological
+##mia
+tel
+bout
+taxi
+emmy
+frost
+rainbow
+compounds
+foundations
+rainfall
+assassination
+nightmare
+dominican
+##win
+achievements
+deserve
+orlando
+intact
+armenia
+##nte
+calgary
+valentine
+106
+marion
+proclaimed
+theodore
+bells
+courtyard
+thigh
+gonzalez
+console
+troop
+minimal
+monte
+everyday
+##ence
+##if
+supporter
+terrorism
+buck
+openly
+presbyterian
+activists
+carpet
+##iers
+rubbing
+uprising
+##yi
+cute
+conceived
+legally
+##cht
+millennium
+cello
+velocity
+ji
+rescued
+cardiff
+1835
+rex
+concentrate
+senators
+beard
+rendered
+glowing
+battalions
+scouts
+competitors
+sculptor
+catalogue
+arctic
+ion
+raja
+bicycle
+wow
+glancing
+lawn
+##woman
+gentleman
+lighthouse
+publish
+predicted
+calculated
+##val
+variants
+##gne
+strain
+##ui
+winston
+deceased
+##nus
+touchdowns
+brady
+caleb
+sinking
+echoed
+crush
+hon
+blessed
+protagonist
+hayes
+endangered
+magnitude
+editors
+##tine
+estimate
+responsibilities
+##mel
+backup
+laying
+consumed
+sealed
+zurich
+lovers
+frustrated
+##eau
+ahmed
+kicking
+mit
+treasurer
+1832
+biblical
+refuse
+terrified
+pump
+agrees
+genuine
+imprisonment
+refuses
+plymouth
+##hen
+lou
+##nen
+tara
+trembling
+antarctic
+ton
+learns
+##tas
+crap
+crucial
+faction
+atop
+##borough
+wrap
+lancaster
+odds
+hopkins
+erik
+lyon
+##eon
+bros
+##ode
+snap
+locality
+tips
+empress
+crowned
+cal
+acclaimed
+chuckled
+##ory
+clara
+sends
+mild
+towel
+##fl
+##day
+##а
+wishing
+assuming
+interviewed
+##bal
+##die
+interactions
+eden
+cups
+helena
+##lf
+indie
+beck
+##fire
+batteries
+filipino
+wizard
+parted
+##lam
+traces
+##born
+rows
+idol
+albany
+delegates
+##ees
+##sar
+discussions
+##ex
+notre
+instructed
+belgrade
+highways
+suggestion
+lauren
+possess
+orientation
+alexandria
+abdul
+beats
+salary
+reunion
+ludwig
+alright
+wagner
+intimate
+pockets
+slovenia
+hugged
+brighton
+merchants
+cruel
+stole
+trek
+slopes
+repairs
+enrollment
+politically
+underlying
+promotional
+counting
+boeing
+##bb
+isabella
+naming
+##и
+keen
+bacteria
+listing
+separately
+belfast
+ussr
+450
+lithuanian
+anybody
+ribs
+sphere
+martinez
+cock
+embarrassed
+proposals
+fragments
+nationals
+##fs
+##wski
+premises
+fin
+1500
+alpine
+matched
+freely
+bounded
+jace
+sleeve
+##af
+gaming
+pier
+populated
+evident
+##like
+frances
+flooded
+##dle
+frightened
+pour
+trainer
+framed
+visitor
+challenging
+pig
+wickets
+##fold
+infected
+email
+##pes
+arose
+##aw
+reward
+ecuador
+oblast
+vale
+ch
+shuttle
+##usa
+bach
+rankings
+forbidden
+cornwall
+accordance
+salem
+consumers
+bruno
+fantastic
+toes
+machinery
+resolved
+julius
+remembering
+propaganda
+iceland
+bombardment
+tide
+contacts
+wives
+##rah
+concerto
+macdonald
+albania
+implement
+daisy
+tapped
+sudan
+helmet
+angela
+mistress
+##lic
+crop
+sunk
+finest
+##craft
+hostile
+##ute
+##tsu
+boxer
+fr
+paths
+adjusted
+habit
+ballot
+supervision
+soprano
+##zen
+bullets
+wicked
+sunset
+regiments
+disappear
+lamp
+performs
+app
+##gia
+##oa
+rabbit
+digging
+incidents
+entries
+##cion
+dishes
+##oi
+introducing
+##ati
+##fied
+freshman
+slot
+jill
+tackles
+baroque
+backs
+##iest
+lone
+sponsor
+destiny
+altogether
+convert
+##aro
+consensus
+shapes
+demonstration
+basically
+feminist
+auction
+artifacts
+##bing
+strongest
+twitter
+halifax
+2019
+allmusic
+mighty
+smallest
+precise
+alexandra
+viola
+##los
+##ille
+manuscripts
+##illo
+dancers
+ari
+managers
+monuments
+blades
+barracks
+springfield
+maiden
+consolidated
+electron
+##end
+berry
+airing
+wheat
+nobel
+inclusion
+blair
+payments
+geography
+bee
+cc
+eleanor
+react
+##hurst
+afc
+manitoba
+##yu
+su
+lineup
+fitness
+recreational
+investments
+airborne
+disappointment
+##dis
+edmonton
+viewing
+##row
+renovation
+##cast
+infant
+bankruptcy
+roses
+aftermath
+pavilion
+##yer
+carpenter
+withdrawal
+ladder
+##hy
+discussing
+popped
+reliable
+agreements
+rochester
+##abad
+curves
+bombers
+220
+rao
+reverend
+decreased
+choosing
+107
+stiff
+consulting
+naples
+crawford
+tracy
+ka
+ribbon
+cops
+##lee
+crushed
+deciding
+unified
+teenager
+accepting
+flagship
+explorer
+poles
+sanchez
+inspection
+revived
+skilled
+induced
+exchanged
+flee
+locals
+tragedy
+swallow
+loading
+hanna
+demonstrate
+##ela
+salvador
+flown
+contestants
+civilization
+##ines
+wanna
+rhodes
+fletcher
+hector
+knocking
+considers
+##ough
+nash
+mechanisms
+sensed
+mentally
+walt
+unclear
+##eus
+renovated
+madame
+##cks
+crews
+governmental
+##hin
+undertaken
+monkey
+##ben
+##ato
+fatal
+armored
+copa
+caves
+governance
+grasp
+perception
+certification
+froze
+damp
+tugged
+wyoming
+##rg
+##ero
+newman
+##lor
+nerves
+curiosity
+graph
+115
+##ami
+withdraw
+tunnels
+dull
+meredith
+moss
+exhibits
+neighbors
+communicate
+accuracy
+explored
+raiders
+republicans
+secular
+kat
+superman
+penny
+criticised
+##tch
+freed
+update
+conviction
+wade
+ham
+likewise
+delegation
+gotta
+doll
+promises
+technological
+myth
+nationality
+resolve
+convent
+##mark
+sharon
+dig
+sip
+coordinator
+entrepreneur
+fold
+##dine
+capability
+councillor
+synonym
+blown
+swan
+cursed
+1815
+jonas
+haired
+sofa
+canvas
+keeper
+rivalry
+##hart
+rapper
+speedway
+swords
+postal
+maxwell
+estonia
+potter
+recurring
+##nn
+##ave
+errors
+##oni
+cognitive
+1834
+##²
+claws
+nadu
+roberto
+bce
+wrestler
+ellie
+##ations
+infinite
+ink
+##tia
+presumably
+finite
+staircase
+108
+noel
+patricia
+nacional
+##cation
+chill
+eternal
+tu
+preventing
+prussia
+fossil
+limbs
+##logist
+ernst
+frog
+perez
+rene
+##ace
+pizza
+prussian
+##ios
+##vy
+molecules
+regulatory
+answering
+opinions
+sworn
+lengths
+supposedly
+hypothesis
+upward
+habitats
+seating
+ancestors
+drank
+yield
+hd
+synthesis
+researcher
+modest
+##var
+mothers
+peered
+voluntary
+homeland
+##the
+acclaim
+##igan
+static
+valve
+luxembourg
+alto
+carroll
+fe
+receptor
+norton
+ambulance
+##tian
+johnston
+catholics
+depicting
+jointly
+elephant
+gloria
+mentor
+badge
+ahmad
+distinguish
+remarked
+councils
+precisely
+allison
+advancing
+detection
+crowded
+##10
+cooperative
+ankle
+mercedes
+dagger
+surrendered
+pollution
+commit
+subway
+jeffrey
+lesson
+sculptures
+provider
+##fication
+membrane
+timothy
+rectangular
+fiscal
+heating
+teammate
+basket
+particle
+anonymous
+deployment
+##ple
+missiles
+courthouse
+proportion
+shoe
+sec
+##ller
+complaints
+forbes
+blacks
+abandon
+remind
+sizes
+overwhelming
+autobiography
+natalie
+##awa
+risks
+contestant
+countryside
+babies
+scorer
+invaded
+enclosed
+proceed
+hurling
+disorders
+##cu
+reflecting
+continuously
+cruiser
+graduates
+freeway
+investigated
+ore
+deserved
+maid
+blocking
+phillip
+jorge
+shakes
+dove
+mann
+variables
+lacked
+burden
+accompanying
+que
+consistently
+organizing
+provisional
+complained
+endless
+##rm
+tubes
+juice
+georges
+krishna
+mick
+labels
+thriller
+##uch
+laps
+arcade
+sage
+snail
+##table
+shannon
+fi
+laurence
+seoul
+vacation
+presenting
+hire
+churchill
+surprisingly
+prohibited
+savannah
+technically
+##oli
+170
+##lessly
+testimony
+suited
+speeds
+toys
+romans
+mlb
+flowering
+measurement
+talented
+kay
+settings
+charleston
+expectations
+shattered
+achieving
+triumph
+ceremonies
+portsmouth
+lanes
+mandatory
+loser
+stretching
+cologne
+realizes
+seventy
+cornell
+careers
+webb
+##ulating
+americas
+budapest
+ava
+suspicion
+##ison
+yo
+conrad
+##hai
+sterling
+jessie
+rector
+##az
+1831
+transform
+organize
+loans
+christine
+volcanic
+warrant
+slender
+summers
+subfamily
+newer
+danced
+dynamics
+rhine
+proceeds
+heinrich
+gastropod
+commands
+sings
+facilitate
+easter
+ra
+positioned
+responses
+expense
+fruits
+yanked
+imported
+25th
+velvet
+vic
+primitive
+tribune
+baldwin
+neighbourhood
+donna
+rip
+hay
+pr
+##uro
+1814
+espn
+welcomed
+##aria
+qualifier
+glare
+highland
+timing
+##cted
+shells
+eased
+geometry
+louder
+exciting
+slovakia
+##sion
+##iz
+##lot
+savings
+prairie
+##ques
+marching
+rafael
+tonnes
+##lled
+curtain
+preceding
+shy
+heal
+greene
+worthy
+##pot
+detachment
+bury
+sherman
+##eck
+reinforced
+seeks
+bottles
+contracted
+duchess
+outfit
+walsh
+##sc
+mickey
+##ase
+geoffrey
+archer
+squeeze
+dawson
+eliminate
+invention
+##enberg
+neal
+##eth
+stance
+dealer
+coral
+maple
+retire
+polo
+simplified
+##ht
+1833
+hid
+watts
+backwards
+jules
+##oke
+genesis
+mt
+frames
+rebounds
+burma
+woodland
+moist
+santos
+whispers
+drained
+subspecies
+##aa
+streaming
+ulster
+burnt
+correspondence
+maternal
+gerard
+denis
+stealing
+##load
+genius
+duchy
+##oria
+inaugurated
+momentum
+suits
+placement
+sovereign
+clause
+thames
+##hara
+confederation
+reservation
+sketch
+yankees
+lets
+rotten
+charm
+hal
+verses
+ultra
+commercially
+dot
+salon
+citation
+adopt
+winnipeg
+mist
+allocated
+cairo
+##boy
+jenkins
+interference
+objectives
+##wind
+1820
+portfolio
+armoured
+sectors
+##eh
+initiatives
+##world
+integrity
+exercises
+robe
+tap
+ab
+gazed
+##tones
+distracted
+rulers
+111
+favorable
+jerome
+tended
+cart
+factories
+##eri
+diplomat
+valued
+gravel
+charitable
+##try
+calvin
+exploring
+chang
+shepherd
+terrace
+pdf
+pupil
+##ural
+reflects
+ups
+##rch
+governors
+shelf
+depths
+##nberg
+trailed
+crest
+tackle
+##nian
+##ats
+hatred
+##kai
+clare
+makers
+ethiopia
+longtime
+detected
+embedded
+lacking
+slapped
+rely
+thomson
+anticipation
+iso
+morton
+successive
+agnes
+screenwriter
+straightened
+philippe
+playwright
+haunted
+licence
+iris
+intentions
+sutton
+112
+logical
+correctly
+##weight
+branded
+licked
+tipped
+silva
+ricky
+narrator
+requests
+##ents
+greeted
+supernatural
+cow
+##wald
+lung
+refusing
+employer
+strait
+gaelic
+liner
+##piece
+zoe
+sabha
+##mba
+driveway
+harvest
+prints
+bates
+reluctantly
+threshold
+algebra
+ira
+wherever
+coupled
+240
+assumption
+picks
+##air
+designers
+raids
+gentlemen
+##ean
+roller
+blowing
+leipzig
+locks
+screw
+dressing
+strand
+##lings
+scar
+dwarf
+depicts
+##nu
+nods
+##mine
+differ
+boris
+##eur
+yuan
+flip
+##gie
+mob
+invested
+questioning
+applying
+##ture
+shout
+##sel
+gameplay
+blamed
+illustrations
+bothered
+weakness
+rehabilitation
+##of
+##zes
+envelope
+rumors
+miners
+leicester
+subtle
+kerry
+##ico
+ferguson
+##fu
+premiership
+ne
+##cat
+bengali
+prof
+catches
+remnants
+dana
+##rily
+shouting
+presidents
+baltic
+ought
+ghosts
+dances
+sailors
+shirley
+fancy
+dominic
+##bie
+madonna
+##rick
+bark
+buttons
+gymnasium
+ashes
+liver
+toby
+oath
+providence
+doyle
+evangelical
+nixon
+cement
+carnegie
+embarked
+hatch
+surroundings
+guarantee
+needing
+pirate
+essence
+##bee
+filter
+crane
+hammond
+projected
+immune
+percy
+twelfth
+##ult
+regent
+doctoral
+damon
+mikhail
+##ichi
+lu
+critically
+elect
+realised
+abortion
+acute
+screening
+mythology
+steadily
+##fc
+frown
+nottingham
+kirk
+wa
+minneapolis
+##rra
+module
+algeria
+mc
+nautical
+encounters
+surprising
+statues
+availability
+shirts
+pie
+alma
+brows
+munster
+mack
+soup
+crater
+tornado
+sanskrit
+cedar
+explosive
+bordered
+dixon
+planets
+stamp
+exam
+happily
+##bble
+carriers
+kidnapped
+##vis
+accommodation
+emigrated
+##met
+knockout
+correspondent
+violation
+profits
+peaks
+lang
+specimen
+agenda
+ancestry
+pottery
+spelling
+equations
+obtaining
+ki
+linking
+1825
+debris
+asylum
+##20
+buddhism
+teddy
+##ants
+gazette
+##nger
+##sse
+dental
+eligibility
+utc
+fathers
+averaged
+zimbabwe
+francesco
+coloured
+hissed
+translator
+lynch
+mandate
+humanities
+mackenzie
+uniforms
+lin
+##iana
+##gio
+asset
+mhz
+fitting
+samantha
+genera
+wei
+rim
+beloved
+shark
+riot
+entities
+expressions
+indo
+carmen
+slipping
+owing
+abbot
+neighbor
+sidney
+##av
+rats
+recommendations
+encouraging
+squadrons
+anticipated
+commanders
+conquered
+##oto
+donations
+diagnosed
+##mond
+divide
+##iva
+guessed
+decoration
+vernon
+auditorium
+revelation
+conversations
+##kers
+##power
+herzegovina
+dash
+alike
+protested
+lateral
+herman
+accredited
+mg
+##gent
+freeman
+mel
+fiji
+crow
+crimson
+##rine
+livestock
+##pped
+humanitarian
+bored
+oz
+whip
+##lene
+##ali
+legitimate
+alter
+grinning
+spelled
+anxious
+oriental
+wesley
+##nin
+##hole
+carnival
+controller
+detect
+##ssa
+bowed
+educator
+kosovo
+macedonia
+##sin
+occupy
+mastering
+stephanie
+janeiro
+para
+unaware
+nurses
+noon
+135
+cam
+hopefully
+ranger
+combine
+sociology
+polar
+rica
+##eer
+neill
+##sman
+holocaust
+##ip
+doubled
+lust
+1828
+109
+decent
+cooling
+unveiled
+##card
+1829
+nsw
+homer
+chapman
+meyer
+##gin
+dive
+mae
+reagan
+expertise
+##gled
+darwin
+brooke
+sided
+prosecution
+investigating
+comprised
+petroleum
+genres
+reluctant
+differently
+trilogy
+johns
+vegetables
+corpse
+highlighted
+lounge
+pension
+unsuccessfully
+elegant
+aided
+ivory
+beatles
+amelia
+cain
+dubai
+sunny
+immigrant
+babe
+click
+##nder
+underwater
+pepper
+combining
+mumbled
+atlas
+horns
+accessed
+ballad
+physicians
+homeless
+gestured
+rpm
+freak
+louisville
+corporations
+patriots
+prizes
+rational
+warn
+modes
+decorative
+overnight
+din
+troubled
+phantom
+##ort
+monarch
+sheer
+##dorf
+generals
+guidelines
+organs
+addresses
+##zon
+enhance
+curling
+parishes
+cord
+##kie
+linux
+caesar
+deutsche
+bavaria
+##bia
+coleman
+cyclone
+##eria
+bacon
+petty
+##yama
+##old
+hampton
+diagnosis
+1824
+throws
+complexity
+rita
+disputed
+##₃
+pablo
+##sch
+marketed
+trafficking
+##ulus
+examine
+plague
+formats
+##oh
+vault
+faithful
+##bourne
+webster
+##ox
+highlights
+##ient
+##ann
+phones
+vacuum
+sandwich
+modeling
+##gated
+bolivia
+clergy
+qualities
+isabel
+##nas
+##ars
+wears
+screams
+reunited
+annoyed
+bra
+##ancy
+##rate
+differential
+transmitter
+tattoo
+container
+poker
+##och
+excessive
+resides
+cowboys
+##tum
+augustus
+trash
+providers
+statute
+retreated
+balcony
+reversed
+void
+storey
+preceded
+masses
+leap
+laughs
+neighborhoods
+wards
+schemes
+falcon
+santo
+battlefield
+pad
+ronnie
+thread
+lesbian
+venus
+##dian
+beg
+sandstone
+daylight
+punched
+gwen
+analog
+stroked
+wwe
+acceptable
+measurements
+dec
+toxic
+##kel
+adequate
+surgical
+economist
+parameters
+varsity
+##sberg
+quantity
+ella
+##chy
+##rton
+countess
+generating
+precision
+diamonds
+expressway
+ga
+##ı
+1821
+uruguay
+talents
+galleries
+expenses
+scanned
+colleague
+outlets
+ryder
+lucien
+##ila
+paramount
+##bon
+syracuse
+dim
+fangs
+gown
+sweep
+##sie
+toyota
+missionaries
+websites
+##nsis
+sentences
+adviser
+val
+trademark
+spells
+##plane
+patience
+starter
+slim
+##borg
+toe
+incredibly
+shoots
+elliot
+nobility
+##wyn
+cowboy
+endorsed
+gardner
+tendency
+persuaded
+organisms
+emissions
+kazakhstan
+amused
+boring
+chips
+themed
+##hand
+llc
+constantinople
+chasing
+systematic
+guatemala
+borrowed
+erin
+carey
+##hard
+highlands
+struggles
+1810
+##ifying
+##ced
+wong
+exceptions
+develops
+enlarged
+kindergarten
+castro
+##ern
+##rina
+leigh
+zombie
+juvenile
+##most
+consul
+##nar
+sailor
+hyde
+clarence
+intensive
+pinned
+nasty
+useless
+jung
+clayton
+stuffed
+exceptional
+ix
+apostolic
+230
+transactions
+##dge
+exempt
+swinging
+cove
+religions
+##ash
+shields
+dairy
+bypass
+190
+pursuing
+bug
+joyce
+bombay
+chassis
+southampton
+chat
+interact
+redesignated
+##pen
+nascar
+pray
+salmon
+rigid
+regained
+malaysian
+grim
+publicity
+constituted
+capturing
+toilet
+delegate
+purely
+tray
+drift
+loosely
+striker
+weakened
+trinidad
+mitch
+itv
+defines
+transmitted
+ming
+scarlet
+nodding
+fitzgerald
+fu
+narrowly
+sp
+tooth
+standings
+virtue
+##₁
+##wara
+##cting
+chateau
+gloves
+lid
+##nel
+hurting
+conservatory
+##pel
+sinclair
+reopened
+sympathy
+nigerian
+strode
+advocated
+optional
+chronic
+discharge
+##rc
+suck
+compatible
+laurel
+stella
+shi
+fails
+wage
+dodge
+128
+informal
+sorts
+levi
+buddha
+villagers
+##aka
+chronicles
+heavier
+summoned
+gateway
+3000
+eleventh
+jewelry
+translations
+accordingly
+seas
+##ency
+fiber
+pyramid
+cubic
+dragging
+##ista
+caring
+##ops
+android
+contacted
+lunar
+##dt
+kai
+lisbon
+patted
+1826
+sacramento
+theft
+madagascar
+subtropical
+disputes
+ta
+holidays
+piper
+willow
+mare
+cane
+itunes
+newfoundland
+benny
+companions
+dong
+raj
+observe
+roar
+charming
+plaque
+tibetan
+fossils
+enacted
+manning
+bubble
+tina
+tanzania
+##eda
+##hir
+funk
+swamp
+deputies
+cloak
+ufc
+scenario
+par
+scratch
+metals
+anthem
+guru
+engaging
+specially
+##boat
+dialects
+nineteen
+cecil
+duet
+disability
+messenger
+unofficial
+##lies
+defunct
+eds
+moonlight
+drainage
+surname
+puzzle
+honda
+switching
+conservatives
+mammals
+knox
+broadcaster
+sidewalk
+cope
+##ried
+benson
+princes
+peterson
+##sal
+bedford
+sharks
+eli
+wreck
+alberto
+gasp
+archaeology
+lgbt
+teaches
+securities
+madness
+compromise
+waving
+coordination
+davidson
+visions
+leased
+possibilities
+eighty
+jun
+fernandez
+enthusiasm
+assassin
+sponsorship
+reviewer
+kingdoms
+estonian
+laboratories
+##fy
+##nal
+applies
+verb
+celebrations
+##zzo
+rowing
+lightweight
+sadness
+submit
+mvp
+balanced
+dude
+##vas
+explicitly
+metric
+magnificent
+mound
+brett
+mohammad
+mistakes
+irregular
+##hing
+##ass
+sanders
+betrayed
+shipped
+surge
+##enburg
+reporters
+termed
+georg
+pity
+verbal
+bulls
+abbreviated
+enabling
+appealed
+##are
+##atic
+sicily
+sting
+heel
+sweetheart
+bart
+spacecraft
+brutal
+monarchy
+##tter
+aberdeen
+cameo
+diane
+##ub
+survivor
+clyde
+##aries
+complaint
+##makers
+clarinet
+delicious
+chilean
+karnataka
+coordinates
+1818
+panties
+##rst
+pretending
+ar
+dramatically
+kiev
+bella
+tends
+distances
+113
+catalog
+launching
+instances
+telecommunications
+portable
+lindsay
+vatican
+##eim
+angles
+aliens
+marker
+stint
+screens
+bolton
+##rne
+judy
+wool
+benedict
+plasma
+europa
+spark
+imaging
+filmmaker
+swiftly
+##een
+contributor
+##nor
+opted
+stamps
+apologize
+financing
+butter
+gideon
+sophisticated
+alignment
+avery
+chemicals
+yearly
+speculation
+prominence
+professionally
+##ils
+immortal
+institutional
+inception
+wrists
+identifying
+tribunal
+derives
+gains
+##wo
+papal
+preference
+linguistic
+vince
+operative
+brewery
+##ont
+unemployment
+boyd
+##ured
+##outs
+albeit
+prophet
+1813
+bi
+##rr
+##face
+##rad
+quarterly
+asteroid
+cleaned
+radius
+temper
+##llen
+telugu
+jerk
+viscount
+menu
+##ote
+glimpse
+##aya
+yacht
+hawaiian
+baden
+##rl
+laptop
+readily
+##gu
+monetary
+offshore
+scots
+watches
+##yang
+##arian
+upgrade
+needle
+xbox
+lea
+encyclopedia
+flank
+fingertips
+##pus
+delight
+teachings
+confirm
+roth
+beaches
+midway
+winters
+##iah
+teasing
+daytime
+beverly
+gambling
+bonnie
+##backs
+regulated
+clement
+hermann
+tricks
+knot
+##shing
+##uring
+##vre
+detached
+ecological
+owed
+specialty
+byron
+inventor
+bats
+stays
+screened
+unesco
+midland
+trim
+affection
+##ander
+##rry
+jess
+thoroughly
+feedback
+##uma
+chennai
+strained
+heartbeat
+wrapping
+overtime
+pleaded
+##sworth
+mon
+leisure
+oclc
+##tate
+##ele
+feathers
+angelo
+thirds
+nuts
+surveys
+clever
+gill
+commentator
+##dos
+darren
+rides
+gibraltar
+##nc
+##mu
+dissolution
+dedication
+shin
+meals
+saddle
+elvis
+reds
+chaired
+taller
+appreciation
+functioning
+niece
+favored
+advocacy
+robbie
+criminals
+suffolk
+yugoslav
+passport
+constable
+congressman
+hastings
+vera
+##rov
+consecrated
+sparks
+ecclesiastical
+confined
+##ovich
+muller
+floyd
+nora
+1822
+paved
+1827
+cumberland
+ned
+saga
+spiral
+##flow
+appreciated
+yi
+collaborative
+treating
+similarities
+feminine
+finishes
+##ib
+jade
+import
+##nse
+##hot
+champagne
+mice
+securing
+celebrities
+helsinki
+attributes
+##gos
+cousins
+phases
+ache
+lucia
+gandhi
+submission
+vicar
+spear
+shine
+tasmania
+biting
+detention
+constitute
+tighter
+seasonal
+##gus
+terrestrial
+matthews
+##oka
+effectiveness
+parody
+philharmonic
+##onic
+1816
+strangers
+encoded
+consortium
+guaranteed
+regards
+shifts
+tortured
+collision
+supervisor
+inform
+broader
+insight
+theaters
+armour
+emeritus
+blink
+incorporates
+mapping
+##50
+##ein
+handball
+flexible
+##nta
+substantially
+generous
+thief
+##own
+carr
+loses
+1793
+prose
+ucla
+romeo
+generic
+metallic
+realization
+damages
+mk
+commissioners
+zach
+default
+##ther
+helicopters
+lengthy
+stems
+spa
+partnered
+spectators
+rogue
+indication
+penalties
+teresa
+1801
+sen
+##tric
+dalton
+##wich
+irving
+photographic
+##vey
+dell
+deaf
+peters
+excluded
+unsure
+##vable
+patterson
+crawled
+##zio
+resided
+whipped
+latvia
+slower
+ecole
+pipes
+employers
+maharashtra
+comparable
+va
+textile
+pageant
+##gel
+alphabet
+binary
+irrigation
+chartered
+choked
+antoine
+offs
+waking
+supplement
+##wen
+quantities
+demolition
+regain
+locate
+urdu
+folks
+alt
+114
+##mc
+scary
+andreas
+whites
+##ava
+classrooms
+mw
+aesthetic
+publishes
+valleys
+guides
+cubs
+johannes
+bryant
+conventions
+affecting
+##itt
+drain
+awesome
+isolation
+prosecutor
+ambitious
+apology
+captive
+downs
+atmospheric
+lorenzo
+aisle
+beef
+foul
+##onia
+kidding
+composite
+disturbed
+illusion
+natives
+##ffer
+emi
+rockets
+riverside
+wartime
+painters
+adolf
+melted
+##ail
+uncertainty
+simulation
+hawks
+progressed
+meantime
+builder
+spray
+breach
+unhappy
+regina
+russians
+##urg
+determining
+##tation
+tram
+1806
+##quin
+aging
+##12
+1823
+garion
+rented
+mister
+diaz
+terminated
+clip
+1817
+depend
+nervously
+disco
+owe
+defenders
+shiva
+notorious
+disbelief
+shiny
+worcester
+##gation
+##yr
+trailing
+undertook
+islander
+belarus
+limitations
+watershed
+fuller
+overlooking
+utilized
+raphael
+1819
+synthetic
+breakdown
+klein
+##nate
+moaned
+memoir
+lamb
+practicing
+##erly
+cellular
+arrows
+exotic
+##graphy
+witches
+117
+charted
+rey
+hut
+hierarchy
+subdivision
+freshwater
+giuseppe
+aloud
+reyes
+qatar
+marty
+sideways
+utterly
+sexually
+jude
+prayers
+mccarthy
+softball
+blend
+damien
+##gging
+##metric
+wholly
+erupted
+lebanese
+negro
+revenues
+tasted
+comparative
+teamed
+transaction
+labeled
+maori
+sovereignty
+parkway
+trauma
+gran
+malay
+121
+advancement
+descendant
+2020
+buzz
+salvation
+inventory
+symbolic
+##making
+antarctica
+mps
+##gas
+##bro
+mohammed
+myanmar
+holt
+submarines
+tones
+##lman
+locker
+patriarch
+bangkok
+emerson
+remarks
+predators
+kin
+afghan
+confession
+norwich
+rental
+emerge
+advantages
+##zel
+rca
+##hold
+shortened
+storms
+aidan
+##matic
+autonomy
+compliance
+##quet
+dudley
+atp
+##osis
+1803
+motto
+documentation
+summary
+professors
+spectacular
+christina
+archdiocese
+flashing
+innocence
+remake
+##dell
+psychic
+reef
+scare
+employ
+rs
+sticks
+meg
+gus
+leans
+##ude
+accompany
+bergen
+tomas
+##iko
+doom
+wages
+pools
+##nch
+##bes
+breasts
+scholarly
+alison
+outline
+brittany
+breakthrough
+willis
+realistic
+##cut
+##boro
+competitor
+##stan
+pike
+picnic
+icon
+designing
+commercials
+washing
+villain
+skiing
+micro
+costumes
+auburn
+halted
+executives
+##hat
+logistics
+cycles
+vowel
+applicable
+barrett
+exclaimed
+eurovision
+eternity
+ramon
+##umi
+##lls
+modifications
+sweeping
+disgust
+##uck
+torch
+aviv
+ensuring
+rude
+dusty
+sonic
+donovan
+outskirts
+cu
+pathway
+##band
+##gun
+##lines
+disciplines
+acids
+cadet
+paired
+##40
+sketches
+##sive
+marriages
+##⁺
+folding
+peers
+slovak
+implies
+admired
+##beck
+1880s
+leopold
+instinct
+attained
+weston
+megan
+horace
+##ination
+dorsal
+ingredients
+evolutionary
+##its
+complications
+deity
+lethal
+brushing
+levy
+deserted
+institutes
+posthumously
+delivering
+telescope
+coronation
+motivated
+rapids
+luc
+flicked
+pays
+volcano
+tanner
+weighed
+##nica
+crowds
+frankie
+gifted
+addressing
+granddaughter
+winding
+##rna
+constantine
+gomez
+##front
+landscapes
+rudolf
+anthropology
+slate
+werewolf
+##lio
+astronomy
+circa
+rouge
+dreaming
+sack
+knelt
+drowned
+naomi
+prolific
+tracked
+freezing
+herb
+##dium
+agony
+randall
+twisting
+wendy
+deposit
+touches
+vein
+wheeler
+##bbled
+##bor
+batted
+retaining
+tire
+presently
+compare
+specification
+daemon
+nigel
+##grave
+merry
+recommendation
+czechoslovakia
+sandra
+ng
+roma
+##sts
+lambert
+inheritance
+sheikh
+winchester
+cries
+examining
+##yle
+comeback
+cuisine
+nave
+##iv
+ko
+retrieve
+tomatoes
+barker
+polished
+defining
+irene
+lantern
+personalities
+begging
+tract
+swore
+1809
+175
+##gic
+omaha
+brotherhood
+##rley
+haiti
+##ots
+exeter
+##ete
+##zia
+steele
+dumb
+pearson
+210
+surveyed
+elisabeth
+trends
+##ef
+fritz
+##rf
+premium
+bugs
+fraction
+calmly
+viking
+##birds
+tug
+inserted
+unusually
+##ield
+confronted
+distress
+crashing
+brent
+turks
+resign
+##olo
+cambodia
+gabe
+sauce
+##kal
+evelyn
+116
+extant
+clusters
+quarry
+teenagers
+luna
+##lers
+##ister
+affiliation
+drill
+##ashi
+panthers
+scenic
+libya
+anita
+strengthen
+inscriptions
+##cated
+lace
+sued
+judith
+riots
+##uted
+mint
+##eta
+preparations
+midst
+dub
+challenger
+##vich
+mock
+cf
+displaced
+wicket
+breaths
+enables
+schmidt
+analyst
+##lum
+ag
+highlight
+automotive
+axe
+josef
+newark
+sufficiently
+resembles
+50th
+##pal
+flushed
+mum
+traits
+##ante
+commodore
+incomplete
+warming
+titular
+ceremonial
+ethical
+118
+celebrating
+eighteenth
+cao
+lima
+medalist
+mobility
+strips
+snakes
+##city
+miniature
+zagreb
+barton
+escapes
+umbrella
+automated
+doubted
+differs
+cooled
+georgetown
+dresden
+cooked
+fade
+wyatt
+rna
+jacobs
+carlton
+abundant
+stereo
+boost
+madras
+inning
+##hia
+spur
+ip
+malayalam
+begged
+osaka
+groan
+escaping
+charging
+dose
+vista
+##aj
+bud
+papa
+communists
+advocates
+edged
+tri
+##cent
+resemble
+peaking
+necklace
+fried
+montenegro
+saxony
+goose
+glances
+stuttgart
+curator
+recruit
+grocery
+sympathetic
+##tting
+##fort
+127
+lotus
+randolph
+ancestor
+##rand
+succeeding
+jupiter
+1798
+macedonian
+##heads
+hiking
+1808
+handing
+fischer
+##itive
+garbage
+node
+##pies
+prone
+singular
+papua
+inclined
+attractions
+italia
+pouring
+motioned
+grandma
+garnered
+jacksonville
+corp
+ego
+ringing
+aluminum
+##hausen
+ordering
+##foot
+drawer
+traders
+synagogue
+##play
+##kawa
+resistant
+wandering
+fragile
+fiona
+teased
+var
+hardcore
+soaked
+jubilee
+decisive
+exposition
+mercer
+poster
+valencia
+hale
+kuwait
+1811
+##ises
+##wr
+##eed
+tavern
+gamma
+122
+johan
+##uer
+airways
+amino
+gil
+##ury
+vocational
+domains
+torres
+##sp
+generator
+folklore
+outcomes
+##keeper
+canberra
+shooter
+fl
+beams
+confrontation
+##lling
+##gram
+feb
+aligned
+forestry
+pipeline
+jax
+motorway
+conception
+decay
+##tos
+coffin
+##cott
+stalin
+1805
+escorted
+minded
+##nam
+sitcom
+purchasing
+twilight
+veronica
+additions
+passive
+tensions
+straw
+123
+frequencies
+1804
+refugee
+cultivation
+##iate
+christie
+clary
+bulletin
+crept
+disposal
+##rich
+##zong
+processor
+crescent
+##rol
+bmw
+emphasized
+whale
+nazis
+aurora
+##eng
+dwelling
+hauled
+sponsors
+toledo
+mega
+ideology
+theatres
+tessa
+cerambycidae
+saves
+turtle
+cone
+suspects
+kara
+rusty
+yelling
+greeks
+mozart
+shades
+cocked
+participant
+##tro
+shire
+spit
+freeze
+necessity
+##cos
+inmates
+nielsen
+councillors
+loaned
+uncommon
+omar
+peasants
+botanical
+offspring
+daniels
+formations
+jokes
+1794
+pioneers
+sigma
+licensing
+##sus
+wheelchair
+polite
+1807
+liquor
+pratt
+trustee
+##uta
+forewings
+balloon
+##zz
+kilometre
+camping
+explicit
+casually
+shawn
+foolish
+teammates
+nm
+hassan
+carrie
+judged
+satisfy
+vanessa
+knives
+selective
+cnn
+flowed
+##lice
+eclipse
+stressed
+eliza
+mathematician
+cease
+cultivated
+##roy
+commissions
+browns
+##ania
+destroyers
+sheridan
+meadow
+##rius
+minerals
+##cial
+downstream
+clash
+gram
+memoirs
+ventures
+baha
+seymour
+archie
+midlands
+edith
+fare
+flynn
+invite
+canceled
+tiles
+stabbed
+boulder
+incorporate
+amended
+camden
+facial
+mollusk
+unreleased
+descriptions
+yoga
+grabs
+550
+raises
+ramp
+shiver
+##rose
+coined
+pioneering
+tunes
+qing
+warwick
+tops
+119
+melanie
+giles
+##rous
+wandered
+##inal
+annexed
+nov
+30th
+unnamed
+##ished
+organizational
+airplane
+normandy
+stoke
+whistle
+blessing
+violations
+chased
+holders
+shotgun
+##ctic
+outlet
+reactor
+##vik
+tires
+tearing
+shores
+fortified
+mascot
+constituencies
+nc
+columnist
+productive
+tibet
+##rta
+lineage
+hooked
+oct
+tapes
+judging
+cody
+##gger
+hansen
+kashmir
+triggered
+##eva
+solved
+cliffs
+##tree
+resisted
+anatomy
+protesters
+transparent
+implied
+##iga
+injection
+mattress
+excluding
+##mbo
+defenses
+helpless
+devotion
+##elli
+growl
+liberals
+weber
+phenomena
+atoms
+plug
+##iff
+mortality
+apprentice
+howe
+convincing
+aaa
+swimmer
+barber
+leone
+promptly
+sodium
+def
+nowadays
+arise
+##oning
+gloucester
+corrected
+dignity
+norm
+erie
+##ders
+elders
+evacuated
+sylvia
+compression
+##yar
+hartford
+pose
+backpack
+reasoning
+accepts
+24th
+wipe
+millimetres
+marcel
+##oda
+dodgers
+albion
+1790
+overwhelmed
+aerospace
+oaks
+1795
+showcase
+acknowledge
+recovering
+nolan
+ashe
+hurts
+geology
+fashioned
+disappearance
+farewell
+swollen
+shrug
+marquis
+wimbledon
+124
+rue
+1792
+commemorate
+reduces
+experiencing
+inevitable
+calcutta
+intel
+##court
+murderer
+sticking
+fisheries
+imagery
+bloom
+280
+brake
+##inus
+gustav
+hesitation
+memorable
+po
+viral
+beans
+accidents
+tunisia
+antenna
+spilled
+consort
+treatments
+aye
+perimeter
+##gard
+donation
+hostage
+migrated
+banker
+addiction
+apex
+lil
+trout
+##ously
+conscience
+##nova
+rams
+sands
+genome
+passionate
+troubles
+##lets
+##set
+amid
+##ibility
+##ret
+higgins
+exceed
+vikings
+##vie
+payne
+##zan
+muscular
+##ste
+defendant
+sucking
+##wal
+ibrahim
+fuselage
+claudia
+vfl
+europeans
+snails
+interval
+##garh
+preparatory
+statewide
+tasked
+lacrosse
+viktor
+##lation
+angola
+##hra
+flint
+implications
+employs
+teens
+patrons
+stall
+weekends
+barriers
+scrambled
+nucleus
+tehran
+jenna
+parsons
+lifelong
+robots
+displacement
+5000
+##bles
+precipitation
+##gt
+knuckles
+clutched
+1802
+marrying
+ecology
+marx
+accusations
+declare
+scars
+kolkata
+mat
+meadows
+bermuda
+skeleton
+finalists
+vintage
+crawl
+coordinate
+affects
+subjected
+orchestral
+mistaken
+##tc
+mirrors
+dipped
+relied
+260
+arches
+candle
+##nick
+incorporating
+wildly
+fond
+basilica
+owl
+fringe
+rituals
+whispering
+stirred
+feud
+tertiary
+slick
+goat
+honorable
+whereby
+skip
+ricardo
+stripes
+parachute
+adjoining
+submerged
+synthesizer
+##gren
+intend
+positively
+ninety
+phi
+beaver
+partition
+fellows
+alexis
+prohibition
+carlisle
+bizarre
+fraternity
+##bre
+doubts
+icy
+cbc
+aquatic
+sneak
+sonny
+combines
+airports
+crude
+supervised
+spatial
+merge
+alfonso
+##bic
+corrupt
+scan
+undergo
+##ams
+disabilities
+colombian
+comparing
+dolphins
+perkins
+##lish
+reprinted
+unanimous
+bounced
+hairs
+underworld
+midwest
+semester
+bucket
+paperback
+miniseries
+coventry
+demise
+##leigh
+demonstrations
+sensor
+rotating
+yan
+##hler
+arrange
+soils
+##idge
+hyderabad
+labs
+##dr
+brakes
+grandchildren
+##nde
+negotiated
+rover
+ferrari
+continuation
+directorate
+augusta
+stevenson
+counterpart
+gore
+##rda
+nursery
+rican
+ave
+collectively
+broadly
+pastoral
+repertoire
+asserted
+discovering
+nordic
+styled
+fiba
+cunningham
+harley
+middlesex
+survives
+tumor
+tempo
+zack
+aiming
+lok
+urgent
+##rade
+##nto
+devils
+##ement
+contractor
+turin
+##wl
+##ool
+bliss
+repaired
+simmons
+moan
+astronomical
+cr
+negotiate
+lyric
+1890s
+lara
+bred
+clad
+angus
+pbs
+##ience
+engineered
+posed
+##lk
+hernandez
+possessions
+elbows
+psychiatric
+strokes
+confluence
+electorate
+lifts
+campuses
+lava
+alps
+##ep
+##ution
+##date
+physicist
+woody
+##page
+##ographic
+##itis
+juliet
+reformation
+sparhawk
+320
+complement
+suppressed
+jewel
+##½
+floated
+##kas
+continuity
+sadly
+##ische
+inability
+melting
+scanning
+paula
+flour
+judaism
+safer
+vague
+##lm
+solving
+curb
+##stown
+financially
+gable
+bees
+expired
+miserable
+cassidy
+dominion
+1789
+cupped
+145
+robbery
+facto
+amos
+warden
+resume
+tallest
+marvin
+ing
+pounded
+usd
+declaring
+gasoline
+##aux
+darkened
+270
+650
+sophomore
+##mere
+erection
+gossip
+televised
+risen
+dial
+##eu
+pillars
+##link
+passages
+profound
+##tina
+arabian
+ashton
+silicon
+nail
+##ead
+##lated
+##wer
+##hardt
+fleming
+firearms
+ducked
+circuits
+blows
+waterloo
+titans
+##lina
+atom
+fireplace
+cheshire
+financed
+activation
+algorithms
+##zzi
+constituent
+catcher
+cherokee
+partnerships
+sexuality
+platoon
+tragic
+vivian
+guarded
+whiskey
+meditation
+poetic
+##late
+##nga
+##ake
+porto
+listeners
+dominance
+kendra
+mona
+chandler
+factions
+22nd
+salisbury
+attitudes
+derivative
+##ido
+##haus
+intake
+paced
+javier
+illustrator
+barrels
+bias
+cockpit
+burnett
+dreamed
+ensuing
+##anda
+receptors
+someday
+hawkins
+mattered
+##lal
+slavic
+1799
+jesuit
+cameroon
+wasted
+tai
+wax
+lowering
+victorious
+freaking
+outright
+hancock
+librarian
+sensing
+bald
+calcium
+myers
+tablet
+announcing
+barack
+shipyard
+pharmaceutical
+##uan
+greenwich
+flush
+medley
+patches
+wolfgang
+pt
+speeches
+acquiring
+exams
+nikolai
+##gg
+hayden
+kannada
+##type
+reilly
+##pt
+waitress
+abdomen
+devastated
+capped
+pseudonym
+pharmacy
+fulfill
+paraguay
+1796
+clicked
+##trom
+archipelago
+syndicated
+##hman
+lumber
+orgasm
+rejection
+clifford
+lorraine
+advent
+mafia
+rodney
+brock
+##ght
+##used
+##elia
+cassette
+chamberlain
+despair
+mongolia
+sensors
+developmental
+upstream
+##eg
+##alis
+spanning
+165
+trombone
+basque
+seeded
+interred
+renewable
+rhys
+leapt
+revision
+molecule
+##ages
+chord
+vicious
+nord
+shivered
+23rd
+arlington
+debts
+corpus
+sunrise
+bays
+blackburn
+centimetres
+##uded
+shuddered
+gm
+strangely
+gripping
+cartoons
+isabelle
+orbital
+##ppa
+seals
+proving
+##lton
+refusal
+strengthened
+bust
+assisting
+baghdad
+batsman
+portrayal
+mara
+pushes
+spears
+og
+##cock
+reside
+nathaniel
+brennan
+1776
+confirmation
+caucus
+##worthy
+markings
+yemen
+nobles
+ku
+lazy
+viewer
+catalan
+encompasses
+sawyer
+##fall
+sparked
+substances
+patents
+braves
+arranger
+evacuation
+sergio
+persuade
+dover
+tolerance
+penguin
+cum
+jockey
+insufficient
+townships
+occupying
+declining
+plural
+processed
+projection
+puppet
+flanders
+introduces
+liability
+##yon
+gymnastics
+antwerp
+taipei
+hobart
+candles
+jeep
+wes
+observers
+126
+chaplain
+bundle
+glorious
+##hine
+hazel
+flung
+sol
+excavations
+dumped
+stares
+sh
+bangalore
+triangular
+icelandic
+intervals
+expressing
+turbine
+##vers
+songwriting
+crafts
+##igo
+jasmine
+ditch
+rite
+##ways
+entertaining
+comply
+sorrow
+wrestlers
+basel
+emirates
+marian
+rivera
+helpful
+##some
+caution
+downward
+networking
+##atory
+##tered
+darted
+genocide
+emergence
+replies
+specializing
+spokesman
+convenient
+unlocked
+fading
+augustine
+concentrations
+resemblance
+elijah
+investigator
+andhra
+##uda
+promotes
+bean
+##rrell
+fleeing
+wan
+simone
+announcer
+##ame
+##bby
+lydia
+weaver
+132
+residency
+modification
+##fest
+stretches
+##ast
+alternatively
+nat
+lowe
+lacks
+##ented
+pam
+tile
+concealed
+inferior
+abdullah
+residences
+tissues
+vengeance
+##ided
+moisture
+peculiar
+groove
+zip
+bologna
+jennings
+ninja
+oversaw
+zombies
+pumping
+batch
+livingston
+emerald
+installations
+1797
+peel
+nitrogen
+rama
+##fying
+##star
+schooling
+strands
+responding
+werner
+##ost
+lime
+casa
+accurately
+targeting
+##rod
+underway
+##uru
+hemisphere
+lester
+##yard
+occupies
+2d
+griffith
+angrily
+reorganized
+##owing
+courtney
+deposited
+##dd
+##30
+estadio
+##ifies
+dunn
+exiled
+##ying
+checks
+##combe
+##о
+##fly
+successes
+unexpectedly
+blu
+assessed
+##flower
+##ه
+observing
+sacked
+spiders
+kn
+##tail
+mu
+nodes
+prosperity
+audrey
+divisional
+155
+broncos
+tangled
+adjust
+feeds
+erosion
+paolo
+surf
+directory
+snatched
+humid
+admiralty
+screwed
+gt
+reddish
+##nese
+modules
+trench
+lamps
+bind
+leah
+bucks
+competes
+##nz
+##form
+transcription
+##uc
+isles
+violently
+clutching
+pga
+cyclist
+inflation
+flats
+ragged
+unnecessary
+##hian
+stubborn
+coordinated
+harriet
+baba
+disqualified
+330
+insect
+wolfe
+##fies
+reinforcements
+rocked
+duel
+winked
+embraced
+bricks
+##raj
+hiatus
+defeats
+pending
+brightly
+jealousy
+##xton
+##hm
+##uki
+lena
+gdp
+colorful
+##dley
+stein
+kidney
+##shu
+underwear
+wanderers
+##haw
+##icus
+guardians
+m³
+roared
+habits
+##wise
+permits
+gp
+uranium
+punished
+disguise
+bundesliga
+elise
+dundee
+erotic
+partisan
+pi
+collectors
+float
+individually
+rendering
+behavioral
+bucharest
+ser
+hare
+valerie
+corporal
+nutrition
+proportional
+##isa
+immense
+##kis
+pavement
+##zie
+##eld
+sutherland
+crouched
+1775
+##lp
+suzuki
+trades
+endurance
+operas
+crosby
+prayed
+priory
+rory
+socially
+##urn
+gujarat
+##pu
+walton
+cube
+pasha
+privilege
+lennon
+floods
+thorne
+waterfall
+nipple
+scouting
+approve
+##lov
+minorities
+voter
+dwight
+extensions
+assure
+ballroom
+slap
+dripping
+privileges
+rejoined
+confessed
+demonstrating
+patriotic
+yell
+investor
+##uth
+pagan
+slumped
+squares
+##cle
+##kins
+confront
+bert
+embarrassment
+##aid
+aston
+urging
+sweater
+starr
+yuri
+brains
+williamson
+commuter
+mortar
+structured
+selfish
+exports
+##jon
+cds
+##him
+unfinished
+##rre
+mortgage
+destinations
+##nagar
+canoe
+solitary
+buchanan
+delays
+magistrate
+fk
+##pling
+motivation
+##lier
+##vier
+recruiting
+assess
+##mouth
+malik
+antique
+1791
+pius
+rahman
+reich
+tub
+zhou
+smashed
+airs
+galway
+xii
+conditioning
+honduras
+discharged
+dexter
+##pf
+lionel
+129
+debates
+lemon
+tiffany
+volunteered
+dom
+dioxide
+procession
+devi
+sic
+tremendous
+advertisements
+colts
+transferring
+verdict
+hanover
+decommissioned
+utter
+relate
+pac
+racism
+##top
+beacon
+limp
+similarity
+terra
+occurrence
+ant
+##how
+becky
+capt
+updates
+armament
+richie
+pal
+##graph
+halloween
+mayo
+##ssen
+##bone
+cara
+serena
+fcc
+dolls
+obligations
+##dling
+violated
+lafayette
+jakarta
+exploitation
+##ime
+infamous
+iconic
+##lah
+##park
+kitty
+moody
+reginald
+dread
+spill
+crystals
+olivier
+modeled
+bluff
+equilibrium
+separating
+notices
+ordnance
+extinction
+onset
+cosmic
+attachment
+sammy
+expose
+privy
+anchored
+##bil
+abbott
+admits
+bending
+baritone
+emmanuel
+policeman
+vaughan
+winged
+climax
+dresses
+denny
+polytechnic
+mohamed
+burmese
+authentic
+nikki
+genetics
+grandparents
+homestead
+gaza
+postponed
+metacritic
+una
+##sby
+##bat
+unstable
+dissertation
+##rial
+##cian
+curls
+obscure
+uncovered
+bronx
+praying
+disappearing
+##hoe
+prehistoric
+coke
+turret
+mutations
+nonprofit
+pits
+monaco
+##ي
+##usion
+prominently
+dispatched
+podium
+##mir
+uci
+##uation
+133
+fortifications
+birthplace
+kendall
+##lby
+##oll
+preacher
+rack
+goodman
+##rman
+persistent
+##ott
+countless
+jaime
+recorder
+lexington
+persecution
+jumps
+renewal
+wagons
+##11
+crushing
+##holder
+decorations
+##lake
+abundance
+wrath
+laundry
+£1
+garde
+##rp
+jeanne
+beetles
+peasant
+##sl
+splitting
+caste
+sergei
+##rer
+##ema
+scripts
+##ively
+rub
+satellites
+##vor
+inscribed
+verlag
+scrapped
+gale
+packages
+chick
+potato
+slogan
+kathleen
+arabs
+##culture
+counterparts
+reminiscent
+choral
+##tead
+rand
+retains
+bushes
+dane
+accomplish
+courtesy
+closes
+##oth
+slaughter
+hague
+krakow
+lawson
+tailed
+elias
+ginger
+##ttes
+canopy
+betrayal
+rebuilding
+turf
+##hof
+frowning
+allegiance
+brigades
+kicks
+rebuild
+polls
+alias
+nationalism
+td
+rowan
+audition
+bowie
+fortunately
+recognizes
+harp
+dillon
+horrified
+##oro
+renault
+##tics
+ropes
+##α
+presumed
+rewarded
+infrared
+wiping
+accelerated
+illustration
+##rid
+presses
+practitioners
+badminton
+##iard
+detained
+##tera
+recognizing
+relates
+misery
+##sies
+##tly
+reproduction
+piercing
+potatoes
+thornton
+esther
+manners
+hbo
+##aan
+ours
+bullshit
+ernie
+perennial
+sensitivity
+illuminated
+rupert
+##jin
+##iss
+##ear
+rfc
+nassau
+##dock
+staggered
+socialism
+##haven
+appointments
+nonsense
+prestige
+sharma
+haul
+##tical
+solidarity
+gps
+##ook
+##rata
+igor
+pedestrian
+##uit
+baxter
+tenants
+wires
+medication
+unlimited
+guiding
+impacts
+diabetes
+##rama
+sasha
+pas
+clive
+extraction
+131
+continually
+constraints
+##bilities
+sonata
+hunted
+sixteenth
+chu
+planting
+quote
+mayer
+pretended
+abs
+spat
+##hua
+ceramic
+##cci
+curtains
+pigs
+pitching
+##dad
+latvian
+sore
+dayton
+##sted
+##qi
+patrols
+slice
+playground
+##nted
+shone
+stool
+apparatus
+inadequate
+mates
+treason
+##ija
+desires
+##liga
+##croft
+somalia
+laurent
+mir
+leonardo
+oracle
+grape
+obliged
+chevrolet
+thirteenth
+stunning
+enthusiastic
+##ede
+accounted
+concludes
+currents
+basil
+##kovic
+drought
+##rica
+mai
+##aire
+shove
+posting
+##shed
+pilgrimage
+humorous
+packing
+fry
+pencil
+wines
+smells
+144
+marilyn
+aching
+newest
+clung
+bon
+neighbours
+sanctioned
+##pie
+mug
+##stock
+drowning
+##mma
+hydraulic
+##vil
+hiring
+reminder
+lilly
+investigators
+##ncies
+sour
+##eous
+compulsory
+packet
+##rion
+##graphic
+##elle
+cannes
+##inate
+depressed
+##rit
+heroic
+importantly
+theresa
+##tled
+conway
+saturn
+marginal
+rae
+##xia
+corresponds
+royce
+pact
+jasper
+explosives
+packaging
+aluminium
+##ttered
+denotes
+rhythmic
+spans
+assignments
+hereditary
+outlined
+originating
+sundays
+lad
+reissued
+greeting
+beatrice
+##dic
+pillar
+marcos
+plots
+handbook
+alcoholic
+judiciary
+avant
+slides
+extract
+masculine
+blur
+##eum
+##force
+homage
+trembled
+owens
+hymn
+trey
+omega
+signaling
+socks
+accumulated
+reacted
+attic
+theo
+lining
+angie
+distraction
+primera
+talbot
+##key
+1200
+ti
+creativity
+billed
+##hey
+deacon
+eduardo
+identifies
+proposition
+dizzy
+gunner
+hogan
+##yam
+##pping
+##hol
+ja
+##chan
+jensen
+reconstructed
+##berger
+clearance
+darius
+##nier
+abe
+harlem
+plea
+dei
+circled
+emotionally
+notation
+fascist
+neville
+exceeded
+upwards
+viable
+ducks
+##fo
+workforce
+racer
+limiting
+shri
+##lson
+possesses
+1600
+kerr
+moths
+devastating
+laden
+disturbing
+locking
+##cture
+gal
+fearing
+accreditation
+flavor
+aide
+1870s
+mountainous
+##baum
+melt
+##ures
+motel
+texture
+servers
+soda
+##mb
+herd
+##nium
+erect
+puzzled
+hum
+peggy
+examinations
+gould
+testified
+geoff
+ren
+devised
+sacks
+##law
+denial
+posters
+grunted
+cesar
+tutor
+ec
+gerry
+offerings
+byrne
+falcons
+combinations
+ct
+incoming
+pardon
+rocking
+26th
+avengers
+flared
+mankind
+seller
+uttar
+loch
+nadia
+stroking
+exposing
+##hd
+fertile
+ancestral
+instituted
+##has
+noises
+prophecy
+taxation
+eminent
+vivid
+pol
+##bol
+dart
+indirect
+multimedia
+notebook
+upside
+displaying
+adrenaline
+referenced
+geometric
+##iving
+progression
+##ddy
+blunt
+announce
+##far
+implementing
+##lav
+aggression
+liaison
+cooler
+cares
+headache
+plantations
+gorge
+dots
+impulse
+thickness
+ashamed
+averaging
+kathy
+obligation
+precursor
+137
+fowler
+symmetry
+thee
+225
+hears
+##rai
+undergoing
+ads
+butcher
+bowler
+##lip
+cigarettes
+subscription
+goodness
+##ically
+browne
+##hos
+##tech
+kyoto
+donor
+##erty
+damaging
+friction
+drifting
+expeditions
+hardened
+prostitution
+152
+fauna
+blankets
+claw
+tossing
+snarled
+butterflies
+recruits
+investigative
+coated
+healed
+138
+communal
+hai
+xiii
+academics
+boone
+psychologist
+restless
+lahore
+stephens
+mba
+brendan
+foreigners
+printer
+##pc
+ached
+explode
+27th
+deed
+scratched
+dared
+##pole
+cardiac
+1780
+okinawa
+proto
+commando
+compelled
+oddly
+electrons
+##base
+replica
+thanksgiving
+##rist
+sheila
+deliberate
+stafford
+tidal
+representations
+hercules
+ou
+##path
+##iated
+kidnapping
+lenses
+##tling
+deficit
+samoa
+mouths
+consuming
+computational
+maze
+granting
+smirk
+razor
+fixture
+ideals
+inviting
+aiden
+nominal
+##vs
+issuing
+julio
+pitt
+ramsey
+docks
+##oss
+exhaust
+##owed
+bavarian
+draped
+anterior
+mating
+ethiopian
+explores
+noticing
+##nton
+discarded
+convenience
+hoffman
+endowment
+beasts
+cartridge
+mormon
+paternal
+probe
+sleeves
+interfere
+lump
+deadline
+##rail
+jenks
+bulldogs
+scrap
+alternating
+justified
+reproductive
+nam
+seize
+descending
+secretariat
+kirby
+coupe
+grouped
+smash
+panther
+sedan
+tapping
+##18
+lola
+cheer
+germanic
+unfortunate
+##eter
+unrelated
+##fan
+subordinate
+##sdale
+suzanne
+advertisement
+##ility
+horsepower
+##lda
+cautiously
+discourse
+luigi
+##mans
+##fields
+noun
+prevalent
+mao
+schneider
+everett
+surround
+governorate
+kira
+##avia
+westward
+##take
+misty
+rails
+sustainability
+134
+unused
+##rating
+packs
+toast
+unwilling
+regulate
+thy
+suffrage
+nile
+awe
+assam
+definitions
+travelers
+affordable
+##rb
+conferred
+sells
+undefeated
+beneficial
+torso
+basal
+repeating
+remixes
+##pass
+bahrain
+cables
+fang
+##itated
+excavated
+numbering
+statutory
+##rey
+deluxe
+##lian
+forested
+ramirez
+derbyshire
+zeus
+slamming
+transfers
+astronomer
+banana
+lottery
+berg
+histories
+bamboo
+##uchi
+resurrection
+posterior
+bowls
+vaguely
+##thi
+thou
+preserving
+tensed
+offence
+##inas
+meyrick
+callum
+ridden
+watt
+langdon
+tying
+lowland
+snorted
+daring
+truman
+##hale
+##girl
+aura
+overly
+filing
+weighing
+goa
+infections
+philanthropist
+saunders
+eponymous
+##owski
+latitude
+perspectives
+reviewing
+mets
+commandant
+radial
+##kha
+flashlight
+reliability
+koch
+vowels
+amazed
+ada
+elaine
+supper
+##rth
+##encies
+predator
+debated
+soviets
+cola
+##boards
+##nah
+compartment
+crooked
+arbitrary
+fourteenth
+##ctive
+havana
+majors
+steelers
+clips
+profitable
+ambush
+exited
+packers
+##tile
+nude
+cracks
+fungi
+##е
+limb
+trousers
+josie
+shelby
+tens
+frederic
+##ος
+definite
+smoothly
+constellation
+insult
+baton
+discs
+lingering
+##nco
+conclusions
+lent
+staging
+becker
+grandpa
+shaky
+##tron
+einstein
+obstacles
+sk
+adverse
+elle
+economically
+##moto
+mccartney
+thor
+dismissal
+motions
+readings
+nostrils
+treatise
+##pace
+squeezing
+evidently
+prolonged
+1783
+venezuelan
+je
+marguerite
+beirut
+takeover
+shareholders
+##vent
+denise
+digit
+airplay
+norse
+##bbling
+imaginary
+pills
+hubert
+blaze
+vacated
+eliminating
+##ello
+vine
+mansfield
+##tty
+retrospective
+barrow
+borne
+clutch
+bail
+forensic
+weaving
+##nett
+##witz
+desktop
+citadel
+promotions
+worrying
+dorset
+ieee
+subdivided
+##iating
+manned
+expeditionary
+pickup
+synod
+chuckle
+185
+barney
+##rz
+##ffin
+functionality
+karachi
+litigation
+meanings
+uc
+lick
+turbo
+anders
+##ffed
+execute
+curl
+oppose
+ankles
+typhoon
+##د
+##ache
+##asia
+linguistics
+compassion
+pressures
+grazing
+perfection
+##iting
+immunity
+monopoly
+muddy
+backgrounds
+136
+namibia
+francesca
+monitors
+attracting
+stunt
+tuition
+##ии
+vegetable
+##mates
+##quent
+mgm
+jen
+complexes
+forts
+##ond
+cellar
+bites
+seventeenth
+royals
+flemish
+failures
+mast
+charities
+##cular
+peruvian
+capitals
+macmillan
+ipswich
+outward
+frigate
+postgraduate
+folds
+employing
+##ouse
+concurrently
+fiery
+##tai
+contingent
+nightmares
+monumental
+nicaragua
+##kowski
+lizard
+mal
+fielding
+gig
+reject
+##pad
+harding
+##ipe
+coastline
+##cin
+##nos
+beethoven
+humphrey
+innovations
+##tam
+##nge
+norris
+doris
+solicitor
+huang
+obey
+141
+##lc
+niagara
+##tton
+shelves
+aug
+bourbon
+curry
+nightclub
+specifications
+hilton
+##ndo
+centennial
+dispersed
+worm
+neglected
+briggs
+sm
+font
+kuala
+uneasy
+plc
+##nstein
+##bound
+##aking
+##burgh
+awaiting
+pronunciation
+##bbed
+##quest
+eh
+optimal
+zhu
+raped
+greens
+presided
+brenda
+worries
+##life
+venetian
+marxist
+turnout
+##lius
+refined
+braced
+sins
+grasped
+sunderland
+nickel
+speculated
+lowell
+cyrillic
+communism
+fundraising
+resembling
+colonists
+mutant
+freddie
+usc
+##mos
+gratitude
+##run
+mural
+##lous
+chemist
+wi
+reminds
+28th
+steals
+tess
+pietro
+##ingen
+promoter
+ri
+microphone
+honoured
+rai
+sant
+##qui
+feather
+##nson
+burlington
+kurdish
+terrorists
+deborah
+sickness
+##wed
+##eet
+hazard
+irritated
+desperation
+veil
+clarity
+##rik
+jewels
+xv
+##gged
+##ows
+##cup
+berkshire
+unfair
+mysteries
+orchid
+winced
+exhaustion
+renovations
+stranded
+obe
+infinity
+##nies
+adapt
+redevelopment
+thanked
+registry
+olga
+domingo
+noir
+tudor
+ole
+##atus
+commenting
+behaviors
+##ais
+crisp
+pauline
+probable
+stirling
+wigan
+##bian
+paralympics
+panting
+surpassed
+##rew
+luca
+barred
+pony
+famed
+##sters
+cassandra
+waiter
+carolyn
+exported
+##orted
+andres
+destructive
+deeds
+jonah
+castles
+vacancy
+suv
+##glass
+1788
+orchard
+yep
+famine
+belarusian
+sprang
+##forth
+skinny
+##mis
+administrators
+rotterdam
+zambia
+zhao
+boiler
+discoveries
+##ride
+##physics
+lucius
+disappointing
+outreach
+spoon
+##frame
+qualifications
+unanimously
+enjoys
+regency
+##iidae
+stade
+realism
+veterinary
+rodgers
+dump
+alain
+chestnut
+castile
+censorship
+rumble
+gibbs
+##itor
+communion
+reggae
+inactivated
+logs
+loads
+##houses
+homosexual
+##iano
+ale
+informs
+##cas
+phrases
+plaster
+linebacker
+ambrose
+kaiser
+fascinated
+850
+limerick
+recruitment
+forge
+mastered
+##nding
+leinster
+rooted
+threaten
+##strom
+borneo
+##hes
+suggestions
+scholarships
+propeller
+documentaries
+patronage
+coats
+constructing
+invest
+neurons
+comet
+entirety
+shouts
+identities
+annoying
+unchanged
+wary
+##antly
+##ogy
+neat
+oversight
+##kos
+phillies
+replay
+constance
+##kka
+incarnation
+humble
+skies
+minus
+##acy
+smithsonian
+##chel
+guerrilla
+jar
+cadets
+##plate
+surplus
+audit
+##aru
+cracking
+joanna
+louisa
+pacing
+##lights
+intentionally
+##iri
+diner
+nwa
+imprint
+australians
+tong
+unprecedented
+bunker
+naive
+specialists
+ark
+nichols
+railing
+leaked
+pedal
+##uka
+shrub
+longing
+roofs
+v8
+captains
+neural
+tuned
+##ntal
+##jet
+emission
+medina
+frantic
+codex
+definitive
+sid
+abolition
+intensified
+stocks
+enrique
+sustain
+genoa
+oxide
+##written
+clues
+cha
+##gers
+tributaries
+fragment
+venom
+##rity
+##ente
+##sca
+muffled
+vain
+sire
+laos
+##ingly
+##hana
+hastily
+snapping
+surfaced
+sentiment
+motive
+##oft
+contests
+approximate
+mesa
+luckily
+dinosaur
+exchanges
+propelled
+accord
+bourne
+relieve
+tow
+masks
+offended
+##ues
+cynthia
+##mmer
+rains
+bartender
+zinc
+reviewers
+lois
+##sai
+legged
+arrogant
+rafe
+rosie
+comprise
+handicap
+blockade
+inlet
+lagoon
+copied
+drilling
+shelley
+petals
+##inian
+mandarin
+obsolete
+##inated
+onward
+arguably
+productivity
+cindy
+praising
+seldom
+busch
+discusses
+raleigh
+shortage
+ranged
+stanton
+encouragement
+firstly
+conceded
+overs
+temporal
+##uke
+cbe
+##bos
+woo
+certainty
+pumps
+##pton
+stalked
+##uli
+lizzie
+periodic
+thieves
+weaker
+##night
+gases
+shoving
+chooses
+wc
+##chemical
+prompting
+weights
+##kill
+robust
+flanked
+sticky
+hu
+tuberculosis
+##eb
+##eal
+christchurch
+resembled
+wallet
+reese
+inappropriate
+pictured
+distract
+fixing
+fiddle
+giggled
+burger
+heirs
+hairy
+mechanic
+torque
+apache
+obsessed
+chiefly
+cheng
+logging
+##tag
+extracted
+meaningful
+numb
+##vsky
+gloucestershire
+reminding
+##bay
+unite
+##lit
+breeds
+diminished
+clown
+glove
+1860s
+##ن
+##ug
+archibald
+focal
+freelance
+sliced
+depiction
+##yk
+organism
+switches
+sights
+stray
+crawling
+##ril
+lever
+leningrad
+interpretations
+loops
+anytime
+reel
+alicia
+delighted
+##ech
+inhaled
+xiv
+suitcase
+bernie
+vega
+licenses
+northampton
+exclusion
+induction
+monasteries
+racecourse
+homosexuality
+##right
+##sfield
+##rky
+dimitri
+michele
+alternatives
+ions
+commentators
+genuinely
+objected
+pork
+hospitality
+fencing
+stephan
+warships
+peripheral
+wit
+drunken
+wrinkled
+quentin
+spends
+departing
+chung
+numerical
+spokesperson
+##zone
+johannesburg
+caliber
+killers
+##udge
+assumes
+neatly
+demographic
+abigail
+bloc
+##vel
+mounting
+##lain
+bentley
+slightest
+xu
+recipients
+##jk
+merlin
+##writer
+seniors
+prisons
+blinking
+hindwings
+flickered
+kappa
+##hel
+80s
+strengthening
+appealing
+brewing
+gypsy
+mali
+lashes
+hulk
+unpleasant
+harassment
+bio
+treaties
+predict
+instrumentation
+pulp
+troupe
+boiling
+mantle
+##ffe
+ins
+##vn
+dividing
+handles
+verbs
+##onal
+coconut
+senegal
+340
+thorough
+gum
+momentarily
+##sto
+cocaine
+panicked
+destined
+##turing
+teatro
+denying
+weary
+captained
+mans
+##hawks
+##code
+wakefield
+bollywood
+thankfully
+##16
+cyril
+##wu
+amendments
+##bahn
+consultation
+stud
+reflections
+kindness
+1787
+internally
+##ovo
+tex
+mosaic
+distribute
+paddy
+seeming
+143
+##hic
+piers
+##15
+##mura
+##verse
+popularly
+winger
+kang
+sentinel
+mccoy
+##anza
+covenant
+##bag
+verge
+fireworks
+suppress
+thrilled
+dominate
+##jar
+swansea
+##60
+142
+reconciliation
+##ndi
+stiffened
+cue
+dorian
+##uf
+damascus
+amor
+ida
+foremost
+##aga
+porsche
+unseen
+dir
+##had
+##azi
+stony
+lexi
+melodies
+##nko
+angular
+integer
+podcast
+ants
+inherent
+jaws
+justify
+persona
+##olved
+josephine
+##nr
+##ressed
+customary
+flashes
+gala
+cyrus
+glaring
+backyard
+ariel
+physiology
+greenland
+html
+stir
+avon
+atletico
+finch
+methodology
+ked
+##lent
+mas
+catholicism
+townsend
+branding
+quincy
+fits
+containers
+1777
+ashore
+aragon
+##19
+forearm
+poisoning
+##sd
+adopting
+conquer
+grinding
+amnesty
+keller
+finances
+evaluate
+forged
+lankan
+instincts
+##uto
+guam
+bosnian
+photographed
+workplace
+desirable
+protector
+##dog
+allocation
+intently
+encourages
+willy
+##sten
+bodyguard
+electro
+brighter
+##ν
+bihar
+##chev
+lasts
+opener
+amphibious
+sal
+verde
+arte
+##cope
+captivity
+vocabulary
+yields
+##tted
+agreeing
+desmond
+pioneered
+##chus
+strap
+campaigned
+railroads
+##ович
+emblem
+##dre
+stormed
+501
+##ulous
+marijuana
+northumberland
+##gn
+##nath
+bowen
+landmarks
+beaumont
+##qua
+danube
+##bler
+attorneys
+th
+ge
+flyers
+critique
+villains
+cass
+mutation
+acc
+##0s
+colombo
+mckay
+motif
+sampling
+concluding
+syndicate
+##rell
+neon
+stables
+ds
+warnings
+clint
+mourning
+wilkinson
+##tated
+merrill
+leopard
+evenings
+exhaled
+emil
+sonia
+ezra
+discrete
+stove
+farrell
+fifteenth
+prescribed
+superhero
+##rier
+worms
+helm
+wren
+##duction
+##hc
+expo
+##rator
+hq
+unfamiliar
+antony
+prevents
+acceleration
+fiercely
+mari
+painfully
+calculations
+cheaper
+ign
+clifton
+irvine
+davenport
+mozambique
+##np
+pierced
+##evich
+wonders
+##wig
+##cate
+##iling
+crusade
+ware
+##uel
+enzymes
+reasonably
+mls
+##coe
+mater
+ambition
+bunny
+eliot
+kernel
+##fin
+asphalt
+headmaster
+torah
+aden
+lush
+pins
+waived
+##care
+##yas
+joao
+substrate
+enforce
+##grad
+##ules
+alvarez
+selections
+epidemic
+tempted
+##bit
+bremen
+translates
+ensured
+waterfront
+29th
+forrest
+manny
+malone
+kramer
+reigning
+cookies
+simpler
+absorption
+205
+engraved
+##ffy
+evaluated
+1778
+haze
+146
+comforting
+crossover
+##abe
+thorn
+##rift
+##imo
+##pop
+suppression
+fatigue
+cutter
+##tr
+201
+wurttemberg
+##orf
+enforced
+hovering
+proprietary
+gb
+samurai
+syllable
+ascent
+lacey
+tick
+lars
+tractor
+merchandise
+rep
+bouncing
+defendants
+##yre
+huntington
+##ground
+##oko
+standardized
+##hor
+##hima
+assassinated
+nu
+predecessors
+rainy
+liar
+assurance
+lyrical
+##uga
+secondly
+flattened
+ios
+parameter
+undercover
+##mity
+bordeaux
+punish
+ridges
+markers
+exodus
+inactive
+hesitate
+debbie
+nyc
+pledge
+savoy
+nagar
+offset
+organist
+##tium
+hesse
+marin
+converting
+##iver
+diagram
+propulsion
+pu
+validity
+reverted
+supportive
+##dc
+ministries
+clans
+responds
+proclamation
+##inae
+##ø
+##rea
+ein
+pleading
+patriot
+sf
+birch
+islanders
+strauss
+hates
+##dh
+brandenburg
+concession
+rd
+##ob
+1900s
+killings
+textbook
+antiquity
+cinematography
+wharf
+embarrassing
+setup
+creed
+farmland
+inequality
+centred
+signatures
+fallon
+370
+##ingham
+##uts
+ceylon
+gazing
+directive
+laurie
+##tern
+globally
+##uated
+##dent
+allah
+excavation
+threads
+##cross
+148
+frantically
+icc
+utilize
+determines
+respiratory
+thoughtful
+receptions
+##dicate
+merging
+chandra
+seine
+147
+builders
+builds
+diagnostic
+dev
+visibility
+goddamn
+analyses
+dhaka
+cho
+proves
+chancel
+concurrent
+curiously
+canadians
+pumped
+restoring
+1850s
+turtles
+jaguar
+sinister
+spinal
+traction
+declan
+vows
+1784
+glowed
+capitalism
+swirling
+install
+universidad
+##lder
+##oat
+soloist
+##genic
+##oor
+coincidence
+beginnings
+nissan
+dip
+resorts
+caucasus
+combustion
+infectious
+##eno
+pigeon
+serpent
+##itating
+conclude
+masked
+salad
+jew
+##gr
+surreal
+toni
+##wc
+harmonica
+151
+##gins
+##etic
+##coat
+fishermen
+intending
+bravery
+##wave
+klaus
+titan
+wembley
+taiwanese
+ransom
+40th
+incorrect
+hussein
+eyelids
+jp
+cooke
+dramas
+utilities
+##etta
+##print
+eisenhower
+principally
+granada
+lana
+##rak
+openings
+concord
+##bl
+bethany
+connie
+morality
+sega
+##mons
+##nard
+earnings
+##kara
+##cine
+wii
+communes
+##rel
+coma
+composing
+softened
+severed
+grapes
+##17
+nguyen
+analyzed
+warlord
+hubbard
+heavenly
+behave
+slovenian
+##hit
+##ony
+hailed
+filmmakers
+trance
+caldwell
+skye
+unrest
+coward
+likelihood
+##aging
+bern
+sci
+taliban
+honolulu
+propose
+##wang
+1700
+browser
+imagining
+cobra
+contributes
+dukes
+instinctively
+conan
+violinist
+##ores
+accessories
+gradual
+##amp
+quotes
+sioux
+##dating
+undertake
+intercepted
+sparkling
+compressed
+139
+fungus
+tombs
+haley
+imposing
+rests
+degradation
+lincolnshire
+retailers
+wetlands
+tulsa
+distributor
+dungeon
+nun
+greenhouse
+convey
+atlantis
+aft
+exits
+oman
+dresser
+lyons
+##sti
+joking
+eddy
+judgement
+omitted
+digits
+##cts
+##game
+juniors
+##rae
+cents
+stricken
+une
+##ngo
+wizards
+weir
+breton
+nan
+technician
+fibers
+liking
+royalty
+##cca
+154
+persia
+terribly
+magician
+##rable
+##unt
+vance
+cafeteria
+booker
+camille
+warmer
+##static
+consume
+cavern
+gaps
+compass
+contemporaries
+foyer
+soothing
+graveyard
+maj
+plunged
+blush
+##wear
+cascade
+demonstrates
+ordinance
+##nov
+boyle
+##lana
+rockefeller
+shaken
+banjo
+izzy
+##ense
+breathless
+vines
+##32
+##eman
+alterations
+chromosome
+dwellings
+feudal
+mole
+153
+catalonia
+relics
+tenant
+mandated
+##fm
+fridge
+hats
+honesty
+patented
+raul
+heap
+cruisers
+accusing
+enlightenment
+infants
+wherein
+chatham
+contractors
+zen
+affinity
+hc
+osborne
+piston
+156
+traps
+maturity
+##rana
+lagos
+##zal
+peering
+##nay
+attendant
+dealers
+protocols
+subset
+prospects
+biographical
+##cre
+artery
+##zers
+insignia
+nuns
+endured
+##eration
+recommend
+schwartz
+serbs
+berger
+cromwell
+crossroads
+##ctor
+enduring
+clasped
+grounded
+##bine
+marseille
+twitched
+abel
+choke
+https
+catalyst
+moldova
+italians
+##tist
+disastrous
+wee
+##oured
+##nti
+wwf
+nope
+##piration
+##asa
+expresses
+thumbs
+167
+##nza
+coca
+1781
+cheating
+##ption
+skipped
+sensory
+heidelberg
+spies
+satan
+dangers
+semifinal
+202
+bohemia
+whitish
+confusing
+shipbuilding
+relies
+surgeons
+landings
+ravi
+baku
+moor
+suffix
+alejandro
+##yana
+litre
+upheld
+##unk
+rajasthan
+##rek
+coaster
+insists
+posture
+scenarios
+etienne
+favoured
+appoint
+transgender
+elephants
+poked
+greenwood
+defences
+fulfilled
+militant
+somali
+1758
+chalk
+potent
+##ucci
+migrants
+wink
+assistants
+nos
+restriction
+activism
+niger
+##ario
+colon
+shaun
+##sat
+daphne
+##erated
+swam
+congregations
+reprise
+considerations
+magnet
+playable
+xvi
+##р
+overthrow
+tobias
+knob
+chavez
+coding
+##mers
+propped
+katrina
+orient
+newcomer
+##suke
+temperate
+##pool
+farmhouse
+interrogation
+##vd
+committing
+##vert
+forthcoming
+strawberry
+joaquin
+macau
+ponds
+shocking
+siberia
+##cellular
+chant
+contributors
+##nant
+##ologists
+sped
+absorb
+hail
+1782
+spared
+##hore
+barbados
+karate
+opus
+originates
+saul
+##xie
+evergreen
+leaped
+##rock
+correlation
+exaggerated
+weekday
+unification
+bump
+tracing
+brig
+afb
+pathways
+utilizing
+##ners
+mod
+mb
+disturbance
+kneeling
+##stad
+##guchi
+100th
+pune
+##thy
+decreasing
+168
+manipulation
+miriam
+academia
+ecosystem
+occupational
+rbi
+##lem
+rift
+##14
+rotary
+stacked
+incorporation
+awakening
+generators
+guerrero
+racist
+##omy
+cyber
+derivatives
+culminated
+allie
+annals
+panzer
+sainte
+wikipedia
+pops
+zu
+austro
+##vate
+algerian
+politely
+nicholson
+mornings
+educate
+tastes
+thrill
+dartmouth
+##gating
+db
+##jee
+regan
+differing
+concentrating
+choreography
+divinity
+##media
+pledged
+alexandre
+routing
+gregor
+madeline
+##idal
+apocalypse
+##hora
+gunfire
+culminating
+elves
+fined
+liang
+lam
+programmed
+tar
+guessing
+transparency
+gabrielle
+##gna
+cancellation
+flexibility
+##lining
+accession
+shea
+stronghold
+nets
+specializes
+##rgan
+abused
+hasan
+sgt
+ling
+exceeding
+##₄
+admiration
+supermarket
+##ark
+photographers
+specialised
+tilt
+resonance
+hmm
+perfume
+380
+sami
+threatens
+garland
+botany
+guarding
+boiled
+greet
+puppy
+russo
+supplier
+wilmington
+vibrant
+vijay
+##bius
+paralympic
+grumbled
+paige
+faa
+licking
+margins
+hurricanes
+##gong
+fest
+grenade
+ripping
+##uz
+counseling
+weigh
+##sian
+needles
+wiltshire
+edison
+costly
+##not
+fulton
+tramway
+redesigned
+staffordshire
+cache
+gasping
+watkins
+sleepy
+candidacy
+##group
+monkeys
+timeline
+throbbing
+##bid
+##sos
+berth
+uzbekistan
+vanderbilt
+bothering
+overturned
+ballots
+gem
+##iger
+sunglasses
+subscribers
+hooker
+compelling
+ang
+exceptionally
+saloon
+stab
+##rdi
+carla
+terrifying
+rom
+##vision
+coil
+##oids
+satisfying
+vendors
+31st
+mackay
+deities
+overlooked
+ambient
+bahamas
+felipe
+olympia
+whirled
+botanist
+advertised
+tugging
+##dden
+disciples
+morales
+unionist
+rites
+foley
+morse
+motives
+creepy
+##₀
+soo
+##sz
+bargain
+highness
+frightening
+turnpike
+tory
+reorganization
+##cer
+depict
+biographer
+##walk
+unopposed
+manifesto
+##gles
+institut
+emile
+accidental
+kapoor
+##dam
+kilkenny
+cortex
+lively
+##13
+romanesque
+jain
+shan
+cannons
+##ood
+##ske
+petrol
+echoing
+amalgamated
+disappears
+cautious
+proposes
+sanctions
+trenton
+##ر
+flotilla
+aus
+contempt
+tor
+canary
+cote
+theirs
+##hun
+conceptual
+deleted
+fascinating
+paso
+blazing
+elf
+honourable
+hutchinson
+##eiro
+##outh
+##zin
+surveyor
+tee
+amidst
+wooded
+reissue
+intro
+##ono
+cobb
+shelters
+newsletter
+hanson
+brace
+encoding
+confiscated
+dem
+caravan
+marino
+scroll
+melodic
+cows
+imam
+##adi
+##aneous
+northward
+searches
+biodiversity
+cora
+310
+roaring
+##bers
+connell
+theologian
+halo
+compose
+pathetic
+unmarried
+dynamo
+##oot
+az
+calculation
+toulouse
+deserves
+humour
+nr
+forgiveness
+tam
+undergone
+martyr
+pamela
+myths
+whore
+counselor
+hicks
+290
+heavens
+battleship
+electromagnetic
+##bbs
+stellar
+establishments
+presley
+hopped
+##chin
+temptation
+90s
+wills
+nas
+##yuan
+nhs
+##nya
+seminars
+##yev
+adaptations
+gong
+asher
+lex
+indicator
+sikh
+tobago
+cites
+goin
+##yte
+satirical
+##gies
+characterised
+correspond
+bubbles
+lure
+participates
+##vid
+eruption
+skate
+therapeutic
+1785
+canals
+wholesale
+defaulted
+sac
+460
+petit
+##zzled
+virgil
+leak
+ravens
+256
+portraying
+##yx
+ghetto
+creators
+dams
+portray
+vicente
+##rington
+fae
+namesake
+bounty
+##arium
+joachim
+##ota
+##iser
+aforementioned
+axle
+snout
+depended
+dismantled
+reuben
+480
+##ibly
+gallagher
+##lau
+##pd
+earnest
+##ieu
+##iary
+inflicted
+objections
+##llar
+asa
+gritted
+##athy
+jericho
+##sea
+##was
+flick
+underside
+ceramics
+undead
+substituted
+195
+eastward
+undoubtedly
+wheeled
+chimney
+##iche
+guinness
+cb
+##ager
+siding
+##bell
+traitor
+baptiste
+disguised
+inauguration
+149
+tipperary
+choreographer
+perched
+warmed
+stationary
+eco
+##ike
+##ntes
+bacterial
+##aurus
+flores
+phosphate
+##core
+attacker
+invaders
+alvin
+intersects
+a1
+indirectly
+immigrated
+businessmen
+cornelius
+valves
+narrated
+pill
+sober
+ul
+nationale
+monastic
+applicants
+scenery
+##jack
+161
+motifs
+constitutes
+cpu
+##osh
+jurisdictions
+sd
+tuning
+irritation
+woven
+##uddin
+fertility
+gao
+##erie
+antagonist
+impatient
+glacial
+hides
+boarded
+denominations
+interception
+##jas
+cookie
+nicola
+##tee
+algebraic
+marquess
+bahn
+parole
+buyers
+bait
+turbines
+paperwork
+bestowed
+natasha
+renee
+oceans
+purchases
+157
+vaccine
+215
+##tock
+fixtures
+playhouse
+integrate
+jai
+oswald
+intellectuals
+##cky
+booked
+nests
+mortimer
+##isi
+obsession
+sept
+##gler
+##sum
+440
+scrutiny
+simultaneous
+squinted
+##shin
+collects
+oven
+shankar
+penned
+remarkably
+##я
+slips
+luggage
+spectral
+1786
+collaborations
+louie
+consolidation
+##ailed
+##ivating
+420
+hoover
+blackpool
+harness
+ignition
+vest
+tails
+belmont
+mongol
+skinner
+##nae
+visually
+mage
+derry
+##tism
+##unce
+stevie
+transitional
+##rdy
+redskins
+drying
+prep
+prospective
+##21
+annoyance
+oversee
+##loaded
+fills
+##books
+##iki
+announces
+fda
+scowled
+respects
+prasad
+mystic
+tucson
+##vale
+revue
+springer
+bankrupt
+1772
+aristotle
+salvatore
+habsburg
+##geny
+dal
+natal
+nut
+pod
+chewing
+darts
+moroccan
+walkover
+rosario
+lenin
+punjabi
+##ße
+grossed
+scattering
+wired
+invasive
+hui
+polynomial
+corridors
+wakes
+gina
+portrays
+##cratic
+arid
+retreating
+erich
+irwin
+sniper
+##dha
+linen
+lindsey
+maneuver
+butch
+shutting
+socio
+bounce
+commemorative
+postseason
+jeremiah
+pines
+275
+mystical
+beads
+bp
+abbas
+furnace
+bidding
+consulted
+assaulted
+empirical
+rubble
+enclosure
+sob
+weakly
+cancel
+polly
+yielded
+##emann
+curly
+prediction
+battered
+70s
+vhs
+jacqueline
+render
+sails
+barked
+detailing
+grayson
+riga
+sloane
+raging
+##yah
+herbs
+bravo
+##athlon
+alloy
+giggle
+imminent
+suffers
+assumptions
+waltz
+##itate
+accomplishments
+##ited
+bathing
+remixed
+deception
+prefix
+##emia
+deepest
+##tier
+##eis
+balkan
+frogs
+##rong
+slab
+##pate
+philosophers
+peterborough
+grains
+imports
+dickinson
+rwanda
+##atics
+1774
+dirk
+lan
+tablets
+##rove
+clone
+##rice
+caretaker
+hostilities
+mclean
+##gre
+regimental
+treasures
+norms
+impose
+tsar
+tango
+diplomacy
+variously
+complain
+192
+recognise
+arrests
+1779
+celestial
+pulitzer
+##dus
+bing
+libretto
+##moor
+adele
+splash
+##rite
+expectation
+lds
+confronts
+##izer
+spontaneous
+harmful
+wedge
+entrepreneurs
+buyer
+##ope
+bilingual
+translate
+rugged
+conner
+circulated
+uae
+eaton
+##gra
+##zzle
+lingered
+lockheed
+vishnu
+reelection
+alonso
+##oom
+joints
+yankee
+headline
+cooperate
+heinz
+laureate
+invading
+##sford
+echoes
+scandinavian
+##dham
+hugging
+vitamin
+salute
+micah
+hind
+trader
+##sper
+radioactive
+##ndra
+militants
+poisoned
+ratified
+remark
+campeonato
+deprived
+wander
+prop
+##dong
+outlook
+##tani
+##rix
+##eye
+chiang
+darcy
+##oping
+mandolin
+spice
+statesman
+babylon
+182
+walled
+forgetting
+afro
+##cap
+158
+giorgio
+buffer
+##polis
+planetary
+##gis
+overlap
+terminals
+kinda
+centenary
+##bir
+arising
+manipulate
+elm
+ke
+1770
+ak
+##tad
+chrysler
+mapped
+moose
+pomeranian
+quad
+macarthur
+assemblies
+shoreline
+recalls
+stratford
+##rted
+noticeable
+##evic
+imp
+##rita
+##sque
+accustomed
+supplying
+tents
+disgusted
+vogue
+sipped
+filters
+khz
+reno
+selecting
+luftwaffe
+mcmahon
+tyne
+masterpiece
+carriages
+collided
+dunes
+exercised
+flare
+remembers
+muzzle
+##mobile
+heck
+##rson
+burgess
+lunged
+middleton
+boycott
+bilateral
+##sity
+hazardous
+lumpur
+multiplayer
+spotlight
+jackets
+goldman
+liege
+porcelain
+rag
+waterford
+benz
+attracts
+hopeful
+battling
+ottomans
+kensington
+baked
+hymns
+cheyenne
+lattice
+levine
+borrow
+polymer
+clashes
+michaels
+monitored
+commitments
+denounced
+##25
+##von
+cavity
+##oney
+hobby
+akin
+##holders
+futures
+intricate
+cornish
+patty
+##oned
+illegally
+dolphin
+##lag
+barlow
+yellowish
+maddie
+apologized
+luton
+plagued
+##puram
+nana
+##rds
+sway
+fanny
+łodz
+##rino
+psi
+suspicions
+hanged
+##eding
+initiate
+charlton
+##por
+nak
+competent
+235
+analytical
+annex
+wardrobe
+reservations
+##rma
+sect
+162
+fairfax
+hedge
+piled
+buckingham
+uneven
+bauer
+simplicity
+snyder
+interpret
+accountability
+donors
+moderately
+byrd
+continents
+##cite
+##max
+disciple
+hr
+jamaican
+ping
+nominees
+##uss
+mongolian
+diver
+attackers
+eagerly
+ideological
+pillows
+miracles
+apartheid
+revolver
+sulfur
+clinics
+moran
+163
+##enko
+ile
+katy
+rhetoric
+##icated
+chronology
+recycling
+##hrer
+elongated
+mughal
+pascal
+profiles
+vibration
+databases
+domination
+##fare
+##rant
+matthias
+digest
+rehearsal
+polling
+weiss
+initiation
+reeves
+clinging
+flourished
+impress
+ngo
+##hoff
+##ume
+buckley
+symposium
+rhythms
+weed
+emphasize
+transforming
+##taking
+##gence
+##yman
+accountant
+analyze
+flicker
+foil
+priesthood
+voluntarily
+decreases
+##80
+##hya
+slater
+sv
+charting
+mcgill
+##lde
+moreno
+##iu
+besieged
+zur
+robes
+##phic
+admitting
+api
+deported
+turmoil
+peyton
+earthquakes
+##ares
+nationalists
+beau
+clair
+brethren
+interrupt
+welch
+curated
+galerie
+requesting
+164
+##ested
+impending
+steward
+viper
+##vina
+complaining
+beautifully
+brandy
+foam
+nl
+1660
+##cake
+alessandro
+punches
+laced
+explanations
+##lim
+attribute
+clit
+reggie
+discomfort
+##cards
+smoothed
+whales
+##cene
+adler
+countered
+duffy
+disciplinary
+widening
+recipe
+reliance
+conducts
+goats
+gradient
+preaching
+##shaw
+matilda
+quasi
+striped
+meridian
+cannabis
+cordoba
+certificates
+##agh
+##tering
+graffiti
+hangs
+pilgrims
+repeats
+##ych
+revive
+urine
+etat
+##hawk
+fueled
+belts
+fuzzy
+susceptible
+##hang
+mauritius
+salle
+sincere
+beers
+hooks
+##cki
+arbitration
+entrusted
+advise
+sniffed
+seminar
+junk
+donnell
+processors
+principality
+strapped
+celia
+mendoza
+everton
+fortunes
+prejudice
+starving
+reassigned
+steamer
+##lund
+tuck
+evenly
+foreman
+##ffen
+dans
+375
+envisioned
+slit
+##xy
+baseman
+liberia
+rosemary
+##weed
+electrified
+periodically
+potassium
+stride
+contexts
+sperm
+slade
+mariners
+influx
+bianca
+subcommittee
+##rane
+spilling
+icao
+estuary
+##nock
+delivers
+iphone
+##ulata
+isa
+mira
+bohemian
+dessert
+##sbury
+welcoming
+proudly
+slowing
+##chs
+musee
+ascension
+russ
+##vian
+waits
+##psy
+africans
+exploit
+##morphic
+gov
+eccentric
+crab
+peck
+##ull
+entrances
+formidable
+marketplace
+groom
+bolted
+metabolism
+patton
+robbins
+courier
+payload
+endure
+##ifier
+andes
+refrigerator
+##pr
+ornate
+##uca
+ruthless
+illegitimate
+masonry
+strasbourg
+bikes
+adobe
+##³
+apples
+quintet
+willingly
+niche
+bakery
+corpses
+energetic
+##cliffe
+##sser
+##ards
+177
+centimeters
+centro
+fuscous
+cretaceous
+rancho
+##yde
+andrei
+telecom
+tottenham
+oasis
+ordination
+vulnerability
+presiding
+corey
+cp
+penguins
+sims
+##pis
+malawi
+piss
+##48
+correction
+##cked
+##ffle
+##ryn
+countdown
+detectives
+psychiatrist
+psychedelic
+dinosaurs
+blouse
+##get
+choi
+vowed
+##oz
+randomly
+##pol
+49ers
+scrub
+blanche
+bruins
+dusseldorf
+##using
+unwanted
+##ums
+212
+dominique
+elevations
+headlights
+om
+laguna
+##oga
+1750
+famously
+ignorance
+shrewsbury
+##aine
+ajax
+breuning
+che
+confederacy
+greco
+overhaul
+##screen
+paz
+skirts
+disagreement
+cruelty
+jagged
+phoebe
+shifter
+hovered
+viruses
+##wes
+mandy
+##lined
+##gc
+landlord
+squirrel
+dashed
+##ι
+ornamental
+gag
+wally
+grange
+literal
+spurs
+undisclosed
+proceeding
+yin
+##text
+billie
+orphan
+spanned
+humidity
+indy
+weighted
+presentations
+explosions
+lucian
+##tary
+vaughn
+hindus
+##anga
+##hell
+psycho
+171
+daytona
+protects
+efficiently
+rematch
+sly
+tandem
+##oya
+rebranded
+impaired
+hee
+metropolis
+peach
+godfrey
+diaspora
+ethnicity
+prosperous
+gleaming
+dar
+grossing
+playback
+##rden
+stripe
+pistols
+##tain
+births
+labelled
+##cating
+172
+rudy
+alba
+##onne
+aquarium
+hostility
+##gb
+##tase
+shudder
+sumatra
+hardest
+lakers
+consonant
+creeping
+demos
+homicide
+capsule
+zeke
+liberties
+expulsion
+pueblo
+##comb
+trait
+transporting
+##ddin
+##neck
+##yna
+depart
+gregg
+mold
+ledge
+hangar
+oldham
+playboy
+termination
+analysts
+gmbh
+romero
+##itic
+insist
+cradle
+filthy
+brightness
+slash
+shootout
+deposed
+bordering
+##truct
+isis
+microwave
+tumbled
+sheltered
+cathy
+werewolves
+messy
+andersen
+convex
+clapped
+clinched
+satire
+wasting
+edo
+vc
+rufus
+##jak
+mont
+##etti
+poznan
+##keeping
+restructuring
+transverse
+##rland
+azerbaijani
+slovene
+gestures
+roommate
+choking
+shear
+##quist
+vanguard
+oblivious
+##hiro
+disagreed
+baptism
+##lich
+coliseum
+##aceae
+salvage
+societe
+cory
+locke
+relocation
+relying
+versailles
+ahl
+swelling
+##elo
+cheerful
+##word
+##edes
+gin
+sarajevo
+obstacle
+diverted
+##nac
+messed
+thoroughbred
+fluttered
+utrecht
+chewed
+acquaintance
+assassins
+dispatch
+mirza
+##wart
+nike
+salzburg
+swell
+yen
+##gee
+idle
+ligue
+samson
+##nds
+##igh
+playful
+spawned
+##cise
+tease
+##case
+burgundy
+##bot
+stirring
+skeptical
+interceptions
+marathi
+##dies
+bedrooms
+aroused
+pinch
+##lik
+preferences
+tattoos
+buster
+digitally
+projecting
+rust
+##ital
+kitten
+priorities
+addison
+pseudo
+##guard
+dusk
+icons
+sermon
+##psis
+##iba
+bt
+##lift
+##xt
+ju
+truce
+rink
+##dah
+##wy
+defects
+psychiatry
+offences
+calculate
+glucose
+##iful
+##rized
+##unda
+francaise
+##hari
+richest
+warwickshire
+carly
+1763
+purity
+redemption
+lending
+##cious
+muse
+bruises
+cerebral
+aero
+carving
+##name
+preface
+terminology
+invade
+monty
+##int
+anarchist
+blurred
+##iled
+rossi
+treats
+guts
+shu
+foothills
+ballads
+undertaking
+premise
+cecilia
+affiliates
+blasted
+conditional
+wilder
+minors
+drone
+rudolph
+buffy
+swallowing
+horton
+attested
+##hop
+rutherford
+howell
+primetime
+livery
+penal
+##bis
+minimize
+hydro
+wrecked
+wrought
+palazzo
+##gling
+cans
+vernacular
+friedman
+nobleman
+shale
+walnut
+danielle
+##ection
+##tley
+sears
+##kumar
+chords
+lend
+flipping
+streamed
+por
+dracula
+gallons
+sacrifices
+gamble
+orphanage
+##iman
+mckenzie
+##gible
+boxers
+daly
+##balls
+##ان
+208
+##ific
+##rative
+##iq
+exploited
+slated
+##uity
+circling
+hillary
+pinched
+goldberg
+provost
+campaigning
+lim
+piles
+ironically
+jong
+mohan
+successors
+usaf
+##tem
+##ught
+autobiographical
+haute
+preserves
+##ending
+acquitted
+comparisons
+203
+hydroelectric
+gangs
+cypriot
+torpedoes
+rushes
+chrome
+derive
+bumps
+instability
+fiat
+pets
+##mbe
+silas
+dye
+reckless
+settler
+##itation
+info
+heats
+##writing
+176
+canonical
+maltese
+fins
+mushroom
+stacy
+aspen
+avid
+##kur
+##loading
+vickers
+gaston
+hillside
+statutes
+wilde
+gail
+kung
+sabine
+comfortably
+motorcycles
+##rgo
+169
+pneumonia
+fetch
+##sonic
+axel
+faintly
+parallels
+##oop
+mclaren
+spouse
+compton
+interdisciplinary
+miner
+##eni
+181
+clamped
+##chal
+##llah
+separates
+versa
+##mler
+scarborough
+labrador
+##lity
+##osing
+rutgers
+hurdles
+como
+166
+burt
+divers
+##100
+wichita
+cade
+coincided
+##erson
+bruised
+mla
+##pper
+vineyard
+##ili
+##brush
+notch
+mentioning
+jase
+hearted
+kits
+doe
+##acle
+pomerania
+##ady
+ronan
+seizure
+pavel
+problematic
+##zaki
+domenico
+##ulin
+catering
+penelope
+dependence
+parental
+emilio
+ministerial
+atkinson
+##bolic
+clarkson
+chargers
+colby
+grill
+peeked
+arises
+summon
+##aged
+fools
+##grapher
+faculties
+qaeda
+##vial
+garner
+refurbished
+##hwa
+geelong
+disasters
+nudged
+bs
+shareholder
+lori
+algae
+reinstated
+rot
+##ades
+##nous
+invites
+stainless
+183
+inclusive
+##itude
+diocesan
+til
+##icz
+denomination
+##xa
+benton
+floral
+registers
+##ider
+##erman
+##kell
+absurd
+brunei
+guangzhou
+hitter
+retaliation
+##uled
+##eve
+blanc
+nh
+consistency
+contamination
+##eres
+##rner
+dire
+palermo
+broadcasters
+diaries
+inspire
+vols
+brewer
+tightening
+ky
+mixtape
+hormone
+##tok
+stokes
+##color
+##dly
+##ssi
+pg
+##ometer
+##lington
+sanitation
+##tility
+intercontinental
+apps
+##adt
+¹⁄₂
+cylinders
+economies
+favourable
+unison
+croix
+gertrude
+odyssey
+vanity
+dangling
+##logists
+upgrades
+dice
+middleweight
+practitioner
+##ight
+206
+henrik
+parlor
+orion
+angered
+lac
+python
+blurted
+##rri
+sensual
+intends
+swings
+angled
+##phs
+husky
+attain
+peerage
+precinct
+textiles
+cheltenham
+shuffled
+dai
+confess
+tasting
+bhutan
+##riation
+tyrone
+segregation
+abrupt
+ruiz
+##rish
+smirked
+blackwell
+confidential
+browning
+amounted
+##put
+vase
+scarce
+fabulous
+raided
+staple
+guyana
+unemployed
+glider
+shay
+##tow
+carmine
+troll
+intervene
+squash
+superstar
+##uce
+cylindrical
+len
+roadway
+researched
+handy
+##rium
+##jana
+meta
+lao
+declares
+##rring
+##tadt
+##elin
+##kova
+willem
+shrubs
+napoleonic
+realms
+skater
+qi
+volkswagen
+##ł
+tad
+hara
+archaeologist
+awkwardly
+eerie
+##kind
+wiley
+##heimer
+##24
+titus
+organizers
+cfl
+crusaders
+lama
+usb
+vent
+enraged
+thankful
+occupants
+maximilian
+##gaard
+possessing
+textbooks
+##oran
+collaborator
+quaker
+##ulo
+avalanche
+mono
+silky
+straits
+isaiah
+mustang
+surged
+resolutions
+potomac
+descend
+cl
+kilograms
+plato
+strains
+saturdays
+##olin
+bernstein
+##ype
+holstein
+ponytail
+##watch
+belize
+conversely
+heroine
+perpetual
+##ylus
+charcoal
+piedmont
+glee
+negotiating
+backdrop
+prologue
+##jah
+##mmy
+pasadena
+climbs
+ramos
+sunni
+##holm
+##tner
+##tri
+anand
+deficiency
+hertfordshire
+stout
+##avi
+aperture
+orioles
+##irs
+doncaster
+intrigued
+bombed
+coating
+otis
+##mat
+cocktail
+##jit
+##eto
+amir
+arousal
+sar
+##proof
+##act
+##ories
+dixie
+pots
+##bow
+whereabouts
+159
+##fted
+drains
+bullying
+cottages
+scripture
+coherent
+fore
+poe
+appetite
+##uration
+sampled
+##ators
+##dp
+derrick
+rotor
+jays
+peacock
+installment
+##rro
+advisors
+##coming
+rodeo
+scotch
+##mot
+##db
+##fen
+##vant
+ensued
+rodrigo
+dictatorship
+martyrs
+twenties
+##н
+towed
+incidence
+marta
+rainforest
+sai
+scaled
+##cles
+oceanic
+qualifiers
+symphonic
+mcbride
+dislike
+generalized
+aubrey
+colonization
+##iation
+##lion
+##ssing
+disliked
+lublin
+salesman
+##ulates
+spherical
+whatsoever
+sweating
+avalon
+contention
+punt
+severity
+alderman
+atari
+##dina
+##grant
+##rop
+scarf
+seville
+vertices
+annexation
+fairfield
+fascination
+inspiring
+launches
+palatinate
+regretted
+##rca
+feral
+##iom
+elk
+nap
+olsen
+reddy
+yong
+##leader
+##iae
+garment
+transports
+feng
+gracie
+outrage
+viceroy
+insides
+##esis
+breakup
+grady
+organizer
+softer
+grimaced
+222
+murals
+galicia
+arranging
+vectors
+##rsten
+bas
+##sb
+##cens
+sloan
+##eka
+bitten
+ara
+fender
+nausea
+bumped
+kris
+banquet
+comrades
+detector
+persisted
+##llan
+adjustment
+endowed
+cinemas
+##shot
+sellers
+##uman
+peek
+epa
+kindly
+neglect
+simpsons
+talon
+mausoleum
+runaway
+hangul
+lookout
+##cic
+rewards
+coughed
+acquainted
+chloride
+##ald
+quicker
+accordion
+neolithic
+##qa
+artemis
+coefficient
+lenny
+pandora
+tx
+##xed
+ecstasy
+litter
+segunda
+chairperson
+gemma
+hiss
+rumor
+vow
+nasal
+antioch
+compensate
+patiently
+transformers
+##eded
+judo
+morrow
+penis
+posthumous
+philips
+bandits
+husbands
+denote
+flaming
+##any
+##phones
+langley
+yorker
+1760
+walters
+##uo
+##kle
+gubernatorial
+fatty
+samsung
+leroy
+outlaw
+##nine
+unpublished
+poole
+jakob
+##ᵢ
+##ₙ
+crete
+distorted
+superiority
+##dhi
+intercept
+crust
+mig
+claus
+crashes
+positioning
+188
+stallion
+301
+frontal
+armistice
+##estinal
+elton
+aj
+encompassing
+camel
+commemorated
+malaria
+woodward
+calf
+cigar
+penetrate
+##oso
+willard
+##rno
+##uche
+illustrate
+amusing
+convergence
+noteworthy
+##lma
+##rva
+journeys
+realise
+manfred
+##sable
+410
+##vocation
+hearings
+fiance
+##posed
+educators
+provoked
+adjusting
+##cturing
+modular
+stockton
+paterson
+vlad
+rejects
+electors
+selena
+maureen
+##tres
+uber
+##rce
+swirled
+##num
+proportions
+nanny
+pawn
+naturalist
+parma
+apostles
+awoke
+ethel
+wen
+##bey
+monsoon
+overview
+##inating
+mccain
+rendition
+risky
+adorned
+##ih
+equestrian
+germain
+nj
+conspicuous
+confirming
+##yoshi
+shivering
+##imeter
+milestone
+rumours
+flinched
+bounds
+smacked
+token
+##bei
+lectured
+automobiles
+##shore
+impacted
+##iable
+nouns
+nero
+##leaf
+ismail
+prostitute
+trams
+##lace
+bridget
+sud
+stimulus
+impressions
+reins
+revolves
+##oud
+##gned
+giro
+honeymoon
+##swell
+criterion
+##sms
+##uil
+libyan
+prefers
+##osition
+211
+preview
+sucks
+accusation
+bursts
+metaphor
+diffusion
+tolerate
+faye
+betting
+cinematographer
+liturgical
+specials
+bitterly
+humboldt
+##ckle
+flux
+rattled
+##itzer
+archaeologists
+odor
+authorised
+marshes
+discretion
+##ов
+alarmed
+archaic
+inverse
+##leton
+explorers
+##pine
+drummond
+tsunami
+woodlands
+##minate
+##tland
+booklet
+insanity
+owning
+insert
+crafted
+calculus
+##tore
+receivers
+##bt
+stung
+##eca
+##nched
+prevailing
+travellers
+eyeing
+lila
+graphs
+##borne
+178
+julien
+##won
+morale
+adaptive
+therapist
+erica
+cw
+libertarian
+bowman
+pitches
+vita
+##ional
+crook
+##ads
+##entation
+caledonia
+mutiny
+##sible
+1840s
+automation
+##ß
+flock
+##pia
+ironic
+pathology
+##imus
+remarried
+##22
+joker
+withstand
+energies
+##att
+shropshire
+hostages
+madeleine
+tentatively
+conflicting
+mateo
+recipes
+euros
+ol
+mercenaries
+nico
+##ndon
+albuquerque
+augmented
+mythical
+bel
+freud
+##child
+cough
+##lica
+365
+freddy
+lillian
+genetically
+nuremberg
+calder
+209
+bonn
+outdoors
+paste
+suns
+urgency
+vin
+restraint
+tyson
+##cera
+##selle
+barrage
+bethlehem
+kahn
+##par
+mounts
+nippon
+barony
+happier
+ryu
+makeshift
+sheldon
+blushed
+castillo
+barking
+listener
+taped
+bethel
+fluent
+headlines
+pornography
+rum
+disclosure
+sighing
+mace
+doubling
+gunther
+manly
+##plex
+rt
+interventions
+physiological
+forwards
+emerges
+##tooth
+##gny
+compliment
+rib
+recession
+visibly
+barge
+faults
+connector
+exquisite
+prefect
+##rlin
+patio
+##cured
+elevators
+brandt
+italics
+pena
+173
+wasp
+satin
+ea
+botswana
+graceful
+respectable
+##jima
+##rter
+##oic
+franciscan
+generates
+##dl
+alfredo
+disgusting
+##olate
+##iously
+sherwood
+warns
+cod
+promo
+cheryl
+sino
+##ة
+##escu
+twitch
+##zhi
+brownish
+thom
+ortiz
+##dron
+densely
+##beat
+carmel
+reinforce
+##bana
+187
+anastasia
+downhill
+vertex
+contaminated
+remembrance
+harmonic
+homework
+##sol
+fiancee
+gears
+olds
+angelica
+loft
+ramsay
+quiz
+colliery
+sevens
+##cape
+autism
+##hil
+walkway
+##boats
+ruben
+abnormal
+ounce
+khmer
+##bbe
+zachary
+bedside
+morphology
+punching
+##olar
+sparrow
+convinces
+##35
+hewitt
+queer
+remastered
+rods
+mabel
+solemn
+notified
+lyricist
+symmetric
+##xide
+174
+encore
+passports
+wildcats
+##uni
+baja
+##pac
+mildly
+##ease
+bleed
+commodity
+mounds
+glossy
+orchestras
+##omo
+damian
+prelude
+ambitions
+##vet
+awhile
+remotely
+##aud
+asserts
+imply
+##iques
+distinctly
+modelling
+remedy
+##dded
+windshield
+dani
+xiao
+##endra
+audible
+powerplant
+1300
+invalid
+elemental
+acquisitions
+##hala
+immaculate
+libby
+plata
+smuggling
+ventilation
+denoted
+minh
+##morphism
+430
+differed
+dion
+kelley
+lore
+mocking
+sabbath
+spikes
+hygiene
+drown
+runoff
+stylized
+tally
+liberated
+aux
+interpreter
+righteous
+aba
+siren
+reaper
+pearce
+millie
+##cier
+##yra
+gaius
+##iso
+captures
+##ttering
+dorm
+claudio
+##sic
+benches
+knighted
+blackness
+##ored
+discount
+fumble
+oxidation
+routed
+##ς
+novak
+perpendicular
+spoiled
+fracture
+splits
+##urt
+pads
+topology
+##cats
+axes
+fortunate
+offenders
+protestants
+esteem
+221
+broadband
+convened
+frankly
+hound
+prototypes
+isil
+facilitated
+keel
+##sher
+sahara
+awaited
+bubba
+orb
+prosecutors
+186
+hem
+520
+##xing
+relaxing
+remnant
+romney
+sorted
+slalom
+stefano
+ulrich
+##active
+exemption
+folder
+pauses
+foliage
+hitchcock
+epithet
+204
+criticisms
+##aca
+ballistic
+brody
+hinduism
+chaotic
+youths
+equals
+##pala
+pts
+thicker
+analogous
+capitalist
+improvised
+overseeing
+sinatra
+ascended
+beverage
+##tl
+straightforward
+##kon
+curran
+##west
+bois
+325
+induce
+surveying
+emperors
+sax
+unpopular
+##kk
+cartoonist
+fused
+##mble
+unto
+##yuki
+localities
+##cko
+##ln
+darlington
+slain
+academie
+lobbying
+sediment
+puzzles
+##grass
+defiance
+dickens
+manifest
+tongues
+alumnus
+arbor
+coincide
+184
+appalachian
+mustafa
+examiner
+cabaret
+traumatic
+yves
+bracelet
+draining
+heroin
+magnum
+baths
+odessa
+consonants
+mitsubishi
+##gua
+kellan
+vaudeville
+##fr
+joked
+null
+straps
+probation
+##ław
+ceded
+interfaces
+##pas
+##zawa
+blinding
+viet
+224
+rothschild
+museo
+640
+huddersfield
+##vr
+tactic
+##storm
+brackets
+dazed
+incorrectly
+##vu
+reg
+glazed
+fearful
+manifold
+benefited
+irony
+##sun
+stumbling
+##rte
+willingness
+balkans
+mei
+wraps
+##aba
+injected
+##lea
+gu
+syed
+harmless
+##hammer
+bray
+takeoff
+poppy
+timor
+cardboard
+astronaut
+purdue
+weeping
+southbound
+cursing
+stalls
+diagonal
+##neer
+lamar
+bryce
+comte
+weekdays
+harrington
+##uba
+negatively
+##see
+lays
+grouping
+##cken
+##henko
+affirmed
+halle
+modernist
+##lai
+hodges
+smelling
+aristocratic
+baptized
+dismiss
+justification
+oilers
+##now
+coupling
+qin
+snack
+healer
+##qing
+gardener
+layla
+battled
+formulated
+stephenson
+gravitational
+##gill
+##jun
+1768
+granny
+coordinating
+suites
+##cd
+##ioned
+monarchs
+##cote
+##hips
+sep
+blended
+apr
+barrister
+deposition
+fia
+mina
+policemen
+paranoid
+##pressed
+churchyard
+covert
+crumpled
+creep
+abandoning
+tr
+transmit
+conceal
+barr
+understands
+readiness
+spire
+##cology
+##enia
+##erry
+610
+startling
+unlock
+vida
+bowled
+slots
+##nat
+##islav
+spaced
+trusting
+admire
+rig
+##ink
+slack
+##70
+mv
+207
+casualty
+##wei
+classmates
+##odes
+##rar
+##rked
+amherst
+furnished
+evolve
+foundry
+menace
+mead
+##lein
+flu
+wesleyan
+##kled
+monterey
+webber
+##vos
+wil
+##mith
+##на
+bartholomew
+justices
+restrained
+##cke
+amenities
+191
+mediated
+sewage
+trenches
+ml
+mainz
+##thus
+1800s
+##cula
+##inski
+caine
+bonding
+213
+converts
+spheres
+superseded
+marianne
+crypt
+sweaty
+ensign
+historia
+##br
+spruce
+##post
+##ask
+forks
+thoughtfully
+yukon
+pamphlet
+ames
+##uter
+karma
+##yya
+bryn
+negotiation
+sighs
+incapable
+##mbre
+##ntial
+actresses
+taft
+##mill
+luce
+prevailed
+##amine
+1773
+motionless
+envoy
+testify
+investing
+sculpted
+instructors
+provence
+kali
+cullen
+horseback
+##while
+goodwin
+##jos
+gaa
+norte
+##ldon
+modify
+wavelength
+abd
+214
+skinned
+sprinter
+forecast
+scheduling
+marries
+squared
+tentative
+##chman
+boer
+##isch
+bolts
+swap
+fisherman
+assyrian
+impatiently
+guthrie
+martins
+murdoch
+194
+tanya
+nicely
+dolly
+lacy
+med
+##45
+syn
+decks
+fashionable
+millionaire
+##ust
+surfing
+##ml
+##ision
+heaved
+tammy
+consulate
+attendees
+routinely
+197
+fuse
+saxophonist
+backseat
+malaya
+##lord
+scowl
+tau
+##ishly
+193
+sighted
+steaming
+##rks
+303
+911
+##holes
+##hong
+ching
+##wife
+bless
+conserved
+jurassic
+stacey
+unix
+zion
+chunk
+rigorous
+blaine
+198
+peabody
+slayer
+dismay
+brewers
+nz
+##jer
+det
+##glia
+glover
+postwar
+int
+penetration
+sylvester
+imitation
+vertically
+airlift
+heiress
+knoxville
+viva
+##uin
+390
+macon
+##rim
+##fighter
+##gonal
+janice
+##orescence
+##wari
+marius
+belongings
+leicestershire
+196
+blanco
+inverted
+preseason
+sanity
+sobbing
+##due
+##elt
+##dled
+collingwood
+regeneration
+flickering
+shortest
+##mount
+##osi
+feminism
+##lat
+sherlock
+cabinets
+fumbled
+northbound
+precedent
+snaps
+##mme
+researching
+##akes
+guillaume
+insights
+manipulated
+vapor
+neighbour
+sap
+gangster
+frey
+f1
+stalking
+scarcely
+callie
+barnett
+tendencies
+audi
+doomed
+assessing
+slung
+panchayat
+ambiguous
+bartlett
+##etto
+distributing
+violating
+wolverhampton
+##hetic
+swami
+histoire
+##urus
+liable
+pounder
+groin
+hussain
+larsen
+popping
+surprises
+##atter
+vie
+curt
+##station
+mute
+relocate
+musicals
+authorization
+richter
+##sef
+immortality
+tna
+bombings
+##press
+deteriorated
+yiddish
+##acious
+robbed
+colchester
+cs
+pmid
+ao
+verified
+balancing
+apostle
+swayed
+recognizable
+oxfordshire
+retention
+nottinghamshire
+contender
+judd
+invitational
+shrimp
+uhf
+##icient
+cleaner
+longitudinal
+tanker
+##mur
+acronym
+broker
+koppen
+sundance
+suppliers
+##gil
+4000
+clipped
+fuels
+petite
+##anne
+landslide
+helene
+diversion
+populous
+landowners
+auspices
+melville
+quantitative
+##xes
+ferries
+nicky
+##llus
+doo
+haunting
+roche
+carver
+downed
+unavailable
+##pathy
+approximation
+hiroshima
+##hue
+garfield
+valle
+comparatively
+keyboardist
+traveler
+##eit
+congestion
+calculating
+subsidiaries
+##bate
+serb
+modernization
+fairies
+deepened
+ville
+averages
+##lore
+inflammatory
+tonga
+##itch
+co₂
+squads
+##hea
+gigantic
+serum
+enjoyment
+retailer
+verona
+35th
+cis
+##phobic
+magna
+technicians
+##vati
+arithmetic
+##sport
+levin
+##dation
+amtrak
+chow
+sienna
+##eyer
+backstage
+entrepreneurship
+##otic
+learnt
+tao
+##udy
+worcestershire
+formulation
+baggage
+hesitant
+bali
+sabotage
+##kari
+barren
+enhancing
+murmur
+pl
+freshly
+putnam
+syntax
+aces
+medicines
+resentment
+bandwidth
+##sier
+grins
+chili
+guido
+##sei
+framing
+implying
+gareth
+lissa
+genevieve
+pertaining
+admissions
+geo
+thorpe
+proliferation
+sato
+bela
+analyzing
+parting
+##gor
+awakened
+##isman
+huddled
+secrecy
+##kling
+hush
+gentry
+540
+dungeons
+##ego
+coasts
+##utz
+sacrificed
+##chule
+landowner
+mutually
+prevalence
+programmer
+adolescent
+disrupted
+seaside
+gee
+trusts
+vamp
+georgie
+##nesian
+##iol
+schedules
+sindh
+##market
+etched
+hm
+sparse
+bey
+beaux
+scratching
+gliding
+unidentified
+216
+collaborating
+gems
+jesuits
+oro
+accumulation
+shaping
+mbe
+anal
+##xin
+231
+enthusiasts
+newscast
+##egan
+janata
+dewey
+parkinson
+179
+ankara
+biennial
+towering
+dd
+inconsistent
+950
+##chet
+thriving
+terminate
+cabins
+furiously
+eats
+advocating
+donkey
+marley
+muster
+phyllis
+leiden
+##user
+grassland
+glittering
+iucn
+loneliness
+217
+memorandum
+armenians
+##ddle
+popularized
+rhodesia
+60s
+lame
+##illon
+sans
+bikini
+header
+orbits
+##xx
+##finger
+##ulator
+sharif
+spines
+biotechnology
+strolled
+naughty
+yates
+##wire
+fremantle
+milo
+##mour
+abducted
+removes
+##atin
+humming
+wonderland
+##chrome
+##ester
+hume
+pivotal
+##rates
+armand
+grams
+believers
+elector
+rte
+apron
+bis
+scraped
+##yria
+endorsement
+initials
+##llation
+eps
+dotted
+hints
+buzzing
+emigration
+nearer
+##tom
+indicators
+##ulu
+coarse
+neutron
+protectorate
+##uze
+directional
+exploits
+pains
+loire
+1830s
+proponents
+guggenheim
+rabbits
+ritchie
+305
+hectare
+inputs
+hutton
+##raz
+verify
+##ako
+boilers
+longitude
+##lev
+skeletal
+yer
+emilia
+citrus
+compromised
+##gau
+pokemon
+prescription
+paragraph
+eduard
+cadillac
+attire
+categorized
+kenyan
+weddings
+charley
+##bourg
+entertain
+monmouth
+##lles
+nutrients
+davey
+mesh
+incentive
+practised
+ecosystems
+kemp
+subdued
+overheard
+##rya
+bodily
+maxim
+##nius
+apprenticeship
+ursula
+##fight
+lodged
+rug
+silesian
+unconstitutional
+patel
+inspected
+coyote
+unbeaten
+##hak
+34th
+disruption
+convict
+parcel
+##cl
+##nham
+collier
+implicated
+mallory
+##iac
+##lab
+susannah
+winkler
+##rber
+shia
+phelps
+sediments
+graphical
+robotic
+##sner
+adulthood
+mart
+smoked
+##isto
+kathryn
+clarified
+##aran
+divides
+convictions
+oppression
+pausing
+burying
+##mt
+federico
+mathias
+eileen
+##tana
+kite
+hunched
+##acies
+189
+##atz
+disadvantage
+liza
+kinetic
+greedy
+paradox
+yokohama
+dowager
+trunks
+ventured
+##gement
+gupta
+vilnius
+olaf
+##thest
+crimean
+hopper
+##ej
+progressively
+arturo
+mouthed
+arrondissement
+##fusion
+rubin
+simulcast
+oceania
+##orum
+##stra
+##rred
+busiest
+intensely
+navigator
+cary
+##vine
+##hini
+##bies
+fife
+rowe
+rowland
+posing
+insurgents
+shafts
+lawsuits
+activate
+conor
+inward
+culturally
+garlic
+265
+##eering
+eclectic
+##hui
+##kee
+##nl
+furrowed
+vargas
+meteorological
+rendezvous
+##aus
+culinary
+commencement
+##dition
+quota
+##notes
+mommy
+salaries
+overlapping
+mule
+##iology
+##mology
+sums
+wentworth
+##isk
+##zione
+mainline
+subgroup
+##illy
+hack
+plaintiff
+verdi
+bulb
+differentiation
+engagements
+multinational
+supplemented
+bertrand
+caller
+regis
+##naire
+##sler
+##arts
+##imated
+blossom
+propagation
+kilometer
+viaduct
+vineyards
+##uate
+beckett
+optimization
+golfer
+songwriters
+seminal
+semitic
+thud
+volatile
+evolving
+ridley
+##wley
+trivial
+distributions
+scandinavia
+jiang
+##ject
+wrestled
+insistence
+##dio
+emphasizes
+napkin
+##ods
+adjunct
+rhyme
+##ricted
+##eti
+hopeless
+surrounds
+tremble
+32nd
+smoky
+##ntly
+oils
+medicinal
+padded
+steer
+wilkes
+219
+255
+concessions
+hue
+uniquely
+blinded
+landon
+yahoo
+##lane
+hendrix
+commemorating
+dex
+specify
+chicks
+##ggio
+intercity
+1400
+morley
+##torm
+highlighting
+##oting
+pang
+oblique
+stalled
+##liner
+flirting
+newborn
+1769
+bishopric
+shaved
+232
+currie
+##ush
+dharma
+spartan
+##ooped
+favorites
+smug
+novella
+sirens
+abusive
+creations
+espana
+##lage
+paradigm
+semiconductor
+sheen
+##rdo
+##yen
+##zak
+nrl
+renew
+##pose
+##tur
+adjutant
+marches
+norma
+##enity
+ineffective
+weimar
+grunt
+##gat
+lordship
+plotting
+expenditure
+infringement
+lbs
+refrain
+av
+mimi
+mistakenly
+postmaster
+1771
+##bara
+ras
+motorsports
+tito
+199
+subjective
+##zza
+bully
+stew
+##kaya
+prescott
+1a
+##raphic
+##zam
+bids
+styling
+paranormal
+reeve
+sneaking
+exploding
+katz
+akbar
+migrant
+syllables
+indefinitely
+##ogical
+destroys
+replaces
+applause
+##phine
+pest
+##fide
+218
+articulated
+bertie
+##thing
+##cars
+##ptic
+courtroom
+crowley
+aesthetics
+cummings
+tehsil
+hormones
+titanic
+dangerously
+##ibe
+stadion
+jaenelle
+auguste
+ciudad
+##chu
+mysore
+partisans
+##sio
+lucan
+philipp
+##aly
+debating
+henley
+interiors
+##rano
+##tious
+homecoming
+beyonce
+usher
+henrietta
+prepares
+weeds
+##oman
+ely
+plucked
+##pire
+##dable
+luxurious
+##aq
+artifact
+password
+pasture
+juno
+maddy
+minsk
+##dder
+##ologies
+##rone
+assessments
+martian
+royalist
+1765
+examines
+##mani
+##rge
+nino
+223
+parry
+scooped
+relativity
+##eli
+##uting
+##cao
+congregational
+noisy
+traverse
+##agawa
+strikeouts
+nickelodeon
+obituary
+transylvania
+binds
+depictions
+polk
+trolley
+##yed
+##lard
+breeders
+##under
+dryly
+hokkaido
+1762
+strengths
+stacks
+bonaparte
+connectivity
+neared
+prostitutes
+stamped
+anaheim
+gutierrez
+sinai
+##zzling
+bram
+fresno
+madhya
+##86
+proton
+##lena
+##llum
+##phon
+reelected
+wanda
+##anus
+##lb
+ample
+distinguishing
+##yler
+grasping
+sermons
+tomato
+bland
+stimulation
+avenues
+##eux
+spreads
+scarlett
+fern
+pentagon
+assert
+baird
+chesapeake
+ir
+calmed
+distortion
+fatalities
+##olis
+correctional
+pricing
+##astic
+##gina
+prom
+dammit
+ying
+collaborate
+##chia
+welterweight
+33rd
+pointer
+substitution
+bonded
+umpire
+communicating
+multitude
+paddle
+##obe
+federally
+intimacy
+##insky
+betray
+ssr
+##lett
+##lean
+##lves
+##therapy
+airbus
+##tery
+functioned
+ud
+bearer
+biomedical
+netflix
+##hire
+##nca
+condom
+brink
+ik
+##nical
+macy
+##bet
+flap
+gma
+experimented
+jelly
+lavender
+##icles
+##ulia
+munro
+##mian
+##tial
+rye
+##rle
+60th
+gigs
+hottest
+rotated
+predictions
+fuji
+bu
+##erence
+##omi
+barangay
+##fulness
+##sas
+clocks
+##rwood
+##liness
+cereal
+roe
+wight
+decker
+uttered
+babu
+onion
+xml
+forcibly
+##df
+petra
+sarcasm
+hartley
+peeled
+storytelling
+##42
+##xley
+##ysis
+##ffa
+fibre
+kiel
+auditor
+fig
+harald
+greenville
+##berries
+geographically
+nell
+quartz
+##athic
+cemeteries
+##lr
+crossings
+nah
+holloway
+reptiles
+chun
+sichuan
+snowy
+660
+corrections
+##ivo
+zheng
+ambassadors
+blacksmith
+fielded
+fluids
+hardcover
+turnover
+medications
+melvin
+academies
+##erton
+ro
+roach
+absorbing
+spaniards
+colton
+##founded
+outsider
+espionage
+kelsey
+245
+edible
+##ulf
+dora
+establishes
+##sham
+##tries
+contracting
+##tania
+cinematic
+costello
+nesting
+##uron
+connolly
+duff
+##nology
+mma
+##mata
+fergus
+sexes
+gi
+optics
+spectator
+woodstock
+banning
+##hee
+##fle
+differentiate
+outfielder
+refinery
+226
+312
+gerhard
+horde
+lair
+drastically
+##udi
+landfall
+##cheng
+motorsport
+odi
+##achi
+predominant
+quay
+skins
+##ental
+edna
+harshly
+complementary
+murdering
+##aves
+wreckage
+##90
+ono
+outstretched
+lennox
+munitions
+galen
+reconcile
+470
+scalp
+bicycles
+gillespie
+questionable
+rosenberg
+guillermo
+hostel
+jarvis
+kabul
+volvo
+opium
+yd
+##twined
+abuses
+decca
+outpost
+##cino
+sensible
+neutrality
+##64
+ponce
+anchorage
+atkins
+turrets
+inadvertently
+disagree
+libre
+vodka
+reassuring
+weighs
+##yal
+glide
+jumper
+ceilings
+repertory
+outs
+stain
+##bial
+envy
+##ucible
+smashing
+heightened
+policing
+hyun
+mixes
+lai
+prima
+##ples
+celeste
+##bina
+lucrative
+intervened
+kc
+manually
+##rned
+stature
+staffed
+bun
+bastards
+nairobi
+priced
+##auer
+thatcher
+##kia
+tripped
+comune
+##ogan
+##pled
+brasil
+incentives
+emanuel
+hereford
+musica
+##kim
+benedictine
+biennale
+##lani
+eureka
+gardiner
+rb
+knocks
+sha
+##ael
+##elled
+##onate
+efficacy
+ventura
+masonic
+sanford
+maize
+leverage
+##feit
+capacities
+santana
+##aur
+novelty
+vanilla
+##cter
+##tour
+benin
+##oir
+##rain
+neptune
+drafting
+tallinn
+##cable
+humiliation
+##boarding
+schleswig
+fabian
+bernardo
+liturgy
+spectacle
+sweeney
+pont
+routledge
+##tment
+cosmos
+ut
+hilt
+sleek
+universally
+##eville
+##gawa
+typed
+##dry
+favors
+allegheny
+glaciers
+##rly
+recalling
+aziz
+##log
+parasite
+requiem
+auf
+##berto
+##llin
+illumination
+##breaker
+##issa
+festivities
+bows
+govern
+vibe
+vp
+333
+sprawled
+larson
+pilgrim
+bwf
+leaping
+##rts
+##ssel
+alexei
+greyhound
+hoarse
+##dler
+##oration
+seneca
+##cule
+gaping
+##ulously
+##pura
+cinnamon
+##gens
+##rricular
+craven
+fantasies
+houghton
+engined
+reigned
+dictator
+supervising
+##oris
+bogota
+commentaries
+unnatural
+fingernails
+spirituality
+tighten
+##tm
+canadiens
+protesting
+intentional
+cheers
+sparta
+##ytic
+##iere
+##zine
+widen
+belgarath
+controllers
+dodd
+iaaf
+navarre
+##ication
+defect
+squire
+steiner
+whisky
+##mins
+560
+inevitably
+tome
+##gold
+chew
+##uid
+##lid
+elastic
+##aby
+streaked
+alliances
+jailed
+regal
+##ined
+##phy
+czechoslovak
+narration
+absently
+##uld
+bluegrass
+guangdong
+quran
+criticizing
+hose
+hari
+##liest
+##owa
+skier
+streaks
+deploy
+##lom
+raft
+bose
+dialed
+huff
+##eira
+haifa
+simplest
+bursting
+endings
+ib
+sultanate
+##titled
+franks
+whitman
+ensures
+sven
+##ggs
+collaborators
+forster
+organising
+ui
+banished
+napier
+injustice
+teller
+layered
+thump
+##otti
+roc
+battleships
+evidenced
+fugitive
+sadie
+robotics
+##roud
+equatorial
+geologist
+##iza
+yielding
+##bron
+##sr
+internationale
+mecca
+##diment
+sbs
+skyline
+toad
+uploaded
+reflective
+undrafted
+lal
+leafs
+bayern
+##dai
+lakshmi
+shortlisted
+##stick
+##wicz
+camouflage
+donate
+af
+christi
+lau
+##acio
+disclosed
+nemesis
+1761
+assemble
+straining
+northamptonshire
+tal
+##asi
+bernardino
+premature
+heidi
+42nd
+coefficients
+galactic
+reproduce
+buzzed
+sensations
+zionist
+monsieur
+myrtle
+##eme
+archery
+strangled
+musically
+viewpoint
+antiquities
+bei
+trailers
+seahawks
+cured
+pee
+preferring
+tasmanian
+lange
+sul
+##mail
+##working
+colder
+overland
+lucivar
+massey
+gatherings
+haitian
+##smith
+disapproval
+flaws
+##cco
+##enbach
+1766
+npr
+##icular
+boroughs
+creole
+forums
+techno
+1755
+dent
+abdominal
+streetcar
+##eson
+##stream
+procurement
+gemini
+predictable
+##tya
+acheron
+christoph
+feeder
+fronts
+vendor
+bernhard
+jammu
+tumors
+slang
+##uber
+goaltender
+twists
+curving
+manson
+vuelta
+mer
+peanut
+confessions
+pouch
+unpredictable
+allowance
+theodor
+vascular
+##factory
+bala
+authenticity
+metabolic
+coughing
+nanjing
+##cea
+pembroke
+##bard
+splendid
+36th
+ff
+hourly
+##ahu
+elmer
+handel
+##ivate
+awarding
+thrusting
+dl
+experimentation
+##hesion
+##46
+caressed
+entertained
+steak
+##rangle
+biologist
+orphans
+baroness
+oyster
+stepfather
+##dridge
+mirage
+reefs
+speeding
+##31
+barons
+1764
+227
+inhabit
+preached
+repealed
+##tral
+honoring
+boogie
+captives
+administer
+johanna
+##imate
+gel
+suspiciously
+1767
+sobs
+##dington
+backbone
+hayward
+garry
+##folding
+##nesia
+maxi
+##oof
+##ppe
+ellison
+galileo
+##stand
+crimea
+frenzy
+amour
+bumper
+matrices
+natalia
+baking
+garth
+palestinians
+##grove
+smack
+conveyed
+ensembles
+gardening
+##manship
+##rup
+##stituting
+1640
+harvesting
+topography
+jing
+shifters
+dormitory
+##carriage
+##lston
+ist
+skulls
+##stadt
+dolores
+jewellery
+sarawak
+##wai
+##zier
+fences
+christy
+confinement
+tumbling
+credibility
+fir
+stench
+##bria
+##plication
+##nged
+##sam
+virtues
+##belt
+marjorie
+pba
+##eem
+##made
+celebrates
+schooner
+agitated
+barley
+fulfilling
+anthropologist
+##pro
+restrict
+novi
+regulating
+##nent
+padres
+##rani
+##hesive
+loyola
+tabitha
+milky
+olson
+proprietor
+crambidae
+guarantees
+intercollegiate
+ljubljana
+hilda
+##sko
+ignorant
+hooded
+##lts
+sardinia
+##lidae
+##vation
+frontman
+privileged
+witchcraft
+##gp
+jammed
+laude
+poking
+##than
+bracket
+amazement
+yunnan
+##erus
+maharaja
+linnaeus
+264
+commissioning
+milano
+peacefully
+##logies
+akira
+rani
+regulator
+##36
+grasses
+##rance
+luzon
+crows
+compiler
+gretchen
+seaman
+edouard
+tab
+buccaneers
+ellington
+hamlets
+whig
+socialists
+##anto
+directorial
+easton
+mythological
+##kr
+##vary
+rhineland
+semantic
+taut
+dune
+inventions
+succeeds
+##iter
+replication
+branched
+##pired
+jul
+prosecuted
+kangaroo
+penetrated
+##avian
+middlesbrough
+doses
+bleak
+madam
+predatory
+relentless
+##vili
+reluctance
+##vir
+hailey
+crore
+silvery
+1759
+monstrous
+swimmers
+transmissions
+hawthorn
+informing
+##eral
+toilets
+caracas
+crouch
+kb
+##sett
+295
+cartel
+hadley
+##aling
+alexia
+yvonne
+##biology
+cinderella
+eton
+superb
+blizzard
+stabbing
+industrialist
+maximus
+##gm
+##orus
+groves
+maud
+clade
+oversized
+comedic
+##bella
+rosen
+nomadic
+fulham
+montane
+beverages
+galaxies
+redundant
+swarm
+##rot
+##folia
+##llis
+buckinghamshire
+fen
+bearings
+bahadur
+##rom
+gilles
+phased
+dynamite
+faber
+benoit
+vip
+##ount
+##wd
+booking
+fractured
+tailored
+anya
+spices
+westwood
+cairns
+auditions
+inflammation
+steamed
+##rocity
+##acion
+##urne
+skyla
+thereof
+watford
+torment
+archdeacon
+transforms
+lulu
+demeanor
+fucked
+serge
+##sor
+mckenna
+minas
+entertainer
+##icide
+caress
+originate
+residue
+##sty
+1740
+##ilised
+##org
+beech
+##wana
+subsidies
+##ghton
+emptied
+gladstone
+ru
+firefighters
+voodoo
+##rcle
+het
+nightingale
+tamara
+edmond
+ingredient
+weaknesses
+silhouette
+285
+compatibility
+withdrawing
+hampson
+##mona
+anguish
+giggling
+##mber
+bookstore
+##jiang
+southernmost
+tilting
+##vance
+bai
+economical
+rf
+briefcase
+dreadful
+hinted
+projections
+shattering
+totaling
+##rogate
+analogue
+indicted
+periodical
+fullback
+##dman
+haynes
+##tenberg
+##ffs
+##ishment
+1745
+thirst
+stumble
+penang
+vigorous
+##ddling
+##kor
+##lium
+octave
+##ove
+##enstein
+##inen
+##ones
+siberian
+##uti
+cbn
+repeal
+swaying
+##vington
+khalid
+tanaka
+unicorn
+otago
+plastered
+lobe
+riddle
+##rella
+perch
+##ishing
+croydon
+filtered
+graeme
+tripoli
+##ossa
+crocodile
+##chers
+sufi
+mined
+##tung
+inferno
+lsu
+##phi
+swelled
+utilizes
+£2
+cale
+periodicals
+styx
+hike
+informally
+coop
+lund
+##tidae
+ala
+hen
+qui
+transformations
+disposed
+sheath
+chickens
+##cade
+fitzroy
+sas
+silesia
+unacceptable
+odisha
+1650
+sabrina
+pe
+spokane
+ratios
+athena
+massage
+shen
+dilemma
+##drum
+##riz
+##hul
+corona
+doubtful
+niall
+##pha
+##bino
+fines
+cite
+acknowledging
+bangor
+ballard
+bathurst
+##resh
+huron
+mustered
+alzheimer
+garments
+kinase
+tyre
+warship
+##cp
+flashback
+pulmonary
+braun
+cheat
+kamal
+cyclists
+constructions
+grenades
+ndp
+traveller
+excuses
+stomped
+signalling
+trimmed
+futsal
+mosques
+relevance
+##wine
+wta
+##23
+##vah
+##lter
+hoc
+##riding
+optimistic
+##´s
+deco
+sim
+interacting
+rejecting
+moniker
+waterways
+##ieri
+##oku
+mayors
+gdansk
+outnumbered
+pearls
+##ended
+##hampton
+fairs
+totals
+dominating
+262
+notions
+stairway
+compiling
+pursed
+commodities
+grease
+yeast
+##jong
+carthage
+griffiths
+residual
+amc
+contraction
+laird
+sapphire
+##marine
+##ivated
+amalgamation
+dissolve
+inclination
+lyle
+packaged
+altitudes
+suez
+canons
+graded
+lurched
+narrowing
+boasts
+guise
+wed
+enrico
+##ovsky
+rower
+scarred
+bree
+cub
+iberian
+protagonists
+bargaining
+proposing
+trainers
+voyages
+vans
+fishes
+##aea
+##ivist
+##verance
+encryption
+artworks
+kazan
+sabre
+cleopatra
+hepburn
+rotting
+supremacy
+mecklenburg
+##brate
+burrows
+hazards
+outgoing
+flair
+organizes
+##ctions
+scorpion
+##usions
+boo
+234
+chevalier
+dunedin
+slapping
+##34
+ineligible
+pensions
+##38
+##omic
+manufactures
+emails
+bismarck
+238
+weakening
+blackish
+ding
+mcgee
+quo
+##rling
+northernmost
+xx
+manpower
+greed
+sampson
+clicking
+##ange
+##horpe
+##inations
+##roving
+torre
+##eptive
+##moral
+symbolism
+38th
+asshole
+meritorious
+outfits
+splashed
+biographies
+sprung
+astros
+##tale
+302
+737
+filly
+raoul
+nw
+tokugawa
+linden
+clubhouse
+##apa
+tracts
+romano
+##pio
+putin
+tags
+##note
+chained
+dickson
+gunshot
+moe
+gunn
+rashid
+##tails
+zipper
+##bas
+##nea
+contrasted
+##ply
+##udes
+plum
+pharaoh
+##pile
+aw
+comedies
+ingrid
+sandwiches
+subdivisions
+1100
+mariana
+nokia
+kamen
+hz
+delaney
+veto
+herring
+##words
+possessive
+outlines
+##roup
+siemens
+stairwell
+rc
+gallantry
+messiah
+palais
+yells
+233
+zeppelin
+##dm
+bolivar
+##cede
+smackdown
+mckinley
+##mora
+##yt
+muted
+geologic
+finely
+unitary
+avatar
+hamas
+maynard
+rees
+bog
+contrasting
+##rut
+liv
+chico
+disposition
+pixel
+##erate
+becca
+dmitry
+yeshiva
+narratives
+##lva
+##ulton
+mercenary
+sharpe
+tempered
+navigate
+stealth
+amassed
+keynes
+##lini
+untouched
+##rrie
+havoc
+lithium
+##fighting
+abyss
+graf
+southward
+wolverine
+balloons
+implements
+ngos
+transitions
+##icum
+ambushed
+concacaf
+dormant
+economists
+##dim
+costing
+csi
+rana
+universite
+boulders
+verity
+##llon
+collin
+mellon
+misses
+cypress
+fluorescent
+lifeless
+spence
+##ulla
+crewe
+shepard
+pak
+revelations
+##م
+jolly
+gibbons
+paw
+##dro
+##quel
+freeing
+##test
+shack
+fries
+palatine
+##51
+##hiko
+accompaniment
+cruising
+recycled
+##aver
+erwin
+sorting
+synthesizers
+dyke
+realities
+sg
+strides
+enslaved
+wetland
+##ghan
+competence
+gunpowder
+grassy
+maroon
+reactors
+objection
+##oms
+carlson
+gearbox
+macintosh
+radios
+shelton
+##sho
+clergyman
+prakash
+254
+mongols
+trophies
+oricon
+228
+stimuli
+twenty20
+cantonese
+cortes
+mirrored
+##saurus
+bhp
+cristina
+melancholy
+##lating
+enjoyable
+nuevo
+##wny
+downfall
+schumacher
+##ind
+banging
+lausanne
+rumbled
+paramilitary
+reflex
+ax
+amplitude
+migratory
+##gall
+##ups
+midi
+barnard
+lastly
+sherry
+##hp
+##nall
+keystone
+##kra
+carleton
+slippery
+##53
+coloring
+foe
+socket
+otter
+##rgos
+mats
+##tose
+consultants
+bafta
+bison
+topping
+##km
+490
+primal
+abandonment
+transplant
+atoll
+hideous
+mort
+pained
+reproduced
+tae
+howling
+##turn
+unlawful
+billionaire
+hotter
+poised
+lansing
+##chang
+dinamo
+retro
+messing
+nfc
+domesday
+##mina
+blitz
+timed
+##athing
+##kley
+ascending
+gesturing
+##izations
+signaled
+tis
+chinatown
+mermaid
+savanna
+jameson
+##aint
+catalina
+##pet
+##hers
+cochrane
+cy
+chatting
+##kus
+alerted
+computation
+mused
+noelle
+majestic
+mohawk
+campo
+octagonal
+##sant
+##hend
+241
+aspiring
+##mart
+comprehend
+iona
+paralyzed
+shimmering
+swindon
+rhone
+##eley
+reputed
+configurations
+pitchfork
+agitation
+francais
+gillian
+lipstick
+##ilo
+outsiders
+pontifical
+resisting
+bitterness
+sewer
+rockies
+##edd
+##ucher
+misleading
+1756
+exiting
+galloway
+##nging
+risked
+##heart
+246
+commemoration
+schultz
+##rka
+integrating
+##rsa
+poses
+shrieked
+##weiler
+guineas
+gladys
+jerking
+owls
+goldsmith
+nightly
+penetrating
+##unced
+lia
+##33
+ignited
+betsy
+##aring
+##thorpe
+follower
+vigorously
+##rave
+coded
+kiran
+knit
+zoology
+tbilisi
+##28
+##bered
+repository
+govt
+deciduous
+dino
+growling
+##bba
+enhancement
+unleashed
+chanting
+pussy
+biochemistry
+##eric
+kettle
+repression
+toxicity
+nrhp
+##arth
+##kko
+##bush
+ernesto
+commended
+outspoken
+242
+mca
+parchment
+sms
+kristen
+##aton
+bisexual
+raked
+glamour
+navajo
+a2
+conditioned
+showcased
+##hma
+spacious
+youthful
+##esa
+usl
+appliances
+junta
+brest
+layne
+conglomerate
+enchanted
+chao
+loosened
+picasso
+circulating
+inspect
+montevideo
+##centric
+##kti
+piazza
+spurred
+##aith
+bari
+freedoms
+poultry
+stamford
+lieu
+##ect
+indigo
+sarcastic
+bahia
+stump
+attach
+dvds
+frankenstein
+lille
+approx
+scriptures
+pollen
+##script
+nmi
+overseen
+##ivism
+tides
+proponent
+newmarket
+inherit
+milling
+##erland
+centralized
+##rou
+distributors
+credentials
+drawers
+abbreviation
+##lco
+##xon
+downing
+uncomfortably
+ripe
+##oes
+erase
+franchises
+##ever
+populace
+##bery
+##khar
+decomposition
+pleas
+##tet
+daryl
+sabah
+##stle
+##wide
+fearless
+genie
+lesions
+annette
+##ogist
+oboe
+appendix
+nair
+dripped
+petitioned
+maclean
+mosquito
+parrot
+rpg
+hampered
+1648
+operatic
+reservoirs
+##tham
+irrelevant
+jolt
+summarized
+##fp
+medallion
+##taff
+##−
+clawed
+harlow
+narrower
+goddard
+marcia
+bodied
+fremont
+suarez
+altering
+tempest
+mussolini
+porn
+##isms
+sweetly
+oversees
+walkers
+solitude
+grimly
+shrines
+hk
+ich
+supervisors
+hostess
+dietrich
+legitimacy
+brushes
+expressive
+##yp
+dissipated
+##rse
+localized
+systemic
+##nikov
+gettysburg
+##js
+##uaries
+dialogues
+muttering
+251
+housekeeper
+sicilian
+discouraged
+##frey
+beamed
+kaladin
+halftime
+kidnap
+##amo
+##llet
+1754
+synonymous
+depleted
+instituto
+insulin
+reprised
+##opsis
+clashed
+##ctric
+interrupting
+radcliffe
+insisting
+medici
+1715
+ejected
+playfully
+turbulent
+##47
+starvation
+##rini
+shipment
+rebellious
+petersen
+verification
+merits
+##rified
+cakes
+##charged
+1757
+milford
+shortages
+spying
+fidelity
+##aker
+emitted
+storylines
+harvested
+seismic
+##iform
+cheung
+kilda
+theoretically
+barbie
+lynx
+##rgy
+##tius
+goblin
+mata
+poisonous
+##nburg
+reactive
+residues
+obedience
+##евич
+conjecture
+##rac
+401
+hating
+sixties
+kicker
+moaning
+motown
+##bha
+emancipation
+neoclassical
+##hering
+consoles
+ebert
+professorship
+##tures
+sustaining
+assaults
+obeyed
+affluent
+incurred
+tornadoes
+##eber
+##zow
+emphasizing
+highlanders
+cheated
+helmets
+##ctus
+internship
+terence
+bony
+executions
+legislators
+berries
+peninsular
+tinged
+##aco
+1689
+amplifier
+corvette
+ribbons
+lavish
+pennant
+##lander
+worthless
+##chfield
+##forms
+mariano
+pyrenees
+expenditures
+##icides
+chesterfield
+mandir
+tailor
+39th
+sergey
+nestled
+willed
+aristocracy
+devotees
+goodnight
+raaf
+rumored
+weaponry
+remy
+appropriations
+harcourt
+burr
+riaa
+##lence
+limitation
+unnoticed
+guo
+soaking
+swamps
+##tica
+collapsing
+tatiana
+descriptive
+brigham
+psalm
+##chment
+maddox
+##lization
+patti
+caliph
+##aja
+akron
+injuring
+serra
+##ganj
+basins
+##sari
+astonished
+launcher
+##church
+hilary
+wilkins
+sewing
+##sf
+stinging
+##fia
+##ncia
+underwood
+startup
+##ition
+compilations
+vibrations
+embankment
+jurist
+##nity
+bard
+juventus
+groundwater
+kern
+palaces
+helium
+boca
+cramped
+marissa
+soto
+##worm
+jae
+princely
+##ggy
+faso
+bazaar
+warmly
+##voking
+229
+pairing
+##lite
+##grate
+##nets
+wien
+freaked
+ulysses
+rebirth
+##alia
+##rent
+mummy
+guzman
+jimenez
+stilled
+##nitz
+trajectory
+tha
+woken
+archival
+professions
+##pts
+##pta
+hilly
+shadowy
+shrink
+##bolt
+norwood
+glued
+migrate
+stereotypes
+devoid
+##pheus
+625
+evacuate
+horrors
+infancy
+gotham
+knowles
+optic
+downloaded
+sachs
+kingsley
+parramatta
+darryl
+mor
+##onale
+shady
+commence
+confesses
+kan
+##meter
+##placed
+marlborough
+roundabout
+regents
+frigates
+io
+##imating
+gothenburg
+revoked
+carvings
+clockwise
+convertible
+intruder
+##sche
+banged
+##ogo
+vicky
+bourgeois
+##mony
+dupont
+footing
+##gum
+pd
+##real
+buckle
+yun
+penthouse
+sane
+720
+serviced
+stakeholders
+neumann
+bb
+##eers
+comb
+##gam
+catchment
+pinning
+rallies
+typing
+##elles
+forefront
+freiburg
+sweetie
+giacomo
+widowed
+goodwill
+worshipped
+aspirations
+midday
+##vat
+fishery
+##trick
+bournemouth
+turk
+243
+hearth
+ethanol
+guadalajara
+murmurs
+sl
+##uge
+afforded
+scripted
+##hta
+wah
+##jn
+coroner
+translucent
+252
+memorials
+puck
+progresses
+clumsy
+##race
+315
+candace
+recounted
+##27
+##slin
+##uve
+filtering
+##mac
+howl
+strata
+heron
+leveled
+##ays
+dubious
+##oja
+##т
+##wheel
+citations
+exhibiting
+##laya
+##mics
+##pods
+turkic
+##lberg
+injunction
+##ennial
+##mit
+antibodies
+##44
+organise
+##rigues
+cardiovascular
+cushion
+inverness
+##zquez
+dia
+cocoa
+sibling
+##tman
+##roid
+expanse
+feasible
+tunisian
+algiers
+##relli
+rus
+bloomberg
+dso
+westphalia
+bro
+tacoma
+281
+downloads
+##ours
+konrad
+duran
+##hdi
+continuum
+jett
+compares
+legislator
+secession
+##nable
+##gues
+##zuka
+translating
+reacher
+##gley
+##ła
+aleppo
+##agi
+tc
+orchards
+trapping
+linguist
+versatile
+drumming
+postage
+calhoun
+superiors
+##mx
+barefoot
+leary
+##cis
+ignacio
+alfa
+kaplan
+##rogen
+bratislava
+mori
+##vot
+disturb
+haas
+313
+cartridges
+gilmore
+radiated
+salford
+tunic
+hades
+##ulsive
+archeological
+delilah
+magistrates
+auditioned
+brewster
+charters
+empowerment
+blogs
+cappella
+dynasties
+iroquois
+whipping
+##krishna
+raceway
+truths
+myra
+weaken
+judah
+mcgregor
+##horse
+mic
+refueling
+37th
+burnley
+bosses
+markus
+premio
+query
+##gga
+dunbar
+##economic
+darkest
+lyndon
+sealing
+commendation
+reappeared
+##mun
+addicted
+ezio
+slaughtered
+satisfactory
+shuffle
+##eves
+##thic
+##uj
+fortification
+warrington
+##otto
+resurrected
+fargo
+mane
+##utable
+##lei
+##space
+foreword
+ox
+##aris
+##vern
+abrams
+hua
+##mento
+sakura
+##alo
+uv
+sentimental
+##skaya
+midfield
+##eses
+sturdy
+scrolls
+macleod
+##kyu
+entropy
+##lance
+mitochondrial
+cicero
+excelled
+thinner
+convoys
+perceive
+##oslav
+##urable
+systematically
+grind
+burkina
+287
+##tagram
+ops
+##aman
+guantanamo
+##cloth
+##tite
+forcefully
+wavy
+##jou
+pointless
+##linger
+##tze
+layton
+portico
+superficial
+clerical
+outlaws
+##hism
+burials
+muir
+##inn
+creditors
+hauling
+rattle
+##leg
+calais
+monde
+archers
+reclaimed
+dwell
+wexford
+hellenic
+falsely
+remorse
+##tek
+dough
+furnishings
+##uttered
+gabon
+neurological
+novice
+##igraphy
+contemplated
+pulpit
+nightstand
+saratoga
+##istan
+documenting
+pulsing
+taluk
+##firmed
+busted
+marital
+##rien
+disagreements
+wasps
+##yes
+hodge
+mcdonnell
+mimic
+fran
+pendant
+dhabi
+musa
+##nington
+congratulations
+argent
+darrell
+concussion
+losers
+regrets
+thessaloniki
+reversal
+donaldson
+hardwood
+thence
+achilles
+ritter
+##eran
+demonic
+jurgen
+prophets
+goethe
+eki
+classmate
+buff
+##cking
+yank
+irrational
+##inging
+perished
+seductive
+qur
+sourced
+##crat
+##typic
+mustard
+ravine
+barre
+horizontally
+characterization
+phylogenetic
+boise
+##dit
+##runner
+##tower
+brutally
+intercourse
+seduce
+##bbing
+fay
+ferris
+ogden
+amar
+nik
+unarmed
+##inator
+evaluating
+kyrgyzstan
+sweetness
+##lford
+##oki
+mccormick
+meiji
+notoriety
+stimulate
+disrupt
+figuring
+instructional
+mcgrath
+##zoo
+groundbreaking
+##lto
+flinch
+khorasan
+agrarian
+bengals
+mixer
+radiating
+##sov
+ingram
+pitchers
+nad
+tariff
+##cript
+tata
+##codes
+##emi
+##ungen
+appellate
+lehigh
+##bled
+##giri
+brawl
+duct
+texans
+##ciation
+##ropolis
+skipper
+speculative
+vomit
+doctrines
+stresses
+253
+davy
+graders
+whitehead
+jozef
+timely
+cumulative
+haryana
+paints
+appropriately
+boon
+cactus
+##ales
+##pid
+dow
+legions
+##pit
+perceptions
+1730
+picturesque
+##yse
+periphery
+rune
+wr
+##aha
+celtics
+sentencing
+whoa
+##erin
+confirms
+variance
+425
+moines
+mathews
+spade
+rave
+m1
+fronted
+fx
+blending
+alleging
+reared
+##gl
+237
+##paper
+grassroots
+eroded
+##free
+##physical
+directs
+ordeal
+##sław
+accelerate
+hacker
+rooftop
+##inia
+lev
+buys
+cebu
+devote
+##lce
+specialising
+##ulsion
+choreographed
+repetition
+warehouses
+##ryl
+paisley
+tuscany
+analogy
+sorcerer
+hash
+huts
+shards
+descends
+exclude
+nix
+chaplin
+gaga
+ito
+vane
+##drich
+causeway
+misconduct
+limo
+orchestrated
+glands
+jana
+##kot
+u2
+##mple
+##sons
+branching
+contrasts
+scoop
+longed
+##virus
+chattanooga
+##75
+syrup
+cornerstone
+##tized
+##mind
+##iaceae
+careless
+precedence
+frescoes
+##uet
+chilled
+consult
+modelled
+snatch
+peat
+##thermal
+caucasian
+humane
+relaxation
+spins
+temperance
+##lbert
+occupations
+lambda
+hybrids
+moons
+mp3
+##oese
+247
+rolf
+societal
+yerevan
+ness
+##ssler
+befriended
+mechanized
+nominate
+trough
+boasted
+cues
+seater
+##hom
+bends
+##tangle
+conductors
+emptiness
+##lmer
+eurasian
+adriatic
+tian
+##cie
+anxiously
+lark
+propellers
+chichester
+jock
+ev
+2a
+##holding
+credible
+recounts
+tori
+loyalist
+abduction
+##hoot
+##redo
+nepali
+##mite
+ventral
+tempting
+##ango
+##crats
+steered
+##wice
+javelin
+dipping
+laborers
+prentice
+looming
+titanium
+##ː
+badges
+emir
+tensor
+##ntation
+egyptians
+rash
+denies
+hawthorne
+lombard
+showers
+wehrmacht
+dietary
+trojan
+##reus
+welles
+executing
+horseshoe
+lifeboat
+##lak
+elsa
+infirmary
+nearing
+roberta
+boyer
+mutter
+trillion
+joanne
+##fine
+##oked
+sinks
+vortex
+uruguayan
+clasp
+sirius
+##block
+accelerator
+prohibit
+sunken
+byu
+chronological
+diplomats
+ochreous
+510
+symmetrical
+1644
+maia
+##tology
+salts
+reigns
+atrocities
+##ия
+hess
+bared
+issn
+##vyn
+cater
+saturated
+##cycle
+##isse
+sable
+voyager
+dyer
+yusuf
+##inge
+fountains
+wolff
+##39
+##nni
+engraving
+rollins
+atheist
+ominous
+##ault
+herr
+chariot
+martina
+strung
+##fell
+##farlane
+horrific
+sahib
+gazes
+saetan
+erased
+ptolemy
+##olic
+flushing
+lauderdale
+analytic
+##ices
+530
+navarro
+beak
+gorilla
+herrera
+broom
+guadalupe
+raiding
+sykes
+311
+bsc
+deliveries
+1720
+invasions
+carmichael
+tajikistan
+thematic
+ecumenical
+sentiments
+onstage
+##rians
+##brand
+##sume
+catastrophic
+flanks
+molten
+##arns
+waller
+aimee
+terminating
+##icing
+alternately
+##oche
+nehru
+printers
+outraged
+##eving
+empires
+template
+banners
+repetitive
+za
+##oise
+vegetarian
+##tell
+guiana
+opt
+cavendish
+lucknow
+synthesized
+##hani
+##mada
+finalized
+##ctable
+fictitious
+mayoral
+unreliable
+##enham
+embracing
+peppers
+rbis
+##chio
+##neo
+inhibition
+slashed
+togo
+orderly
+embroidered
+safari
+salty
+236
+barron
+benito
+totaled
+##dak
+pubs
+simulated
+caden
+devin
+tolkien
+momma
+welding
+sesame
+##ept
+gottingen
+hardness
+630
+shaman
+temeraire
+620
+adequately
+pediatric
+##kit
+ck
+assertion
+radicals
+composure
+cadence
+seafood
+beaufort
+lazarus
+mani
+warily
+cunning
+kurdistan
+249
+cantata
+##kir
+ares
+##41
+##clusive
+nape
+townland
+geared
+insulted
+flutter
+boating
+violate
+draper
+dumping
+malmo
+##hh
+##romatic
+firearm
+alta
+bono
+obscured
+##clave
+exceeds
+panorama
+unbelievable
+##train
+preschool
+##essed
+disconnected
+installing
+rescuing
+secretaries
+accessibility
+##castle
+##drive
+##ifice
+##film
+bouts
+slug
+waterway
+mindanao
+##buro
+##ratic
+halves
+##ل
+calming
+liter
+maternity
+adorable
+bragg
+electrification
+mcc
+##dote
+roxy
+schizophrenia
+##body
+munoz
+kaye
+whaling
+239
+mil
+tingling
+tolerant
+##ago
+unconventional
+volcanoes
+##finder
+deportivo
+##llie
+robson
+kaufman
+neuroscience
+wai
+deportation
+masovian
+scraping
+converse
+##bh
+hacking
+bulge
+##oun
+administratively
+yao
+580
+amp
+mammoth
+booster
+claremont
+hooper
+nomenclature
+pursuits
+mclaughlin
+melinda
+##sul
+catfish
+barclay
+substrates
+taxa
+zee
+originals
+kimberly
+packets
+padma
+##ality
+borrowing
+ostensibly
+solvent
+##bri
+##genesis
+##mist
+lukas
+shreveport
+veracruz
+##ь
+##lou
+##wives
+cheney
+tt
+anatolia
+hobbs
+##zyn
+cyclic
+radiant
+alistair
+greenish
+siena
+dat
+independents
+##bation
+conform
+pieter
+hyper
+applicant
+bradshaw
+spores
+telangana
+vinci
+inexpensive
+nuclei
+322
+jang
+nme
+soho
+spd
+##ign
+cradled
+receptionist
+pow
+##43
+##rika
+fascism
+##ifer
+experimenting
+##ading
+##iec
+##region
+345
+jocelyn
+maris
+stair
+nocturnal
+toro
+constabulary
+elgin
+##kker
+msc
+##giving
+##schen
+##rase
+doherty
+doping
+sarcastically
+batter
+maneuvers
+##cano
+##apple
+##gai
+##git
+intrinsic
+##nst
+##stor
+1753
+showtime
+cafes
+gasps
+lviv
+ushered
+##thed
+fours
+restart
+astonishment
+transmitting
+flyer
+shrugs
+##sau
+intriguing
+cones
+dictated
+mushrooms
+medial
+##kovsky
+##elman
+escorting
+gaped
+##26
+godfather
+##door
+##sell
+djs
+recaptured
+timetable
+vila
+1710
+3a
+aerodrome
+mortals
+scientology
+##orne
+angelina
+mag
+convection
+unpaid
+insertion
+intermittent
+lego
+##nated
+endeavor
+kota
+pereira
+##lz
+304
+bwv
+glamorgan
+insults
+agatha
+fey
+##cend
+fleetwood
+mahogany
+protruding
+steamship
+zeta
+##arty
+mcguire
+suspense
+##sphere
+advising
+urges
+##wala
+hurriedly
+meteor
+gilded
+inline
+arroyo
+stalker
+##oge
+excitedly
+revered
+##cure
+earle
+introductory
+##break
+##ilde
+mutants
+puff
+pulses
+reinforcement
+##haling
+curses
+lizards
+stalk
+correlated
+##fixed
+fallout
+macquarie
+##unas
+bearded
+denton
+heaving
+802
+##ocation
+winery
+assign
+dortmund
+##lkirk
+everest
+invariant
+charismatic
+susie
+##elling
+bled
+lesley
+telegram
+sumner
+bk
+##ogen
+##к
+wilcox
+needy
+colbert
+duval
+##iferous
+##mbled
+allotted
+attends
+imperative
+##hita
+replacements
+hawker
+##inda
+insurgency
+##zee
+##eke
+casts
+##yla
+680
+ives
+transitioned
+##pack
+##powering
+authoritative
+baylor
+flex
+cringed
+plaintiffs
+woodrow
+##skie
+drastic
+ape
+aroma
+unfolded
+commotion
+nt
+preoccupied
+theta
+routines
+lasers
+privatization
+wand
+domino
+ek
+clenching
+nsa
+strategically
+showered
+bile
+handkerchief
+pere
+storing
+christophe
+insulting
+316
+nakamura
+romani
+asiatic
+magdalena
+palma
+cruises
+stripping
+405
+konstantin
+soaring
+##berman
+colloquially
+forerunner
+havilland
+incarcerated
+parasites
+sincerity
+##utus
+disks
+plank
+saigon
+##ining
+corbin
+homo
+ornaments
+powerhouse
+##tlement
+chong
+fastened
+feasibility
+idf
+morphological
+usable
+##nish
+##zuki
+aqueduct
+jaguars
+keepers
+##flies
+aleksandr
+faust
+assigns
+ewing
+bacterium
+hurled
+tricky
+hungarians
+integers
+wallis
+321
+yamaha
+##isha
+hushed
+oblivion
+aviator
+evangelist
+friars
+##eller
+monograph
+ode
+##nary
+airplanes
+labourers
+charms
+##nee
+1661
+hagen
+tnt
+rudder
+fiesta
+transcript
+dorothea
+ska
+inhibitor
+maccabi
+retorted
+raining
+encompassed
+clauses
+menacing
+1642
+lineman
+##gist
+vamps
+##ape
+##dick
+gloom
+##rera
+dealings
+easing
+seekers
+##nut
+##pment
+helens
+unmanned
+##anu
+##isson
+basics
+##amy
+##ckman
+adjustments
+1688
+brutality
+horne
+##zell
+sui
+##55
+##mable
+aggregator
+##thal
+rhino
+##drick
+##vira
+counters
+zoom
+##01
+##rting
+mn
+montenegrin
+packard
+##unciation
+##♭
+##kki
+reclaim
+scholastic
+thugs
+pulsed
+##icia
+syriac
+quan
+saddam
+banda
+kobe
+blaming
+buddies
+dissent
+##lusion
+##usia
+corbett
+jaya
+delle
+erratic
+lexie
+##hesis
+435
+amiga
+hermes
+##pressing
+##leen
+chapels
+gospels
+jamal
+##uating
+compute
+revolving
+warp
+##sso
+##thes
+armory
+##eras
+##gol
+antrim
+loki
+##kow
+##asian
+##good
+##zano
+braid
+handwriting
+subdistrict
+funky
+pantheon
+##iculate
+concurrency
+estimation
+improper
+juliana
+##his
+newcomers
+johnstone
+staten
+communicated
+##oco
+##alle
+sausage
+stormy
+##stered
+##tters
+superfamily
+##grade
+acidic
+collateral
+tabloid
+##oped
+##rza
+bladder
+austen
+##ellant
+mcgraw
+##hay
+hannibal
+mein
+aquino
+lucifer
+wo
+badger
+boar
+cher
+christensen
+greenberg
+interruption
+##kken
+jem
+244
+mocked
+bottoms
+cambridgeshire
+##lide
+sprawling
+##bbly
+eastwood
+ghent
+synth
+##buck
+advisers
+##bah
+nominally
+hapoel
+qu
+daggers
+estranged
+fabricated
+towels
+vinnie
+wcw
+misunderstanding
+anglia
+nothin
+unmistakable
+##dust
+##lova
+chilly
+marquette
+truss
+##edge
+##erine
+reece
+##lty
+##chemist
+##connected
+272
+308
+41st
+bash
+raion
+waterfalls
+##ump
+##main
+labyrinth
+queue
+theorist
+##istle
+bharatiya
+flexed
+soundtracks
+rooney
+leftist
+patrolling
+wharton
+plainly
+alleviate
+eastman
+schuster
+topographic
+engages
+immensely
+unbearable
+fairchild
+1620
+dona
+lurking
+parisian
+oliveira
+ia
+indictment
+hahn
+bangladeshi
+##aster
+vivo
+##uming
+##ential
+antonia
+expects
+indoors
+kildare
+harlan
+##logue
+##ogenic
+##sities
+forgiven
+##wat
+childish
+tavi
+##mide
+##orra
+plausible
+grimm
+successively
+scooted
+##bola
+##dget
+##rith
+spartans
+emery
+flatly
+azure
+epilogue
+##wark
+flourish
+##iny
+##tracted
+##overs
+##oshi
+bestseller
+distressed
+receipt
+spitting
+hermit
+topological
+##cot
+drilled
+subunit
+francs
+##layer
+eel
+##fk
+##itas
+octopus
+footprint
+petitions
+ufo
+##say
+##foil
+interfering
+leaking
+palo
+##metry
+thistle
+valiant
+##pic
+narayan
+mcpherson
+##fast
+gonzales
+##ym
+##enne
+dustin
+novgorod
+solos
+##zman
+doin
+##raph
+##patient
+##meyer
+soluble
+ashland
+cuffs
+carole
+pendleton
+whistling
+vassal
+##river
+deviation
+revisited
+constituents
+rallied
+rotate
+loomed
+##eil
+##nting
+amateurs
+augsburg
+auschwitz
+crowns
+skeletons
+##cona
+bonnet
+257
+dummy
+globalization
+simeon
+sleeper
+mandal
+differentiated
+##crow
+##mare
+milne
+bundled
+exasperated
+talmud
+owes
+segregated
+##feng
+##uary
+dentist
+piracy
+props
+##rang
+devlin
+##torium
+malicious
+paws
+##laid
+dependency
+##ergy
+##fers
+##enna
+258
+pistons
+rourke
+jed
+grammatical
+tres
+maha
+wig
+512
+ghostly
+jayne
+##achal
+##creen
+##ilis
+##lins
+##rence
+designate
+##with
+arrogance
+cambodian
+clones
+showdown
+throttle
+twain
+##ception
+lobes
+metz
+nagoya
+335
+braking
+##furt
+385
+roaming
+##minster
+amin
+crippled
+##37
+##llary
+indifferent
+hoffmann
+idols
+intimidating
+1751
+261
+influenza
+memo
+onions
+1748
+bandage
+consciously
+##landa
+##rage
+clandestine
+observes
+swiped
+tangle
+##ener
+##jected
+##trum
+##bill
+##lta
+hugs
+congresses
+josiah
+spirited
+##dek
+humanist
+managerial
+filmmaking
+inmate
+rhymes
+debuting
+grimsby
+ur
+##laze
+duplicate
+vigor
+##tf
+republished
+bolshevik
+refurbishment
+antibiotics
+martini
+methane
+newscasts
+royale
+horizons
+levant
+iain
+visas
+##ischen
+paler
+##around
+manifestation
+snuck
+alf
+chop
+futile
+pedestal
+rehab
+##kat
+bmg
+kerman
+res
+fairbanks
+jarrett
+abstraction
+saharan
+##zek
+1746
+procedural
+clearer
+kincaid
+sash
+luciano
+##ffey
+crunch
+helmut
+##vara
+revolutionaries
+##tute
+creamy
+leach
+##mmon
+1747
+permitting
+nes
+plight
+wendell
+##lese
+contra
+ts
+clancy
+ipa
+mach
+staples
+autopsy
+disturbances
+nueva
+karin
+pontiac
+##uding
+proxy
+venerable
+haunt
+leto
+bergman
+expands
+##helm
+wal
+##pipe
+canning
+celine
+cords
+obesity
+##enary
+intrusion
+planner
+##phate
+reasoned
+sequencing
+307
+harrow
+##chon
+##dora
+marred
+mcintyre
+repay
+tarzan
+darting
+248
+harrisburg
+margarita
+repulsed
+##hur
+##lding
+belinda
+hamburger
+novo
+compliant
+runways
+bingham
+registrar
+skyscraper
+ic
+cuthbert
+improvisation
+livelihood
+##corp
+##elial
+admiring
+##dened
+sporadic
+believer
+casablanca
+popcorn
+##29
+asha
+shovel
+##bek
+##dice
+coiled
+tangible
+##dez
+casper
+elsie
+resin
+tenderness
+rectory
+##ivision
+avail
+sonar
+##mori
+boutique
+##dier
+guerre
+bathed
+upbringing
+vaulted
+sandals
+blessings
+##naut
+##utnant
+1680
+306
+foxes
+pia
+corrosion
+hesitantly
+confederates
+crystalline
+footprints
+shapiro
+tirana
+valentin
+drones
+45th
+microscope
+shipments
+texted
+inquisition
+wry
+guernsey
+unauthorized
+resigning
+760
+ripple
+schubert
+stu
+reassure
+felony
+##ardo
+brittle
+koreans
+##havan
+##ives
+dun
+implicit
+tyres
+##aldi
+##lth
+magnolia
+##ehan
+##puri
+##poulos
+aggressively
+fei
+gr
+familiarity
+##poo
+indicative
+##trust
+fundamentally
+jimmie
+overrun
+395
+anchors
+moans
+##opus
+britannia
+armagh
+##ggle
+purposely
+seizing
+##vao
+bewildered
+mundane
+avoidance
+cosmopolitan
+geometridae
+quartermaster
+caf
+415
+chatter
+engulfed
+gleam
+purge
+##icate
+juliette
+jurisprudence
+guerra
+revisions
+##bn
+casimir
+brew
+##jm
+1749
+clapton
+cloudy
+conde
+hermitage
+278
+simulations
+torches
+vincenzo
+matteo
+##rill
+hidalgo
+booming
+westbound
+accomplishment
+tentacles
+unaffected
+##sius
+annabelle
+flopped
+sloping
+##litz
+dreamer
+interceptor
+vu
+##loh
+consecration
+copying
+messaging
+breaker
+climates
+hospitalized
+1752
+torino
+afternoons
+winfield
+witnessing
+##teacher
+breakers
+choirs
+sawmill
+coldly
+##ege
+sipping
+haste
+uninhabited
+conical
+bibliography
+pamphlets
+severn
+edict
+##oca
+deux
+illnesses
+grips
+##pl
+rehearsals
+sis
+thinkers
+tame
+##keepers
+1690
+acacia
+reformer
+##osed
+##rys
+shuffling
+##iring
+##shima
+eastbound
+ionic
+rhea
+flees
+littered
+##oum
+rocker
+vomiting
+groaning
+champ
+overwhelmingly
+civilizations
+paces
+sloop
+adoptive
+##tish
+skaters
+##vres
+aiding
+mango
+##joy
+nikola
+shriek
+##ignon
+pharmaceuticals
+##mg
+tuna
+calvert
+gustavo
+stocked
+yearbook
+##urai
+##mana
+computed
+subsp
+riff
+hanoi
+kelvin
+hamid
+moors
+pastures
+summons
+jihad
+nectar
+##ctors
+bayou
+untitled
+pleasing
+vastly
+republics
+intellect
+##η
+##ulio
+##tou
+crumbling
+stylistic
+sb
+##ی
+consolation
+frequented
+h₂o
+walden
+widows
+##iens
+404
+##ignment
+chunks
+improves
+288
+grit
+recited
+##dev
+snarl
+sociological
+##arte
+##gul
+inquired
+##held
+bruise
+clube
+consultancy
+homogeneous
+hornets
+multiplication
+pasta
+prick
+savior
+##grin
+##kou
+##phile
+yoon
+##gara
+grimes
+vanishing
+cheering
+reacting
+bn
+distillery
+##quisite
+##vity
+coe
+dockyard
+massif
+##jord
+escorts
+voss
+##valent
+byte
+chopped
+hawke
+illusions
+workings
+floats
+##koto
+##vac
+kv
+annapolis
+madden
+##onus
+alvaro
+noctuidae
+##cum
+##scopic
+avenge
+steamboat
+forte
+illustrates
+erika
+##trip
+570
+dew
+nationalities
+bran
+manifested
+thirsty
+diversified
+muscled
+reborn
+##standing
+arson
+##lessness
+##dran
+##logram
+##boys
+##kushima
+##vious
+willoughby
+##phobia
+286
+alsace
+dashboard
+yuki
+##chai
+granville
+myspace
+publicized
+tricked
+##gang
+adjective
+##ater
+relic
+reorganisation
+enthusiastically
+indications
+saxe
+##lassified
+consolidate
+iec
+padua
+helplessly
+ramps
+renaming
+regulars
+pedestrians
+accents
+convicts
+inaccurate
+lowers
+mana
+##pati
+barrie
+bjp
+outta
+someplace
+berwick
+flanking
+invoked
+marrow
+sparsely
+excerpts
+clothed
+rei
+##ginal
+wept
+##straße
+##vish
+alexa
+excel
+##ptive
+membranes
+aquitaine
+creeks
+cutler
+sheppard
+implementations
+ns
+##dur
+fragrance
+budge
+concordia
+magnesium
+marcelo
+##antes
+gladly
+vibrating
+##rral
+##ggles
+montrose
+##omba
+lew
+seamus
+1630
+cocky
+##ament
+##uen
+bjorn
+##rrick
+fielder
+fluttering
+##lase
+methyl
+kimberley
+mcdowell
+reductions
+barbed
+##jic
+##tonic
+aeronautical
+condensed
+distracting
+##promising
+huffed
+##cala
+##sle
+claudius
+invincible
+missy
+pious
+balthazar
+ci
+##lang
+butte
+combo
+orson
+##dication
+myriad
+1707
+silenced
+##fed
+##rh
+coco
+netball
+yourselves
+##oza
+clarify
+heller
+peg
+durban
+etudes
+offender
+roast
+blackmail
+curvature
+##woods
+vile
+309
+illicit
+suriname
+##linson
+overture
+1685
+bubbling
+gymnast
+tucking
+##mming
+##ouin
+maldives
+##bala
+gurney
+##dda
+##eased
+##oides
+backside
+pinto
+jars
+racehorse
+tending
+##rdial
+baronetcy
+wiener
+duly
+##rke
+barbarian
+cupping
+flawed
+##thesis
+bertha
+pleistocene
+puddle
+swearing
+##nob
+##tically
+fleeting
+prostate
+amulet
+educating
+##mined
+##iti
+##tler
+75th
+jens
+respondents
+analytics
+cavaliers
+papacy
+raju
+##iente
+##ulum
+##tip
+funnel
+271
+disneyland
+##lley
+sociologist
+##iam
+2500
+faulkner
+louvre
+menon
+##dson
+276
+##ower
+afterlife
+mannheim
+peptide
+referees
+comedians
+meaningless
+##anger
+##laise
+fabrics
+hurley
+renal
+sleeps
+##bour
+##icle
+breakout
+kristin
+roadside
+animator
+clover
+disdain
+unsafe
+redesign
+##urity
+firth
+barnsley
+portage
+reset
+narrows
+268
+commandos
+expansive
+speechless
+tubular
+##lux
+essendon
+eyelashes
+smashwords
+##yad
+##bang
+##claim
+craved
+sprinted
+chet
+somme
+astor
+wrocław
+orton
+266
+bane
+##erving
+##uing
+mischief
+##amps
+##sund
+scaling
+terre
+##xious
+impairment
+offenses
+undermine
+moi
+soy
+contiguous
+arcadia
+inuit
+seam
+##tops
+macbeth
+rebelled
+##icative
+##iot
+590
+elaborated
+frs
+uniformed
+##dberg
+259
+powerless
+priscilla
+stimulated
+980
+qc
+arboretum
+frustrating
+trieste
+bullock
+##nified
+enriched
+glistening
+intern
+##adia
+locus
+nouvelle
+ollie
+ike
+lash
+starboard
+ee
+tapestry
+headlined
+hove
+rigged
+##vite
+pollock
+##yme
+thrive
+clustered
+cas
+roi
+gleamed
+olympiad
+##lino
+pressured
+regimes
+##hosis
+##lick
+ripley
+##ophone
+kickoff
+gallon
+rockwell
+##arable
+crusader
+glue
+revolutions
+scrambling
+1714
+grover
+##jure
+englishman
+aztec
+263
+contemplating
+coven
+ipad
+preach
+triumphant
+tufts
+##esian
+rotational
+##phus
+328
+falkland
+##brates
+strewn
+clarissa
+rejoin
+environmentally
+glint
+banded
+drenched
+moat
+albanians
+johor
+rr
+maestro
+malley
+nouveau
+shaded
+taxonomy
+v6
+adhere
+bunk
+airfields
+##ritan
+1741
+encompass
+remington
+tran
+##erative
+amelie
+mazda
+friar
+morals
+passions
+##zai
+breadth
+vis
+##hae
+argus
+burnham
+caressing
+insider
+rudd
+##imov
+##mini
+##rso
+italianate
+murderous
+textual
+wainwright
+armada
+bam
+weave
+timer
+##taken
+##nh
+fra
+##crest
+ardent
+salazar
+taps
+tunis
+##ntino
+allegro
+gland
+philanthropic
+##chester
+implication
+##optera
+esq
+judas
+noticeably
+wynn
+##dara
+inched
+indexed
+crises
+villiers
+bandit
+royalties
+patterned
+cupboard
+interspersed
+accessory
+isla
+kendrick
+entourage
+stitches
+##esthesia
+headwaters
+##ior
+interlude
+distraught
+draught
+1727
+##basket
+biased
+sy
+transient
+triad
+subgenus
+adapting
+kidd
+shortstop
+##umatic
+dimly
+spiked
+mcleod
+reprint
+nellie
+pretoria
+windmill
+##cek
+singled
+##mps
+273
+reunite
+##orous
+747
+bankers
+outlying
+##omp
+##ports
+##tream
+apologies
+cosmetics
+patsy
+##deh
+##ocks
+##yson
+bender
+nantes
+serene
+##nad
+lucha
+mmm
+323
+##cius
+##gli
+cmll
+coinage
+nestor
+juarez
+##rook
+smeared
+sprayed
+twitching
+sterile
+irina
+embodied
+juveniles
+enveloped
+miscellaneous
+cancers
+dq
+gulped
+luisa
+crested
+swat
+donegal
+ref
+##anov
+##acker
+hearst
+mercantile
+##lika
+doorbell
+ua
+vicki
+##alla
+##som
+bilbao
+psychologists
+stryker
+sw
+horsemen
+turkmenistan
+wits
+##national
+anson
+mathew
+screenings
+##umb
+rihanna
+##agne
+##nessy
+aisles
+##iani
+##osphere
+hines
+kenton
+saskatoon
+tasha
+truncated
+##champ
+##itan
+mildred
+advises
+fredrik
+interpreting
+inhibitors
+##athi
+spectroscopy
+##hab
+##kong
+karim
+panda
+##oia
+##nail
+##vc
+conqueror
+kgb
+leukemia
+##dity
+arrivals
+cheered
+pisa
+phosphorus
+shielded
+##riated
+mammal
+unitarian
+urgently
+chopin
+sanitary
+##mission
+spicy
+drugged
+hinges
+##tort
+tipping
+trier
+impoverished
+westchester
+##caster
+267
+epoch
+nonstop
+##gman
+##khov
+aromatic
+centrally
+cerro
+##tively
+##vio
+billions
+modulation
+sedimentary
+283
+facilitating
+outrageous
+goldstein
+##eak
+##kt
+ld
+maitland
+penultimate
+pollard
+##dance
+fleets
+spaceship
+vertebrae
+##nig
+alcoholism
+als
+recital
+##bham
+##ference
+##omics
+m2
+##bm
+trois
+##tropical
+##в
+commemorates
+##meric
+marge
+##raction
+1643
+670
+cosmetic
+ravaged
+##ige
+catastrophe
+eng
+##shida
+albrecht
+arterial
+bellamy
+decor
+harmon
+##rde
+bulbs
+synchronized
+vito
+easiest
+shetland
+shielding
+wnba
+##glers
+##ssar
+##riam
+brianna
+cumbria
+##aceous
+##rard
+cores
+thayer
+##nsk
+brood
+hilltop
+luminous
+carts
+keynote
+larkin
+logos
+##cta
+##ا
+##mund
+##quay
+lilith
+tinted
+277
+wrestle
+mobilization
+##uses
+sequential
+siam
+bloomfield
+takahashi
+274
+##ieving
+presenters
+ringo
+blazed
+witty
+##oven
+##ignant
+devastation
+haydn
+harmed
+newt
+therese
+##peed
+gershwin
+molina
+rabbis
+sudanese
+001
+innate
+restarted
+##sack
+##fus
+slices
+wb
+##shah
+enroll
+hypothetical
+hysterical
+1743
+fabio
+indefinite
+warped
+##hg
+exchanging
+525
+unsuitable
+##sboro
+gallo
+1603
+bret
+cobalt
+homemade
+##hunter
+mx
+operatives
+##dhar
+terraces
+durable
+latch
+pens
+whorls
+##ctuated
+##eaux
+billing
+ligament
+succumbed
+##gly
+regulators
+spawn
+##brick
+##stead
+filmfare
+rochelle
+##nzo
+1725
+circumstance
+saber
+supplements
+##nsky
+##tson
+crowe
+wellesley
+carrot
+##9th
+##movable
+primate
+drury
+sincerely
+topical
+##mad
+##rao
+callahan
+kyiv
+smarter
+tits
+undo
+##yeh
+announcements
+anthologies
+barrio
+nebula
+##islaus
+##shaft
+##tyn
+bodyguards
+2021
+assassinate
+barns
+emmett
+scully
+##mah
+##yd
+##eland
+##tino
+##itarian
+demoted
+gorman
+lashed
+prized
+adventist
+writ
+##gui
+alla
+invertebrates
+##ausen
+1641
+amman
+1742
+align
+healy
+redistribution
+##gf
+##rize
+insulation
+##drop
+adherents
+hezbollah
+vitro
+ferns
+yanking
+269
+php
+registering
+uppsala
+cheerleading
+confines
+mischievous
+tully
+##ross
+49th
+docked
+roam
+stipulated
+pumpkin
+##bry
+prompt
+##ezer
+blindly
+shuddering
+craftsmen
+frail
+scented
+katharine
+scramble
+shaggy
+sponge
+helix
+zaragoza
+279
+##52
+43rd
+backlash
+fontaine
+seizures
+posse
+cowan
+nonfiction
+telenovela
+wwii
+hammered
+undone
+##gpur
+encircled
+irs
+##ivation
+artefacts
+oneself
+searing
+smallpox
+##belle
+##osaurus
+shandong
+breached
+upland
+blushing
+rankin
+infinitely
+psyche
+tolerated
+docking
+evicted
+##col
+unmarked
+##lving
+gnome
+lettering
+litres
+musique
+##oint
+benevolent
+##jal
+blackened
+##anna
+mccall
+racers
+tingle
+##ocene
+##orestation
+introductions
+radically
+292
+##hiff
+##باد
+1610
+1739
+munchen
+plead
+##nka
+condo
+scissors
+##sight
+##tens
+apprehension
+##cey
+##yin
+hallmark
+watering
+formulas
+sequels
+##llas
+aggravated
+bae
+commencing
+##building
+enfield
+prohibits
+marne
+vedic
+civilized
+euclidean
+jagger
+beforehand
+blasts
+dumont
+##arney
+##nem
+740
+conversions
+hierarchical
+rios
+simulator
+##dya
+##lellan
+hedges
+oleg
+thrusts
+shadowed
+darby
+maximize
+1744
+gregorian
+##nded
+##routed
+sham
+unspecified
+##hog
+emory
+factual
+##smo
+##tp
+fooled
+##rger
+ortega
+wellness
+marlon
+##oton
+##urance
+casket
+keating
+ley
+enclave
+##ayan
+char
+influencing
+jia
+##chenko
+412
+ammonia
+erebidae
+incompatible
+violins
+cornered
+##arat
+grooves
+astronauts
+columbian
+rampant
+fabrication
+kyushu
+mahmud
+vanish
+##dern
+mesopotamia
+##lete
+ict
+##rgen
+caspian
+kenji
+pitted
+##vered
+999
+grimace
+roanoke
+tchaikovsky
+twinned
+##analysis
+##awan
+xinjiang
+arias
+clemson
+kazakh
+sizable
+1662
+##khand
+##vard
+plunge
+tatum
+vittorio
+##nden
+cholera
+##dana
+##oper
+bracing
+indifference
+projectile
+superliga
+##chee
+realises
+upgrading
+299
+porte
+retribution
+##vies
+nk
+stil
+##resses
+ama
+bureaucracy
+blackberry
+bosch
+testosterone
+collapses
+greer
+##pathic
+ioc
+fifties
+malls
+##erved
+bao
+baskets
+adolescents
+siegfried
+##osity
+##tosis
+mantra
+detecting
+existent
+fledgling
+##cchi
+dissatisfied
+gan
+telecommunication
+mingled
+sobbed
+6000
+controversies
+outdated
+taxis
+##raus
+fright
+slams
+##lham
+##fect
+##tten
+detectors
+fetal
+tanned
+##uw
+fray
+goth
+olympian
+skipping
+mandates
+scratches
+sheng
+unspoken
+hyundai
+tracey
+hotspur
+restrictive
+##buch
+americana
+mundo
+##bari
+burroughs
+diva
+vulcan
+##6th
+distinctions
+thumping
+##ngen
+mikey
+sheds
+fide
+rescues
+springsteen
+vested
+valuation
+##ece
+##ely
+pinnacle
+rake
+sylvie
+##edo
+almond
+quivering
+##irus
+alteration
+faltered
+##wad
+51st
+hydra
+ticked
+##kato
+recommends
+##dicated
+antigua
+arjun
+stagecoach
+wilfred
+trickle
+pronouns
+##pon
+aryan
+nighttime
+##anian
+gall
+pea
+stitch
+##hei
+leung
+milos
+##dini
+eritrea
+nexus
+starved
+snowfall
+kant
+parasitic
+cot
+discus
+hana
+strikers
+appleton
+kitchens
+##erina
+##partisan
+##itha
+##vius
+disclose
+metis
+##channel
+1701
+tesla
+##vera
+fitch
+1735
+blooded
+##tila
+decimal
+##tang
+##bai
+cyclones
+eun
+bottled
+peas
+pensacola
+basha
+bolivian
+crabs
+boil
+lanterns
+partridge
+roofed
+1645
+necks
+##phila
+opined
+patting
+##kla
+##lland
+chuckles
+volta
+whereupon
+##nche
+devout
+euroleague
+suicidal
+##dee
+inherently
+involuntary
+knitting
+nasser
+##hide
+puppets
+colourful
+courageous
+southend
+stills
+miraculous
+hodgson
+richer
+rochdale
+ethernet
+greta
+uniting
+prism
+umm
+##haya
+##itical
+##utation
+deterioration
+pointe
+prowess
+##ropriation
+lids
+scranton
+billings
+subcontinent
+##koff
+##scope
+brute
+kellogg
+psalms
+degraded
+##vez
+stanisław
+##ructured
+ferreira
+pun
+astonishing
+gunnar
+##yat
+arya
+prc
+gottfried
+##tight
+excursion
+##ographer
+dina
+##quil
+##nare
+huffington
+illustrious
+wilbur
+gundam
+verandah
+##zard
+naacp
+##odle
+constructive
+fjord
+kade
+##naud
+generosity
+thrilling
+baseline
+cayman
+frankish
+plastics
+accommodations
+zoological
+##fting
+cedric
+qb
+motorized
+##dome
+##otted
+squealed
+tackled
+canucks
+budgets
+situ
+asthma
+dail
+gabled
+grasslands
+whimpered
+writhing
+judgments
+##65
+minnie
+pv
+##carbon
+bananas
+grille
+domes
+monique
+odin
+maguire
+markham
+tierney
+##estra
+##chua
+libel
+poke
+speedy
+atrium
+laval
+notwithstanding
+##edly
+fai
+kala
+##sur
+robb
+##sma
+listings
+luz
+supplementary
+tianjin
+##acing
+enzo
+jd
+ric
+scanner
+croats
+transcribed
+##49
+arden
+cv
+##hair
+##raphy
+##lver
+##uy
+357
+seventies
+staggering
+alam
+horticultural
+hs
+regression
+timbers
+blasting
+##ounded
+montagu
+manipulating
+##cit
+catalytic
+1550
+troopers
+##meo
+condemnation
+fitzpatrick
+##oire
+##roved
+inexperienced
+1670
+castes
+##lative
+outing
+314
+dubois
+flicking
+quarrel
+ste
+learners
+1625
+iq
+whistled
+##class
+282
+classify
+tariffs
+temperament
+355
+folly
+liszt
+##yles
+immersed
+jordanian
+ceasefire
+apparel
+extras
+maru
+fished
+##bio
+harta
+stockport
+assortment
+craftsman
+paralysis
+transmitters
+##cola
+blindness
+##wk
+fatally
+proficiency
+solemnly
+##orno
+repairing
+amore
+groceries
+ultraviolet
+##chase
+schoolhouse
+##tua
+resurgence
+nailed
+##otype
+##×
+ruse
+saliva
+diagrams
+##tructing
+albans
+rann
+thirties
+1b
+antennas
+hilarious
+cougars
+paddington
+stats
+##eger
+breakaway
+ipod
+reza
+authorship
+prohibiting
+scoffed
+##etz
+##ttle
+conscription
+defected
+trondheim
+##fires
+ivanov
+keenan
+##adan
+##ciful
+##fb
+##slow
+locating
+##ials
+##tford
+cadiz
+basalt
+blankly
+interned
+rags
+rattling
+##tick
+carpathian
+reassured
+sync
+bum
+guildford
+iss
+staunch
+##onga
+astronomers
+sera
+sofie
+emergencies
+susquehanna
+##heard
+duc
+mastery
+vh1
+williamsburg
+bayer
+buckled
+craving
+##khan
+##rdes
+bloomington
+##write
+alton
+barbecue
+##bians
+justine
+##hri
+##ndt
+delightful
+smartphone
+newtown
+photon
+retrieval
+peugeot
+hissing
+##monium
+##orough
+flavors
+lighted
+relaunched
+tainted
+##games
+##lysis
+anarchy
+microscopic
+hopping
+adept
+evade
+evie
+##beau
+inhibit
+sinn
+adjustable
+hurst
+intuition
+wilton
+cisco
+44th
+lawful
+lowlands
+stockings
+thierry
+##dalen
+##hila
+##nai
+fates
+prank
+tb
+maison
+lobbied
+provocative
+1724
+4a
+utopia
+##qual
+carbonate
+gujarati
+purcell
+##rford
+curtiss
+##mei
+overgrown
+arenas
+mediation
+swallows
+##rnik
+respectful
+turnbull
+##hedron
+##hope
+alyssa
+ozone
+##ʻi
+ami
+gestapo
+johansson
+snooker
+canteen
+cuff
+declines
+empathy
+stigma
+##ags
+##iner
+##raine
+taxpayers
+gui
+volga
+##wright
+##copic
+lifespan
+overcame
+tattooed
+enactment
+giggles
+##ador
+##camp
+barrington
+bribe
+obligatory
+orbiting
+peng
+##enas
+elusive
+sucker
+##vating
+cong
+hardship
+empowered
+anticipating
+estrada
+cryptic
+greasy
+detainees
+planck
+sudbury
+plaid
+dod
+marriott
+kayla
+##ears
+##vb
+##zd
+mortally
+##hein
+cognition
+radha
+319
+liechtenstein
+meade
+richly
+argyle
+harpsichord
+liberalism
+trumpets
+lauded
+tyrant
+salsa
+tiled
+lear
+promoters
+reused
+slicing
+trident
+##chuk
+##gami
+##lka
+cantor
+checkpoint
+##points
+gaul
+leger
+mammalian
+##tov
+##aar
+##schaft
+doha
+frenchman
+nirvana
+##vino
+delgado
+headlining
+##eron
+##iography
+jug
+tko
+1649
+naga
+intersections
+##jia
+benfica
+nawab
+##suka
+ashford
+gulp
+##deck
+##vill
+##rug
+brentford
+frazier
+pleasures
+dunne
+potsdam
+shenzhen
+dentistry
+##tec
+flanagan
+##dorff
+##hear
+chorale
+dinah
+prem
+quezon
+##rogated
+relinquished
+sutra
+terri
+##pani
+flaps
+##rissa
+poly
+##rnet
+homme
+aback
+##eki
+linger
+womb
+##kson
+##lewood
+doorstep
+orthodoxy
+threaded
+westfield
+##rval
+dioceses
+fridays
+subsided
+##gata
+loyalists
+##biotic
+##ettes
+letterman
+lunatic
+prelate
+tenderly
+invariably
+souza
+thug
+winslow
+##otide
+furlongs
+gogh
+jeopardy
+##runa
+pegasus
+##umble
+humiliated
+standalone
+tagged
+##roller
+freshmen
+klan
+##bright
+attaining
+initiating
+transatlantic
+logged
+viz
+##uance
+1723
+combatants
+intervening
+stephane
+chieftain
+despised
+grazed
+317
+cdc
+galveston
+godzilla
+macro
+simulate
+##planes
+parades
+##esses
+960
+##ductive
+##unes
+equator
+overdose
+##cans
+##hosh
+##lifting
+joshi
+epstein
+sonora
+treacherous
+aquatics
+manchu
+responsive
+##sation
+supervisory
+##christ
+##llins
+##ibar
+##balance
+##uso
+kimball
+karlsruhe
+mab
+##emy
+ignores
+phonetic
+reuters
+spaghetti
+820
+almighty
+danzig
+rumbling
+tombstone
+designations
+lured
+outset
+##felt
+supermarkets
+##wt
+grupo
+kei
+kraft
+susanna
+##blood
+comprehension
+genealogy
+##aghan
+##verted
+redding
+##ythe
+1722
+bowing
+##pore
+##roi
+lest
+sharpened
+fulbright
+valkyrie
+sikhs
+##unds
+swans
+bouquet
+merritt
+##tage
+##venting
+commuted
+redhead
+clerks
+leasing
+cesare
+dea
+hazy
+##vances
+fledged
+greenfield
+servicemen
+##gical
+armando
+blackout
+dt
+sagged
+downloadable
+intra
+potion
+pods
+##4th
+##mism
+xp
+attendants
+gambia
+stale
+##ntine
+plump
+asteroids
+rediscovered
+buds
+flea
+hive
+##neas
+1737
+classifications
+debuts
+##eles
+olympus
+scala
+##eurs
+##gno
+##mute
+hummed
+sigismund
+visuals
+wiggled
+await
+pilasters
+clench
+sulfate
+##ances
+bellevue
+enigma
+trainee
+snort
+##sw
+clouded
+denim
+##rank
+##rder
+churning
+hartman
+lodges
+riches
+sima
+##missible
+accountable
+socrates
+regulates
+mueller
+##cr
+1702
+avoids
+solids
+himalayas
+nutrient
+pup
+##jevic
+squat
+fades
+nec
+##lates
+##pina
+##rona
+##ου
+privateer
+tequila
+##gative
+##mpton
+apt
+hornet
+immortals
+##dou
+asturias
+cleansing
+dario
+##rries
+##anta
+etymology
+servicing
+zhejiang
+##venor
+##nx
+horned
+erasmus
+rayon
+relocating
+£10
+##bags
+escalated
+promenade
+stubble
+2010s
+artisans
+axial
+liquids
+mora
+sho
+yoo
+##tsky
+bundles
+oldies
+##nally
+notification
+bastion
+##ths
+sparkle
+##lved
+1728
+leash
+pathogen
+highs
+##hmi
+immature
+880
+gonzaga
+ignatius
+mansions
+monterrey
+sweets
+bryson
+##loe
+polled
+regatta
+brightest
+pei
+rosy
+squid
+hatfield
+payroll
+addict
+meath
+cornerback
+heaviest
+lodging
+##mage
+capcom
+rippled
+##sily
+barnet
+mayhem
+ymca
+snuggled
+rousseau
+##cute
+blanchard
+284
+fragmented
+leighton
+chromosomes
+risking
+##md
+##strel
+##utter
+corinne
+coyotes
+cynical
+hiroshi
+yeomanry
+##ractive
+ebook
+grading
+mandela
+plume
+agustin
+magdalene
+##rkin
+bea
+femme
+trafford
+##coll
+##lun
+##tance
+52nd
+fourier
+upton
+##mental
+camilla
+gust
+iihf
+islamabad
+longevity
+##kala
+feldman
+netting
+##rization
+endeavour
+foraging
+mfa
+orr
+##open
+greyish
+contradiction
+graz
+##ruff
+handicapped
+marlene
+tweed
+oaxaca
+spp
+campos
+miocene
+pri
+configured
+cooks
+pluto
+cozy
+pornographic
+##entes
+70th
+fairness
+glided
+jonny
+lynne
+rounding
+sired
+##emon
+##nist
+remade
+uncover
+##mack
+complied
+lei
+newsweek
+##jured
+##parts
+##enting
+##pg
+293
+finer
+guerrillas
+athenian
+deng
+disused
+stepmother
+accuse
+gingerly
+seduction
+521
+confronting
+##walker
+##going
+gora
+nostalgia
+sabres
+virginity
+wrenched
+##minated
+syndication
+wielding
+eyre
+##56
+##gnon
+##igny
+behaved
+taxpayer
+sweeps
+##growth
+childless
+gallant
+##ywood
+amplified
+geraldine
+scrape
+##ffi
+babylonian
+fresco
+##rdan
+##kney
+##position
+1718
+restricting
+tack
+fukuoka
+osborn
+selector
+partnering
+##dlow
+318
+gnu
+kia
+tak
+whitley
+gables
+##54
+##mania
+mri
+softness
+immersion
+##bots
+##evsky
+1713
+chilling
+insignificant
+pcs
+##uis
+elites
+lina
+purported
+supplemental
+teaming
+##americana
+##dding
+##inton
+proficient
+rouen
+##nage
+##rret
+niccolo
+selects
+##bread
+fluffy
+1621
+gruff
+knotted
+mukherjee
+polgara
+thrash
+nicholls
+secluded
+smoothing
+thru
+corsica
+loaf
+whitaker
+inquiries
+##rrier
+##kam
+indochina
+289
+marlins
+myles
+peking
+##tea
+extracts
+pastry
+superhuman
+connacht
+vogel
+##ditional
+##het
+##udged
+##lash
+gloss
+quarries
+refit
+teaser
+##alic
+##gaon
+20s
+materialized
+sling
+camped
+pickering
+tung
+tracker
+pursuant
+##cide
+cranes
+soc
+##cini
+##typical
+##viere
+anhalt
+overboard
+workout
+chores
+fares
+orphaned
+stains
+##logie
+fenton
+surpassing
+joyah
+triggers
+##itte
+grandmaster
+##lass
+##lists
+clapping
+fraudulent
+ledger
+nagasaki
+##cor
+##nosis
+##tsa
+eucalyptus
+tun
+##icio
+##rney
+##tara
+dax
+heroism
+ina
+wrexham
+onboard
+unsigned
+##dates
+moshe
+galley
+winnie
+droplets
+exiles
+praises
+watered
+noodles
+##aia
+fein
+adi
+leland
+multicultural
+stink
+bingo
+comets
+erskine
+modernized
+canned
+constraint
+domestically
+chemotherapy
+featherweight
+stifled
+##mum
+darkly
+irresistible
+refreshing
+hasty
+isolate
+##oys
+kitchener
+planners
+##wehr
+cages
+yarn
+implant
+toulon
+elects
+childbirth
+yue
+##lind
+##lone
+cn
+rightful
+sportsman
+junctions
+remodeled
+specifies
+##rgh
+291
+##oons
+complimented
+##urgent
+lister
+ot
+##logic
+bequeathed
+cheekbones
+fontana
+gabby
+##dial
+amadeus
+corrugated
+maverick
+resented
+triangles
+##hered
+##usly
+nazareth
+tyrol
+1675
+assent
+poorer
+sectional
+aegean
+##cous
+296
+nylon
+ghanaian
+##egorical
+##weig
+cushions
+forbid
+fusiliers
+obstruction
+somerville
+##scia
+dime
+earrings
+elliptical
+leyte
+oder
+polymers
+timmy
+atm
+midtown
+piloted
+settles
+continual
+externally
+mayfield
+##uh
+enrichment
+henson
+keane
+persians
+1733
+benji
+braden
+pep
+324
+##efe
+contenders
+pepsi
+valet
+##isches
+298
+##asse
+##earing
+goofy
+stroll
+##amen
+authoritarian
+occurrences
+adversary
+ahmedabad
+tangent
+toppled
+dorchester
+1672
+modernism
+marxism
+islamist
+charlemagne
+exponential
+racks
+unicode
+brunette
+mbc
+pic
+skirmish
+##bund
+##lad
+##powered
+##yst
+hoisted
+messina
+shatter
+##ctum
+jedi
+vantage
+##music
+##neil
+clemens
+mahmoud
+corrupted
+authentication
+lowry
+nils
+##washed
+omnibus
+wounding
+jillian
+##itors
+##opped
+serialized
+narcotics
+handheld
+##arm
+##plicity
+intersecting
+stimulating
+##onis
+crate
+fellowships
+hemingway
+casinos
+climatic
+fordham
+copeland
+drip
+beatty
+leaflets
+robber
+brothel
+madeira
+##hedral
+sphinx
+ultrasound
+##vana
+valor
+forbade
+leonid
+villas
+##aldo
+duane
+marquez
+##cytes
+disadvantaged
+forearms
+kawasaki
+reacts
+consular
+lax
+uncles
+uphold
+##hopper
+concepcion
+dorsey
+lass
+##izan
+arching
+passageway
+1708
+researches
+tia
+internationals
+##graphs
+##opers
+distinguishes
+javanese
+divert
+##uven
+plotted
+##listic
+##rwin
+##erik
+##tify
+affirmative
+signifies
+validation
+##bson
+kari
+felicity
+georgina
+zulu
+##eros
+##rained
+##rath
+overcoming
+##dot
+argyll
+##rbin
+1734
+chiba
+ratification
+windy
+earls
+parapet
+##marks
+hunan
+pristine
+astrid
+punta
+##gart
+brodie
+##kota
+##oder
+malaga
+minerva
+rouse
+##phonic
+bellowed
+pagoda
+portals
+reclamation
+##gur
+##odies
+##⁄₄
+parentheses
+quoting
+allergic
+palette
+showcases
+benefactor
+heartland
+nonlinear
+##tness
+bladed
+cheerfully
+scans
+##ety
+##hone
+1666
+girlfriends
+pedersen
+hiram
+sous
+##liche
+##nator
+1683
+##nery
+##orio
+##umen
+bobo
+primaries
+smiley
+##cb
+unearthed
+uniformly
+fis
+metadata
+1635
+ind
+##oted
+recoil
+##titles
+##tura
+##ια
+406
+hilbert
+jamestown
+mcmillan
+tulane
+seychelles
+##frid
+antics
+coli
+fated
+stucco
+##grants
+1654
+bulky
+accolades
+arrays
+caledonian
+carnage
+optimism
+puebla
+##tative
+##cave
+enforcing
+rotherham
+seo
+dunlop
+aeronautics
+chimed
+incline
+zoning
+archduke
+hellenistic
+##oses
+##sions
+candi
+thong
+##ople
+magnate
+rustic
+##rsk
+projective
+slant
+##offs
+danes
+hollis
+vocalists
+##ammed
+congenital
+contend
+gesellschaft
+##ocating
+##pressive
+douglass
+quieter
+##cm
+##kshi
+howled
+salim
+spontaneously
+townsville
+buena
+southport
+##bold
+kato
+1638
+faerie
+stiffly
+##vus
+##rled
+297
+flawless
+realising
+taboo
+##7th
+bytes
+straightening
+356
+jena
+##hid
+##rmin
+cartwright
+berber
+bertram
+soloists
+411
+noses
+417
+coping
+fission
+hardin
+inca
+##cen
+1717
+mobilized
+vhf
+##raf
+biscuits
+curate
+##85
+##anial
+331
+gaunt
+neighbourhoods
+1540
+##abas
+blanca
+bypassed
+sockets
+behold
+coincidentally
+##bane
+nara
+shave
+splinter
+terrific
+##arion
+##erian
+commonplace
+juris
+redwood
+waistband
+boxed
+caitlin
+fingerprints
+jennie
+naturalized
+##ired
+balfour
+craters
+jody
+bungalow
+hugely
+quilt
+glitter
+pigeons
+undertaker
+bulging
+constrained
+goo
+##sil
+##akh
+assimilation
+reworked
+##person
+persuasion
+##pants
+felicia
+##cliff
+##ulent
+1732
+explodes
+##dun
+##inium
+##zic
+lyman
+vulture
+hog
+overlook
+begs
+northwards
+ow
+spoil
+##urer
+fatima
+favorably
+accumulate
+sargent
+sorority
+corresponded
+dispersal
+kochi
+toned
+##imi
+##lita
+internacional
+newfound
+##agger
+##lynn
+##rigue
+booths
+peanuts
+##eborg
+medicare
+muriel
+nur
+##uram
+crates
+millennia
+pajamas
+worsened
+##breakers
+jimi
+vanuatu
+yawned
+##udeau
+carousel
+##hony
+hurdle
+##ccus
+##mounted
+##pod
+rv
+##eche
+airship
+ambiguity
+compulsion
+recapture
+##claiming
+arthritis
+##osomal
+1667
+asserting
+ngc
+sniffing
+dade
+discontent
+glendale
+ported
+##amina
+defamation
+rammed
+##scent
+fling
+livingstone
+##fleet
+875
+##ppy
+apocalyptic
+comrade
+lcd
+##lowe
+cessna
+eine
+persecuted
+subsistence
+demi
+hoop
+reliefs
+710
+coptic
+progressing
+stemmed
+perpetrators
+1665
+priestess
+##nio
+dobson
+ebony
+rooster
+itf
+tortricidae
+##bbon
+##jian
+cleanup
+##jean
+##øy
+1721
+eighties
+taxonomic
+holiness
+##hearted
+##spar
+antilles
+showcasing
+stabilized
+##nb
+gia
+mascara
+michelangelo
+dawned
+##uria
+##vinsky
+extinguished
+fitz
+grotesque
+£100
+##fera
+##loid
+##mous
+barges
+neue
+throbbed
+cipher
+johnnie
+##a1
+##mpt
+outburst
+##swick
+spearheaded
+administrations
+c1
+heartbreak
+pixels
+pleasantly
+##enay
+lombardy
+plush
+##nsed
+bobbie
+##hly
+reapers
+tremor
+xiang
+minogue
+substantive
+hitch
+barak
+##wyl
+kwan
+##encia
+910
+obscene
+elegance
+indus
+surfer
+bribery
+conserve
+##hyllum
+##masters
+horatio
+##fat
+apes
+rebound
+psychotic
+##pour
+iteration
+##mium
+##vani
+botanic
+horribly
+antiques
+dispose
+paxton
+##hli
+##wg
+timeless
+1704
+disregard
+engraver
+hounds
+##bau
+##version
+looted
+uno
+facilitates
+groans
+masjid
+rutland
+antibody
+disqualification
+decatur
+footballers
+quake
+slacks
+48th
+rein
+scribe
+stabilize
+commits
+exemplary
+tho
+##hort
+##chison
+pantry
+traversed
+##hiti
+disrepair
+identifiable
+vibrated
+baccalaureate
+##nnis
+csa
+interviewing
+##iensis
+##raße
+greaves
+wealthiest
+343
+classed
+jogged
+£5
+##58
+##atal
+illuminating
+knicks
+respecting
+##uno
+scrubbed
+##iji
+##dles
+kruger
+moods
+growls
+raider
+silvia
+chefs
+kam
+vr
+cree
+percival
+##terol
+gunter
+counterattack
+defiant
+henan
+ze
+##rasia
+##riety
+equivalence
+submissions
+##fra
+##thor
+bautista
+mechanically
+##heater
+cornice
+herbal
+templar
+##mering
+outputs
+ruining
+ligand
+renumbered
+extravagant
+mika
+blockbuster
+eta
+insurrection
+##ilia
+darkening
+ferocious
+pianos
+strife
+kinship
+##aer
+melee
+##anor
+##iste
+##may
+##oue
+decidedly
+weep
+##jad
+##missive
+##ppel
+354
+puget
+unease
+##gnant
+1629
+hammering
+kassel
+ob
+wessex
+##lga
+bromwich
+egan
+paranoia
+utilization
+##atable
+##idad
+contradictory
+provoke
+##ols
+##ouring
+##tangled
+knesset
+##very
+##lette
+plumbing
+##sden
+##¹
+greensboro
+occult
+sniff
+338
+zev
+beaming
+gamer
+haggard
+mahal
+##olt
+##pins
+mendes
+utmost
+briefing
+gunnery
+##gut
+##pher
+##zh
+##rok
+1679
+khalifa
+sonya
+##boot
+principals
+urbana
+wiring
+##liffe
+##minating
+##rrado
+dahl
+nyu
+skepticism
+np
+townspeople
+ithaca
+lobster
+somethin
+##fur
+##arina
+##−1
+freighter
+zimmerman
+biceps
+contractual
+##herton
+amend
+hurrying
+subconscious
+##anal
+336
+meng
+clermont
+spawning
+##eia
+##lub
+dignitaries
+impetus
+snacks
+spotting
+twigs
+##bilis
+##cz
+##ouk
+libertadores
+nic
+skylar
+##aina
+##firm
+gustave
+asean
+##anum
+dieter
+legislatures
+flirt
+bromley
+trolls
+umar
+##bbies
+##tyle
+blah
+parc
+bridgeport
+crank
+negligence
+##nction
+46th
+constantin
+molded
+bandages
+seriousness
+00pm
+siegel
+carpets
+compartments
+upbeat
+statehood
+##dner
+##edging
+marko
+730
+platt
+##hane
+paving
+##iy
+1738
+abbess
+impatience
+limousine
+nbl
+##talk
+441
+lucille
+mojo
+nightfall
+robbers
+##nais
+karel
+brisk
+calves
+replicate
+ascribed
+telescopes
+##olf
+intimidated
+##reen
+ballast
+specialization
+##sit
+aerodynamic
+caliphate
+rainer
+visionary
+##arded
+epsilon
+##aday
+##onte
+aggregation
+auditory
+boosted
+reunification
+kathmandu
+loco
+robyn
+402
+acknowledges
+appointing
+humanoid
+newell
+redeveloped
+restraints
+##tained
+barbarians
+chopper
+1609
+italiana
+##lez
+##lho
+investigates
+wrestlemania
+##anies
+##bib
+690
+##falls
+creaked
+dragoons
+gravely
+minions
+stupidity
+volley
+##harat
+##week
+musik
+##eries
+##uously
+fungal
+massimo
+semantics
+malvern
+##ahl
+##pee
+discourage
+embryo
+imperialism
+1910s
+profoundly
+##ddled
+jiangsu
+sparkled
+stat
+##holz
+sweatshirt
+tobin
+##iction
+sneered
+##cheon
+##oit
+brit
+causal
+smyth
+##neuve
+diffuse
+perrin
+silvio
+##ipes
+##recht
+detonated
+iqbal
+selma
+##nism
+##zumi
+roasted
+##riders
+tay
+##ados
+##mament
+##mut
+##rud
+840
+completes
+nipples
+cfa
+flavour
+hirsch
+##laus
+calderon
+sneakers
+moravian
+##ksha
+1622
+rq
+294
+##imeters
+bodo
+##isance
+##pre
+##ronia
+anatomical
+excerpt
+##lke
+dh
+kunst
+##tablished
+##scoe
+biomass
+panted
+unharmed
+gael
+housemates
+montpellier
+##59
+coa
+rodents
+tonic
+hickory
+singleton
+##taro
+451
+1719
+aldo
+breaststroke
+dempsey
+och
+rocco
+##cuit
+merton
+dissemination
+midsummer
+serials
+##idi
+haji
+polynomials
+##rdon
+gs
+enoch
+prematurely
+shutter
+taunton
+£3
+##grating
+##inates
+archangel
+harassed
+##asco
+326
+archway
+dazzling
+##ecin
+1736
+sumo
+wat
+##kovich
+1086
+honneur
+##ently
+##nostic
+##ttal
+##idon
+1605
+403
+1716
+blogger
+rents
+##gnan
+hires
+##ikh
+##dant
+howie
+##rons
+handler
+retracted
+shocks
+1632
+arun
+duluth
+kepler
+trumpeter
+##lary
+peeking
+seasoned
+trooper
+##mara
+laszlo
+##iciencies
+##rti
+heterosexual
+##inatory
+##ssion
+indira
+jogging
+##inga
+##lism
+beit
+dissatisfaction
+malice
+##ately
+nedra
+peeling
+##rgeon
+47th
+stadiums
+475
+vertigo
+##ains
+iced
+restroom
+##plify
+##tub
+illustrating
+pear
+##chner
+##sibility
+inorganic
+rappers
+receipts
+watery
+##kura
+lucinda
+##oulos
+reintroduced
+##8th
+##tched
+gracefully
+saxons
+nutritional
+wastewater
+rained
+favourites
+bedrock
+fisted
+hallways
+likeness
+upscale
+##lateral
+1580
+blinds
+prequel
+##pps
+##tama
+deter
+humiliating
+restraining
+tn
+vents
+1659
+laundering
+recess
+rosary
+tractors
+coulter
+federer
+##ifiers
+##plin
+persistence
+##quitable
+geschichte
+pendulum
+quakers
+##beam
+bassett
+pictorial
+buffet
+koln
+##sitor
+drills
+reciprocal
+shooters
+##57
+##cton
+##tees
+converge
+pip
+dmitri
+donnelly
+yamamoto
+aqua
+azores
+demographics
+hypnotic
+spitfire
+suspend
+wryly
+roderick
+##rran
+sebastien
+##asurable
+mavericks
+##fles
+##200
+himalayan
+prodigy
+##iance
+transvaal
+demonstrators
+handcuffs
+dodged
+mcnamara
+sublime
+1726
+crazed
+##efined
+##till
+ivo
+pondered
+reconciled
+shrill
+sava
+##duk
+bal
+cad
+heresy
+jaipur
+goran
+##nished
+341
+lux
+shelly
+whitehall
+##hre
+israelis
+peacekeeping
+##wled
+1703
+demetrius
+ousted
+##arians
+##zos
+beale
+anwar
+backstroke
+raged
+shrinking
+cremated
+##yck
+benign
+towing
+wadi
+darmstadt
+landfill
+parana
+soothe
+colleen
+sidewalks
+mayfair
+tumble
+hepatitis
+ferrer
+superstructure
+##gingly
+##urse
+##wee
+anthropological
+translators
+##mies
+closeness
+hooves
+##pw
+mondays
+##roll
+##vita
+landscaping
+##urized
+purification
+sock
+thorns
+thwarted
+jalan
+tiberius
+##taka
+saline
+##rito
+confidently
+khyber
+sculptors
+##ij
+brahms
+hammersmith
+inspectors
+battista
+fivb
+fragmentation
+hackney
+##uls
+arresting
+exercising
+antoinette
+bedfordshire
+##zily
+dyed
+##hema
+1656
+racetrack
+variability
+##tique
+1655
+austrians
+deteriorating
+madman
+theorists
+aix
+lehman
+weathered
+1731
+decreed
+eruptions
+1729
+flaw
+quinlan
+sorbonne
+flutes
+nunez
+1711
+adored
+downwards
+fable
+rasped
+1712
+moritz
+mouthful
+renegade
+shivers
+stunts
+dysfunction
+restrain
+translit
+327
+pancakes
+##avio
+##cision
+##tray
+351
+vial
+##lden
+bain
+##maid
+##oxide
+chihuahua
+malacca
+vimes
+##rba
+##rnier
+1664
+donnie
+plaques
+##ually
+337
+bangs
+floppy
+huntsville
+loretta
+nikolay
+##otte
+eater
+handgun
+ubiquitous
+##hett
+eras
+zodiac
+1634
+##omorphic
+1820s
+##zog
+cochran
+##bula
+##lithic
+warring
+##rada
+dalai
+excused
+blazers
+mcconnell
+reeling
+bot
+este
+##abi
+geese
+hoax
+taxon
+##bla
+guitarists
+##icon
+condemning
+hunts
+inversion
+moffat
+taekwondo
+##lvis
+1624
+stammered
+##rest
+##rzy
+sousa
+fundraiser
+marylebone
+navigable
+uptown
+cabbage
+daniela
+salman
+shitty
+whimper
+##kian
+##utive
+programmers
+protections
+rm
+##rmi
+##rued
+forceful
+##enes
+fuss
+##tao
+##wash
+brat
+oppressive
+reykjavik
+spartak
+ticking
+##inkles
+##kiewicz
+adolph
+horst
+maui
+protege
+straighten
+cpc
+landau
+concourse
+clements
+resultant
+##ando
+imaginative
+joo
+reactivated
+##rem
+##ffled
+##uising
+consultative
+##guide
+flop
+kaitlyn
+mergers
+parenting
+somber
+##vron
+supervise
+vidhan
+##imum
+courtship
+exemplified
+harmonies
+medallist
+refining
+##rrow
+##ка
+amara
+##hum
+780
+goalscorer
+sited
+overshadowed
+rohan
+displeasure
+secretive
+multiplied
+osman
+##orth
+engravings
+padre
+##kali
+##veda
+miniatures
+mis
+##yala
+clap
+pali
+rook
+##cana
+1692
+57th
+antennae
+astro
+oskar
+1628
+bulldog
+crotch
+hackett
+yucatan
+##sure
+amplifiers
+brno
+ferrara
+migrating
+##gree
+thanking
+turing
+##eza
+mccann
+ting
+andersson
+onslaught
+gaines
+ganga
+incense
+standardization
+##mation
+sentai
+scuba
+stuffing
+turquoise
+waivers
+alloys
+##vitt
+regaining
+vaults
+##clops
+##gizing
+digger
+furry
+memorabilia
+probing
+##iad
+payton
+rec
+deutschland
+filippo
+opaque
+seamen
+zenith
+afrikaans
+##filtration
+disciplined
+inspirational
+##merie
+banco
+confuse
+grafton
+tod
+##dgets
+championed
+simi
+anomaly
+biplane
+##ceptive
+electrode
+##para
+1697
+cleavage
+crossbow
+swirl
+informant
+##lars
+##osta
+afi
+bonfire
+spec
+##oux
+lakeside
+slump
+##culus
+##lais
+##qvist
+##rrigan
+1016
+facades
+borg
+inwardly
+cervical
+xl
+pointedly
+050
+stabilization
+##odon
+chests
+1699
+hacked
+ctv
+orthogonal
+suzy
+##lastic
+gaulle
+jacobite
+rearview
+##cam
+##erted
+ashby
+##drik
+##igate
+##mise
+##zbek
+affectionately
+canine
+disperse
+latham
+##istles
+##ivar
+spielberg
+##orin
+##idium
+ezekiel
+cid
+##sg
+durga
+middletown
+##cina
+customized
+frontiers
+harden
+##etano
+##zzy
+1604
+bolsheviks
+##66
+coloration
+yoko
+##bedo
+briefs
+slabs
+debra
+liquidation
+plumage
+##oin
+blossoms
+dementia
+subsidy
+1611
+proctor
+relational
+jerseys
+parochial
+ter
+##ici
+esa
+peshawar
+cavalier
+loren
+cpi
+idiots
+shamrock
+1646
+dutton
+malabar
+mustache
+##endez
+##ocytes
+referencing
+terminates
+marche
+yarmouth
+##sop
+acton
+mated
+seton
+subtly
+baptised
+beige
+extremes
+jolted
+kristina
+telecast
+##actic
+safeguard
+waldo
+##baldi
+##bular
+endeavors
+sloppy
+subterranean
+##ensburg
+##itung
+delicately
+pigment
+tq
+##scu
+1626
+##ound
+collisions
+coveted
+herds
+##personal
+##meister
+##nberger
+chopra
+##ricting
+abnormalities
+defective
+galician
+lucie
+##dilly
+alligator
+likened
+##genase
+burundi
+clears
+complexion
+derelict
+deafening
+diablo
+fingered
+champaign
+dogg
+enlist
+isotope
+labeling
+mrna
+##erre
+brilliance
+marvelous
+##ayo
+1652
+crawley
+ether
+footed
+dwellers
+deserts
+hamish
+rubs
+warlock
+skimmed
+##lizer
+870
+buick
+embark
+heraldic
+irregularities
+##ajan
+kiara
+##kulam
+##ieg
+antigen
+kowalski
+##lge
+oakley
+visitation
+##mbit
+vt
+##suit
+1570
+murderers
+##miento
+##rites
+chimneys
+##sling
+condemn
+custer
+exchequer
+havre
+##ghi
+fluctuations
+##rations
+dfb
+hendricks
+vaccines
+##tarian
+nietzsche
+biking
+juicy
+##duced
+brooding
+scrolling
+selangor
+##ragan
+352
+annum
+boomed
+seminole
+sugarcane
+##dna
+departmental
+dismissing
+innsbruck
+arteries
+ashok
+batavia
+daze
+kun
+overtook
+##rga
+##tlan
+beheaded
+gaddafi
+holm
+electronically
+faulty
+galilee
+fractures
+kobayashi
+##lized
+gunmen
+magma
+aramaic
+mala
+eastenders
+inference
+messengers
+bf
+##qu
+407
+bathrooms
+##vere
+1658
+flashbacks
+ideally
+misunderstood
+##jali
+##weather
+mendez
+##grounds
+505
+uncanny
+##iii
+1709
+friendships
+##nbc
+sacrament
+accommodated
+reiterated
+logistical
+pebbles
+thumped
+##escence
+administering
+decrees
+drafts
+##flight
+##cased
+##tula
+futuristic
+picket
+intimidation
+winthrop
+##fahan
+interfered
+339
+afar
+francoise
+morally
+uta
+cochin
+croft
+dwarfs
+##bruck
+##dents
+##nami
+biker
+##hner
+##meral
+nano
+##isen
+##ometric
+##pres
+##ан
+brightened
+meek
+parcels
+securely
+gunners
+##jhl
+##zko
+agile
+hysteria
+##lten
+##rcus
+bukit
+champs
+chevy
+cuckoo
+leith
+sadler
+theologians
+welded
+##section
+1663
+jj
+plurality
+xander
+##rooms
+##formed
+shredded
+temps
+intimately
+pau
+tormented
+##lok
+##stellar
+1618
+charred
+ems
+essen
+##mmel
+alarms
+spraying
+ascot
+blooms
+twinkle
+##abia
+##apes
+internment
+obsidian
+##chaft
+snoop
+##dav
+##ooping
+malibu
+##tension
+quiver
+##itia
+hays
+mcintosh
+travers
+walsall
+##ffie
+1623
+beverley
+schwarz
+plunging
+structurally
+m3
+rosenthal
+vikram
+##tsk
+770
+ghz
+##onda
+##tiv
+chalmers
+groningen
+pew
+reckon
+unicef
+##rvis
+55th
+##gni
+1651
+sulawesi
+avila
+cai
+metaphysical
+screwing
+turbulence
+##mberg
+augusto
+samba
+56th
+baffled
+momentary
+toxin
+##urian
+##wani
+aachen
+condoms
+dali
+steppe
+##3d
+##app
+##oed
+##year
+adolescence
+dauphin
+electrically
+inaccessible
+microscopy
+nikita
+##ega
+atv
+##cel
+##enter
+##oles
+##oteric
+##ы
+accountants
+punishments
+wrongly
+bribes
+adventurous
+clinch
+flinders
+southland
+##hem
+##kata
+gough
+##ciency
+lads
+soared
+##ה
+undergoes
+deformation
+outlawed
+rubbish
+##arus
+##mussen
+##nidae
+##rzburg
+arcs
+##ingdon
+##tituted
+1695
+wheelbase
+wheeling
+bombardier
+campground
+zebra
+##lices
+##oj
+##bain
+lullaby
+##ecure
+donetsk
+wylie
+grenada
+##arding
+##ης
+squinting
+eireann
+opposes
+##andra
+maximal
+runes
+##broken
+##cuting
+##iface
+##ror
+##rosis
+additive
+britney
+adultery
+triggering
+##drome
+detrimental
+aarhus
+containment
+jc
+swapped
+vichy
+##ioms
+madly
+##oric
+##rag
+brant
+##ckey
+##trix
+1560
+1612
+broughton
+rustling
+##stems
+##uder
+asbestos
+mentoring
+##nivorous
+finley
+leaps
+##isan
+apical
+pry
+slits
+substitutes
+##dict
+intuitive
+fantasia
+insistent
+unreasonable
+##igen
+##vna
+domed
+hannover
+margot
+ponder
+##zziness
+impromptu
+jian
+lc
+rampage
+stemming
+##eft
+andrey
+gerais
+whichever
+amnesia
+appropriated
+anzac
+clicks
+modifying
+ultimatum
+cambrian
+maids
+verve
+yellowstone
+##mbs
+conservatoire
+##scribe
+adherence
+dinners
+spectra
+imperfect
+mysteriously
+sidekick
+tatar
+tuba
+##aks
+##ifolia
+distrust
+##athan
+##zle
+c2
+ronin
+zac
+##pse
+celaena
+instrumentalist
+scents
+skopje
+##mbling
+comical
+compensated
+vidal
+condor
+intersect
+jingle
+wavelengths
+##urrent
+mcqueen
+##izzly
+carp
+weasel
+422
+kanye
+militias
+postdoctoral
+eugen
+gunslinger
+##ɛ
+faux
+hospice
+##for
+appalled
+derivation
+dwarves
+##elis
+dilapidated
+##folk
+astoria
+philology
+##lwyn
+##otho
+##saka
+inducing
+philanthropy
+##bf
+##itative
+geek
+markedly
+sql
+##yce
+bessie
+indices
+rn
+##flict
+495
+frowns
+resolving
+weightlifting
+tugs
+cleric
+contentious
+1653
+mania
+rms
+##miya
+##reate
+##ruck
+##tucket
+bien
+eels
+marek
+##ayton
+##cence
+discreet
+unofficially
+##ife
+leaks
+##bber
+1705
+332
+dung
+compressor
+hillsborough
+pandit
+shillings
+distal
+##skin
+381
+##tat
+##you
+nosed
+##nir
+mangrove
+undeveloped
+##idia
+textures
+##inho
+##500
+##rise
+ae
+irritating
+nay
+amazingly
+bancroft
+apologetic
+compassionate
+kata
+symphonies
+##lovic
+airspace
+##lch
+930
+gifford
+precautions
+fulfillment
+sevilla
+vulgar
+martinique
+##urities
+looting
+piccolo
+tidy
+##dermott
+quadrant
+armchair
+incomes
+mathematicians
+stampede
+nilsson
+##inking
+##scan
+foo
+quarterfinal
+##ostal
+shang
+shouldered
+squirrels
+##owe
+344
+vinegar
+##bner
+##rchy
+##systems
+delaying
+##trics
+ars
+dwyer
+rhapsody
+sponsoring
+##gration
+bipolar
+cinder
+starters
+##olio
+##urst
+421
+signage
+##nty
+aground
+figurative
+mons
+acquaintances
+duets
+erroneously
+soyuz
+elliptic
+recreated
+##cultural
+##quette
+##ssed
+##tma
+##zcz
+moderator
+scares
+##itaire
+##stones
+##udence
+juniper
+sighting
+##just
+##nsen
+britten
+calabria
+ry
+bop
+cramer
+forsyth
+stillness
+##л
+airmen
+gathers
+unfit
+##umber
+##upt
+taunting
+##rip
+seeker
+streamlined
+##bution
+holster
+schumann
+tread
+vox
+##gano
+##onzo
+strive
+dil
+reforming
+covent
+newbury
+predicting
+##orro
+decorate
+tre
+##puted
+andover
+ie
+asahi
+dept
+dunkirk
+gills
+##tori
+buren
+huskies
+##stis
+##stov
+abstracts
+bets
+loosen
+##opa
+1682
+yearning
+##glio
+##sir
+berman
+effortlessly
+enamel
+napoli
+persist
+##peration
+##uez
+attache
+elisa
+b1
+invitations
+##kic
+accelerating
+reindeer
+boardwalk
+clutches
+nelly
+polka
+starbucks
+##kei
+adamant
+huey
+lough
+unbroken
+adventurer
+embroidery
+inspecting
+stanza
+##ducted
+naia
+taluka
+##pone
+##roids
+chases
+deprivation
+florian
+##jing
+##ppet
+earthly
+##lib
+##ssee
+colossal
+foreigner
+vet
+freaks
+patrice
+rosewood
+triassic
+upstate
+##pkins
+dominates
+ata
+chants
+ks
+vo
+##400
+##bley
+##raya
+##rmed
+555
+agra
+infiltrate
+##ailing
+##ilation
+##tzer
+##uppe
+##werk
+binoculars
+enthusiast
+fujian
+squeak
+##avs
+abolitionist
+almeida
+boredom
+hampstead
+marsden
+rations
+##ands
+inflated
+334
+bonuses
+rosalie
+patna
+##rco
+329
+detachments
+penitentiary
+54th
+flourishing
+woolf
+##dion
+##etched
+papyrus
+##lster
+##nsor
+##toy
+bobbed
+dismounted
+endelle
+inhuman
+motorola
+tbs
+wince
+wreath
+##ticus
+hideout
+inspections
+sanjay
+disgrace
+infused
+pudding
+stalks
+##urbed
+arsenic
+leases
+##hyl
+##rrard
+collarbone
+##waite
+##wil
+dowry
+##bant
+##edance
+genealogical
+nitrate
+salamanca
+scandals
+thyroid
+necessitated
+##!
+##"
+###
+##$
+##%
+##&
+##'
+##(
+##)
+##*
+##+
+##,
+##-
+##.
+##/
+##:
+##;
+##<
+##=
+##>
+##?
+##@
+##[
+##\
+##]
+##^
+##_
+##`
+##{
+##|
+##}
+##~
+##¡
+##¢
+##£
+##¤
+##¥
+##¦
+##§
+##¨
+##©
+##ª
+##«
+##¬
+##®
+##±
+##´
+##µ
+##¶
+##·
+##º
+##»
+##¼
+##¾
+##¿
+##æ
+##ð
+##÷
+##þ
+##đ
+##ħ
+##ŋ
+##œ
+##ƒ
+##ɐ
+##ɑ
+##ɒ
+##ɔ
+##ɕ
+##ə
+##ɡ
+##ɣ
+##ɨ
+##ɪ
+##ɫ
+##ɬ
+##ɯ
+##ɲ
+##ɴ
+##ɹ
+##ɾ
+##ʀ
+##ʁ
+##ʂ
+##ʃ
+##ʉ
+##ʊ
+##ʋ
+##ʌ
+##ʎ
+##ʐ
+##ʑ
+##ʒ
+##ʔ
+##ʰ
+##ʲ
+##ʳ
+##ʷ
+##ʸ
+##ʻ
+##ʼ
+##ʾ
+##ʿ
+##ˈ
+##ˡ
+##ˢ
+##ˣ
+##ˤ
+##β
+##γ
+##δ
+##ε
+##ζ
+##θ
+##κ
+##λ
+##μ
+##ξ
+##ο
+##π
+##ρ
+##σ
+##τ
+##υ
+##φ
+##χ
+##ψ
+##ω
+##б
+##г
+##д
+##ж
+##з
+##м
+##п
+##с
+##у
+##ф
+##х
+##ц
+##ч
+##ш
+##щ
+##ъ
+##э
+##ю
+##ђ
+##є
+##і
+##ј
+##љ
+##њ
+##ћ
+##ӏ
+##ա
+##բ
+##գ
+##դ
+##ե
+##թ
+##ի
+##լ
+##կ
+##հ
+##մ
+##յ
+##ն
+##ո
+##պ
+##ս
+##վ
+##տ
+##ր
+##ւ
+##ք
+##־
+##א
+##ב
+##ג
+##ד
+##ו
+##ז
+##ח
+##ט
+##י
+##ך
+##כ
+##ל
+##ם
+##מ
+##ן
+##נ
+##ס
+##ע
+##ף
+##פ
+##ץ
+##צ
+##ק
+##ר
+##ש
+##ת
+##،
+##ء
+##ب
+##ت
+##ث
+##ج
+##ح
+##خ
+##ذ
+##ز
+##س
+##ش
+##ص
+##ض
+##ط
+##ظ
+##ع
+##غ
+##ـ
+##ف
+##ق
+##ك
+##و
+##ى
+##ٹ
+##پ
+##چ
+##ک
+##گ
+##ں
+##ھ
+##ہ
+##ے
+##अ
+##आ
+##उ
+##ए
+##क
+##ख
+##ग
+##च
+##ज
+##ट
+##ड
+##ण
+##त
+##थ
+##द
+##ध
+##न
+##प
+##ब
+##भ
+##म
+##य
+##र
+##ल
+##व
+##श
+##ष
+##स
+##ह
+##ा
+##ि
+##ी
+##ो
+##।
+##॥
+##ং
+##অ
+##আ
+##ই
+##উ
+##এ
+##ও
+##ক
+##খ
+##গ
+##চ
+##ছ
+##জ
+##ট
+##ড
+##ণ
+##ত
+##থ
+##দ
+##ধ
+##ন
+##প
+##ব
+##ভ
+##ম
+##য
+##র
+##ল
+##শ
+##ষ
+##স
+##হ
+##া
+##ি
+##ী
+##ে
+##க
+##ச
+##ட
+##த
+##ந
+##ன
+##ப
+##ம
+##ய
+##ர
+##ல
+##ள
+##வ
+##ா
+##ி
+##ு
+##ே
+##ை
+##ನ
+##ರ
+##ಾ
+##ක
+##ය
+##ර
+##ල
+##ව
+##ා
+##ก
+##ง
+##ต
+##ท
+##น
+##พ
+##ม
+##ย
+##ร
+##ล
+##ว
+##ส
+##อ
+##า
+##เ
+##་
+##།
+##ག
+##ང
+##ད
+##ན
+##པ
+##བ
+##མ
+##འ
+##ར
+##ལ
+##ས
+##မ
+##ა
+##ბ
+##გ
+##დ
+##ე
+##ვ
+##თ
+##ი
+##კ
+##ლ
+##მ
+##ნ
+##ო
+##რ
+##ს
+##ტ
+##უ
+##ᄀ
+##ᄂ
+##ᄃ
+##ᄅ
+##ᄆ
+##ᄇ
+##ᄉ
+##ᄊ
+##ᄋ
+##ᄌ
+##ᄎ
+##ᄏ
+##ᄐ
+##ᄑ
+##ᄒ
+##ᅡ
+##ᅢ
+##ᅥ
+##ᅦ
+##ᅧ
+##ᅩ
+##ᅪ
+##ᅭ
+##ᅮ
+##ᅯ
+##ᅲ
+##ᅳ
+##ᅴ
+##ᅵ
+##ᆨ
+##ᆫ
+##ᆯ
+##ᆷ
+##ᆸ
+##ᆼ
+##ᴬ
+##ᴮ
+##ᴰ
+##ᴵ
+##ᴺ
+##ᵀ
+##ᵃ
+##ᵇ
+##ᵈ
+##ᵉ
+##ᵍ
+##ᵏ
+##ᵐ
+##ᵒ
+##ᵖ
+##ᵗ
+##ᵘ
+##ᵣ
+##ᵤ
+##ᵥ
+##ᶜ
+##ᶠ
+##‐
+##‑
+##‒
+##–
+##—
+##―
+##‖
+##‘
+##’
+##‚
+##“
+##”
+##„
+##†
+##‡
+##•
+##…
+##‰
+##′
+##″
+##›
+##‿
+##⁄
+##⁰
+##ⁱ
+##⁴
+##⁵
+##⁶
+##⁷
+##⁸
+##⁹
+##⁻
+##ⁿ
+##₅
+##₆
+##₇
+##₈
+##₉
+##₊
+##₍
+##₎
+##ₐ
+##ₑ
+##ₒ
+##ₓ
+##ₕ
+##ₖ
+##ₗ
+##ₘ
+##ₚ
+##ₛ
+##ₜ
+##₤
+##₩
+##€
+##₱
+##₹
+##ℓ
+##№
+##ℝ
+##™
+##⅓
+##⅔
+##←
+##↑
+##→
+##↓
+##↔
+##↦
+##⇄
+##⇌
+##⇒
+##∂
+##∅
+##∆
+##∇
+##∈
+##∗
+##∘
+##√
+##∞
+##∧
+##∨
+##∩
+##∪
+##≈
+##≡
+##≤
+##≥
+##⊂
+##⊆
+##⊕
+##⊗
+##⋅
+##─
+##│
+##■
+##▪
+##●
+##★
+##☆
+##☉
+##♠
+##♣
+##♥
+##♦
+##♯
+##⟨
+##⟩
+##ⱼ
+##⺩
+##⺼
+##⽥
+##、
+##。
+##〈
+##〉
+##《
+##》
+##「
+##」
+##『
+##』
+##〜
+##あ
+##い
+##う
+##え
+##お
+##か
+##き
+##く
+##け
+##こ
+##さ
+##し
+##す
+##せ
+##そ
+##た
+##ち
+##っ
+##つ
+##て
+##と
+##な
+##に
+##ぬ
+##ね
+##の
+##は
+##ひ
+##ふ
+##へ
+##ほ
+##ま
+##み
+##む
+##め
+##も
+##や
+##ゆ
+##よ
+##ら
+##り
+##る
+##れ
+##ろ
+##を
+##ん
+##ァ
+##ア
+##ィ
+##イ
+##ウ
+##ェ
+##エ
+##オ
+##カ
+##キ
+##ク
+##ケ
+##コ
+##サ
+##シ
+##ス
+##セ
+##タ
+##チ
+##ッ
+##ツ
+##テ
+##ト
+##ナ
+##ニ
+##ノ
+##ハ
+##ヒ
+##フ
+##ヘ
+##ホ
+##マ
+##ミ
+##ム
+##メ
+##モ
+##ャ
+##ュ
+##ョ
+##ラ
+##リ
+##ル
+##レ
+##ロ
+##ワ
+##ン
+##・
+##ー
+##一
+##三
+##上
+##下
+##不
+##世
+##中
+##主
+##久
+##之
+##也
+##事
+##二
+##五
+##井
+##京
+##人
+##亻
+##仁
+##介
+##代
+##仮
+##伊
+##会
+##佐
+##侍
+##保
+##信
+##健
+##元
+##光
+##八
+##公
+##内
+##出
+##分
+##前
+##劉
+##力
+##加
+##勝
+##北
+##区
+##十
+##千
+##南
+##博
+##原
+##口
+##古
+##史
+##司
+##合
+##吉
+##同
+##名
+##和
+##囗
+##四
+##国
+##國
+##土
+##地
+##坂
+##城
+##堂
+##場
+##士
+##夏
+##外
+##大
+##天
+##太
+##夫
+##奈
+##女
+##子
+##学
+##宀
+##宇
+##安
+##宗
+##定
+##宣
+##宮
+##家
+##宿
+##寺
+##將
+##小
+##尚
+##山
+##岡
+##島
+##崎
+##川
+##州
+##巿
+##帝
+##平
+##年
+##幸
+##广
+##弘
+##張
+##彳
+##後
+##御
+##德
+##心
+##忄
+##志
+##忠
+##愛
+##成
+##我
+##戦
+##戸
+##手
+##扌
+##政
+##文
+##新
+##方
+##日
+##明
+##星
+##春
+##昭
+##智
+##曲
+##書
+##月
+##有
+##朝
+##木
+##本
+##李
+##村
+##東
+##松
+##林
+##森
+##楊
+##樹
+##橋
+##歌
+##止
+##正
+##武
+##比
+##氏
+##民
+##水
+##氵
+##氷
+##永
+##江
+##沢
+##河
+##治
+##法
+##海
+##清
+##漢
+##瀬
+##火
+##版
+##犬
+##王
+##生
+##田
+##男
+##疒
+##発
+##白
+##的
+##皇
+##目
+##相
+##省
+##真
+##石
+##示
+##社
+##神
+##福
+##禾
+##秀
+##秋
+##空
+##立
+##章
+##竹
+##糹
+##美
+##義
+##耳
+##良
+##艹
+##花
+##英
+##華
+##葉
+##藤
+##行
+##街
+##西
+##見
+##訁
+##語
+##谷
+##貝
+##貴
+##車
+##軍
+##辶
+##道
+##郎
+##郡
+##部
+##都
+##里
+##野
+##金
+##鈴
+##镇
+##長
+##門
+##間
+##阝
+##阿
+##陳
+##陽
+##雄
+##青
+##面
+##風
+##食
+##香
+##馬
+##高
+##龍
+##龸
+##ﬁ
+##ﬂ
+##！
+##（
+##）
+##，
+##－
+##．
+##／
+##：
+##？
+##～
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/refs/main b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/refs/main
new file mode 100644
index 0000000..7f8345e
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/refs/main
@@ -0,0 +1 @@
+c9a2bfebc254878aee8c3aca9e6844d5bbb102d1
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/1_Pooling/config.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/1_Pooling/config.json
new file mode 120000
index 0000000..ee46132
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/1_Pooling/config.json
@@ -0,0 +1 @@
+../../../blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/README.md b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/README.md
new file mode 120000
index 0000000..8c06fb1
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/README.md
@@ -0,0 +1 @@
+../../blobs/152b56c8ff5229192e0b1f405f5bf07699854738
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config.json
new file mode 120000
index 0000000..621db3c
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config.json
@@ -0,0 +1 @@
+../../blobs/d931afc983d9be7f3ca1d98032eadd4dd2ac7d69
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config_sentence_transformers.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config_sentence_transformers.json
new file mode 120000
index 0000000..f75609a
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/config_sentence_transformers.json
@@ -0,0 +1 @@
+../../blobs/b974b349cb2d419ada11181750a733ff82f291ad
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/model.safetensors b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/model.safetensors
new file mode 120000
index 0000000..ca271ad
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/model.safetensors
@@ -0,0 +1 @@
+../../blobs/2ce4480dc3b2f8edeee50c43765c72768e79fc0113d3f73773dded4887cca298
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/modules.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/modules.json
new file mode 120000
index 0000000..140f6da
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/modules.json
@@ -0,0 +1 @@
+../../blobs/f7640f94e81bb7f4f04daf1668850b38763a13d9
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/sentence_bert_config.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/sentence_bert_config.json
new file mode 120000
index 0000000..1edd472
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/sentence_bert_config.json
@@ -0,0 +1 @@
+../../blobs/5fd10429389515d3e5cccdeda08cae5fea1ae82e
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/special_tokens_map.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/special_tokens_map.json
new file mode 120000
index 0000000..9782670
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/special_tokens_map.json
@@ -0,0 +1 @@
+../../blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer.json
new file mode 120000
index 0000000..f7196be
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer.json
@@ -0,0 +1 @@
+../../blobs/40c4a0f6c414c8218190234bbce9bf4cc04fa3ac
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer_config.json b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer_config.json
new file mode 120000
index 0000000..5773fce
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/tokenizer_config.json
@@ -0,0 +1 @@
+../../blobs/7410db66f06de178beeadfdd11b1fc241b04f683
\ No newline at end of file
diff --git a/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/vocab.txt b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/vocab.txt
new file mode 120000
index 0000000..970a8a0
--- /dev/null
+++ b/simtag/models/models--sentence-transformers--paraphrase-MiniLM-L6-v2/snapshots/c9a2bfebc254878aee8c3aca9e6844d5bbb102d1/vocab.txt
@@ -0,0 +1 @@
+../../blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938
\ No newline at end of file
diff --git a/simtag/module/ai_handler.py b/simtag/module/ai_handler.py
deleted file mode 100644
index 4b74afe..0000000
--- a/simtag/module/ai_handler.py
+++ /dev/null
@@ -1,189 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-import logging
-from typing import Dict, Any, List
-
-from simtag.services.rag_service import RAGService
-from simtag.core.graph_service import GraphService
-from simtag.utils.unified_tag_processor import normalize_payload
-
-logger = logging.getLogger(__name__)
-
-class AIHandler:
-    """
-    Dedicated handler for AI-powered chat commands.
-    Provides specialized endpoints for tag suggestions, content expansion, and knowledge connections.
-    """
-    
-    def __init__(self, rag_service: RAGService, graph_service: GraphService):
-        self.rag_service = rag_service
-        self.graph_service = graph_service
-        self.logger = logging.getLogger(__name__)
-
-    async def suggest_tags(self, payload: Dict) -> Dict[str, Any]:
-        """
-        Generate intelligent tag suggestions for given content.
-        
-        Args:
-            payload: Dict containing 'content' key with text to analyze
-            
-        Returns:
-            Dict with suggested tags and confidence scores
-        """
-        try:
-            data = normalize_payload(payload)
-            content = data.get('content')
-            
-            if not content:
-                return {"error": "Content is required for tag suggestions"}
-                
-            self.logger.info(f"Generating tag suggestions for content: {content[:100]}...")
-            
-            # Use RAG service for intelligent tag generation
-            prompt = f"""
-            Analyze the following content and suggest relevant tags for organizing this information.
-            Focus on key concepts, topics, themes, and actionable tags that would help categorize this content.
-            
-            Content: {content}
-            
-            Provide 5-10 relevant tags as a JSON array of strings.
-            """
-            
-            response = await self.rag_service.query(prompt)
-            
-            # Extract tags from response
-            if response.get("status") == "success":
-                answer = response.get("answer", "")
-                # Parse tags from the response
-                import re
-                tags = re.findall(r'["\']([^"\']+)["\']', answer)
-                if not tags:
-                    # Fallback: split by comma or newlines
-                    tags = [tag.strip() for tag in re.split(r'[,\n]', answer) if tag.strip()]
-                
-                return {
-                    "status": "success",
-                    "suggestions": tags[:10],  # Limit to 10 tags
-                    "source": "ai-analysis"
-                }
-            else:
-                return {"error": "Failed to generate tag suggestions", "details": response}
-                
-        except Exception as e:
-            self.logger.error(f"Error in suggest_tags: {e}", exc_info=True)
-            return {"error": f"Failed to generate tags: {str(e)}"}
-
-    async def find_connections(self, payload: Dict) -> Dict[str, Any]:
-        """
-        Find knowledge connections for a given tag.
-        
-        Args:
-            payload: Dict containing 'tag' key with tag name
-            
-        Returns:
-            Dict with related nodes, tags, and connection insights
-        """
-        try:
-            data = normalize_payload(payload)
-            tag = data.get('tag')
-            
-            if not tag:
-                return {"error": "Tag is required for finding connections"}
-                
-            self.logger.info(f"Finding connections for tag: {tag}")
-            
-            # Get nodes with this tag
-            nodes_with_tag = await self.graph_service.find_nodes_by_tag(tag)
-            
-            # Get related tags via co-occurrence
-            related_tags = await self.graph_service.get_related_tags(tag)
-            
-            # Get similar concepts
-            prompt = f"""
-            Find knowledge connections and relationships for the tag "{tag}".
-            Consider:
-            1. Related concepts and themes
-            2. Practical applications and use cases
-            3. Connections to other knowledge areas
-            4. Common workflows or processes involving this tag
-            
-            Provide insights about how this tag connects to the broader knowledge graph.
-            """
-            
-            ai_insights = await self.rag_service.query(prompt)
-            
-            return {
-                "status": "success",
-                "tag": tag,
-                "nodes_count": len(nodes_with_tag),
-                "related_tags": related_tags[:10],
-                "ai_insights": ai_insights.get("answer", "") if ai_insights.get("status") == "success" else "",
-                "nodes": nodes_with_tag[:5]  # Limit to 5 nodes for brevity
-            }
-            
-        except Exception as e:
-            self.logger.error(f"Error in find_connections: {e}", exc_info=True)
-            return {"error": f"Failed to find connections: {str(e)}"}
-
-    async def expand_content(self, payload: Dict) -> Dict[str, Any]:
-        """
-        Expand and elaborate on given content or topic.
-        
-        Args:
-            payload: Dict containing 'content' and optional 'context' keys
-            
-        Returns:
-            Dict with expanded content and insights
-        """
-        try:
-            data = normalize_payload(payload)
-            content = data.get('content')
-            context_text = data.get('context', '')
-            
-            if not content:
-                return {"error": "Content is required for expansion"}
-                
-            self.logger.info(f"Expanding content: {content[:100]}...")
-            
-            # Build contextual prompt
-            if context_text:
-                prompt = f"""
-                Expand on the following topic within the given context:
-                
-                Topic to expand: {content}
-                
-                Context: {context_text}
-                
-                Provide detailed insights, examples, related concepts, and practical applications.
-                Structure the response with clear sections and actionable information.
-                """
-            else:
-                prompt = f"""
-                Expand on the following topic:
-                
-                {content}
-                
-                Provide comprehensive insights including:
-                1. Detailed explanation
-                2. Practical examples
-                3. Related concepts
-                4. Implementation guidance
-                5. Common use cases
-                """
-            
-            response = await self.rag_service.query(prompt)
-            
-            if response.get("status") == "success":
-                return {
-                    "status": "success",
-                    "expanded_content": response.get("answer", ""),
-                    "original_content": content,
-                    "context_used": bool(context_text)
-                }
-            else:
-                return {"error": "Failed to expand content", "details": response}
-                
-        except Exception as e:
-            self.logger.error(f"Error in expand_content: {e}", exc_info=True)
-            return {"error": f"Failed to expand content: {str(e)}"}
\ No newline at end of file
diff --git a/simtag/module/autotag_handler.py b/simtag/module/autotag_handler.py
index 1d657b6..ab06b2f 100755
--- a/simtag/module/autotag_handler.py
+++ b/simtag/module/autotag_handler.py
@@ -1,113 +1,213 @@
 import logging
-import json
 import asyncio
-from typing import Dict, Any, List
+from platform import node
+from typing import List, Dict, Any, Optional
+from dependency_injector.wiring import inject, Provide
 
-from .. import prompts
-from ..services.llm_client import LLMClient
-from ..utils.unified_tag_processor import normalize_payload, TagResult
-from ..prompts import AUTOTAG_SUGGESTION_PROMPT
-from ..utils.utils import extract_json_from_response
+from ..utils.unified_tag_processor import TagResult, normalize_payload
 
 logger = logging.getLogger(__name__)
 
 class AutotagHandler:
-    """
-    Handles autotagging of notes by generating suggested tags using an LLM.
-    """
-    def __init__(self, llm_client: LLMClient):
+    """处理自动标签生成的业务逻辑 (异步)"""
+    
+    @inject
+    def __init__(self, 
+                 llm_client=Provide['llm_client'],
+                 ner_service=Provide['ner_service']):
         self.llm_client = llm_client
-        self.logger = logging.getLogger(__name__)
+        self.ner_service = ner_service
+        logger.info("AutotagHandler initialized with injected dependencies")
 
-    async def _suggest_tags_for_node(self, node: Dict[str, Any], model_config: Dict) -> List[str]:
+    async def batch_generate_tags(self, *args) -> Dict[str, Any]:
         """
-        Core logic to generate tags for a single, clean node dictionary.
-        This now uses the dedicated AUTOTAG_SUGGESTION_PROMPT.
-        """
-        if not isinstance(node, dict):
-            raise ValueError(f"Expected a 'node' dictionary, got {type(node)}")
-
-        content = node.get('content', '')
-        if not content:
-            return []
-
-        prompt = AUTOTAG_SUGGESTION_PROMPT.format(text_content=content)
-        llm_result = await self.llm_client.generate(prompt, format_json=False, **model_config)
-
-        if not llm_result.success:
-            self.logger.error(f"LLM generation failed for node {node.get('id')}: {llm_result.error_message}")
-            return []
-
-        response_json = extract_json_from_response(llm_result.content)
-        if response_json and isinstance(response_json.get("tags"), list):
-            return response_json["tags"]
+        批量生成标签 (异步版本)
         
-        self.logger.warning(f"Failed to extract valid 'tags' list from LLM response for node {node.get('id')}")
-        return []
-
-    async def suggest_tags_for_single_node_elisp(self, payload: Any) -> Dict[str, Any]:
+        The bridge passes the raw Elisp payload here. We use normalize_payload
+        to convert it into a standard Python dictionary.
         """
-        Adapter method for single node tag generation from Elisp.
-        It normalizes the payload and then calls the core logic.
+        try:
+            logger.debug(f"batch_generate_tags received raw args tuple: {args}")
+            logger.debug(f"args length: {len(args)}")
+            if args:
+                logger.debug(f"first arg type: {type(args[0])}")
+                logger.debug(f"first arg content: {args[0]}")
+            
+            # The entire payload is passed as the first argument.
+            if not args:
+                raise ValueError("No arguments provided to batch_generate_tags")
+            
+            # Use the new, unified payload normalizer.
+            # The EPC bridge should already wrap the payload in a list.
+            data_dict = normalize_payload(list(args))
+            logger.debug(f"Normalized data_dict: {data_dict}")
+            logger.debug(f"data_dict keys: {list(data_dict.keys()) if isinstance(data_dict, dict) else 'not a dict'}")
+
+            if 'error' in data_dict:
+                raise ValueError(f"Payload normalization failed: {data_dict.get('message')}")
+            
+            nodes_data = data_dict.get('nodes', [])
+            model_config = data_dict.get('model_config', {})
+            
+            # Fix: Handle case where nodes_data is a dict instead of list
+            if isinstance(nodes_data, dict):
+                # If it's a single node dict, wrap it in a list
+                nodes_list = [nodes_data]
+                logger.debug(f"Single node detected, wrapped in list")
+            elif isinstance(nodes_data, list):
+                nodes_list = nodes_data
+                logger.debug(f"Node list detected with {len(nodes_list)} items")
+            else:
+                logger.warning(f"Unexpected nodes data type: {type(nodes_data)}")
+                nodes_list = []
+            
+            # Fix: Handle case where model_config is a list instead of dict
+            if isinstance(model_config, list):
+                # Convert plist-style list to dict
+                config_dict = {}
+                for i in range(0, len(model_config), 2):
+                    if i + 1 < len(model_config):
+                        key = model_config[i]
+                        value = model_config[i + 1]
+                        config_dict[key] = value
+                model_config = config_dict
+                logger.debug(f"Converted model_config list to dict: {model_config}")
+            
+            logger.debug(f"Final nodes_list length: {len(nodes_list)}")
+            logger.debug(f"Final model_config: {model_config}")
+            
+            if not nodes_list:
+                logger.warning("No nodes found in payload for batch tag generation.")
+                logger.warning(f"Available keys in data_dict: {list(data_dict.keys()) if isinstance(data_dict, dict) else 'not a dict'}")
+                logger.warning(f"Original nodes_data type: {type(nodes_data)}, value: {nodes_data}")
+                return {'results': [], 'total_processed': 0, 'successful': 0, 'failed': 0}
+
+            logger.info(f"Processing batch tag generation for {len(nodes_list)} nodes")
+
+            # Use asyncio.gather to run all tag generations concurrently
+            tasks = [self._generate_tags_for_node(node_data, model_config) for node_data in nodes_list]
+            results = await asyncio.gather(*tasks, return_exceptions=True)
+
+            # Process results
+            final_results = []
+            for i, res in enumerate(results):
+                node_data = nodes_list[i]
+                node_id = "unknown"
+                try:
+                    node_id = dict(node_data).get('id', f'node_{i}')
+                except (TypeError, ValueError):
+                    logger.warning(f"Could not extract node_id from node_data at index {i}")
+
+                if isinstance(res, Exception):
+                    logger.error(f"Error processing node {i} (id: {node_id}): {res}", exc_info=True)
+                    final_results.append({
+                        'node_id': node_id,
+                        'tags': [],
+                        'status': 'error',
+                        'error': str(res)
+                    })
+                else:
+                    final_results.append({
+                        'node_id': node_id,
+                        'tags': [tag.__dict__ for tag in res],
+                        'status': 'success'
+                    })
+            
+            return {
+                'results': final_results,
+                'total_processed': len(nodes_list),
+                'successful': len([r for r in final_results if r['status'] == 'success']),
+                'failed': len([r for r in final_results if r['status'] == 'error'])
+            }
+            
+        except Exception as e:
+            logger.error(f"Error in batch_generate_tags: {str(e)}", exc_info=True)
+            return {
+                'error': str(e),
+                'results': [],
+                'total_processed': 0,
+                'successful': 0,
+                'failed': 0
+            }
+
+    async def suggest_tags_for_single_node_elisp(self, *args) -> Dict[str, Any]:
+        """
+        为单个节点生成标签 (异步版本), a new wrapper for the EPC call.
         """
-        node_id = 'unknown'
         try:
-            data_dict = normalize_payload(payload)
-            node = data_dict.get('node')
-            if not node:
-                nodes = data_dict.get('nodes', [])
-                if nodes and len(nodes) > 0:
-                    node = nodes[0]
-
-            if not node:
-                return {'node_id': 'unknown', 'tags': [], 'status': 'no_node_data'}
+            logger.debug(f"suggest_tags_for_single_node_elisp received raw args tuple: {args}")
+            
+            if not args:
+                raise ValueError("No arguments provided")
 
-            node_id = node.get('id', 'unknown')
-            model_config = data_dict.get("model_config", {})
+            data_dict = normalize_payload(list(args))
+            if 'error' in data_dict:
+                raise ValueError(f"Payload normalization failed: {data_dict.get('message')}")
 
-            tags = await self._suggest_tags_for_node(node, model_config)
+            node = data_dict.get('node')
+            model_config = data_dict.get('model_config', {})
 
-            return {'node_id': node_id, 'tags': tags, 'status': 'success'}
+            if not isinstance(node, dict):
+                raise ValueError(f"Expected a 'node' dictionary in payload, but got {type(node)}")
 
+            node_id = node.get('id', 'unknown')
+            logger.info(f"Processing single node tag generation for node: {node_id}")
+            
+            tags = await self._generate_tags_for_node(node, model_config)
+            
+            return {
+                'node_id': node_id,
+                'tags': [tag.__dict__ for tag in tags],
+                'status': 'success'
+            }
+            
         except Exception as e:
-            self.logger.error(f"Error in suggest_tags_for_single_node_elisp (node: {node_id}): {str(e)}", exc_info=True)
-            return {'node_id': node_id, 'tags': [], 'status': 'error', 'error': str(e)}
-
-    async def batch_generate_tags(self, payload: Dict[str, Any]) -> List[Dict[str, Any]]:
+            logger.error(f"Error in suggest_tags_for_single_node_elisp: {str(e)}", exc_info=True)
+            return {
+                'node_id': 'unknown',
+                'tags': [],
+                'status': 'error',
+                'error': str(e)
+            }
+
+    async def _generate_tags_for_node(self, node_data: Dict, model_config: Dict[str, Any]) -> List[TagResult]:
         """
-        Generates tags for a batch of nodes concurrently by calling the core node processing logic.
+        为单个节点生成标签的内部方法 (异步版本)
         """
-        data = normalize_payload(payload)
-        nodes = data.get("nodes")
-        model_config = data.get("model_config", {})
-
-        if not nodes:
-            logger.warning("batch_generate_tags called with no 'nodes' in payload.")
-            return [{"error": "Payload missing 'nodes' key."}]
-
-        logger.info(f"Starting batch tag generation for {len(nodes)} nodes.")
-
-        tasks = [self._suggest_tags_for_node(node, model_config) for node in nodes]
-
-        results = await asyncio.gather(*tasks, return_exceptions=True)
-
-        processed_results = []
-        for i, result in enumerate(results):
-            node_info = nodes[i] if i < len(nodes) else {}
-            node_id = node_info.get('id', f'unknown_node_{i}')
-
-            if isinstance(result, Exception):
-                logger.error(f"Error processing node {node_id} in batch: {result}", exc_info=result)
-                processed_results.append({
-                    "node_id": node_id,
-                    "tags": [],
-                    "error": str(result)
-                })
-            else:
-                processed_results.append({
-                    "node_id": node_id,
-                    "tags": result
-                })
+        if not isinstance(node_data, dict):
+            logger.error(f"Could not process node_data, expected a dict but got {type(node_data)}")
+            return []
 
-        logger.info(f"Batch tag generation completed for {len(nodes)} nodes.")
-        return processed_results
+        try:
+            node_id = node_data.get('id', 'unknown')
+            content = node_data.get('content', '')
+            
+            if not content:
+                logger.warning(f"Node {node_id} has no content")
+                return []
+            
+            # 使用正确的 NERService 方法: suggest_tags_batch (async)
+            suggested_tags_raw_batch = await self.ner_service.suggest_tags_batch([content], [[]])
+            suggested_tags_raw = suggested_tags_raw_batch[0] if suggested_tags_raw_batch else []
+            logger.debug(f"NER service suggested {len(suggested_tags_raw)} tags for node {node_id}")
+
+            # 将原始建议转换为 TagResult 格式
+            tags = []
+            for suggestion in suggested_tags_raw:
+                if isinstance(suggestion, dict):
+                    tag = TagResult(
+                        tag_name=suggestion.get('tag_name', ''),
+                        confidence=suggestion.get('confidence', 0.5),
+                        reasoning=suggestion.get('reasoning', 'Suggested by LLM.'),
+                        source="llm_ner"
+                    )
+                    tags.append(tag)
+                else:
+                    logger.warning(f"Received non-dict suggestion for node {node_id}: {suggestion}")
+            
+            logger.debug(f"Generated {len(tags)} tags for node {node_id}")
+            return tags
+            
+        except Exception as e:
+            logger.error(f"Error generating tags for node {node_data.get('id', 'unknown')}: {str(e)}", exc_info=True)
+            return [] 
\ No newline at end of file
diff --git a/simtag/module/diagnostics_handler.py b/simtag/module/diagnostics_handler.py
index 8f7033e..07f16c2 100755
--- a/simtag/module/diagnostics_handler.py
+++ b/simtag/module/diagnostics_handler.py
@@ -12,27 +12,35 @@ from simtag.core.graph_service import GraphService
 logger = logging.getLogger(__name__)
 
 class DiagnosticsHandler:
-    def __init__(self, config, graph_service: GraphService, llm_client, embedding_service):
+    def __init__(self, config, llm_client, graph_service: GraphService, memory_engine, entity_extractor, rag_engine, emacs_client, data_dir, content_processor):
         self.config = config
-        self.graph_service = graph_service
         self.llm_client = llm_client
-        self.embedding_service = embedding_service
-        logger.info("DiagnosticsHandler initialized with modern dependencies.")
+        self.graph_service = graph_service
+        self.memory_engine = memory_engine
+        self.entity_extractor = entity_extractor
+        self.rag_engine = rag_engine
+        self.emacs_client = emacs_client
+        self.data_dir = data_dir
+        self.content_processor = content_processor
+        logger.info("DiagnosticsHandler initialized with extended dependencies.")
 
     def get_status(self) -> Dict[str, Any]:
         """
-        Gets the system status.
+        Gets the system status. Moved from SimTagBridge.
         """
         logger.info("get_status called")
         try:
-            llm_available = self.llm_client is not None
+            # Check LLM client availability
+            llm_available = False
             llm_model = "unknown"
-            if llm_available:
-                try:
-                    llm_model = self.llm_client.get_default_model()
-                except Exception:
-                    pass
+            try:
+                if self.llm_client:
+                    llm_available = True
+                    llm_model = getattr(self.config, 'llm_client_config', {}).get('default_model', 'unknown')
+            except Exception as e:
+                logger.warning(f"Could not check LLM status: {e}")
 
+            # Check storage stats
             storage_stats = {}
             try:
                 if self.graph_service:
@@ -44,12 +52,16 @@ class DiagnosticsHandler:
                 "llm_available": llm_available,
                 "llm_model": llm_model,
                 "graph_service_ready": self.graph_service is not None,
-                "embedding_service_ready": self.embedding_service is not None,
                 "storage_stats": storage_stats,
+                "memory_engine_ready": self.memory_engine is not None,
+                "entity_extractor_ready": self.entity_extractor is not None,
+                "rag_engine_ready": self.rag_engine is not None,
                 "server_running": True,
+                "emacs_client_connected": self.emacs_client is not None,
                 "config": {
-                    "data_directory": self.config.data_directory,
-                    "db_path": self.config.db_path,
+                    "vector_db_path": self.config.vector_db_path,
+                    "data_directory": self.data_dir,
+                    "llm_model": llm_model
                 }
             }
             return {"status": "success", "result": status}
@@ -59,13 +71,13 @@ class DiagnosticsHandler:
 
     def get_config(self) -> Dict[str, Any]:
         """
-        Gets the current configuration.
+        Gets the current configuration. Moved from SimTagBridge.
         """
         logger.info("EPC Call: get_config")
         try:
             return {
                 "status": "success",
-                "config": self.config.model_dump()
+                "config": self.config.to_dict()  # Assuming Config has a to_dict method
             }
         except Exception as e:
             logger.error(f"Get config failed: {e}\n{traceback.format_exc()}")
@@ -93,53 +105,31 @@ class DiagnosticsHandler:
             logger.error(msg)
             return {"status": "error", "message": msg, "missing": missing_modules}
 
-    def get_processing_status(self) -> Dict[str, Any]:
-        return {"status": "error", "message": "Log monitor not available in this version."}
-    
-    def print_processing_report(self) -> Dict[str, Any]:
-        return {"status": "error", "message": "Log monitor not available in this version."}
-
-
+    def test_embedding_retrieval(self, text: str):
+        from simtag.services.content_processor import ProcessingConfig, ProcessingMode, ContentItem
+        import time
 
-    async def test_embedding_retrieval(self, text: str) -> Dict[str, Any]:
-        """Tests the full embedding retrieval pipeline for a given text."""
-        logger.info(f"Running embedding test for text: '{text[:30]}...'")
-        if not self.embedding_service:
-            return {"status": "error", "message": "EmbeddingService is not available."}
+        logger.info(f"Received test_embedding_retrieval request with text: '{text}'")
+        if not self.content_processor:
+            return "Error: ContentProcessor not available."
         
-        start_time = time.time()
-        result = await self.embedding_service.get_embedding(text)
-        end_time = time.time()
-        
-        if result.success:
-            return {
-                "status": "success",
-                "embedding_length": len(result.embedding) if result.embedding else 0,
-                "model_used": result.model_used,
-                "backend_used": result.backend_used,
-                "processing_time_ms": (end_time - start_time) * 1000
-            }
-        else:
-            return {
-                "status": "error",
-                "message": result.error_message,
-                "model_used": result.model_used,
-                "backend_used": result.backend_used,
-            }
-
-
-
-    async def get_available_models(self) -> Dict[str, Any]:
-        """
-        Gets the available models from the LLM client.
-        """
         try:
-            if self.llm_client:
-                # This assumes the llm_client has a method to get available models.
-                # We will need to add this to the LLMClient class.
-                models = await self.llm_client.get_available_models()
-                return {"status": "success", "models": models}
-            return {"status": "error", "message": "LLM client not available."}
+            embedding_config = ProcessingConfig(mode=ProcessingMode.EMBEDDING_ONLY)
+            embedding_processor = self.content_processor.__class__(embedding_config)
+            
+            content_item = ContentItem(id=f"test_{int(time.time())}", text=text)
+            processing_result = asyncio.run(embedding_processor.process_single(content_item))
+            
+            if processing_result.success and processing_result.embedding_result and processing_result.embedding_result.success:
+                embedding = processing_result.embedding_result.embedding
+                return f"Success! Got embedding of dimension {len(embedding)}."
+            else:
+                return f"Error: {processing_result.error or 'Unknown error'}"
         except Exception as e:
-            logger.error(f"Failed to get available models: {e}\n{traceback.format_exc()}")
-            return {"status": "error", "message": str(e)} 
\ No newline at end of file
+            return f"Error: Exception occurred: {e}"
+
+    def get_processing_status(self) -> Dict[str, Any]:
+        return {"status": "error", "message": "Log monitor not available in this version."}
+    
+    def print_processing_report(self) -> Dict[str, Any]:
+        return {"status": "error", "message": "Log monitor not available in this version."} 
\ No newline at end of file
diff --git a/simtag/module/epc_handler.py b/simtag/module/epc_handler.py
new file mode 100755
index 0000000..8ee69e9
--- /dev/null
+++ b/simtag/module/epc_handler.py
@@ -0,0 +1,595 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import logging
+import asyncio
+import json
+import dataclasses
+import time
+from typing import Dict, Any, List, Optional
+import traceback
+
+from simtag.core.memory_engine import MemoryItem, MemoryItemType
+from simtag.services.content_processor import ContentProcessor, ProcessingConfig, ContentItem
+from simtag.core.entity_extractor import ExtractedEntity
+from simtag.utils.unified_tag_processor import normalize_payload
+from simtag.core.graph_service import GraphService
+
+logger = logging.getLogger(__name__)
+
+class EPCHandler:
+    def __init__(self, entity_extractor, graph_service: GraphService, memory_engine, rag_engine, config, content_processor, memory_synthesizer, engine, llm_client, ner_service, node_processor):
+        self.entity_extractor = entity_extractor
+        self.graph_service = graph_service
+        self.memory_engine = memory_engine
+        self.rag_engine = rag_engine
+        self.config = config
+        self.content_processor = content_processor
+        self.memory_synthesizer = memory_synthesizer
+        self.engine = engine
+        self.llm_client = llm_client
+        self.ner_service = ner_service
+        self.node_processor = node_processor
+        logger.info("EPCHandler initialized with extended dependencies (including node processor).")
+
+    async def _async_analyze_input(self, input_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+        """
+        Asynchronous core logic for analyzing input.
+        """
+        try:
+            logger.info(f"_async_analyze_input called with data keys: {list(input_data.keys())}")
+            text = input_data.get('text')
+            node_id = input_data.get('node_id')
+            org_tags = input_data.get('tags') # Optional list of strings
+
+            if not text or not node_id:
+                logger.warning("analyze_input: 'text' or 'node_id' missing from input_data.")
+                return {'status': 'error', 'message': "'text' and 'node_id' are required."}
+
+            # 1. Extract entities and relations
+            extracted_entities = await self.entity_extractor.extract_from_org_node(
+                node_content=text,
+                node_id=node_id,
+                existing_tags=org_tags
+            )
+            # Relations extraction is not yet implemented in the entity extractor
+            extracted_relations = []
+            logger.info(f"Extracted {len(extracted_entities)} entities and {len(extracted_relations)} relations from node {node_id}.")
+
+            # 2. Convert to format expected by GraphService and upsert
+            if extracted_entities:
+                tags_for_gs = [{
+                    'tag_id': e.name, # Use name as ID for tag entities
+                    'name': e.name,
+                    'description': e.attributes.get('description', '')
+                } for e in extracted_entities]
+                self.graph_service.bulk_upsert_tags(tags_for_gs)
+
+            if extracted_relations:
+                # Assuming extracted_relations are in a compatible format
+                self.graph_service.bulk_upsert_relations(extracted_relations)
+
+            logger.info(f"Upserted entities and relations for node {node_id} into GraphService.")
+
+            # 3. Record analysis in memory
+            analysis_summary = {
+                'node_id': node_id,
+                'num_entities_extracted': len(extracted_entities),
+                'num_relations_extracted': len(extracted_relations),
+                'tags_provided': org_tags
+            }
+            mem_item = MemoryItem(
+                id=f"analysis_{node_id}_{int(time.time())}",
+                type=MemoryItemType.SYSTEM_STATE,
+                content=analysis_summary,
+                metadata={'source': 'analyze_input'}
+            )
+            await self.memory_engine.add_memory_item(mem_item)
+
+            return {
+                'status': 'success',
+                'message': f"Analyzed node {node_id}.",
+                'num_entities': len(extracted_entities),
+                'num_relations': len(extracted_relations)
+            }
+
+        except Exception as e:
+            logger.error(f"Error in _async_analyze_input: {e}\n{traceback.format_exc()}")
+            return {'status': 'error', 'message': str(e)}
+
+    def analyze_input(self, input_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+        """
+        EPC method: Analyzes input text, extracts entities/relations, updates KG, and stores analysis in memory.
+        """
+        return asyncio.run(self._async_analyze_input(input_data))
+
+    async def _async_ask_question(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Asynchronous core logic for asking a question.
+        """
+        try:
+            query_text = question_data.get('query_text')
+            session_id = question_data.get('session_id', f"session_{int(time.time())}")
+            logger.info(f"_async_ask_question called for session {session_id} with query: '{query_text[:50]}...'")
+
+            if not query_text:
+                logger.warning("ask_question: 'query_text' missing.")
+                return {'answer': "Error: 'query_text' is required.", 'session_id': session_id, 'status': 'error', 'metadata': {}}
+
+            dialogue_mode = await self.memory_engine.get_user_preference(key=f"session_mode_{session_id}", default='normal')
+            logger.info(f"Using dialogue mode '{dialogue_mode}' for session {session_id}")
+
+            embedding_config = ProcessingConfig(mode=ProcessingConfig.EMBEDDING_ONLY)
+            embedding_processor = ContentProcessor(embedding_config)
+            
+            content_item = ContentItem(id=f"query_{session_id}_{int(time.time())}", text=query_text)
+            processing_result = await embedding_processor.process_single(content_item)
+            
+            if not (processing_result.success and processing_result.embedding_result and processing_result.embedding_result.success):
+                logger.error("Failed to get query embedding for ask_question.")
+                return {'answer': "Error: Could not generate query embedding.", 'session_id': session_id, 'status': 'error', 'metadata': {}}
+            
+            query_embedding = processing_result.embedding_result.embedding
+            query_extracted_entities = await self.entity_extractor.extract_from_org_node(
+                node_content=query_text, node_id=f"query_epc_{session_id}_{int(time.time())}"
+            )
+
+            active_mode_config = self.config.rag_mode_presets.get(dialogue_mode, self.config.rag_mode_presets.get('normal', {}))
+            rag_strategy_name = active_mode_config.get('rag_retrieval_mode', getattr(self.config, 'rag_retrieval_mode', 'light'))
+            rag_top_k = active_mode_config.get('rag_vector_results', getattr(self.config, 'rag_vector_results', 10))
+
+            rag_context_docs = await self.rag_engine.retrieve_context(
+                query_text=query_text,
+                query_embedding=query_embedding,
+                extracted_entities=query_extracted_entities,
+                strategy=rag_strategy_name,
+                top_k=rag_top_k,
+            )
+
+            prompt_template_override = active_mode_config.get('prompt_template')
+            final_answer_text: str
+            response_metadata = {'mode': dialogue_mode}
+
+            if dialogue_mode == 'socratic':
+                socratic_prompt_template = prompt_template_override or """...""" # Socratic prompt
+                socratic_response_str = await self.rag_engine.generate_response(
+                    query_text=query_text, context_docs=rag_context_docs, prompt_template=socratic_prompt_template
+                )
+                try:
+                    socratic_data = json.loads(socratic_response_str)
+                    final_answer_text = socratic_data.get("question", "I'm not sure what to ask next. Could you elaborate?")
+                    response_metadata['hint'] = socratic_data.get("hint", "")
+                except (json.JSONDecodeError, TypeError):
+                    final_answer_text = socratic_response_str
+                    response_metadata['hint'] = "Error: The AI response was not in the expected structured format."
+            else:
+                final_answer_text = await self.rag_engine.generate_response(
+                    query_text=query_text, context_docs=rag_context_docs, prompt_template=prompt_template_override
+                )
+
+            await self.memory_engine.add_dialogue_turn(session_id, "user", query_text, metadata={'mode': dialogue_mode})
+            ai_turn_metadata = {'mode': dialogue_mode, 'rag_docs_count': len(rag_context_docs)}
+            if 'hint' in response_metadata and response_metadata['hint']:
+                ai_turn_metadata['hint'] = response_metadata['hint']
+            await self.memory_engine.add_dialogue_turn(session_id, "ai", final_answer_text, metadata=ai_turn_metadata)
+            
+            snapshot_content = {
+                'query': query_text, 'dialogue_mode': dialogue_mode, 'rag_strategy': rag_strategy_name,
+                'retrieved_docs_count': len(rag_context_docs),
+                'retrieved_docs_preview': [dataclasses.asdict(doc) if not isinstance(doc, dict) else doc for doc in rag_context_docs[:3]],
+            }
+            await self.memory_engine.record_context_snapshot(
+                triggering_query=query_text, context_content=snapshot_content, llm_response=final_answer_text
+            )
+
+            return {'answer': final_answer_text, 'session_id': session_id, 'status': 'success', 'metadata': response_metadata}
+
+        except Exception as e:
+            logger.error(f"Error in _async_ask_question: {e}\n{traceback.format_exc()}")
+            return {'answer': f"An error occurred: {str(e)}", 'session_id': question_data.get('session_id', 'error_session'), 'status': 'error', 'metadata': {}}
+
+    def ask_question(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        EPC method: Handles a question from the user.
+        """
+        return asyncio.run(self._async_ask_question(question_data))
+
+    async def _async_get_suggestions(self, context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
+        """
+        Asynchronous core logic for getting suggestions.
+        """
+        try:
+            if isinstance(context_data, list) and len(context_data) == 1 and isinstance(context_data[0], dict):
+                context_data = context_data[0]
+
+            node_content = context_data.get('current_node_content')
+            node_id = context_data.get('current_node_id')
+
+            if not node_content or not node_id:
+                return []
+
+            embedding_config = ProcessingConfig(mode=ProcessingConfig.EMBEDDING_ONLY)
+            embedding_processor = ContentProcessor(embedding_config)
+            
+            content_item = ContentItem(id=node_id, text=node_content)
+            processing_result = await embedding_processor.process_single(content_item)
+            
+            if not (processing_result.success and processing_result.embedding_result and processing_result.embedding_result.success):
+                return []
+            
+            content_embedding = processing_result.embedding_result.embedding
+            extracted_node_entities = await self.entity_extractor.extract_from_org_node(
+                node_content=node_content, node_id=node_id
+            )
+
+            suggestion_rag_strategy = getattr(self.config, 'suggestion_rag_strategy', 'light') 
+            suggestion_top_k = getattr(self.config, 'suggestion_top_k', 5)
+
+            related_docs = await self.rag_engine.retrieve_context(
+                query_text=node_content[:1000],
+                query_embedding=content_embedding,
+                extracted_entities=extracted_node_entities,
+                strategy=suggestion_rag_strategy,
+                top_k=suggestion_top_k 
+            )
+
+            suggestions = []
+            for doc in related_docs:
+                if doc.get('id') == node_id: continue
+
+                suggestions.append({
+                    'type': doc.get('source', 'related_document'),
+                    'id': str(doc.get('id', '')),
+                    'title': doc.get('text', '')[:80] + "..." if len(doc.get('text', '')) > 80 else doc.get('text', ''),
+                    'snippet': doc.get('text', '')[:200] + "..." if len(doc.get('text', '')) > 200 else doc.get('text', ''),
+                    'score': float(doc.get('score', 0.0)),
+                    'metadata': {'entity_type': doc.get('entity_type')} if doc.get('entity_type') else {}
+                })
+            
+            suggestions.sort(key=lambda s: s.get('score', 0.0), reverse=True)
+            
+            return suggestions[:suggestion_top_k]
+
+        except Exception as e:
+            logger.error(f"Error in _async_get_suggestions: {e}\n{traceback.format_exc()}")
+            return []
+
+    def get_suggestions(self, context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
+        """
+        EPC method: Provides suggestions based on the current context.
+        """
+        return asyncio.run(self._async_get_suggestions(context_data))
+
+    async def _async_set_dialogue_mode(self, session_id: str, mode_name: str, mode_config: Optional[Dict[str, Any]] = None) -> bool:
+        """
+        Asynchronous core logic for setting dialogue mode.
+        """
+        try:
+            if mode_config:
+                await self.memory_engine.update_user_preference(
+                    key=f"dialogue_mode_custom_config_{mode_name}", 
+                    value=mode_config
+                )
+            
+            session_mode_key = f"session_mode_{session_id}"
+            await self.memory_engine.update_user_preference(key=session_mode_key, value=mode_name)
+            
+            return True
+        except Exception as e:
+            logger.error(f"Error in _async_set_dialogue_mode for session '{session_id}': {e}\n{traceback.format_exc()}")
+            return False
+
+    def set_dialogue_mode(self, session_id: str, mode_name: str, mode_config: Optional[Dict[str, Any]] = None) -> bool:
+        """
+        EPC method: Sets the active dialogue mode for a specific session.
+        """
+        return asyncio.run(self._async_set_dialogue_mode(session_id, mode_name, mode_config))
+
+    async def _async_get_memory_dashboard(self) -> Dict[str, Any]:
+        """
+        Asynchronous core logic for getting the memory dashboard.
+        """
+        try:
+            dashboard_data = await self.memory_engine.get_memory_dashboard_data()
+            return dashboard_data
+        except Exception as e:
+            logger.error(f"Error in _async_get_memory_dashboard: {e}\n{traceback.format_exc()}")
+            return {'status': 'error', 'message': str(e)}
+
+    def get_memory_dashboard(self) -> Dict[str, Any]:
+        """
+        EPC method: Retrieves data for the memory management dashboard.
+        """
+        return asyncio.run(self._async_get_memory_dashboard())
+
+    async def _async_update_memory_epc(self, update_data: Dict[str, Any]) -> bool:
+        """
+        Asynchronous core logic for updating memory via EPC.
+        """
+        try:
+            if isinstance(update_data, list) and len(update_data) == 1 and isinstance(update_data[0], dict):
+                update_data = update_data[0]
+
+            item_type_str = update_data.get('item_type')
+            
+            if not item_type_str:
+                return False
+
+            if item_type_str == MemoryItemType.USER_PREFERENCE.value:
+                key = update_data.get('key')
+                value = update_data.get('value')
+                if key is not None:
+                    await self.memory_engine.update_user_preference(key, value)
+                    return True
+                else:
+                    return False
+            
+            elif item_type_str == MemoryItemType.USER_FEEDBACK.value:
+                feedback_content = update_data.get('content')
+                if feedback_content and isinstance(feedback_content, dict):
+                    feedback_item = MemoryItem(
+                        id=f"feedback_{int(time.time())}",
+                        type=MemoryItemType.USER_FEEDBACK,
+                        content=feedback_content,
+                        metadata=update_data.get('metadata', {})
+                    )
+                    await self.memory_engine.add_memory_item(feedback_item)
+                    return True
+                else:
+                    return False
+            
+            else:
+                return False
+
+        except Exception as e:
+            logger.error(f"Error in _async_update_memory_epc: {e}\n{traceback.format_exc()}")
+            return False
+
+    def update_memory_epc(self, update_data: Dict[str, Any]) -> bool:
+        """
+        EPC method: Allows Emacs to update specific memory items.
+        """
+        return asyncio.run(self._async_update_memory_epc(update_data))
+
+    async def _async_trigger_memory_synthesis(self, session_id: str) -> Dict[str, Any]:
+        """Triggers the memory synthesis process for a given session."""
+        try:
+            candidates = await self.memory_synthesizer.synthesize_from_dialogue(session_id)
+            return {"status": "success", "message": f"Found {len(candidates)} new candidates.", "new_candidate_count": len(candidates)}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def trigger_memory_synthesis_for_session(self, session_id: str) -> Dict[str, Any]:
+        """EPC Method: Triggers memory synthesis for a session."""
+        return asyncio.run(self._async_trigger_memory_synthesis(session_id))
+
+    def get_candidate_memories(self) -> Dict[str, Any]:
+        """Returns all pending candidate memories, formatted for EPC."""
+        try:
+            candidates = self.memory_synthesizer.get_pending_candidates()
+            serializable_candidates = [dataclasses.asdict(c) for c in candidates]
+            for cand_dict in serializable_candidates:
+                if 'proposed_item' in cand_dict and isinstance(cand_dict['proposed_item'], MemoryItem):
+                     cand_dict['proposed_item'] = dataclasses.asdict(cand_dict['proposed_item'])
+            return {"status": "success", "result": serializable_candidates}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+            
+    async def _async_process_candidate_memory(self, candidate_id: str, action: str) -> Dict[str, Any]:
+        """Processes a user's decision on a candidate memory."""
+        candidate = self.memory_synthesizer.get_candidate_by_id(candidate_id)
+        if not candidate:
+            return {"status": "error", "message": f"Candidate with ID {candidate_id} not found."}
+
+        try:
+            if action == "accept":
+                await self.memory_engine.add_memory_item(candidate.proposed_item)
+                self.memory_synthesizer.discard_candidate(candidate_id)
+                return {"status": "success", "message": f"Accepted candidate {candidate_id}."}
+            elif action == "reject":
+                self.memory_synthesizer.discard_candidate(candidate_id)
+                return {"status": "success", "message": f"Rejected candidate {candidate_id}."}
+            else:
+                return {"status": "error", "message": f"Invalid action '{action}'."}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def process_candidate_memory(self, candidate_id: str, action: str) -> Dict[str, Any]:
+        """EPC Method: Processes user decision on a candidate memory."""
+        return asyncio.run(self._async_process_candidate_memory(candidate_id, action))
+
+    # --- Knowledge Archaeology ---
+    async def _async_knowledge_archaeology_dig(self, query_text: str, top_k: int = 50) -> Dict[str, Any]:
+        """Finds nodes related to a query and returns them sorted by date."""
+        try:
+            similar_nodes_result = await asyncio.to_thread(self.engine.find_similar_nodes, query_text, top_k)
+            if not similar_nodes_result: return {"status": "success", "result": []}
+            node_ids = [node_id for node_id, score in similar_nodes_result]
+            node_details = self.graph_service.get_nodes_by_ids(node_ids)
+            if not node_details: return {"status": "success", "result": []}
+            valid_nodes = [n for n in node_details if n.get('document_date')]
+            sorted_nodes = sorted(valid_nodes, key=lambda x: x['document_date'], reverse=True)
+            return {"status": "success", "result": sorted_nodes}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def knowledge_archaeology_dig(self, query_text: str, top_k: int = 50) -> Dict[str, Any]:
+        """EPC method for the Knowledge Archaeology feature."""
+        return asyncio.run(self._async_knowledge_archaeology_dig(query_text, top_k))
+
+    # --- NER-based Features ---
+    async def _async_get_ner_tag_suggestions(self, payload: Dict) -> Dict[str, Any]:
+        try:
+            payload = normalize_payload(payload)
+            note_content = payload.get("note_content")
+            if not note_content: return {"status": "error", "message": "note_content is required."}
+            suggestions = await self.ner_service.suggest_tags_for_note(note_content, payload.get("existing_tags"))
+            return {"status": "success", "result": suggestions}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+        
+    def get_ner_tag_suggestions(self, payload):
+        return asyncio.run(self._async_get_ner_tag_suggestions(payload))
+
+    async def _async_discover_inferred_relationships(self, payload: Dict) -> Dict[str, Any]:
+        try:
+            payload = normalize_payload(payload)
+            note_content = payload.get("note_content")
+            if not note_content: return {"status": "error", "message": "note_content is required."}
+            relationships = await self.ner_service.discover_tag_relationships(note_content)
+            return {"status": "success", "result": relationships}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def discover_inferred_relationships(self, payload: Dict) -> Dict[str, Any]:
+        return asyncio.run(self._async_discover_inferred_relationships(payload))
+
+    # --- Generic Text Generation ---
+    async def _async_generate_text(self, prompt: str) -> Dict[str, Any]:
+        try:
+            if not prompt: return {"status": "error", "message": "Prompt cannot be empty."}
+            response_text = await self.llm_client.generate(prompt)
+            return {"status": "success", "result": response_text}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def generate_text(self, prompt: str) -> Dict[str, Any]:
+        return asyncio.run(self._async_generate_text(prompt))
+
+    # --- MemoryEngine EPC Helpers ---
+    def epc_get_user_preference(self, key: str, default: Optional[Any] = None) -> Optional[Any]:
+        try:
+            return asyncio.run(self.memory_engine.get_user_preference(key, default))
+        except Exception:
+            return default
+
+    def epc_add_dialogue_turn(self, session_id: str, speaker: str, text: str, metadata: Optional[Dict[str, Any]]=None) -> Dict[str, Any]:
+        try:
+            dialogue_history_obj = asyncio.run(self.memory_engine.add_dialogue_turn(session_id, speaker, text, metadata or {}))
+            if dialogue_history_obj:
+                return {"status": "success", "session_id": dialogue_history_obj.session_id, "turn_count": len(dialogue_history_obj.turns)}
+            return {"status": "error", "message": "Failed to add dialogue turn"}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    def epc_get_dialogue_history(self, session_id: str, max_turns: Optional[int] = None) -> Optional[Dict[str, Any]]:
+        try:
+            history_obj = asyncio.run(self.memory_engine.get_dialogue_history(session_id, max_turns))
+            if history_obj:
+                return {
+                    "id": history_obj.id, "session_id": history_obj.session_id,
+                    "turns": [{"speaker": t.speaker, "text": t.text, "timestamp": t.timestamp, "metadata": t.metadata} for t in history_obj.turns],
+                    "summary": history_obj.summary, "timestamp": history_obj.timestamp, "metadata": history_obj.metadata
+                }
+            return None
+        except Exception:
+            return None
+
+    def epc_summarize_dialogue_history(self, dialogue_history_id: str) -> Dict[str, Any]:
+        try:
+            success = asyncio.run(self.memory_engine.summarize_dialogue_history(dialogue_history_id))
+            return {"status": "success" if success else "failure"}
+        except Exception as e:
+            return {"status": "error", "message": str(e)}
+
+    async def _async_analyze_conceptual_resonance(self, context_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+        """
+        Asynchronous core logic for analyzing conceptual resonance.
+        """
+        if isinstance(context_data, list) and len(context_data) == 1 and isinstance(context_data[0], dict):
+            context_data = context_data[0]
+
+        source_node_id = context_data.get('source_node_id')
+        target_node_id = context_data.get('target_node_id')
+
+        if not source_node_id or not target_node_id:
+            return None
+
+        try:
+            # 1. Fetch node details
+            node_details_list = self.graph_service.get_nodes_by_ids([source_node_id, target_node_id])
+            if len(node_details_list) < 2:
+                return None
+            
+            nodes_by_id = {n['node_id']: n for n in node_details_list}
+            source_node = nodes_by_id[source_node_id]
+            target_node = nodes_by_id[target_node_id]
+
+            # 2. Find common neighbors/tags
+            source_neighbors = self.graph_service.get_neighbors(source_node_id)
+            target_neighbors = self.graph_service.get_neighbors(target_node_id)
+            source_neighbor_ids = {n['node_id'] for n in source_neighbors}
+            target_neighbor_ids = {n['node_id'] for n in target_neighbors}
+            common_neighbor_ids = list(source_neighbor_ids.intersection(target_neighbor_ids))
+
+            # 3. Formulate a prompt for the LLM
+            prompt = f"""
+Analyze the conceptual resonance between two nodes.
+
+Source Node:
+Title: {source_node.get('title', 'N/A')}
+Content: {source_node.get('content', '')[:500]}...
+
+Target Node:
+Title: {target_node.get('title', 'N/A')}
+Content: {target_node.get('content', '')[:500]}...
+
+They share {len(common_neighbor_ids)} common neighbors/tags.
+
+Based on this information, provide:
+1.  A "resonance_score" between 0.0 and 1.0.
+2.  A brief, one-paragraph "explanation" of their relationship.
+3.  A list of "connecting_concepts" (strings).
+
+Return the response as a single JSON object.
+"""
+            # 4. Generate response from LLM
+            response_text = await self.llm_client.generate(prompt)
+            
+            # 5. Parse and return the structured response
+            try:
+                structured_response = json.loads(response_text)
+                return {"status": "success", "result": structured_response}
+            except json.JSONDecodeError:
+                return {"status": "error", "message": "LLM returned non-JSON response.", "raw_response": response_text}
+
+        except Exception as e:
+            logger.error(f"Error in _async_analyze_conceptual_resonance: {e}", exc_info=True)
+            return {"status": "error", "message": str(e)}
+
+    def analyze_conceptual_resonance(self, context_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+        return asyncio.run(self._async_analyze_conceptual_resonance(context_data))
+
+    async def _async_analyze_node_context(self, context_data: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Asynchronous core logic for analyzing a node's context.
+        """
+        logger.warning("analyze_node_context is temporarily simplified during refactoring.")
+        if isinstance(context_data, list) and len(context_data) == 1 and isinstance(context_data[0], dict):
+            context_data = context_data[0]
+        
+        node_id = context_data.get('node_id')
+        if not node_id:
+            return {"status": "error", "message": "node_id is required."}
+
+        # Simplified implementation: just get the node and its direct neighbors.
+        node_data = self.graph_service.get_node_by_id(node_id)
+        if not node_data:
+            return {"status": "error", "message": f"Node with ID {node_id} not found."}
+        
+        neighbors = self.graph_service.get_neighbors(node_id)
+        
+        return {
+            "status": "success",
+            "result": {
+                "node": node_data,
+                "neighbors": neighbors,
+                "relations": [] # Stubbed for now
+            }
+        }
+
+    def analyze_node_context(self, context_data: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        EPC method: Gets a node's local context from the knowledge graph.
+        """
+        return asyncio.run(self._async_analyze_node_context(context_data)) 
\ No newline at end of file
diff --git a/simtag/module/feedback_handler.py b/simtag/module/feedback_handler.py
deleted file mode 100644
index 118dcc0..0000000
--- a/simtag/module/feedback_handler.py
+++ /dev/null
@@ -1,47 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-import logging
-from typing import Dict, Any
-
-logger = logging.getLogger(__name__)
-
-class FeedbackHandler:
-    """
-    Handles user feedback. This is a simplified version after the removal of MemoryEngine.
-    Currently, it only logs the feedback received.
-    """
-    def __init__(self, config):
-        """
-        Initializes the simplified FeedbackHandler.
-        """
-        self.config = config
-        logger.info("FeedbackHandler initialized (simplified).")
-
-    async def submit_feedback(self, feedback_data: Dict[str, Any]) -> Dict[str, Any]:
-        """
-        Receives and logs user feedback. Does not store it permanently.
-
-        Args:
-            feedback_data: A dictionary containing feedback details.
-
-        Returns:
-            A dictionary confirming the receipt of the feedback.
-        """
-        try:
-            item_id = feedback_data.get("item_id")
-            feedback_type = feedback_data.get("feedback_type")
-
-            if not item_id or not feedback_type:
-                logger.warning("Received feedback with missing 'item_id' or 'feedback_type'.")
-                return {"status": "error", "message": "Missing 'item_id' or 'feedback_type'."}
-
-            # In this simplified version, we just log the feedback.
-            # In the future, this could be stored in the graph DB or another system.
-            logger.info(f"Received feedback for item '{item_id}': {feedback_data}")
-            
-            return {"status": "success", "message": "Feedback received and logged."}
-
-        except Exception as e:
-            logger.error(f"Error processing feedback: {e}", exc_info=True)
-            return {"status": "error", "message": str(e)}
diff --git a/simtag/module/knowledge_handler.py b/simtag/module/knowledge_handler.py
deleted file mode 100644
index 85dd9f8..0000000
--- a/simtag/module/knowledge_handler.py
+++ /dev/null
@@ -1,194 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-KnowledgeHandler
-================
-Background knowledge processor responsible for staged entity extraction and relation inference.
-Stage definitions:
-1. PENDING              —— Pending entity extraction
-2. ENTITIES_EXTRACTED   —— Entities extracted, pending relation inference
-3. COMPLETED            —— Relations written, process finished
-
-This implementation focuses on the framework and state transitions; specific LLM Prompt and parsing logic can be refined later.
-"""
-import logging
-import asyncio
-from typing import Dict, Any, List
-
-from simtag.core.graph_service import GraphService
-from simtag.services.embedding_service import EmbeddingService
-from simtag.services.llm_client import LLMClient
-from simtag.services.rag_service import RAGService, Relationship
-from ..prompts import (
-    ENTITY_EXTRACTION_PROMPT,
-    QUERY_ANALYSIS_PROMPT,
-    QA_PROMPT,
-    DEFAULT_ENTITY_TYPES,
-    RELATION_INFERENCE_PROMPT,
-    ENTITY_ONLY_PROMPT,
-)
-
-logger = logging.getLogger(__name__)
-
-class KnowledgeHandler:
-    """Staged background knowledge generation processor."""
-
-    def __init__(self, config, graph_service: GraphService, embedding_service: EmbeddingService,
-                 rag_service: RAGService, llm_client: LLMClient):
-        self.config = config
-        self.graph_service = graph_service
-        self.embedding_service = embedding_service
-        self.rag_service = rag_service
-        self.llm_client = llm_client
-        self.llm_semaphore = asyncio.Semaphore(config.sync_max_concurrent_llm_tasks)
-        logger.info("KnowledgeHandler initialized. Concurrency limit: %s", config.sync_max_concurrent_llm_tasks)
-
-    # ---------------------------------------------------------------------
-    # Public EPC entry
-    # ---------------------------------------------------------------------
-    async def run_extraction_cycle(self, limit: int = 5) -> Dict[str, Any]:
-        """Runs one knowledge processing cycle, including entity extraction and relation inference stages."""
-        logger.info("KnowledgeHandler: starting extraction cycle with limit=%d", limit)
-        stage_a_processed = await self._run_entity_extraction(limit)
-        stage_b_processed = await self._run_relation_inference(limit)
-        total = stage_a_processed + stage_b_processed
-        return {
-            "status": "success",
-            "stage_a_processed": stage_a_processed,
-            "stage_b_processed": stage_b_processed,
-            "total_processed": total,
-        }
-
-    # ------------------------------------------------------------------
-    # Stage A: Entity Extraction
-    # ------------------------------------------------------------------
-
-    async def extract_entities(self, text: str) -> List[Dict[str, Any]]:
-        """Return list of entity dicts (name,type,description) using lightweight prompt."""
-        try:
-            prompt = ENTITY_ONLY_PROMPT.format(entity_types=DEFAULT_ENTITY_TYPES, input_text=text)
-            llm_res = await self.llm_client.generate(prompt, use_chat_endpoint=False)
-            if not llm_res or not llm_res.content:
-                return []
-            txt = llm_res.content.strip()
-            # strip fences if any
-            import re, json
-            txt = re.sub(r"^```[a-zA-Z]*\s*", "", txt).strip()
-            txt = re.sub(r"```$", "", txt).strip()
-            data = json.loads(txt)
-            return data.get("entities", []) if isinstance(data, dict) else []
-        except Exception as e:
-            logger.error("extract_entities failed: %s", e)
-            return []
-        
-    async def _run_entity_extraction(self, limit: int) -> int:
-        node_ids = self.graph_service.get_nodes_by_status("PENDING", limit)
-        if not node_ids:
-            return 0
-        logger.info("Stage A: %d nodes pending entity extraction", len(node_ids))
-        tasks = [self._extract_entities_for_node(nid) for nid in node_ids]
-        results = await asyncio.gather(*tasks, return_exceptions=True)
-        success_count = sum(1 for r in results if r is True)
-        return success_count
-
-    async def _extract_entities_for_node(self, node_id: str) -> bool:
-        """Placeholder implementation: only handles state transition, custom Prompt can be integrated later."""
-        try:
-            node = self.graph_service.get_node_by_id(node_id)
-            if not node:
-                logger.warning("Node %s not found during entity extraction", node_id)
-                return False
-
-            raw_text = node.get("content") or node.get("title") or ""
-            if not raw_text.strip():
-                # If no content, skip entity extraction and mark as completed.
-                self.graph_service.update_node_status(node_id, "COMPLETED")
-                return True
-
-            # --- Entity Extraction Logic ---
-            try:
-                entities_data = await self.rag_service.extract_entities(raw_text)
-                entities = [e.get("name") for e in entities_data if e.get("name")]
-            except Exception as inner_e:
-                logger.error("_extract_structured_data_from_text failed for node %s: %s", node_id, inner_e, exc_info=True)
-                entities = []
-
-            # Temporarily write entities to properties.extracted_entities
-            node_props = node.get("properties", {}) or {}
-            node_props["extracted_entities"] = entities
-            node_updated = {
-                "node_id": node_id,
-                "properties": node_props,
-            }
-            self.graph_service.upsert_entity(node_updated)
-
-            # Update status
-            new_status = "ENTITIES_EXTRACTED" if entities else "COMPLETED"
-            self.graph_service.update_node_status(node_id, new_status)
-            return True
-        except Exception as e:
-            logger.error("Entity extraction failed for node %s: %s", node_id, e, exc_info=True)
-            return False
-
-    # ------------------------------------------------------------------
-    # Stage B: Relation Inference
-    # ------------------------------------------------------------------
-    async def _run_relation_inference(self, limit: int) -> int:
-        node_ids = self.graph_service.get_nodes_by_status("ENTITIES_EXTRACTED", limit)
-        if not node_ids:
-            return 0
-        logger.info("Stage B: %d nodes pending relation inference", len(node_ids))
-        tasks = [self._infer_relations_for_node(nid) for nid in node_ids]
-        results = await asyncio.gather(*tasks, return_exceptions=True)
-        success_count = sum(1 for r in results if r is True)
-        return success_count
-
-    async def _infer_relations_for_node(self, node_id: str) -> bool:
-        """Placeholder implementation: infers relations and writes back to database."""
-        try:
-            node = self.graph_service.get_node_by_id(node_id)
-            if not node:
-                logger.warning("Node %s not found during relation inference", node_id)
-                return False
-
-            props = node.get("properties", {}) or {}
-            entities: List[str] = props.get("extracted_entities", [])
-            raw_text = node.get("content") or node.get("title") or ""
-
-            if not entities:
-                # No entities -> No relation inference, mark as completed directly
-                self.graph_service.update_node_status(node_id, "COMPLETED")
-                return True
-
-            # --- Relation Inference Logic ---
-            try:
-                relations = await self.rag_service.infer_relations(raw_text, entities)
-                for rel in relations:
-                    self.graph_service.upsert_relationship(rel.source, rel.target, rel.type)
-            except Exception as inner_e:
-                logger.error("infer_relations failed for node %s: %s", node_id, inner_e, exc_info=True)
-
-            # Completed status
-            self.graph_service.update_node_status(node_id, "COMPLETED")
-            return True
-        except Exception as e:
-            logger.error("Relation inference failed for node %s: %s", node_id, e, exc_info=True)
-            return False
-
-    # ------------------------------------------------------------------
-    # Diagnostics helpers
-    # ------------------------------------------------------------------
-
-    def get_queue_status(self) -> Dict[str, Any]:
-        """Returns the count of nodes in each knowledge status, for frontend display."""
-        try:
-            pending = len(self.graph_service.get_nodes_by_status("PENDING", 100000))
-            extracted = len(self.graph_service.get_nodes_by_status("ENTITIES_EXTRACTED", 100000))
-            return {
-                "status": "success",
-                "pending": pending,
-                "entities_extracted": extracted,
-            }
-        except Exception as e:
-            logger.error("Failed to get knowledge queue status: %s", e, exc_info=True)
-            return {"status": "error", "message": str(e)}
\ No newline at end of file
diff --git a/simtag/module/node_processor.py b/simtag/module/node_processor.py
new file mode 100755
index 0000000..e12fc03
--- /dev/null
+++ b/simtag/module/node_processor.py
@@ -0,0 +1,317 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import logging
+import time
+import asyncio
+import json
+from typing import List, Dict, Any, Optional
+import numpy as np
+from dependency_injector.wiring import inject, Provide
+import concurrent.futures
+import os
+
+from simtag.services.content_processor import ContentItem, ProcessingResult, EmbeddingResult, NERResult
+from simtag.core.entity_extractor import ExtractedEntity, LLMEntityExtractor
+from simtag.module.utils import compute_node_hash, compute_content_hash
+from simtag.core.graph_service import GraphService
+from simtag.services.embedding_service import EmbeddingService
+from simtag.services.ner_service import NERService
+
+
+logger = logging.getLogger(__name__)
+
+class NodeProcessor:
+    """
+    Handles the processing of individual nodes and batches of nodes,
+    including vectorization, NER, and knowledge graph updates.
+    """
+    def __init__(self, config, llm_client, ner_service, graph_service: GraphService, content_processor, emacs_client=None, embedding_service: EmbeddingService = None, entity_extractor: LLMEntityExtractor = None):
+        self.config = config
+        self.llm_client = llm_client
+        self.ner_service = ner_service
+        self.graph_service = graph_service
+        self.content_processor = content_processor
+        self.emacs_client = emacs_client
+        self.embedding_service = embedding_service
+        self.entity_extractor = entity_extractor
+        logger.info("NodeProcessor initialized.")
+
+    def _eval_in_emacs(self, *args):
+        # This is a placeholder. The actual implementation is in the bridge.
+        # This should be passed in or handled via a callback.
+        if self.emacs_client:
+            # A real implementation would call self.emacs_client.call(...)
+            logger.info(f"Placeholder: Would call Emacs with: {args}")
+            pass
+        else:
+            logger.warning("Emacs client not available to NodeProcessor.")
+
+    def _get_content_for_embedding(self, node_data: Dict) -> str:
+        """
+        Determines the most relevant text content for embedding.
+        Prioritizes the 'content' field if it's not empty, otherwise falls back to 'title'.
+        A node is considered to have no content only if both are empty.
+        """
+        content = node_data.get('content')
+        if content and isinstance(content, str) and content.strip():
+            return content.strip()
+
+        title = node_data.get('title')
+        if title and isinstance(title, str) and title.strip():
+            return title.strip()
+            
+        return ""
+
+    def process_nodes_batch(self, nodes: List[Dict[str, Any]], total_items: int) -> Dict[str, Any]:
+        """
+        Processes a batch of nodes from the snapshot in parallel.
+        Worker threads handle I/O-bound tasks (LLM calls), and the main thread
+        handles all database writes to prevent locking issues.
+        """
+        valid_nodes = [node for node in nodes if isinstance(node, dict) and 'id' in node]
+        invalid_nodes_count = len(nodes) - len(valid_nodes)
+
+        if invalid_nodes_count > 0:
+            logger.warning(f"Found {invalid_nodes_count} nodes failing validation.")
+
+        max_workers = self.config.processing_config.get('processing_workers', 4)
+        logger.info(f"✅ Starting to process {len(valid_nodes)} valid nodes with up to {max_workers} parallel workers...")
+
+        successful_count = 0
+        failed_nodes = []
+        all_processing_results = []
+
+        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
+            future_to_node = {executor.submit(self._process_single_node, node): node for node in valid_nodes}
+            
+            for future in concurrent.futures.as_completed(future_to_node):
+                node = future_to_node[future]
+                try:
+                    result_package = future.result()
+                    if result_package:
+                        all_processing_results.append(result_package)
+                    successful_count += 1
+                except Exception as e:
+                    node_id = node.get('id', 'N/A')
+                    logger.error(f"Error processing node {node_id}: {e}", exc_info=True)
+                    failed_nodes.append({'id': node_id, 'error': str(e)})
+
+        logger.info(f"Parallel processing completed. Success: {successful_count}/{len(valid_nodes)}")
+
+        # --- SEQUENTIAL DATABASE WRITES ---
+        if all_processing_results:
+            logger.info(f"Starting sequential database writes for {len(all_processing_results)} processed nodes...")
+            # 1. Collect all data to be written
+            nodes_to_upsert = []
+            embeddings_to_upsert = []
+            entities_to_upsert = []
+            relations_to_upsert = []
+
+            for result in all_processing_results:
+                if result.get('node_for_upsert'):
+                    nodes_to_upsert.append(result['node_for_upsert'])
+                if result.get('embedding_vector') is not None:
+                    embeddings_to_upsert.append((result['node_id'], result['embedding_vector']))
+                if result.get('inferred_entities'):
+                    entities_to_upsert.extend(result['inferred_entities'])
+                if result.get('inferred_relations'):
+                    relations_to_upsert.extend(result['inferred_relations'])
+
+            # 2. Perform batch writes
+            try:
+                if nodes_to_upsert:
+                    # Note: upsert_text_node handles one at a time, but its internals could be batched.
+                    # For now, we call it sequentially. A future optimization could be a bulk version.
+                    for node_up in nodes_to_upsert:
+                        self.graph_service.upsert_text_node(node_up)
+                    logger.info(f"Upserted metadata for {len(nodes_to_upsert)} text nodes.")
+
+                if embeddings_to_upsert:
+                    for node_id, vector in embeddings_to_upsert:
+                        self.graph_service.upsert_node_embedding(node_id, vector)
+                    logger.info(f"Upserted {len(embeddings_to_upsert)} embeddings.")
+
+                if entities_to_upsert:
+                    self.graph_service.bulk_upsert_entity_nodes(entities_to_upsert)
+                    logger.info(f"Upserted {len(entities_to_upsert)} inferred entity nodes.")
+
+                if relations_to_upsert:
+                    # This step needs pre-calculation of entity IDs after they are upserted.
+                    # For simplicity, we get IDs now. A more optimized way would be to get them in bulk.
+                    final_relations = []
+                    for rel in relations_to_upsert:
+                        source_id = self.graph_service._get_entity_id_by_name(rel['source_name'])
+                        target_id = self.graph_service._get_entity_id_by_name(rel['target_name'])
+                        if source_id and target_id:
+                            final_relations.append({
+                                'source_id': source_id,
+                                'target_id': target_id,
+                                'type': rel['type'],
+                                'properties': rel['properties']
+                            })
+                    if final_relations:
+                        self.graph_service.bulk_upsert_relations(final_relations)
+                        logger.info(f"Upserted {len(final_relations)} inferred relations.")
+
+            except Exception as e:
+                logger.error(f"A database error occurred during sequential writes: {e}", exc_info=True)
+                # Here you might want to add logic to handle partial writes or rollback.
+
+        if failed_nodes:
+            logger.error(f"Failed to process {len(failed_nodes)} nodes. First 5 failures:")
+            for i, failure in enumerate(failed_nodes[:5]):
+                logger.error(f"  Node {failure.get('id')} failed: {failure.get('error')}")
+
+        return {
+            'total_processed': len(valid_nodes),
+            'successful_count': successful_count,
+            'failed_count': len(failed_nodes),
+            'format_errors': invalid_nodes_count
+        }
+
+    def _process_single_node(self, node: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+        """
+        Prepares all data for a single node (embeddings, inferred relations) without
+        writing to the database. Returns a dictionary of data to be written by the main thread.
+        """
+        try:
+            loop = asyncio.get_running_loop()
+        except RuntimeError:
+            loop = asyncio.new_event_loop()
+            asyncio.set_event_loop(loop)
+
+        node_id = node.get('id')
+        content_for_embedding = self._get_content_for_embedding(node)
+        logger.debug(f"Preparing node {node_id}. Content length: {len(content_for_embedding)}")
+        
+        # 1. Prepare node data for upsert
+        node_for_upsert = node.copy()
+        properties_raw = node_for_upsert.get('properties', {})
+        properties_dict = properties_raw[0] if isinstance(properties_raw, list) and properties_raw else properties_raw
+        
+        if isinstance(properties_dict, dict):
+            tags_string = properties_dict.get('TAGS', '')
+            if isinstance(tags_string, str):
+                node_for_upsert['tags'] = [tag.strip() for tag in tags_string.split(':') if tag.strip()]
+
+        for key in ['properties', 'olp', 'tags', 'ref_to', 'ref_from', 'priority']:
+            if key in node_for_upsert and isinstance(node_for_upsert[key], (dict, list)):
+                node_for_upsert[key] = json.dumps(node_for_upsert[key])
+        
+        node_for_upsert['node_id'] = node_for_upsert.pop('id')
+
+        # This will hold all results from this thread
+        result_package = {
+            "node_id": node_id,
+            "node_for_upsert": node_for_upsert,
+            "embedding_vector": None,
+            "inferred_entities": [],
+            "inferred_relations": []
+        }
+
+        # 2. Generate embedding if there is content
+        if content_for_embedding:
+            embedding_config = self.config.embedding_config
+            embedding_result = loop.run_until_complete(self.embedding_service.get_embedding(
+                content_for_embedding,
+                backend=embedding_config.get('primary_backend'),
+                model=self.config.llm_client_config.get('default_embedding_model')
+            ))
+            if embedding_result.success:
+                result_package["embedding_vector"] = np.array(embedding_result.embedding, dtype=np.float32)
+            else:
+                logger.error(f"Embedding generation failed for node {node_id}: {embedding_result.error_message}")
+        
+        # 3. Infer relations from content if enabled
+        if self.config.analysis_config.get("enable_inferred_relations") and self.entity_extractor and content_for_embedding:
+            logger.debug(f"Inferring relations for node {node_id} as feature is enabled.")
+            existing_tags = node_for_upsert.get('tags', [])
+            
+            extraction_result = loop.run_until_complete(
+                self.entity_extractor.extract(content_for_embedding, existing_tags)
+            )
+            
+            inferred_entities_data = extraction_result.get("entities", [])
+            inferred_relations_data = extraction_result.get("relations", [])
+
+            if inferred_entities_data:
+                entities_to_add = []
+                for entity_data in inferred_entities_data:
+                    entities_to_add.append({
+                        'name': entity_data.get('name'),
+                        'description': entity_data.get('description', '')
+                    })
+                result_package["inferred_entities"] = entities_to_add
+                logger.debug(f"Prepared {len(entities_to_add)} entities for node {node_id}")
+
+            if inferred_relations_data:
+                relations_to_add = []
+                for rel_data in inferred_relations_data:
+                    source_name = rel_data.get('source')
+                    target_name = rel_data.get('target')
+                    if not target_name or not source_name:
+                        logger.warning(f"Skipping malformed relation from LLM for node {node_id}, missing source or target. Data: {rel_data}")
+                        continue
+                    
+                    relations_to_add.append({
+                        'source_name': source_name,
+                        'target_name': target_name,
+                        'type': rel_data.get('type', 'RELATED_TO'),
+                        'properties': json.dumps({'description': rel_data.get('description', '')})
+                    })
+                result_package["inferred_relations"] = relations_to_add
+                logger.debug(f"Prepared {len(relations_to_add)} relations for node {node_id}")
+
+        logger.debug(f"Finished preparing node {node_id}.")
+        return result_package
+
+    def find_similar_nodes(self, node_id: str, top_k: int = 5) -> List[Dict]:
+        """
+        Find nodes similar to the given node_id.
+        """
+        try:
+            # 1. Get the content of the source node
+            source_node = self.graph_service.get_node(node_id)
+            if not source_node:
+                logger.warning(f"Cannot find similar nodes: source node {node_id} not found in graph.")
+                return []
+            
+            content = source_node.get('content')
+            if not content:
+                logger.warning(f"Source node {node_id} has no content to compare.")
+                return []
+
+            # 2. Use the embedding service to find similar documents
+            similar_items = self.embedding_service.search(content, top_k=top_k)
+            
+            # 3. Format results
+            results = []
+            for item in similar_items:
+                # Assuming the search result item has 'id' and 'score'
+                results.append({
+                    "id": item.get('id'),
+                    "title": item.get('title', 'N/A'),
+                    "score": item.get('score', 0.0)
+                })
+            
+            return results
+        except Exception as e:
+            logger.error(f"Error finding similar nodes for {node_id}: {e}", exc_info=True)
+            return [] 
+
+    def get_all_nodes(self):
+        # This seems out of place for a "processor". Should be in a query handler.
+        # Removing for now to clarify responsibilities.
+        pass
+
+    def _sanitize_value(self, value: Any) -> Any:
+        """
+        Recursively sanitizes values to be compatible with SQLite.
+        """
+        if isinstance(value, (dict, list)):
+            return json.dumps(value)
+        elif isinstance(value, str):
+            return value.strip()
+        else:
+            return value 
\ No newline at end of file
diff --git a/simtag/module/query_handler.py b/simtag/module/query_handler.py
index 28fc12e..9ffe023 100755
--- a/simtag/module/query_handler.py
+++ b/simtag/module/query_handler.py
@@ -2,142 +2,154 @@
 # -*- coding: utf-8 -*-
 
 import logging
+from typing import Dict, Any, List
 import time
 import traceback
 
-from typing import Dict, Any, List
-
-from ..core.graph_service import GraphService
-from ..utils.unified_tag_processor import normalize_payload
-
-try:
-    import numpy as np
-except ImportError:
-    np = None
+from simtag.core.graph_service import GraphService
 
 logger = logging.getLogger(__name__)
 
 class QueryHandler:
-    """
-    Handles simple, direct queries against the knowledge graph, such as
-    keyword search and fetching specific node details.
-    """
-    def __init__(self, graph_service: GraphService):
+    def __init__(self, engine, user_interface, graph_service: GraphService, emacs_client=None):
+        self.engine = engine
+        self.user_interface = user_interface
         self.graph_service = graph_service
-        self.logger = logging.getLogger(__name__)
+        self.emacs_client = emacs_client
+        logger.info("QueryHandler initialized.")
 
-    def search(self, payload: Dict) -> List[Dict[str, Any]]:
+    def get_similar_nodes(self, query_input: str, top_k: int = 10):
         """
-        Performs a keyword search against node titles and content.
+        Finds similar nodes using the core tagging engine.
+        Moved from SimTagBridge.
         """
+        logger.info(f"QueryHandler.get_similar_nodes called for: '{query_input[:100]}...', top_k: {top_k}")
+        start_time = time.time()
         try:
-            data = normalize_payload(payload)
-            query_text = data.get("query")
-            limit = data.get("limit", 10)
-
-            if not query_text:
-                self.logger.warning("Search query is empty.")
-                return []
-
-            self.logger.info(f"Performing keyword search for: '{query_text}' with limit {limit}")
-            return self.graph_service.search_nodes_by_title_content(
-                search_query=query_text,
-                limit=limit
-            )
-
+            if not hasattr(self, 'engine') or self.engine is None:
+                logger.error("QueryHandler.get_similar_nodes: TaggingEngine (self.engine) is not initialized.")
+                return {"status": "error", "message": "TaggingEngine not initialized"}
+            
+            similar_nodes_result = self.engine.find_similar_nodes(query_input, top_k)
+            
+            logger.info(f"QueryHandler.get_similar_nodes for '{query_input[:100]}...' completed in {time.time() - start_time:.4f} seconds. Found {len(similar_nodes_result)} nodes.")
+            return {"status": "success", "result": similar_nodes_result}
         except Exception as e:
-            self.logger.error(f"Error during keyword search: {e}", exc_info=True)
-            return []
+            logger.error(f"QueryHandler.get_similar_nodes for '{query_input[:100]}...' failed: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
 
-    def get_node_details(self, payload: Dict) -> Dict[str, Any]:
+    def find_similar_nodes_friendly(self, query_input: str, top_k: int = 10):
         """
-        Retrieves full details for a given node_id, including its neighbors.
+        User-friendly method to find similar nodes, returning rich context.
+        Moved from SimTagBridge.
         """
+        logger.info(f"QueryHandler.find_similar_nodes_friendly: '{query_input[:100]}...', top_k: {top_k}")
+        start_time = time.time()
+        
         try:
-            data = normalize_payload(payload)
-            node_id = data.get("node_id")
-
-            if not node_id:
-                self.logger.warning("Node ID is missing for get_node_details.")
-                return {"error": "Node ID is required."}
-
-            self.logger.info(f"Fetching details for node: {node_id}")
-            node_details = self.graph_service.get_node_with_neighbors(node_id)
-
-            if not node_details:
-                return {"error": f"Node with ID '{node_id}' not found."}
-
-            return node_details
-
+            if not hasattr(self, 'user_interface') or self.user_interface is None:
+                logger.error("QueryHandler.find_similar_nodes_friendly: UserInterfaceService not initialized.")
+                return {"status": "error", "message": "UserInterfaceService not initialized"}
+            
+            if not hasattr(self, 'engine') or self.engine is None:
+                logger.error("QueryHandler.find_similar_nodes_friendly: TaggingEngine not initialized.")
+                return {"status": "error", "message": "TaggingEngine not initialized"}
+            
+            # Resolve user input to UUID if possible
+            resolved_query = self.user_interface.resolve_user_input_to_uuid(query_input) or query_input
+            
+            # Use the engine to find similar nodes (returns UUIDs)
+            uuid_results = self.engine.find_similar_nodes(resolved_query, top_k)
+            
+            # Convert to user-friendly format
+            friendly_results = self.user_interface.convert_similar_nodes_to_user_friendly(uuid_results)
+            
+            logger.info(f"QueryHandler.find_similar_nodes_friendly: Completed for '{query_input[:100]}...' in {time.time() - start_time:.4f}s. Found {len(friendly_results)} nodes.")
+            
+            return {
+                "status": "success", 
+                "result": friendly_results,
+                "query_info": {
+                    "original_query": query_input,
+                    "resolved_query": resolved_query,
+                    "total_results": len(friendly_results)
+                }
+            }
+            
         except Exception as e:
-            self.logger.error(f"Error fetching node details for {node_id}: {e}", exc_info=True)
-            return {"error": "An internal error occurred."}
+            logger.error(f"QueryHandler.find_similar_nodes_friendly failed for '{query_input[:100]}...': {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
 
-    async def get_similar_entities(self, *args) -> Dict[str, Any]:
+    def search_nodes_by_content(self, search_query: str, top_k: int = 10, fuzzy_match: bool = True):
         """
-        Finds nodes with similar content based on vector embeddings.
-        This is an async method intended to be called from the EPC bridge.
+        Searches for nodes by their content (title, keywords, etc.).
+        Moved from SimTagBridge.
         """
-        node_id = None  # Initialize for logging in case of early failure
+        logger.info(f"QueryHandler.search_nodes_by_content: '{search_query}', top_k: {top_k}, fuzzy: {fuzzy_match}")
+        start_time = time.time()
+        
         try:
-            payload = normalize_payload(args)
-            node_id = payload.get("node_id")
-            top_k = payload.get("top_k", 5)
-
-            if not node_id:
-                self.logger.warning("Node ID is missing for get_similar_entities.")
-                return {"status": "error", "message": "Node ID is required."}
-
-            self.logger.info(f"Fetching similar entities for node: {node_id}")
-
-            # Note: graph_service methods are synchronous.
-            # We don't need to await them unless they are converted to async.
-            query_embedding = self.graph_service.get_entity_embedding_by_id(node_id)
-
-            if query_embedding is None:
-                self.logger.warning(f"No embedding found for node {node_id}.")
-                # It's not an error if a node simply has no embedding yet.
-                # Return success with an empty list.
-                return {"status": "success", "result": []}
-
-            # The find_similar_nodes expects a list of floats
-            if np and isinstance(query_embedding, np.ndarray):
-                embedding_list = query_embedding.tolist()
-            else:
-                embedding_list = query_embedding
-
-
-            similar_nodes = self.graph_service.find_similar_nodes(
-                query_embedding=embedding_list,
-                top_k=top_k + 1  # Fetch one more to account for the source node
+            if not hasattr(self, 'user_interface') or self.user_interface is None:
+                logger.error("QueryHandler.search_nodes_by_content: UserInterfaceService not initialized.")
+                return {"status": "error", "message": "UserInterfaceService not initialized"}
+            
+            # Use the user interface service to perform the search
+            search_results = self.user_interface.search_nodes_by_title_or_content(
+                query=search_query,
+                top_k=top_k,
+                fuzzy_match=fuzzy_match
             )
-
-            # Exclude the query node itself from the results
-            results = [node for node in similar_nodes if node.get("node_id") != node_id]
-
-            # Trim to top_k if necessary
-            results = results[:top_k]
-
-            self.logger.info(f"Found {len(results)} similar entities for node {node_id}.")
-            return {"status": "success", "result": results}
-
+            
+            logger.info(f"QueryHandler.search_nodes_by_content: Completed for '{search_query}' in {time.time() - start_time:.4f}s. Found {len(search_results)} results.")
+            
+            return {
+                "status": "success",
+                "result": search_results,
+                "search_info": {
+                    "query": search_query,
+                    "total_results": len(search_results),
+                    "fuzzy_match_enabled": fuzzy_match
+                }
+            }
+            
         except Exception as e:
-            self.logger.error(f"Error fetching similar entities for {node_id}: {e}", exc_info=True)
-            return {"status": "error", "message": f"An internal error occurred: {e}"}
+            logger.error(f"QueryHandler.search_nodes_by_content failed for '{search_query}': {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
 
-    def get_similar_nodes(self, query_input: str, top_k: int = 10):
-        logger.info(f"QueryHandler.get_similar_nodes: '{query_input[:100]}...', top_k: {top_k}")
+    def get_node_context_friendly(self, node_identifier: str):
+        """
+        Gets the full context for a node in a user-friendly format.
+        Moved from SimTagBridge.
+        """
+        logger.info(f"QueryHandler.get_node_context_friendly: '{node_identifier}'")
         start_time = time.time()
+        
         try:
-            # This is a temporary solution. The engine concept will be removed.
-            # Directly call embedding service and graph service.
-            try:
-                similar_nodes_result = self.graph_service.search_nodes_by_title_content(query_input, limit=top_k)
-                logger.info(f"QueryHandler.get_similar_nodes completed in {time.time() - start_time:.4f} seconds. Found {len(similar_nodes_result)} nodes.")
-                return {"status": "success", "result": similar_nodes_result}
-            except Exception as search_error:
-                logger.error(f"Text search also failed: {search_error}")
-                return {"status": "error", "message": "Both embedding and text search failed"}
+            if not hasattr(self, 'user_interface') or self.user_interface is None:
+                logger.error("QueryHandler.get_node_context_friendly: UserInterfaceService not initialized.")
+                return {"status": "error", "message": "UserInterfaceService not initialized"}
+            
+            # Resolve user input to a UUID
+            node_uuid = self.user_interface.resolve_user_input_to_uuid(node_identifier)
+            if not node_uuid:
+                return {"status": "error", "message": f"Could not resolve node identifier: {node_identifier}"}
+            
+            # Get the node context
+            context_info = self.user_interface.get_node_context_by_uuid(node_uuid)
+            if not context_info:
+                return {"status": "error", "message": f"Node not found: {node_identifier}"}
+            
+            logger.info(f"QueryHandler.get_node_context_friendly: Completed for '{node_identifier}' in {time.time() - start_time:.4f}s.")
+            
+            return {
+                "status": "success",
+                "result": context_info,
+                "node_info": {
+                    "identifier": node_identifier,
+                    "resolved_uuid": node_uuid
+                }
+            }
+            
         except Exception as e:
-            logger.error(f"QueryHandler.get_similar_nodes failed: {e}\n{traceback.format_exc()}")
-            return {"status": "error", "message": str(e)} 
+            logger.error(f"QueryHandler.get_node_context_friendly failed for '{node_identifier}': {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)} 
\ No newline at end of file
diff --git a/simtag/module/rag_handler.py b/simtag/module/rag_handler.py
deleted file mode 100644
index 2e4f935..0000000
--- a/simtag/module/rag_handler.py
+++ /dev/null
@@ -1,81 +0,0 @@
-import logging
-from typing import Dict, Any
-
-from ..services.rag_service import RAGService
-from ..utils.unified_tag_processor import normalize_payload
-from ..prompts import generate_query_with_language_instruction
-
-logger = logging.getLogger(__name__)
-
-class RAGHandler:
-    """
-    A clean, simple handler that acts as an API endpoint for the RAGService.
-    """
-    def __init__(self, rag_service: RAGService):
-        self.rag_service = rag_service
-        self.logger = logging.getLogger(__name__)
-
-    async def query(self, payload: Dict) -> Dict[str, Any]:
-        """
-        Receives a query payload from the Elisp front-end, passes it to the
-        RAGService, and returns the result.
-        """
-        try:
-            # Handle chat query data format (different from snapshot data)
-            data = self._normalize_chat_payload(payload)
-            self.logger.debug(f"Normalized chat data keys: {list(data.keys())}")
-            query_text = data.get("query") or data.get("query_text")
-            history = data.get("history", [])
-            lang = data.get("lang") # Extract language setting
-            command = data.get("command") # Extract command setting
-
-            if not query_text:
-                self.logger.error("Query payload did not contain 'query' text.")
-                return {"error": "Query text is missing."}
-
-            if isinstance(query_text, list):
-                query_text = " ".join(str(x) for x in query_text)
-
-            # Generate a language-specific query if lang is provided and no command is active
-            # Skip language prefix for custom commands as they already contain complete instructions
-            if lang and not command:
-                final_query = generate_query_with_language_instruction(lang, query_text)
-            else:
-                final_query = query_text
-
-            self.logger.info(f"RAGHandler received query: '{final_query}' (history len {len(history)}, command: {command})")
-            return await self.rag_service.query(final_query, history=history, command=command)
-
-        except Exception as e:
-            self.logger.error(f"Error in RAGHandler query: {e}", exc_info=True)
-            return {"error": f"An unexpected error occurred in RAGHandler: {e}"}
-
-    def _normalize_chat_payload(self, payload: Any) -> Dict:
-        """
-        Normalize chat query payload from Elisp.
-        This is a simplified version for chat data that doesn't use the full snapshot validation.
-        """
-        logger.debug(f"_normalize_chat_payload received raw payload of type: {type(payload)}")
-
-        current_data = payload
-        # Elisp often wraps data in a single-element list or tuple
-        while isinstance(current_data, (list, tuple)) and len(current_data) == 1:
-            current_data = current_data[0]
-            logger.debug(f"Unwrapped payload, current type: {type(current_data)}")
-
-        # After unwrapping, we should have the core alist (as a list/tuple of lists/tuples)
-        if isinstance(current_data, (list, tuple)):
-            from ..utils.unified_tag_processor import _parse_elisp_data
-            parsed_dict = _parse_elisp_data(current_data)
-            if isinstance(parsed_dict, dict):
-                logger.debug(f"Successfully parsed chat payload to dict: {list(parsed_dict.keys())}")
-                return parsed_dict
-            else:
-                logger.error(f"Parsed chat data is not a dict, but {type(parsed_dict)}. Returning empty dict.")
-                return {}
-        
-        if isinstance(current_data, dict):
-            return current_data
-
-        logger.error(f"Could not normalize chat payload. Final type was {type(current_data)}. Returning empty dict.")
-        return {}
\ No newline at end of file
diff --git a/simtag/module/resonance_handler.py b/simtag/module/resonance_handler.py
new file mode 100755
index 0000000..39858ee
--- /dev/null
+++ b/simtag/module/resonance_handler.py
@@ -0,0 +1,120 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import logging
+import json
+from typing import Dict, Any, List
+
+from simtag.core.graph_service import GraphService
+from simtag.services.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+class ResonanceHandler:
+    """
+    Handles the logic for the "Core Resonance Loop" MVP.
+    Provides proactive suggestions by finding non-obvious connections
+    between the current note and existing notes in the knowledge base.
+    """
+    def __init__(self, graph_service: GraphService, llm_client: LLMClient, config: Dict[str, Any] = None):
+        """
+        Initializes the ResonanceHandler.
+
+        Args:
+            graph_service: The service for interacting with the knowledge graph.
+            llm_client: The client for interacting with Large Language Models.
+            config: Optional configuration dictionary.
+        """
+        self.graph_service = graph_service
+        self.llm_client = llm_client
+        self.config = config or {}
+        self.top_k = self.config.get("resonance_top_k", 5)
+        self.min_results_for_resonance = self.config.get("min_results_for_resonance", 2)
+        logger.info(f"ResonanceHandler initialized. Top-K: {self.top_k}, Min Results: {self.min_results_for_resonance}")
+
+    def _build_prompt(self, source_content: str, related_nodes: List[Dict[str, Any]]) -> str:
+        """Builds the prompt for the LLM to find conceptual resonance."""
+        
+        source_summary = (source_content[:200] + '...') if len(source_content) > 200 else source_content
+        
+        related_items_str = ""
+        for i, node in enumerate(related_nodes):
+            title = node.get('title', f"Node {node.get('id')}")
+            content_summary = node.get('content', '')
+            summary = (content_summary[:150] + '...') if len(content_summary) > 150 else content_summary
+            related_items_str += f"{i+1}. Title: {title}\n   Summary: {summary}\n\n"
+
+        prompt = f"""
+Analyze the following source text and a list of related items from a knowledge base. Your task is to identify a single, non-obvious, and insightful connection, theme, or contradiction between the source text and the related items. Express this connection in one compelling sentence.
+
+**Source Text:**
+---
+{source_summary}
+---
+
+**Related Items:**
+---
+{related_items_str}
+---
+
+**Your Task:**
+In one sentence, what is the most interesting and non-obvious connection?
+"""
+        return prompt.strip()
+
+    def get_resonance(self, content: str, current_node_id: str = None) -> Dict[str, Any]:
+        """
+        The core logic for finding and describing a conceptual resonance.
+
+        Args:
+            content: The text content of the current note being edited.
+            current_node_id: The ID of the node to exclude from results.
+
+        Returns:
+            A dictionary containing the resonance description and related nodes,
+            or an empty dictionary if no resonance is found.
+        """
+        logger.debug(f"Getting resonance for content (node_id: {current_node_id})")
+        
+        try:
+            # 1. Vector search for top-k similar nodes
+            exclude_ids = [current_node_id] if current_node_id else []
+            similar_nodes = self.graph_service.search_nodes_by_content(
+                query_text=content, 
+                top_k=self.top_k, 
+                exclude_ids=exclude_ids
+            )
+
+            if not similar_nodes or len(similar_nodes) < self.min_results_for_resonance:
+                logger.info("Not enough similar nodes found to generate a meaningful resonance.")
+                return {"status": "no_resonance", "message": "Not enough connections found."}
+
+            # 2. Build a prompt for the LLM
+            prompt = self._build_prompt(content, similar_nodes)
+            logger.debug(f"Generated resonance prompt:\n{prompt}")
+
+            # 3. Call LLM to get the connection insight
+            llm_response_str = self.llm_client.generate(prompt)
+            if not llm_response_str:
+                logger.warning("LLM returned an empty response for resonance prompt.")
+                return {"status": "error", "message": "LLM returned empty response."}
+
+            # For simplicity, we assume the LLM returns just the sentence.
+            # In a real scenario, we might want to parse a JSON object.
+            insight = llm_response_str.strip()
+            
+            # 4. Format and return the result
+            related_node_ids = [node.get('id') for node in similar_nodes]
+
+            result = {
+                "status": "success",
+                "insight": insight,
+                "related_nodes": related_node_ids,
+                "full_prompt_for_debug": prompt
+            }
+            logger.info(f"Successfully found resonance: '{insight}'")
+            return result
+
+        except Exception as e:
+            logger.error(f"Error getting resonance: {e}", exc_info=True)
+            return {"status": "error", "message": str(e)} 
\ No newline at end of file
diff --git a/simtag/module/smart_companion_handler.py b/simtag/module/smart_companion_handler.py
deleted file mode 100644
index 72963b1..0000000
--- a/simtag/module/smart_companion_handler.py
+++ /dev/null
@@ -1,64 +0,0 @@
-import logging
-import json
-import asyncio
-from typing import Dict, Any, List, Optional
-from dataclasses import dataclass
-
-from ..utils.unified_tag_processor import normalize_payload
-
-logger = logging.getLogger(__name__)
-
-
-@dataclass
-class TagSuggestion:
-    """Data structure for tag suggestions"""
-    name: str
-    confidence: float
-    source_type: str = "llm"  # llm, vector, graph
-    reasoning: Optional[str] = None
-
-
-class SmartCompanionHandler:
-    """
-    Smart Knowledge Companion Handler
-    
-    Provides tag suggestions for untagged nodes by delegating to auto-tag infrastructure
-    """
-    
-    def __init__(self, autotag_handler):
-        self.autotag_handler = autotag_handler
-        self.logger = logging.getLogger(__name__)
-        
-    async def analyze_tag_context(self, payload: Dict) -> Dict[str, Any]:
-        """
-        Analyze tag context and return intelligent suggestions.
-        
-        This method now delegates to the autotag handler for tag suggestions.
-        The smart companion frontend should call autotag endpoints directly.
-        
-        Args:
-            payload: Dictionary containing node information for tag suggestions
-            
-        Returns:
-            Dictionary containing tag suggestions results
-        """
-        try:
-            data = normalize_payload(payload)
-            self.logger.info("Smart companion delegating to autotag handler for tag suggestions")
-            
-            # Delegate to autotag handler
-            if hasattr(self.autotag_handler, 'generate_tags_for_nodes'):
-                result = await self.autotag_handler.generate_tags_for_nodes(payload)
-                return result
-            else:
-                return {
-                    "status": "error", 
-                    "message": "Autotag handler not available for tag suggestions"
-                }
-                
-        except Exception as e:
-            self.logger.error(f"Smart companion analysis failed: {e}", exc_info=True)
-            return {"status": "error", "message": str(e)}
-
-    # Additional methods can be added here for future smart companion features
-    # For now, we delegate all tag suggestion logic to the autotag handler
\ No newline at end of file
diff --git a/simtag/module/sync_handler.py b/simtag/module/sync_handler.py
index b84d56a..ac413e3 100755
--- a/simtag/module/sync_handler.py
+++ b/simtag/module/sync_handler.py
@@ -2,232 +2,303 @@
 # -*- coding: utf-8 -*-
 
 import logging
-from typing import Dict, Any, List, TYPE_CHECKING
+from typing import Dict, Any, List
 import json
 import traceback
 import asyncio
 import concurrent.futures
-import numpy as np
-
-if TYPE_CHECKING:
-    from ..core.graph_service import GraphService
-    from simtag.services.embedding_service import EmbeddingService
-    from simtag.services.rag_service import RAGService
-else:
-    from ..core.graph_service import GraphService
-    from simtag.services.embedding_service import EmbeddingService
-    from simtag.services.rag_service import RAGService
-from simtag.utils.unified_tag_processor import normalize_payload
-from ..utils.text_processing import prepare_node_text_for_embedding, generate_semantic_id
-from ..config import Config
-
-
 
 logger = logging.getLogger(__name__)
 
 class SyncHandler:
-    """
-    Coordinates the synchronization of data between Emacs and the Python backend.
-    It orchestrates GraphService, EmbeddingService, and RAGService to process
-    and store nodes, embeddings, and inferred relations.
-    """
-    def __init__(
-        self,
-        graph_service: "GraphService",
-        embedding_service: "EmbeddingService", 
-        rag_service: "RAGService",
-        config: Config
-    ):
-        self.graph_service = graph_service
-        self.embedding_service = embedding_service
-        self.rag_service = rag_service
-        self.config = config
-        # Create a semaphore to limit concurrent LLM calls
-        self.llm_semaphore = asyncio.Semaphore(config.sync_max_concurrent_llm_tasks)
-        logger.info(f"SyncHandler initialized. LLM concurrency limit: {config.sync_max_concurrent_llm_tasks}")
-
-    def _get_content_for_node(self, node_data: Dict) -> str:
-        """
-        Determines the most relevant text content for processing.
-        """
-        content = node_data.get('content')
-        if content and isinstance(content, str) and content.strip():
-            return content.strip()
-        title = node_data.get('title')
-        if title and isinstance(title, str) and title.strip():
-            return title.strip()
-        return ""
-
-    async def _process_knowledge_with_limit(self, raw_content: str, node_id: str):
-        """
-        A wrapper function that acquires a semaphore before calling the LLM service,
-        thus limiting concurrency.
-        """
-        async with self.llm_semaphore:
-            logger.debug(f"Semaphore acquired for node {node_id}. Processing knowledge...")
-            await self.rag_service.process_and_store_text(raw_content, node_id)
-            logger.debug(f"Semaphore released for node {node_id}.")
+    def __init__(self, node_processor, engine, emacs_client=None):
+        self.node_processor = node_processor
+        self.engine = engine
+        self.emacs_client = emacs_client
+        logger.info("SyncHandler initialized.")
+
+    def sync_library(self, db_file, db_snapshot_json_str):
+        logger.info(f"sync_library called with db_file: {db_file}, receiving DB snapshot as JSON string.")
+        
+        num_tags = "N/A"
+        num_nodes = "N/A"
+        parsed_snapshot = None
 
-    async def _process_single_entity_for_sync(self, entity: Dict[str, Any]):
+        try:
+            if not isinstance(db_snapshot_json_str, str):
+                logger.error(f"DB snapshot is not a JSON string as expected (type: {type(db_snapshot_json_str)}). Aborting sync.")
+                return {"status": "error", "message": "Invalid snapshot format (not a JSON string)"}
+
+            logger.debug(f"Attempting to parse JSON snapshot string (length: {len(db_snapshot_json_str)} chars). Preview: {db_snapshot_json_str[:200]}...")
+            parsed_snapshot = json.loads(db_snapshot_json_str) # Parse the JSON string
+            logger.info("Successfully parsed DB snapshot JSON string.")
+
+            if isinstance(parsed_snapshot, dict):
+                num_tags = len(parsed_snapshot.get("tags", []))
+                num_nodes = len(parsed_snapshot.get("nodes", []))
+                logger.info(f"Parsed snapshot contains: tags={num_tags}, nodes={num_nodes}.")
+            else:
+                logger.error(f"Parsed snapshot is not a dictionary (type: {type(parsed_snapshot)}). Aborting sync.")
+                return {"status": "error", "message": "Parsed snapshot is not a dictionary"}
+
+            # Call the engine method with the parsed Python dictionary
+            result = self.engine.sync_full_snapshot(parsed_snapshot) 
+            
+            return {"status": "success", "result": result}
+        except json.JSONDecodeError as je:
+            logger.error(f"JSON decoding failed for DB snapshot: {je}\nFull string was (approx first 500 chars): {db_snapshot_json_str[:500]}")
+            return {"status": "error", "message": f"JSON decoding error: {je}"}
+        except Exception as e:
+            logger.error(f"Sync library with full snapshot failed: {e}\n{traceback.format_exc()}")
+            # Include details about the snapshot if it was parsed, to help debug if error is in engine
+            parsed_info = f"Parsed snapshot (tags: {num_tags}, nodes: {num_nodes})" if parsed_snapshot else "Snapshot not parsed."
+            return {"status": "error", "message": f"General error during sync: {e}. {parsed_info}"}
+
+    async def _async_sync_node_from_elisp(self, node_props: Dict[str, Any]) -> Dict[str, Any]:
         """
-        处理单个实体：存储数据并根据实体类型生成嵌入向量
-        
-        现在期望接收已经验证过的实体数据字典
+        Asynchronously processes a single node from Elisp.
+        This is the new, preferred method for syncing individual nodes.
+        The logic is moved here from SimTagBridge.
         """
-        entity_id = entity.get('id')  # 注意：使用 'id' 而不是 'node_id'
-        entity_type = entity.get('type')
+        node_id = node_props.get("id")
+        if not node_id:
+            logger.error("sync_node_from_elisp: Received node properties without an 'id'.")
+            return {"status": "error", "message": "Node 'id' is required."}
 
-        if not entity_id:
-            logger.warning(f"Skipping entity without an ID. Original data: {entity}")
-            return
+        logger.info(f"SyncHandler: Processing node {node_id}.")
 
         try:
-            # 将 'id' 映射为 'node_id' 以兼容 graph_service
-            if 'id' in entity and 'node_id' not in entity:
-                entity['node_id'] = entity['id']
-
-            # 只要是节点类型，就将其状态设置为 PENDING，以便知识周期可以处理
-            if entity_type == 'node':
-                # 确保 properties 存在
-                if 'properties' not in entity or not isinstance(entity.get('properties'), dict):
-                    entity['properties'] = {}
-                # 设置状态以进行知识提取
-                entity['properties']['status'] = 'PENDING'
-                logger.debug(f"Set status to PENDING for node {entity_id}")
-
-            # 如果 content 为空但 title 不为空，则将 title 作为 content 以便后续嵌入和 RAG 使用
-            if entity_type == 'node' and (not entity.get('content')) and entity.get('title'):
-                entity['content'] = entity['title']
-            # 1. Upsert the entity's data first.
-            self.graph_service.upsert_entity(entity)
-
-            # 2. If it's a node, proceed with embedding and knowledge extraction.
-            if entity_type == 'node':
-                raw_content = entity.get('content', '') or entity.get('title', '')
-
-                # Fast path：只做嵌入，不做耗时的 LLM 推理
-                logger.info(f"Starting embedding process for node {entity_id}")
-                embedding_result = await self.embedding_service.get_node_embedding(entity)
-
-                if embedding_result is not None:
-                    # 根据配置决定是否使用语义标识符
-                    if self.config.use_semantic_embedding_ids:
-                        semantic_id = generate_semantic_id(entity)
-                        self.graph_service.upsert_entity_embedding(semantic_id, embedding_result)
-                        logger.info(f"Successfully embedded entity {entity_id} with semantic ID '{semantic_id}' and shape {embedding_result.shape}")
-                    else:
-                        self.graph_service.upsert_entity_embedding(entity_id, embedding_result)
-                        logger.info(f"Successfully embedded entity {entity_id} with shape {embedding_result.shape}")
-                else:
-                    logger.warning(f"Failed to get embedding for entity {entity_id}")
-
-            # 3. If it's a tag, we'll handle embedding later in batch
-            elif entity_type == 'tag':
-                logger.debug(f"Tag entity {entity_id} will be processed in batch embedding")
+            title = node_props.get("title", "")
+            content_body = node_props.get("content", "")
+            tags = node_props.get("tags", [])
+            
+            # The content for processing is the combination of title and body.
+            content = f"{title}\n\n{content_body}".strip()
+
+            # Delegate the core processing to NodeProcessor
+            success = await self.node_processor.process_single_node_data(
+                node_id=node_id, 
+                content=content, 
+                tags=tags, 
+                title=title
+            )
+
+            if success:
+                logger.info(f"SyncHandler: Successfully processed node {node_id}.")
+                return {"status": "success", "node_id": node_id}
+            else:
+                logger.warning(f"SyncHandler: NodeProcessor failed to process node {node_id}.")
+                return {"status": "error", "message": f"NodeProcessor failed for node {node_id}.", "node_id": node_id}
 
         except Exception as e:
-            logger.error(f"Error processing entity {entity_id} in worker: {e}", exc_info=True)
+            logger.error(f"SyncHandler: Error syncing node {node_id}: {e}", exc_info=True)
+            return {"status": "error", "message": str(e), "node_id": node_id}
+
+    def sync_node_from_elisp(self, node_props: Dict[str, Any]) -> Dict[str, Any]:
+        """Synchronous EPC wrapper for _async_sync_node_from_elisp."""
+        return asyncio.run(self._async_sync_node_from_elisp(node_props))
 
-    async def bulk_process_snapshot(self, snapshot_data: Any) -> Dict[str, Any]:
+    async def _async_bulk_process_snapshot(self, snapshot_data: Dict[str, Any]) -> Dict[str, Any]:
         """
-        Main async entry point for periodic background sync.
+        Main entry point for periodic background sync.
         Processes an incremental snapshot of changes from the Elisp database.
-        
-        Now receiving data that has been validated by normalize_payload
         """
-        # Data has been validated and cleaned by normalize_payload
-        normalized_data = normalize_payload(snapshot_data)
-        entities_to_upsert = normalized_data.get("entities", [])
-        links_to_upsert = normalized_data.get("links", [])
-        ids_to_delete = normalized_data.get("ids_to_delete", [])
-
-        logger.info(f"Received snapshot: {len(entities_to_upsert)} entities, {len(links_to_upsert)} links, {len(ids_to_delete)} deletions.")
-
-        # 1. Handle Deletions
-        if ids_to_delete:
-            self.graph_service.delete_nodes_by_ids(ids_to_delete)
-            logger.info(f"Deleted {len(ids_to_delete)} entities.")
-
-        # 2. Process Entities (Nodes and Tags) Concurrently
-        tag_ids_to_embed = []
-        if entities_to_upsert:
-            # Separate tags and nodes for subsequent batch embedding
-            for entity in entities_to_upsert:
-                if entity.get('type') == 'tag':
-                    tag_ids_to_embed.append(entity.get('id'))
-
-            # Process all entities concurrently
-            tasks = [self._process_single_entity_for_sync(entity) for entity in entities_to_upsert]
-            await asyncio.gather(*tasks)
-            logger.info(f"Finished processing {len(entities_to_upsert)} entities.")
-
-        # 3. Process Tags for Embedding (Batch process tag embedding)
-        if tag_ids_to_embed:
-            tag_ids_to_embed = [tid for tid in tag_ids_to_embed if tid]
-            logger.info(f"Starting embedding process for {len(tag_ids_to_embed)} tags.")
-            loop = asyncio.get_running_loop()
-            with concurrent.futures.ThreadPoolExecutor() as pool:
-                await loop.run_in_executor(
-                    pool,
-                    self.embedding_service.batch_embed_tags,
-                    tag_ids_to_embed
-                )
-            logger.info(f"Finished processing {len(tag_ids_to_embed)} tags.")
-
-        # 4. Upsert Org-mode Links as Relations
-        if links_to_upsert:
-            for link in links_to_upsert:
-                # Data has been validated, use directly
-                self.graph_service.upsert_relationship(
-                    source_id=link['source'],
-                    target_id=link['target'],
-                    type=link.get('type', 'REF_TO'),
-                    properties={'raw_link': link}
-                )
-            logger.info(f"Upserted {len(links_to_upsert)} link-based relations.")
-
-        # 4.5 Refresh embeddings for STALE tags created by new HAS_TAG relations
-        await self.embedding_service.refresh_stale_tags()
-        
-        # 5. Commit any pending transactions
         try:
-            self.graph_service._get_connection().commit()
-            logger.info("Final commit successful.")
+            nodes_to_upsert = snapshot_data.get("nodes", [])
+            links_to_upsert = snapshot_data.get("links", [])
+            ids_to_delete = snapshot_data.get("ids_to_delete", [])
+
+            total_upsert = len(nodes_to_upsert)
+            total_delete = len(ids_to_delete)
+            logger.info(f"Received incremental snapshot: {total_upsert} nodes to upsert, {total_delete} IDs to delete, {len(links_to_upsert)} links to upsert.")
+
+            # 1. Perform deletions first to handle nodes that might be re-added with a new type, etc.
+            if ids_to_delete:
+                logger.info(f"Deleting {len(ids_to_delete)} objects from graph.")
+                self.node_processor.graph_service.delete_nodes_by_ids(ids_to_delete)
+
+            # 2. Process nodes that need to be created or updated.
+            # This will handle embedding and relation inference for new/changed content.
+            if nodes_to_upsert:
+                logger.info(f"Delegating upsert processing of {len(nodes_to_upsert)} nodes to NodeProcessor.")
+                loop = asyncio.get_running_loop()
+                with concurrent.futures.ThreadPoolExecutor() as executor:
+                    report = await loop.run_in_executor(
+                        executor, self.node_processor.process_nodes_batch, nodes_to_upsert, len(nodes_to_upsert)
+                    )
+                logger.info(f"NodeProcessor finished. Report: {report}")
+            else:
+                report = {"status": "no nodes to process"}
+
+            # 3. Upsert link relationships. This is a direct graph operation.
+            if links_to_upsert:
+                relations_from_links = []
+                for link in links_to_upsert:
+                    source_id = link.get('source')
+                    target_id = link.get('target')
+                    if source_id and target_id:
+                        relations_from_links.append({
+                            'source_id': source_id,
+                            'target_id': target_id,
+                            'type': 'REF_TO',  # Correctly hardcode the relation type for org links
+                            'properties': json.dumps({'raw_link': link})
+                        })
+                
+                if relations_from_links:
+                    logger.info(f"Upserting {len(relations_from_links)} link-based relations.")
+                    self.node_processor.graph_service.bulk_upsert_relations(relations_from_links)
+
+            logger.info("Incremental snapshot processing complete.")
+            return {"status": "success", "result": report}
+            
         except Exception as e:
-            logger.error(f"Error during final commit: {e}", exc_info=True)
-
-        return {
-            "status": "success", 
-            "processed_entities": len(entities_to_upsert),
-            "processed_links": len(links_to_upsert),
-            "deleted_entities": len(ids_to_delete)
-        }
-
-    async def _embed_and_store_entity(self, entity_id, entity_data):
-        """Embeds entity data and stores the embedding for a given entity."""
-        logger.info(f"Embedding content for entity {entity_id}...")
-        embedding_result = await self.embedding_service.get_node_embedding(entity_data)
-        if embedding_result is not None:
-            self.graph_service.upsert_entity_embedding(entity_id, embedding_result)
-            logger.info(f"Successfully embedded and stored entity {entity_id}")
-        else:
-            logger.error(f"Failed to embed entity {entity_id}")
+            logger.error(f"SyncHandler: Error in _async_bulk_process_snapshot: {e}", exc_info=True)
+            return {"status": "error", "message": str(e)}
 
+    def bulk_process_snapshot(self, snapshot_data: Any) -> Dict[str, Any]:
+        """Synchronous EPC wrapper for _async_bulk_process_snapshot."""
+        # 统一使用 unified_tag_processor 进行所有数据标准化
+        from simtag.utils.unified_tag_processor import normalize_payload
+        
+        try:
+            # 按照规范，应该接收 [hash-table] 格式，但EPC可能序列化了hash-table
+            normalized_data = normalize_payload(snapshot_data)
+            logger.debug(f"Normalized data keys: {normalized_data.keys() if isinstance(normalized_data, dict) else 'N/A'}")
+            
+            return asyncio.run(self._async_bulk_process_snapshot(normalized_data))
+            
+        except Exception as e:
+            logger.error(f"Failed to normalize snapshot data: {e}", exc_info=True)
+            return {"status": "error", "message": f"Data normalization failed: {str(e)}"}
 
+    def sync_tag_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
+        """同步标签事件到 Python 后端。
+        
+        Args:
+            event_data: 事件数据，包含事件类型和相关数据
+            
+        Returns:
+            同步结果
+        """
+        try:
+            event_type = event_data.get('event_type')
+            tag_data = event_data.get('tag_data')
+            
+            if not event_type or not tag_data:
+                return {"status": "error", "message": "Missing event_type or tag_data"}
+            
+            # 根据事件类型处理
+            if event_type == 'tag:status-changed':
+                return self.update_tag_status(tag_data)
+            elif event_type == 'tag:rules-changed':
+                return self.update_tag_rules(tag_data)
+            elif event_type == 'tag:relation-changed':
+                return self.update_relation_type(tag_data)
+            else:
+                return {"status": "error", "message": f"Unknown event type: {event_type}"}
+                
+        except Exception as e:
+            logger.error(f"Error in sync_tag_event: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
 
-    def sync_deleted_nodes(self, payload):
+    def update_tag_status(self, tag_data: Dict[str, Any]) -> Dict[str, Any]:
+        """更新标签状态。
+        
+        Args:
+            tag_data: 标签数据，包含标签ID和新状态
+            
+        Returns:
+            更新结果
         """
-        Receives a list of deleted node IDs and removes them from the graph.
+        try:
+            tag_id = tag_data.get('id')
+            new_status = tag_data.get('tag-status')
+            
+            if not tag_id or not new_status:
+                return {"status": "error", "message": "Missing tag_id or new_status"}
+            
+            # 更新数据库中的标签状态
+            self.engine.update_tag_status(tag_id, new_status)
+            
+            return {"status": "success", "tag_id": tag_id, "new_status": new_status}
+            
+        except Exception as e:
+            logger.error(f"Error in update_tag_status: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
+
+    def update_tag_rules(self, tag_data: Dict[str, Any]) -> Dict[str, Any]:
+        """更新标签规则。
+        
+        Args:
+            tag_data: 标签数据，包含标签ID和规则
+            
+        Returns:
+            更新结果
         """
-        deleted_node_ids = normalize_payload(payload)
-        logger.info(f"Received request to delete {len(deleted_node_ids)} nodes.")
-        if not deleted_node_ids:
-            return
+        try:
+            tag_id = tag_data.get('id')
+            rules = tag_data.get('tag-rules')
+            
+            if not tag_id or not rules:
+                return {"status": "error", "message": "Missing tag_id or rules"}
+            
+            # 更新数据库中的标签规则
+            self.engine.update_tag_rules(tag_id, rules)
+            
+            return {"status": "success", "tag_id": tag_id}
+            
+        except Exception as e:
+            logger.error(f"Error in update_tag_rules: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
+
+    def update_relation_type(self, relation_data: Dict[str, Any]) -> Dict[str, Any]:
+        """更新标签关系类型。
         
+        Args:
+            relation_data: 关系数据，包含源标签ID、目标标签ID和关系类型
+            
+        Returns:
+            更新结果
+        """
         try:
-            self.graph_service.delete_nodes_by_ids(deleted_node_ids)
-            logger.info(f"Successfully deleted {len(deleted_node_ids)} nodes.")
+            from_tag = relation_data.get('from-tag')
+            to_tag = relation_data.get('to-tag')
+            rel_type = relation_data.get('tag-rel-type')
+            
+            if not from_tag or not to_tag or not rel_type:
+                return {"status": "error", "message": "Missing from_tag, to_tag, or rel_type"}
+            
+            # 更新数据库中的关系类型
+            self.engine.update_relation_type(from_tag, to_tag, rel_type)
+            
+            return {"status": "success", "from_tag": from_tag, "to_tag": to_tag}
+            
         except Exception as e:
-            logger.error(f"Failed to delete nodes: {e}", exc_info=True)
+            logger.error(f"Error in update_relation_type: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)}
 
- 
\ No newline at end of file
+    def update_relation_rules(self, relation_data: Dict[str, Any]) -> Dict[str, Any]:
+        """更新标签关系规则。
+        
+        Args:
+            relation_data: 关系数据，包含源标签ID、目标标签ID和规则
+            
+        Returns:
+            更新结果
+        """
+        try:
+            from_tag = relation_data.get('from-tag')
+            to_tag = relation_data.get('to-tag')
+            rules = relation_data.get('tag-rel-rules')
+            
+            if not from_tag or not to_tag or not rules:
+                return {"status": "error", "message": "Missing from_tag, to_tag, or rules"}
+            
+            # 更新数据库中的关系规则
+            self.engine.update_relation_rules(from_tag, to_tag, rules)
+            
+            return {"status": "success", "from_tag": from_tag, "to_tag": to_tag}
+            
+        except Exception as e:
+            logger.error(f"Error in update_relation_rules: {e}\n{traceback.format_exc()}")
+            return {"status": "error", "message": str(e)} 
\ No newline at end of file
diff --git a/simtag/ollama_bridge.py b/simtag/ollama_bridge.py
new file mode 100644
index 0000000..bab4b9e
--- /dev/null
+++ b/simtag/ollama_bridge.py
@@ -0,0 +1,227 @@
+"""
+SimTag Ollama Bridge Module - Provides interaction with the Ollama model
+"""
+
+import logging
+from typing import Any, Optional, Dict, Literal
+import subprocess
+import traceback
+import requests
+import json
+import sys
+
+class OllamaBridge:
+    """Ollama API integration, providing basic LLM call functionality"""
+    
+    def __init__(self, model: Optional[str] = None):
+        """Initialize the Ollama client
+        
+        Args:
+            model: The name of the model to use
+        """
+        self.logger = logging.getLogger("simtag.ollama_bridge")
+        if not model:
+            model = "hf.co/unsloth/gemma-3-4b-it-GGUF:latest"
+        # Clean up model name - remove any quotes and extra whitespace
+        self.model = str(model).strip().replace('"', '').replace("'", '')
+        self.logger.info(f"Initialized OllamaBridge, default model: {self.model}")
+
+    def run(self, prompt: str, system: Optional[str] = None, model: Optional[str] = None) -> str:
+        """Execute interaction with Ollama model.
+
+        Args:
+            prompt: User prompt text.
+            system: Optional system prompt text.
+            model: Optional model name to override the default for this call.
+
+        Returns:
+            The response content string from the model.
+
+        Raises:
+            Exception: If the Ollama interaction fails.
+        """
+        try:
+            # Determine the model to use for this specific call
+            model_to_use = model.strip().replace('"', '').replace("'", '') if model and model.strip() else self.model
+            self.logger.info(f"Executing Ollama call. Model: {model_to_use}. Prompt length: {len(prompt)}")
+            if system:
+                self.logger.info(f"System prompt provided (length: {len(system)}).")
+
+            # Build request data
+            data = {
+                "model": model_to_use,
+                "prompt": prompt,
+                "stream": False,  # Do not use streaming response
+                "options": {
+                    "temperature": 0.7,  # Control the randomness of the output
+                    "num_predict": 1024,  # Maximum output length
+                    "stop": []  # Stop markers
+                }
+            }
+            
+            # Add system prompt
+            if system:
+                data["system"] = system
+            
+            # Log the generated request data (excluding sensitive content)
+            self.logger.info(f"Sending API request to model: {model_to_use}")
+            
+            # Send the request
+            try:
+                response = requests.post(
+                    "http://127.0.0.1:11434/api/generate",
+                    json=data,
+                    headers={"Content-Type": "application/json"},
+                    timeout=60  # Add timeout setting
+                )
+            except requests.RequestException as e:
+                self.logger.error(f"Request exception: {e}")
+                raise Exception(f"Request exception: {e}")
+            
+            # Check the response status code
+            if response.status_code == 200:
+                try:
+                    response_data = response.json()
+                    result = response_data.get('response', '').strip()
+                    
+                    # Log the generation statistics
+                    if 'eval_duration' in response_data:
+                        eval_duration = response_data['eval_duration']
+                        eval_count = response_data.get('eval_count', 0)
+                        tokens_per_second = eval_count / (eval_duration / 1e9) if eval_duration > 0 else 0
+                        self.logger.info(f"Generation speed: {tokens_per_second:.2f} tokens/s")
+                    
+                    if not result:
+                        self.logger.warning("Ollama returned an empty response")
+                        
+                    self.logger.info("Ollama API call successful")
+                    self.logger.debug(f"Response result: {result[:100]}..." if len(result) > 100 else f"Response result: {result}")
+                    return result
+                except json.JSONDecodeError as e:
+                    self.logger.error(f"Failed to parse JSON response: {e}")
+                    self.logger.error(f"Original response content: {response.text[:200]}...")
+                    raise Exception(f"Failed to parse JSON response: {e}")
+            else:
+                error_msg = f"Ollama API call failed: HTTP {response.status_code} - {response.text}"
+                self.logger.error(error_msg)
+                raise Exception(error_msg)
+                
+        except requests.exceptions.ConnectionError as e:
+            error_msg = f"Failed to connect to Ollama service: {str(e)}"
+            self.logger.error(error_msg)
+            raise Exception(error_msg)
+            
+        except Exception as e:
+            error_msg = f"Ollama execution exception: {str(e)}"
+            self.logger.error(error_msg)
+            self.logger.error(traceback.format_exc())
+            raise Exception(error_msg)
+
+    def status(self) -> Dict[str, Any]:
+        """Get Ollama status"""
+        try:
+            self.logger.info("Checking Ollama status")
+            result = subprocess.run(
+                ["ollama", "list"],
+                capture_output=True,
+                text=True
+            )
+            
+            if result.returncode == 0:
+                self.logger.info("Ollama status check successful")
+                return {
+                    "available": True,
+                    "model": self.model,
+                    "models": result.stdout.strip()
+                }
+            else:
+                self.logger.error(f"Ollama status check failed: {result.stderr}")
+                return {
+                    "available": False,
+                    "error": result.stderr
+                }
+                
+        except Exception as e:
+            error_msg = f"Ollama status check exception: {str(e)}"
+            self.logger.error(error_msg)
+            return {
+                "available": False,
+                "error": error_msg
+            }
+
+def _test():
+    """Test Ollama Bridge functionality"""
+    # Set up logging
+    logging.basicConfig(
+        level=logging.DEBUG,
+        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+        datefmt='%Y-%m-%d %H:%M:%S'
+    )
+    logger = logging.getLogger("ollama_bridge_test")
+    
+    try:
+        logger.info("Starting Ollama Bridge test")
+        
+        # 1. Test initialization
+        bridge = OllamaBridge()
+        logger.info(f"OllamaBridge instance created successfully, using model: {bridge.model}")
+        
+        # 2. Test status check
+        logger.info("Testing status check...")
+        status = bridge.status()
+        logger.info(f"Ollama status: {status}")
+        
+        # 3. Test simple dialogue with generate mode
+        logger.info("Testing simple dialogue (generate mode)...")
+        prompt = "Hello, please introduce yourself in one sentence."
+        response = bridge.run(prompt, mode="generate")
+        logger.info(f"Simple dialogue response (generate): {response}")
+        
+        # 4. Test simple dialogue with chat mode
+        logger.info("Testing simple dialogue (chat mode)...")
+        response = bridge.run(prompt, mode="chat")
+        logger.info(f"Simple dialogue response (chat): {response}")
+        
+        # 5. Test dialogue with system prompt (generate mode)
+        logger.info("Testing dialogue with system prompt (generate mode)...")
+        system = "You are a concise assistant, answer should be short."
+        prompt = "Explain what is artificial intelligence."
+        response = bridge.run(prompt, system=system, mode="generate")
+        logger.info(f"Dialogue response with system prompt (generate): {response}")
+        
+        # 6. Test dialogue with system prompt (chat mode)
+        logger.info("Testing dialogue with system prompt (chat mode)...")
+        response = bridge.run(prompt, system=system, mode="chat")
+        logger.info(f"Dialogue response with system prompt (chat): {response}")
+        
+        # 7. Test tag generation scenario
+        logger.info("Testing tag generation scenario...")
+        system = """You are a tag generation expert. Please analyze the given text and generate the most relevant tags.
+Requirements:
+1. Each tag should be concise and accurate
+2. Tags should reflect the main theme and concepts of the text
+3. Return the format as a comma-separated list of tags
+4. Do not explain, just return the list of tags"""
+        
+        test_text = """
+        Python is a popular programming language, known for its concise syntax and rich ecosystem.
+        It is widely used in fields such as web development, data analysis, and artificial intelligence.
+        """
+        # Try both modes
+        response_gen = bridge.run(test_text, system=system, mode="generate")
+        logger.info(f"Tag generation response (generate): {response_gen}")
+        response_chat = bridge.run(test_text, system=system, mode="chat")
+        logger.info(f"Tag generation response (chat): {response_chat}")
+        
+        logger.info("All tests completed")
+        
+    except Exception as e:
+        logger.error(f"Error during testing: {e}")
+        logger.error(traceback.format_exc())
+        return False
+        
+    return True
+
+if __name__ == "__main__":
+    success = _test()
+    sys.exit(0 if success else 1) 
\ No newline at end of file
diff --git a/simtag/prompts.py b/simtag/prompts.py
index c366da6..0d50c6f 100755
--- a/simtag/prompts.py
+++ b/simtag/prompts.py
@@ -3,133 +3,113 @@
 Unified Prompt Management Module
 ================================
 
-Centralized management for all prompt templates.
+Centralized management for a unified tag extraction prompt template.
+This module provides a single, robust prompt for all entity and tag extraction
+tasks, inspired by high-quality prompt engineering principles. It ensures
+format consistency and maintainability.
 """
 
 import logging
 logger = logging.getLogger(__name__)
+# The single, unified prompt template for all tag extraction purposes.
+# It is designed to be clear, structured, and provide few-shot examples
+# to guide the LLM towards a consistent and accurate output format.
+TAG_EXTRACTION_PROMPT = """
+-Goal-
+Your primary mission is to act as an expert tagger. Given a text document (or a batch of them), you will meticulously analyze the content and extract key entities, concepts, and topics that can serve as relevant tags. Your focus is on extracting specific nouns, such as people, places, and key concepts, that are highly relevant to the text's meaning.
+
+-Entity Types-
+You should focus on identifying entities belonging to the following categories. This list helps guide your extraction process:
+[{entity_types}]
+
+-Instructions-
+1.  **Analyze**: Read the provided text content carefully.
+2.  **Identify**: Pinpoint all significant entities and concepts that match the requested entity types.
+3.  **Format**: For each identified tag, create a JSON object with the following fields:
+    - `tag_name`: The name of the tag/entity. It must be formatted for org-mode: replace all spaces with underscores (`_`). For example, "Emacs Lisp" becomes "Emacs_Lisp".
+    - `confidence`: A score from 0.0 to 1.0 indicating your confidence in the tag's relevance.
+    - `reasoning`: A brief, clear explanation of why you chose this tag based on the text.
+    - `note_index`: (For batch processing ONLY) The 0-based index of the note this tag belongs to.
+4.  **Output**: Return your findings as a single, flat JSON array containing all the tag objects. Do not include any other text, explanations, or formatting outside of this JSON array.
+5.  **Avoid Duplicates**: If a list of existing tags is provided for a note, do not suggest the same tags again.
+
+-Critical Output Format-
+The final output MUST be a valid JSON array, structured exactly as shown in the examples below.
 
-# ==============================================================================
-# The single, definitive prompt for all advanced entity, relation,
-# and keyword extraction purposes, based on the high-quality MiniRAG prompt.
-# ==============================================================================
-
-# Delimiters for the custom tuple-based format
-TUPLE_DELIMITER = "<|>"
-RECORD_DELIMITER = "##"
-COMPLETION_DELIMITER = "<|COMPLETE|>"
-
-# Default entity types from MiniRAG
-DEFAULT_ENTITY_TYPES = ["organization", "person", "location", "event"]
-
-ENTITY_EXTRACTION_PROMPT = """
-Task: From the input text, output ONE JSON object with keys:
-  entities        – list of {{name, type, description}}
-  relationships   – list of {{source, target, type, description, strength}}
-  keywords        – list of strings
+######################
+-Examples-
+######################
 
-Rules:
-- Use only the entity types: {entity_types}
-- relationship.type MUST be UPPER_SNAKE_CASE.
-- strength is a number between 1.0 and 10.0.
-- NO extra text, markdown, or code fences.
+--- Example 1: Single Note ---
+[
+  {{"tag_name": "Emacs_Lisp", "confidence": 0.9, "reasoning": "The text explicitly discusses function definitions and variable bindings in Emacs Lisp."}},
+  {{"tag_name": "Asynchronous_Processing", "confidence": 0.85, "reasoning": "The note describes a system for running background tasks and handling their callbacks, which is a core concept of asynchronous programming."}}
+]
 
-Text:
-{input_text}
+--- Example 2: Batch Processing ---
+[
+  {{"tag_name": "Docker", "confidence": 0.95, "reasoning": "Note 0 is a tutorial on how to containerize a Python application using Docker.", "note_index": 0}},
+  {{"tag_name": "REST_API", "confidence": 0.9, "reasoning": "Note 0 mentions creating a REST API with Flask.", "note_index": 0}},
+  {{"tag_name": "Kubernetes", "confidence": 0.8, "reasoning": "Note 1 discusses the challenges of deploying applications at scale using Kubernetes.", "note_index": 1}}
+]
 
-JSON example:
-{{
-  "entities": [
-    {{"name": "Alice", "type": "PERSON", "description": "..."}}
-  ],
-  "relationships": [
-    {{"source": "Alice", "target": "Bob", "type": "COLLABORATES_WITH", "description": "...", "strength": 7.5}}
-  ],
-  "keywords": ["project"]
-}}
+######################
+-Real Data-
+######################
 
-Your JSON:
+{content_section}
 """
 
-# ==============================================================================
-# Prompt for Relation Inference (Stage B)
-# ==============================================================================
+ORG_MODE_TAG_EXTRACTION_PROMPT = """
+You are an expert metadata annotator for Org-mode. Your goal is to extract high-signal, human-readable tags.
 
-RELATION_INFERENCE_PROMPT = """-Role-
-You are a relationship extractor working with a predefined list of entities.
+- Fields to consider:
+  - `concepts`: Key ideas, themes.
+  - `entities`: People, organizations, specific products.
+  - `projects`: Names of projects or initiatives.
 
--Input-
-Entities: {entities}
-Text: {input_text}
+- Rules:
+  - `tag_name` MUST be `lower_case_with_underscores`.
+  - EXCLUDE: machine IDs (UUIDs), file paths, timestamps, generic terms, existing tags.
+  - FOCUS: Meaningful nouns that aid knowledge organization and retrieval.
 
--Goal-
-Identify all direct relationships between **only** the provided entities. Ignore mentions of entities outside the list.
-
--Output Format-
-Return **one** JSON object.
-Key: "relationships"
-Value: list of objects with keys:
-    source  – entity name (string, must exist in Entities)
-    target  – entity name (string, must exist in Entities)
-    type    – UPPER_SNAKE_CASE relationship label (string, e.g., "MENTIONS", "USES")
-
-Example Output:
-{{"relationships": [
-  {{"source": "Python", "target": "FastAPI", "type": "USES"}},
-  {{"source": "FastAPI", "target": "Pydantic", "type": "DEPENDS_ON"}}
-]}}
-
-**Do not** add any explanation, markdown fences, or extra keys.
-"""
+- Output:
+  - Return a single JSON array of objects with tag_name, confidence, and reasoning fields.
+  - **JSON array ONLY. No other text.**
 
-# ==============================================================================
-# Prompt for Auto-Tagging
-# ==============================================================================
-
-AUTOTAG_SUGGESTION_PROMPT = """
-You are an AI assistant specializing in analyzing text to suggest relevant tags.
-
-**Instructions (EN)**
-1. **Text Analysis**
-   • Focus on core themes, key entities (concepts/ people / organizations / places), technologies, and domain-specific terms.
-   • Prioritize terms representing the main subject rather than peripheral details.
-2. **Tag Generation Rules**
-   • Generate **up to 5** tags; fewer is acceptable when content is narrow.
-   • Each tag must be short, no more than 1-3 words.
-   • Convert internal spaces to **snake_case** — e.g. "natural language processing" → "natural_language_processing".
-   • For non-English terms, translate to English first, then apply snake_case rule.
-   • Avoid overly broad terms. Example: use "ai_safety" instead of "technology".
-3. **Output Format**
-   • Return **only** a JSON object with key "tags" whose value is a list of strings.
-   • Do NOT wrap in Markdown, code fences, or add extra keys.
-
-**示例 (Example)**
-Input Summary: "Python in machine learning and knowledge graph"
-Expected Output:
-{{
-    "tags": ["python", "machine_learnning", "knowledge_graph"]
-}}
+- Example:
+  - Text: "ID: 940281B0-1486-4D80-9604-65EEF3EBA7A9. Met with Jane Doe (Acme Inc.) on 'Project Orion' to discuss user authentication."
+  - Existing: ['meeting']
+  - Output: [{{"tag_name": "jane_doe", "confidence": 0.95, "reasoning": "Person entity."}}, {{"tag_name": "acme_inc", "confidence": 0.9, "reasoning": "Organization entity."}}, {{"tag_name": "project_orion", "confidence": 0.9, "reasoning": "Project name."}}, {{"tag_name": "user_authentication", "confidence": 0.85, "reasoning": "Key concept."}}]
 
----
+- Real Data:
+{content_section}
+"""
 
-**Text to Analyze:**
----
-{text_content}
----
+DEFAULT_ENTITY_TYPES = [
+    # Personal & Professional Context
+    "person",          # For contact management (e.g., "John Doe")
+    "organization",    # Companies, teams, groups (e.g., "Acme Inc.")
+    "project",         # Specific projects and initiatives (e.g., "Project_Phoenix")
+    "goal",            # High-level objectives (e.g., "Q3_revenue_target")
+    "task",            # Concrete action items (e.g., "Refactor_the_API")
 
-**Your JSON Output:**
-"""
+    # Knowledge & Technology
+    "concept",         # Abstract ideas, theories, methodologies (e.g., "Agile_Development")
+    "technology",      # Software, languages, tools (e.g., "Python", "Docker")
+    "product",         # Specific products or services (e.g., "Obsidian", "Gmail")
 
-# ==============================================================================
-# Prompts for specific, advanced tasks (Graph building, Querying)
-# ==============================================================================
+    # General Classifiers
+    "location",        # Geographical places
+    "event"            # Meetings, deadlines, conferences
+]
 
-GRAPH_INFERENCE_PROMPT = """
-Analyze the text to find key entities and their relationships for knowledge graph construction.
+INFER_RELATIONS_PROMPT = """
+Analyze the text to find key entities and their relationships.
 
 **Instructions:**
-1.  **Extract Entities:** Identify important concepts or nouns. For each, provide a `name`, `type`, and `description`.
-2.  **Extract Relations:** Describe connections between entities (`source` -> `target`). For each, provide `source`, `target`, `type`, and `description`.
+1.  **Extract Entities:** Identify important concepts or nouns.
+2.  **Extract Relations:** Describe connections between entities (`source` -> `target`).
 3.  **Format:** Return a single JSON object with `entities` and `relations` keys. Do not add any text outside the JSON.
 
 **Existing Entities for Context:**
@@ -142,178 +122,101 @@ Analyze the text to find key entities and their relationships for knowledge grap
 
 **Example:**
 ```json
-{
+{{
   "entities": [
-    {"name": "Concept A", "type": "Concept", "description": "A brief explanation of Concept A."},
-    {"name": "Tool B", "type": "Technology", "description": "A brief explanation of Tool B."}
+    {{"name": "Concept A", "type": "Concept", "description": "A brief explanation of Concept A."}},
+    {{"name": "Tool B", "type": "Technology", "description": "A brief explanation of Tool B."}}
   ],
   "relations": [
-    {"source": "Concept A", "target": "Tool B", "type": "IMPLEMENTED_BY", "description": "Concept A is implemented by Tool B."}
+    {{"source": "Concept A", "target": "Tool B", "type": "IMPLEMENTED_BY", "description": "Concept A is implemented by Tool B."}}
   ]
-}
+}}
 ```
 
 **Output JSON:**
 """
 
-MINIRAG_QUERY_TO_KEYWORDS_PROMPT = """
----Role---
-You are a helpful assistant tasked with identifying both answer-type and low-level keywords in the user's query.
----Goal---
-Given the query, list both answer-type and low-level keywords.
-answer_type_keywords focus on the type of the answer to the certain query, while low-level keywords focus on specific entities, details, or concrete terms.
-The answer_type_keywords must be selected from Answer type pool.
-This pool is in the form of a dictionary, where the key represents the Type you should choose from and the value represents the example samples.
----Instructions---
-- Output the keywords in JSON format.
-- The JSON should have three keys:
-  - "answer_type_keywords" for the types of the answer. In this list, the types with the highest likelihood should be placed at the forefront. No more than 3.
-  - "entities_from_query" for specific entities or details. It must be extracted from the query.
-
-######################
--Example-
-######################
-
-Query: "How does international trade influence global economic stability?"
-Answer type pool: {answer_type_pool}
-################
-Output:
-{{
-  "answer_type_keywords": ["STRATEGY", "CONCEPT"],
-  "entities_from_query": ["international trade", "global economic stability", "trade agreements", "tariffs", "currency exchange"]
-}}
-#############################
-
----Real Data---
-Query: {query}
-Answer type pool: {answer_type_pool}
-################
-Output:
-"""
-
-MINIRAG_RAG_RESPONSE_PROMPT = """
----Role---
-You are an expert AI assistant. Your goal is to provide a comprehensive and accurate answer to the user's query.
-
----Instructions---
-1. **Use Context + Knowledge**: Base your answer primarily on the provided context, but also use your general knowledge when appropriate to provide more comprehensive insights.
-2. **Be Creative When Needed**: For creative tasks (like generating questions, brainstorming, or analysis), don't limit yourself to just the context. Use your knowledge to provide deeper insights.
-3. **Synthesize and Structure**: Do not simply list the retrieved information. Synthesize the context into a coherent, well-structured answer. Use Markdown formatting (like bullet points, bolding, and italics) to improve readability.
-4. **Cite Sources**: If the context contains source IDs (e.g., `[source_id: 123]`), you MUST cite the relevant sources at the end of the sentences or paragraphs they support. This is critical for traceability.
-5. **Answer Directly**: Begin by directly answering the user's query, then provide supporting details and synthesis from the context.
-
----Context---
-{context}
-
----Query---
-{query}
-
----Answer---
-"""
-
-QUERY_ANALYSIS_PROMPT = """
----Role---
-You are an expert query analyzer. Your task is to extract key entities and keywords from the user's query to facilitate a precise search within a knowledge graph.
-
----Goal---
-Given the user's query, identify the main subjects, concepts, or entities being discussed.
-
----Instructions---
-- Output a JSON object.
-- The JSON should have one key: "entities_from_query".
-- The value of "entities_from_query" should be a list of strings.
-- Each string should be a noun or noun phrase that represents a specific person, place, organization, concept, or technology.
-- Focus on terms that are essential to understanding the user's intent. Avoid generic verbs, adjectives, or filler words.
-
-######################
--Examples-
-######################
-Example 1:
-Query: "What is the relationship between langchain and asyncio in the context of building conversational AI?"
-################
-Output:
-{
-    "entities_from_query": ["langchain", "asyncio", "conversational AI"]
-}
-######################
-Example 2:
-Query: "Tell me about the software architecture of the apollo project."
-################
-Output:
-{
-    "entities_from_query": ["apollo project", "software architecture"]
-}
-######################
-Example 3:
-Query: "Summarize the latest advancements in quantum computing."
-################
-Output:
-{
-    "entities_from_query": ["quantum computing", "advancements"]
-}
-######################
-
----Real Data---
-Query: {query}
-################
-Output:
-"""
-
-QA_PROMPT = """
-You are a helpful AI assistant. Answer the user's question based on the context provided below.
-If the context does not contain the answer, state that you don't know.
-
-Context:
-{context}
-
-Question:
-{question}
-
-Answer:
-"""
-
-# ==============================================================================
-# Prompt for Entity Extraction Only (Stage A)
-# ==============================================================================
-
-ENTITY_ONLY_PROMPT = """
-Task: Extract entities from the input text and output ONE JSON object only.
-
-Rules:
-- Use only the entity types: {entity_types}
-- Do NOT include relationships or keywords.
-- No explanation, no markdown, no code fences.
-
-Text:
-{input_text}
-
-JSON example:
-{{"entities": [{{"name": "Alice", "type": "PERSON", "description": "..."}}]}}
-
-Your JSON:
-"""
-
-# ==============================================================================
-# Prompts for Multi-Language Chat
-# ==============================================================================
-
-CHAT_LANGUAGE_INSTRUCTIONS = {
-    "English": "Answer the following question in English: {question}",
-    "Chinese": "请用中文回答以下问题: {question}",
-    "Spanish": "Por favor responda a la siguiente pregunta en español: {question}",
-}
-
-def generate_query_with_language_instruction(language: str, question: str) -> str:
+def create_prompt(
+    contents: list[str],
+    entity_types: list[str] = None,
+    existing_tags_lists: list[list[str]] = None,
+    prompt_template: str = TAG_EXTRACTION_PROMPT
+) -> str:
     """
-    Generates a query string that includes a language instruction.
+    Creates a complete prompt string for single or batch processing.
 
     Args:
-        language: The target language (e.g., "Chinese").
-        question: The user's original question.
+        contents: A list of text contents to be processed.
+        entity_types: A list of entity types to guide the LLM.
+        existing_tags_lists: A list of lists, where each inner list contains
+                             existing tags for the corresponding content.
+        prompt_template: The prompt template string to use. Defaults to
+                         `TAG_EXTRACTION_PROMPT`.
 
     Returns:
-        A formatted query string for the LLM.
+        A formatted prompt string ready to be sent to the LLM.
     """
-    template = CHAT_LANGUAGE_INSTRUCTIONS.get(language, CHAT_LANGUAGE_INSTRUCTIONS["English"])
-    logger.debug(f"Generated prompt for language '{language}': {template.format(question='...')}")
-    return template.format(question=question)
\ No newline at end of file
+    if entity_types is None:
+        entity_types = DEFAULT_ENTITY_TYPES
+
+    if existing_tags_lists is None:
+        existing_tags_lists = [[] for _ in contents]
+
+    # Ensure existing_tags_lists has the same length as contents
+    while len(existing_tags_lists) < len(contents):
+        existing_tags_lists.append([])
+
+    is_batch = len(contents) > 1
+    content_section_parts = []
+
+    for i, content in enumerate(contents):
+        header = f"--- Text Content (Note {i}) ---" if is_batch else "--- Text Content ---"
+        
+        # Truncate content to avoid excessively long prompts
+        truncated_content = content[:4000]
+        
+        existing_tags = existing_tags_lists[i]
+        
+        # Defensive check for existing_tags format
+        if existing_tags:
+            if not isinstance(existing_tags, list):
+                logger.warning(f"Malformed 'existing_tags' detected for note {i}; expected a list, but got {type(existing_tags)}. Content: {existing_tags}. Skipping.")
+                existing_tags = []
+            elif not all(isinstance(t, str) for t in existing_tags):
+                non_string_items = [f"{t} ({type(t)})" for t in existing_tags if not isinstance(t, str)]
+                logger.warning(f"Malformed 'existing_tags' detected for note {i}; expected all items to be strings, but found non-string items: {non_string_items}. Full list: {existing_tags}. Filtering out non-strings.")
+                existing_tags = [t for t in existing_tags if isinstance(t, str)]
+
+        existing_tags_str = ", ".join(f"'{tag}'" for tag in existing_tags) if existing_tags else "None"
+        
+        content_block = (
+            f"{header}\n"
+            f"{truncated_content}\n"
+            f"--- Existing Tags ---\n"
+            f"[{existing_tags_str}]"
+        )
+        content_section_parts.append(content_block)
+
+    content_section = "\n\n".join(content_section_parts)
+    entity_types_str = ", ".join(f'"{t}"' for t in entity_types)
+
+    # Dynamically build format arguments to avoid errors if a placeholder
+    # like {entity_types} is not in the selected prompt_template.
+    format_args = {
+        "content_section": content_section
+    }
+    if "{entity_types}" in prompt_template:
+        format_args["entity_types"] = entity_types_str
+
+    return prompt_template.format(**format_args)
+
+
+def log_prompt_usage(content_lengths: list[int], success: bool):
+    """Log prompt usage details for monitoring and optimization."""
+    logger = logging.getLogger(__name__)
+    status = "✅" if success else "❌"
+    logger.info(
+        f"📝 Prompt usage: count={len(content_lengths)}, "
+        f"avg_len={sum(content_lengths) / len(content_lengths):.0f}, "
+        f"status={status}"
+    ) 
\ No newline at end of file
diff --git a/simtag/requirements.txt b/simtag/requirements.txt
index 1058f34..df1d3df 100755
--- a/simtag/requirements.txt
+++ b/simtag/requirements.txt
@@ -5,7 +5,7 @@ wheel>=0.40.0
 numpy>=1.26.0
 requests>=2.31.0
 epc>=0.0.5
-pydantic>=2.0.0
+dependency-injector>=4.41.0
 
 # AI/ML dependencies (optional but recommended)
 torch>=2.1.0
@@ -21,22 +21,3 @@ sqlite-vec>=0.0.1
 urllib3<2.0.0
 toml>=0.10.2
 
-# Added during refactoring
-fastapi
-uvicorn[standard]
-python-dotenv
-beautifulsoup4
-scipy
-thefuzz
-pydantic-settings
-openai
-anthropic
-google-generativeai
-llama-cpp-python
-llama-index
-llama-index-llms-llama-cpp
-llama-index-embeddings-huggingface
-loguru
-sexpdata
-aiohttp
-rank_bm25>=0.2.2
diff --git a/simtag/services/__init__.py b/simtag/services/__init__.py
index b76ef15..868be05 100755
--- a/simtag/services/__init__.py
+++ b/simtag/services/__init__.py
@@ -4,18 +4,7 @@ Provides EPC server and other external service interfaces
 """
 
 
-from .embedding_service import EmbeddingService, EmbeddingResult, EmbeddingBackend
-from .llama_cpp_embedding_service import LlamaCppEmbeddingService
-from .llm_client import LLMClient, LLMResult, LLMBackend
-from .rag_service import RAGService
+from .ollama import OllamaService
+from .user_interface import UserInterfaceService
 
-__all__ = [
-    "EmbeddingService",
-    "EmbeddingResult",
-    "EmbeddingBackend",
-    "LlamaCppEmbeddingService",
-    "LLMClient",
-    "LLMResult",
-    "LLMBackend",
-    "RAGService"
-]
\ No newline at end of file
+__all__ = ['SimTagEPCServer', 'OllamaService', 'UserInterfaceService']
\ No newline at end of file
diff --git a/simtag/services/bm25_service.py b/simtag/services/bm25_service.py
deleted file mode 100644
index 5cb2f2f..0000000
--- a/simtag/services/bm25_service.py
+++ /dev/null
@@ -1,102 +0,0 @@
-"""bm25_service.py
-Lightweight single-file BM25 index service for SimTag.
-Stores an in-memory BM25Okapi index and serializes it to `index_path` with pickle so that it can be re-loaded quickly on next startup.
-The index is built over the (title + content) of every node in the graph.
-"""
-
-from __future__ import annotations
-
-import os
-import pickle
-import re
-import logging
-from typing import List, Tuple, Optional
-
-try:
-    # rank_bm25 is a pure-python dependency
-    from rank_bm25 import BM25Okapi
-except ImportError as e:  # pragma: no cover – handled at runtime
-    raise ImportError("rank_bm25 library is required. Please add `rank_bm25` to your requirements.") from e
-
-logger = logging.getLogger(__name__)
-
-# Very naive whitespace/regex tokenizer. For production consider jieba/icu for CJK.
-_tokenizer_re = re.compile(r"[\w\-]+", re.UNICODE)
-
-def _tokenize(text: str) -> List[str]:
-    return _tokenizer_re.findall(text.lower())
-
-
-class BM25Service:
-    """A minimal BM25 retrieval service backed by rank_bm25 and a pickle file."""
-
-    def __init__(self, graph_service, index_path: str, rebuild_if_missing: bool = True):
-        from simtag.core.graph_service import GraphService  # Local import to avoid circular dep
-        if not isinstance(graph_service, GraphService):
-            raise TypeError("graph_service must be an instance of GraphService")
-        self.graph_service = graph_service
-        self.index_path = index_path
-        self.bm25: Optional[BM25Okapi] = None
-        self.node_ids: List[str] = []
-        if os.path.exists(self.index_path):
-            try:
-                self._load()
-            except Exception as e:
-                logger.warning(f"Failed to load BM25 index: {e}. Rebuilding…")
-                self._build_and_save()
-        elif rebuild_if_missing:
-            logger.info("BM25 index not found. Building a new one…")
-            self._build_and_save()
-        else:
-            logger.warning("BM25 index missing and rebuild_if_missing=False – service disabled.")
-
-    # ---------------------------------------------------------------------
-    # Public API
-    # ---------------------------------------------------------------------
-
-    def query(self, query_text: str, top_k: int = 10) -> List[Tuple[str, float]]:
-        """Return a list of ``(node_id, score)`` sorted by descending BM25 score."""
-        if not self.bm25:
-            return []
-        tokens = _tokenize(query_text)
-        if not tokens:
-            return []
-        scores = self.bm25.get_scores(tokens)
-        # rank_bm25 returns scores aligned with corpus order
-        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
-        return [(self.node_ids[i], float(scores[i])) for i in top_idx if scores[i] > 0]
-
-    def rebuild(self):
-        """Force rebuild the index from graph data."""
-        self._build_and_save()
-
-    # ---------------------------------------------------------------------
-    # Internal helpers
-    # ---------------------------------------------------------------------
-
-    def _load(self):
-        logger.info(f"Loading BM25 index from {self.index_path}")
-        with open(self.index_path, "rb") as f:
-            data = pickle.load(f)
-        self.bm25 = data["bm25"]
-        self.node_ids = data["node_ids"]
-        logger.info(f"Loaded BM25 index with {len(self.node_ids)} documents.")
-
-    def _build_and_save(self):
-        logger.info("Building BM25 index …")
-        conn = self.graph_service._get_connection()  # Use internal method for efficiency
-        cursor = conn.cursor()
-        cursor.execute("SELECT node_id, COALESCE(title, ''), COALESCE(content, '') FROM nodes")
-        rows = cursor.fetchall()
-        self.node_ids = []
-        corpus_tokens: List[List[str]] = []
-        for node_id, title, content in rows:
-            text = f"{title} {content}"
-            corpus_tokens.append(_tokenize(text))
-            self.node_ids.append(node_id)
-        self.bm25 = BM25Okapi(corpus_tokens)
-        # Persist
-        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
-        with open(self.index_path, "wb") as f:
-            pickle.dump({"bm25": self.bm25, "node_ids": self.node_ids}, f)
-        logger.info(f"BM25 index built with {len(self.node_ids)} documents and saved to {self.index_path}") 
\ No newline at end of file
diff --git a/simtag/services/content_processor.py b/simtag/services/content_processor.py
new file mode 100755
index 0000000..d4b4636
--- /dev/null
+++ b/simtag/services/content_processor.py
@@ -0,0 +1,398 @@
+"""
+内容处理服务：解耦嵌入和NER的统一处理接口
+
+此服务参考 MiniRAG 的设计理念，将内容处理分解为独立的流水线：
+1. 内容验证和预处理
+2. 实体提取 (NER)
+3. 嵌入生成
+4. 结果整合
+
+每个步骤都是独立的，失败不会影响其他步骤的执行。
+"""
+
+import asyncio
+import logging
+import time
+from dataclasses import dataclass, field
+from typing import List, Optional, Dict, Any, Tuple
+from enum import Enum
+
+from simtag.services.embedding_service import get_embedding_service, EmbeddingResult
+from simtag.core.entity_extractor import LLMEntityExtractor, ExtractedEntity
+from simtag.config import Config
+
+logger = logging.getLogger(__name__)
+
+@dataclass
+class NERResult:
+    """NER 处理结果"""
+    success: bool
+    entities: List[ExtractedEntity] = field(default_factory=list)
+    error_message: Optional[str] = None
+
+class ProcessingMode(Enum):
+    """处理模式"""
+    EMBEDDING_ONLY = "embedding_only"  # 只处理嵌入
+    NER_ONLY = "ner_only"  # 只处理实体提取
+    FULL_PROCESSING = "full"  # 完整处理
+    MINIMAL = "minimal"  # 最小处理（快速验证）
+
+@dataclass
+class ContentItem:
+    """内容项"""
+    id: str
+    text: str
+    title: Optional[str] = None
+    metadata: Dict[str, Any] = field(default_factory=dict)
+    hash: Optional[str] = None
+    content_hash: Optional[str] = None
+
+@dataclass 
+class ProcessingResult:
+    """处理结果"""
+    content_id: str
+    success: bool
+    
+    # 嵌入结果
+    embedding_result: Optional[EmbeddingResult] = None
+    
+    # NER 结果（新格式，用于桥接代码兼容）
+    ner_result: Optional[NERResult] = None
+    
+    # NER 结果（原格式，保持向后兼容）
+    extracted_entities: List[ExtractedEntity] = field(default_factory=list)
+    
+    # 错误信息
+    errors: List[str] = field(default_factory=list)
+    warnings: List[str] = field(default_factory=list)
+    
+    # 性能指标
+    processing_time: Optional[float] = None
+    mode_used: Optional[ProcessingMode] = None
+    
+    # 哈希值
+    hash: Optional[str] = None  # 添加哈希字段
+    content_hash: Optional[str] = None  # 添加内容哈希字段
+
+@dataclass
+class ProcessingConfig:
+    """处理配置"""
+    mode: ProcessingMode = ProcessingMode.FULL_PROCESSING
+    
+    # 内容验证配置
+    min_text_length: int = 1
+    max_text_length: int = 50000
+    skip_empty_content: bool = True
+    
+    # 嵌入配置
+    embedding_enabled: bool = True
+    embedding_backend: Optional[str] = None  # None = 使用默认
+    
+    # NER 配置  
+    ner_enabled: bool = True
+    entity_types: Optional[List[str]] = None  # None = 使用默认
+    fast_ner_mode: bool = True  # 使用快速NER模式（小模型+单轮）
+    
+    # 并发配置
+    max_concurrent_items: int = 10
+    
+    # 容错配置
+    continue_on_embedding_failure: bool = True
+    continue_on_ner_failure: bool = True
+    
+    # 增量更新配置
+    incremental_update: bool = True  # 启用增量更新
+    force_update: bool = False  # 强制更新所有内容
+
+class ContentProcessor:
+    """内容处理器"""
+    
+    def __init__(self, config: Config):
+        """
+        Initializes the ContentProcessor.
+
+        Args:
+            config: The main application Config object.
+        """
+        self.config = config
+        self.processing_config = self._create_processing_config_from_main(config)
+        
+        self.embedding_service = get_embedding_service()
+        # Lazily initialize the new extractor
+        self.entity_extractor: Optional[LLMEntityExtractor] = None
+        
+    def _create_processing_config_from_main(self, main_config: Config) -> 'ProcessingConfig':
+        """Creates a ProcessingConfig instance from the main Config."""
+        # This allows mapping main config settings to the specific processing config
+        return ProcessingConfig(
+            embedding_enabled=main_config.embedding_config.get("embedding_enabled", True),
+            ner_enabled=main_config.use_smart_ner_service,
+            # Other fields can be mapped here as needed
+        )
+    
+    async def _init_entity_extractor(self):
+        """
+        Lazily initializes the entity extractor based on the processing config.
+        If fast_ner_mode is enabled, it uses a smaller, faster model configuration.
+        """
+        if self.entity_extractor is not None:
+            return
+
+        if not self.processing_config.ner_enabled:
+            logger.info("NER is disabled in the configuration. Skipping entity extractor initialization.")
+            return
+
+        try:
+            from simtag.services.llm_client import LLMClient
+            
+            # We now directly use the analysis_config, which is consistent
+            # with the NodeProcessor implementation.
+            analysis_conf = self.config.analysis_config
+
+            self.entity_extractor = LLMEntityExtractor(
+                llm_client=LLMClient(config_dict=self.config.llm_client_config),
+                config=analysis_conf
+            )
+            logger.info(f"Entity extractor initialized using analysis_config.")
+        except Exception as e:
+            logger.error(f"Failed to initialize entity extractor: {e}", exc_info=True)
+            self.entity_extractor = None
+    
+    def _validate_content(self, item: ContentItem) -> Tuple[bool, List[str]]:
+        """验证内容"""
+        errors = []
+        
+        if not item.text and self.processing_config.skip_empty_content:
+            errors.append("Empty content skipped")
+            return False, errors
+            
+        if not item.text.strip():
+            errors.append("Content contains only whitespace")
+            return False, errors
+            
+        text_length = len(item.text)
+        if text_length < self.processing_config.min_text_length:
+            errors.append(f"Content too short: {text_length} < {self.processing_config.min_text_length}")
+            return False, errors
+            
+        if text_length > self.processing_config.max_text_length:
+            errors.append(f"Content too long: {text_length} > {self.processing_config.max_text_length}")
+            return False, errors
+            
+        return True, []
+    
+    async def _process_embedding(self, item: ContentItem) -> Optional[EmbeddingResult]:
+        """处理嵌入"""
+        if not self.processing_config.embedding_enabled:
+            logger.debug(f"Embedding disabled for {item.id}")
+            return None
+            
+        try:
+            # The incremental update logic has been removed from here.
+            # The decision to process a node should be made upstream (e.g., in SyncHandler)
+            # based on whether the node is new or has been modified.
+            # This processor will now embed any item it receives.
+            
+            # 合并标题和内容
+            text_to_embed = f"{item.title}\n\n{item.text}" if item.title and item.text else item.title or item.text or ""
+            
+            text_to_embed = text_to_embed.strip()
+            
+            logger.info(f"Starting embedding generation for {item.id}: {len(text_to_embed)} chars")
+            
+            result = await self.embedding_service.get_embedding(
+                text=text_to_embed,
+                backend=self.processing_config.embedding_backend
+            )
+            
+            if result.success:
+                logger.info(f"✅ Embedding generated for {item.id}: dim={result.dimension}, backend={result.backend_used}")
+            else:
+                logger.warning(f"❌ Embedding failed for {item.id}: {result.error_message}")
+                
+            return result
+            
+        except Exception as e:
+            logger.error(f"Error during embedding generation for {item.id}: {e}", exc_info=True)
+            return EmbeddingResult(success=False, error_message=str(e))
+    
+    async def _process_ner(self, item: ContentItem) -> NERResult:
+        """处理实体提取 (NER)"""
+        if not self.processing_config.ner_enabled:
+            logger.debug(f"NER disabled for {item.id}")
+            return NERResult(success=True, entities=[]) # Return success with no entities
+
+        # Lazy initialization of the entity extractor
+        if self.entity_extractor is None:
+            await self._init_entity_extractor()
+            
+        if self.entity_extractor is None:
+            # Initialization must have failed
+            return NERResult(success=False, error_message="Entity extractor could not be initialized.")
+
+        try:
+            logger.info(f"Starting NER for {item.id}: {len(item.text)} chars")
+            
+            # Here we assume existing tags are passed via metadata if needed
+            existing_tags = item.metadata.get('existing_tags', [])
+            
+            # The new extractor expects a simple list of tag names and is now async
+            extraction_result = await self.entity_extractor.extract(
+                text=item.text,
+                existing_entities=existing_tags
+            )
+            
+            # Adapt the new dictionary-based result to the old NERResult structure
+            extracted_entities = extraction_result.get("entities", [])
+
+            # For compatibility, we can wrap this in the older ExtractedEntity structure if needed,
+            # but for now, we'll assume the dict structure is acceptable downstream.
+            
+            logger.info(f"✅ NER completed for {item.id}, found {len(extracted_entities)} entities.")
+            # Note: The new format is a list of dicts, not ExtractedEntity objects.
+            # This part of the code might need further adaptation if the caller
+            # strictly expects ExtractedEntity objects. For now, we pass the dicts.
+            return NERResult(success=True, entities=extracted_entities)
+            
+        except Exception as e:
+            logger.error(f"Error during NER processing for {item.id}: {e}", exc_info=True)
+            return NERResult(success=False, error_message=str(e))
+
+    async def process_single(self, item: ContentItem) -> ProcessingResult:
+        """
+        串行处理单个内容项（嵌入和NER）。
+        """
+        start_time = time.time()
+        
+        is_valid, validation_errors = self._validate_content(item)
+        if not is_valid:
+            logger.warning(f"Content item {item.id} is invalid: {', '.join(validation_errors)}")
+            return ProcessingResult(
+                content_id=item.id,
+                success=False,
+                errors=validation_errors,
+                processing_time=time.time() - start_time,
+                hash=item.hash,
+                content_hash=item.content_hash
+            )
+
+        result = ProcessingResult(
+            content_id=item.id,
+            success=True,
+            mode_used=self.processing_config.mode,
+            hash=item.hash,
+            content_hash=item.content_hash
+        )
+
+        # --- 嵌入处理 ---
+        if self.processing_config.mode in [ProcessingMode.FULL_PROCESSING, ProcessingMode.EMBEDDING_ONLY]:
+            embedding_start = time.time()
+            embedding_result = await self._process_embedding(item)
+            if embedding_result:
+                result.embedding_result = embedding_result
+                if not embedding_result.success:
+                    result.success = False
+                    error_msg = f"Embedding failed: {embedding_result.error_message}"
+                    result.errors.append(error_msg)
+                    if not self.processing_config.continue_on_embedding_failure:
+                        result.processing_time = time.time() - start_time
+                        return result
+            logger.debug(f"Embedding for {item.id} took {time.time() - embedding_start:.2f}s")
+
+        # --- NER 处理 ---
+        if self.processing_config.mode in [ProcessingMode.FULL_PROCESSING, ProcessingMode.NER_ONLY]:
+            ner_start = time.time()
+            ner_result = await self._process_ner(item)
+            result.ner_result = ner_result
+            if ner_result.success:
+                result.extracted_entities = ner_result.entities
+            else:
+                result.success = False
+                error_msg = f"NER failed: {ner_result.error_message}"
+                result.errors.append(error_msg)
+                if not self.processing_config.continue_on_ner_failure:
+                    result.processing_time = time.time() - start_time
+                    return result
+            logger.debug(f"NER for {item.id} took {time.time() - ner_start:.2f}s")
+        
+        result.processing_time = time.time() - start_time
+        logger.info(f"Finished processing {item.id} in {result.processing_time:.2f}s. Success: {result.success}")
+        return result
+
+    async def process_batch(self, items: List[ContentItem]) -> List[ProcessingResult]:
+        """
+        以串行方式处理一批内容项。
+        
+        此方法将迭代项目列表，并按顺序一次处理一个。
+        多进程/并行逻辑已被移除以确保稳定性。
+        """
+        if not items:
+            return []
+
+        logger.info(f"Starting serial processing for a batch of {len(items)} items.")
+        start_time = time.time()
+        
+        results = []
+        for i, item in enumerate(items):
+            logger.info(f"Processing item {i+1}/{len(items)}: {item.id}")
+            result = await self.process_single(item)
+            results.append(result)
+
+        total_time = time.time() - start_time
+        logger.info(f"Finished serial processing of {len(items)} items in {total_time:.2f}s.")
+        
+        return results
+    
+    def get_stats(self) -> Dict[str, Any]:
+        """获取处理器统计信息"""
+        return {
+            "config": {
+                "mode": self.processing_config.mode.value,
+                "embedding_enabled": self.processing_config.embedding_enabled,
+                "ner_enabled": self.processing_config.ner_enabled,
+                "max_concurrent_items": self.processing_config.max_concurrent_items
+            },
+            "embedding_service_available": self.embedding_service is not None,
+            "entity_extractor_available": self.entity_extractor is not None
+        }
+
+# 便利函数
+async def process_text(text: str, 
+                      content_id: str = "default",
+                      title: Optional[str] = None,
+                      mode: ProcessingMode = ProcessingMode.FULL_PROCESSING) -> ProcessingResult:
+    """便捷函数，用于处理单个文本片段"""
+    from simtag.container import Container
+    
+    container = Container()
+    config = container.config()
+    content_processor = ContentProcessor(config=config)
+
+    item = ContentItem(id=content_id, text=text, title=title)
+    
+    # 根据模式调整处理配置
+    content_processor.processing_config.mode = mode
+    
+    return await content_processor.process_single(item)
+
+async def process_node_data(node_id: str,
+                           content: str,
+                           title: Optional[str] = None,
+                           embedding_only: bool = False) -> ProcessingResult:
+    """
+    处理来自Elisp的节点数据。
+    这是一个兼容层，将旧的函数调用适配到新的ContentProcessor。
+    """
+    from simtag.container import Container
+
+    container = Container()
+    config = container.config()
+    content_processor = ContentProcessor(config=config)
+
+    item = ContentItem(id=node_id, text=content, title=title)
+    
+    # 根据 embedding_only 标志调整模式
+    mode = ProcessingMode.EMBEDDING_ONLY if embedding_only else ProcessingMode.FULL_PROCESSING
+    content_processor.processing_config.mode = mode
+    
+    return await content_processor.process_single(item) 
\ No newline at end of file
diff --git a/simtag/services/embedding_service.py b/simtag/services/embedding_service.py
index 9f34745..7bd931a 100755
--- a/simtag/services/embedding_service.py
+++ b/simtag/services/embedding_service.py
@@ -8,22 +8,13 @@ import logging
 import hashlib
 import time
 from abc import ABC, abstractmethod
-from dataclasses import dataclass
-from typing import List, Optional, Dict, Any
+from dataclasses import dataclass, field
+from typing import List, Optional, Dict, Any, Union
 import numpy as np
 import concurrent.futures
 import multiprocessing as mp
 import os
 
-# Import required dependencies
-from ..config import Config
-from ..utils.text_processing import prepare_node_text_for_embedding
-
-# Forward declaration to avoid circular imports
-from typing import TYPE_CHECKING
-if TYPE_CHECKING:
-    from ..core.graph_service import GraphService
-
 # LlamaCpp backend import
 try:
     from .llama_cpp_embedding_service import LlamaCppEmbeddingService
@@ -35,7 +26,7 @@ logger = logging.getLogger(__name__)
 
 @dataclass
 class EmbeddingResult:
-    """Result wrapper containing embedding vector and metadata"""
+    """结果封装，包含嵌入向量和元数据"""
     success: bool
     embedding: Optional[List[float]] = None
     error_message: Optional[str] = None
@@ -45,7 +36,7 @@ class EmbeddingResult:
     processing_time: Optional[float] = None
 
 class EmbeddingBackend(ABC):
-    """Abstract base class for embedding backends"""
+    """嵌入后端抽象基类"""
     
     @property
     @abstractmethod
@@ -66,7 +57,7 @@ class EmbeddingBackend(ABC):
         pass
 
 class OllamaEmbeddingBackend(EmbeddingBackend):
-    """Ollama Embedding Backend"""
+    """Ollama 嵌入后端"""
     
     def __init__(self, base_url: str = "http://localhost:11434", 
                  default_model: str = "nomic-embed-text", timeout: int = 300):
@@ -80,13 +71,13 @@ class OllamaEmbeddingBackend(EmbeddingBackend):
     
     @property 
     def default_dimension(self) -> int:
-        return 768  # Default dimension for nomic-embed-text
+        return 768  # nomic-embed-text 的默认维度
         
     async def get_embedding(self, text: str, model: Optional[str] = None) -> EmbeddingResult:
         start_time = time.time()
         target_model = model or self.default_model
         
-        # Content validation
+        # 内容验证
         if not text or not text.strip():
             return EmbeddingResult(
                 success=False,
@@ -95,8 +86,9 @@ class OllamaEmbeddingBackend(EmbeddingBackend):
             )
         
         try:
-            # Use requests for synchronous calls to avoid httpx issues
+            # 使用 requests 库进行同步调用，避免 httpx 的问题
             import requests
+            import json
             
             api_url = f"{self.base_url}/api/embeddings"
             payload = {
@@ -144,13 +136,13 @@ class OllamaEmbeddingBackend(EmbeddingBackend):
             )
     
     async def get_embeddings_batch(self, texts: List[str], model: Optional[str] = None) -> List[EmbeddingResult]:
-        """Batch processing, calling get_embedding for each text (Ollama API doesn't directly support batch)"""
-        # For Ollama, use async concurrency instead of multiprocessing (network calls)
+        """批量处理，逐个调用（Ollama API 不直接支持批量）"""
+        # 对于Ollama，使用异步并发而不是多进程（网络调用）
         tasks = [self.get_embedding(text, model) for text in texts]
         return await asyncio.gather(*tasks)
 
 class LocalEmbeddingBackend(EmbeddingBackend):
-    """Local Embedding Backend (using Sentence Transformers etc.)"""
+    """本地嵌入后端（使用句子转换器等）"""
     
     def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
         self.model_name = model_name
@@ -163,15 +155,15 @@ class LocalEmbeddingBackend(EmbeddingBackend):
         
     @property
     def default_dimension(self) -> int:
-        return self._dimension or 384  # Default dimension for all-MiniLM-L6-v2
+        return self._dimension or 384  # all-MiniLM-L6-v2 的默认维度
         
     async def _load_model(self):
-        """Lazy load the model"""
+        """延迟加载模型"""
         if self._model is None:
             try:
                 from sentence_transformers import SentenceTransformer
                 self._model = await asyncio.to_thread(SentenceTransformer, self.model_name)
-                # Get actual dimension
+                # 获取实际维度
                 test_embedding = await asyncio.to_thread(self._model.encode, ["test"])
                 self._dimension = test_embedding.shape[1]
                 logger.info(f"Loaded local embedding model {self.model_name} with dimension {self._dimension}")
@@ -213,10 +205,10 @@ class LocalEmbeddingBackend(EmbeddingBackend):
             )
     
     async def get_embeddings_batch(self, texts: List[str], model: Optional[str] = None) -> List[EmbeddingResult]:
-        """Batch processing (local models support multi-core)"""
+        """批量处理（本地模型支持多核心）"""
         start_time = time.time()
         
-        # Filter empty texts
+        # 过滤空文本
         valid_texts = []
         valid_indices = []
         for i, text in enumerate(texts):
@@ -231,7 +223,7 @@ class LocalEmbeddingBackend(EmbeddingBackend):
         try:
             await self._load_model()
             
-            # Use multi-core processing for large batches
+            # 对于大批量，使用多核心处理
             if len(valid_texts) > 32:
                 embeddings = await self._process_batch_multicore(valid_texts)
             else:
@@ -261,15 +253,15 @@ class LocalEmbeddingBackend(EmbeddingBackend):
                    for _ in texts]
     
     async def _process_batch_multicore(self, texts: List[str]) -> np.ndarray:
-        """Process large batches of text embeddings using multiple cores"""
+        """使用多核心处理大批量文本嵌入"""
         cpu_count = os.cpu_count() or 4
-        max_workers = min(cpu_count - 2, 12)  # Optimized for M4 Max
+        max_workers = min(cpu_count - 2, 12)  # 为M4 Max优化
         
         chunk_size = max(len(texts) // max_workers, 16)
         text_chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]
         
         def process_chunk(chunk):
-            """Process a chunk of text in a separate process"""
+            """在单独进程中处理文本块"""
             try:
                 from sentence_transformers import SentenceTransformer
                 model = SentenceTransformer(self.model_name)
@@ -282,10 +274,10 @@ class LocalEmbeddingBackend(EmbeddingBackend):
             max_workers=max_workers,
             mp_context=mp.get_context('spawn')
         ) as executor:
-            # Submit all tasks
+            # 提交所有任务
             futures = [executor.submit(process_chunk, chunk) for chunk in text_chunks]
             
-            # Collect results
+            # 收集结果
             chunk_results = []
             for future in concurrent.futures.as_completed(futures):
                 try:
@@ -295,104 +287,130 @@ class LocalEmbeddingBackend(EmbeddingBackend):
                 except Exception as e:
                     logger.error(f"Multicore embedding future failed: {e}")
         
-        # Concatenate all results
+        # 合并所有结果
         if chunk_results:
             return np.concatenate(chunk_results)
         else:
-            # fallback to single thread
+            # fallback到单线程
             logger.warning("Multicore processing failed, falling back to single thread")
             return await asyncio.to_thread(self._model.encode, texts)
 
 class EmbeddingService:
-    """Service layer to manage all embedding backends"""
-
-    def __init__(self, config: Config, graph_service: "GraphService"):
-        self.config = config
-        self.graph_service = graph_service
-        self.logger = logging.getLogger(__name__)
-        
-        # Initialize backends configuration from config.embedding
-        self.backends_config = {
-            "ollama": self.config.embedding.ollama,
-            "llama_cpp": self.config.embedding.llama_cpp,
-            "local": getattr(self.config.embedding, 'local', {})
-        }
-        self.provider_name = self.config.embedding.provider
-        
-        # Initialize backends storage and cache
-        self.backends = {}
-        self.cache = {} if self.config.embedding.use_cache else None
+    """统一的嵌入服务，支持多后端和故障容错"""
+    
+    def __init__(self, config_dict: Dict[str, Any]):
+        """
+        初始化嵌入服务
         
-        # Initialize backends
+        Args:
+            config_dict: 来自config.py的embedding_config字典
+        """
+        self.config = config_dict
+        self.backends: Dict[str, EmbeddingBackend] = {}
+        self.cache: Dict[str, Dict] = {} if config_dict.get("cache_enabled", True) else None
         self._init_backends()
-
+        
     def _init_backends(self):
-        """Initializes all available backends based on configuration"""
-        # Always try to initialize ollama if configured
-        ollama_config = self.backends_config.get("ollama", {})
-        if ollama_config:
+        """初始化所有后端"""
+        primary_backend = self.config.get("primary_backend", "llama_cpp")
+        fallback_backends = self.config.get("fallback_backends", ["ollama"])
+        
+        all_backends = [primary_backend] + fallback_backends
+        
+        # Ollama 后端
+        if "ollama" in all_backends:
             self.backends["ollama"] = OllamaEmbeddingBackend(
-                base_url=ollama_config.get("base_url", "http://localhost:11434"),
-                default_model=ollama_config.get("model_name", "nomic-embed-text"),
-                timeout=ollama_config.get("timeout", 300)
+                base_url=self.config.get("ollama_base_url", "http://localhost:11434"),
+                default_model=self.config.get("ollama_model", "nomic-embed-text"),
+                timeout=self.config.get("ollama_timeout", 300)
             )
-            logger.info("Initialized 'ollama' embedding backend.")
-
-        # Try to initialize local if configured
-        local_config = self.backends_config.get("local", {})
-        if local_config:
+        
+        # 本地后端 - 只有在配置中指定时才初始化
+        if "local" in all_backends:
             self.backends["local"] = LocalEmbeddingBackend(
-                model_name=local_config.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
+                model_name=self.config.get("local_model", "sentence-transformers/all-MiniLM-L6-v2")
             )
-            logger.info("Initialized 'local' embedding backend.")
-
-        # Try to initialize llama_cpp if configured and available
-        llama_cpp_config = self.backends_config.get("llama_cpp", {})
-        if llama_cpp_config and LLAMA_CPP_AVAILABLE:
-            self.backends["llama_cpp"] = LlamaCppEmbeddingService(
-                model_path=llama_cpp_config.get("model_path"),
-                binary_path=llama_cpp_config.get("binary_path", "llama-embedding"),
-                pooling=llama_cpp_config.get("pooling", "mean"),
-                n_threads=llama_cpp_config.get("n_threads"),
-                n_ctx=llama_cpp_config.get("n_ctx", 4096),
-                config_dict=llama_cpp_config
-            )
-            logger.info("Initialized 'llama_cpp' embedding backend directly.")
-        elif llama_cpp_config:
-            logger.warning("Llama.cpp backend configured but not available. Skipping.")
-
-        # Set the primary backend based on the provider name from config
-        self.primary_backend = self.provider_name
-        if self.primary_backend not in self.backends:
-            logger.warning(
-                f"Primary backend '{self.primary_backend}' not available. "
-                f"Falling back to the first available backend."
-            )
-            if self.backends:
-                self.primary_backend = list(self.backends.keys())[0]
-            else:
-                self.primary_backend = None
-                logger.error("No embedding backends are available or configured.")
-
+        
+        # LlamaCpp 后端
+        if "llama_cpp" in all_backends and LLAMA_CPP_AVAILABLE:
+            try:
+                from .llama_cpp_embedding_service import create_qwen3_embedding_service
+                llama_cpp_service = create_qwen3_embedding_service(
+                    model_path=self.config.get("llama_cpp_model_path"),
+                    binary_path=self.config.get("llama_cpp_binary", "llama-embedding"),
+                    config_dict=self.config  # 传入完整的配置字典
+                )
+                
+                # 创建一个适配器来匹配 EmbeddingBackend 接口
+                class LlamaCppBackendAdapter(EmbeddingBackend):
+                    def __init__(self, service):
+                        self.service = service
+                    
+                    @property
+                    def backend_name(self) -> str:
+                        return "llama_cpp"
+                    
+                    @property
+                    def default_dimension(self) -> int:
+                        return 1024  # Qwen3-Embedding-0.6B 的维度
+                    
+                    async def get_embedding(self, text: str, model: Optional[str] = None) -> EmbeddingResult:
+                        result = await self.service.get_embedding(text)
+                        # 转换为兼容的 EmbeddingResult 格式
+                        return EmbeddingResult(
+                            success=result.success,
+                            embedding=result.embedding,
+                            error_message=result.error_message,
+                            model_used="qwen3-embedding-0.6b",
+                            backend_used="llama_cpp",
+                            dimension=result.dimension,
+                            processing_time=result.processing_time
+                        )
+                    
+                    async def get_embeddings_batch(self, texts: List[str], model: Optional[str] = None) -> List[EmbeddingResult]:
+                        results = await self.service.get_embeddings_batch(texts)
+                        # 转换为兼容的 EmbeddingResult 格式
+                        converted_results = []
+                        for result in results:
+                            converted_results.append(EmbeddingResult(
+                                success=result.success,
+                                embedding=result.embedding,
+                                error_message=result.error_message,
+                                model_used="qwen3-embedding-0.6b",
+                                backend_used="llama_cpp",
+                                dimension=result.dimension,
+                                processing_time=result.processing_time
+                            ))
+                        return converted_results
+                
+                self.backends["llama_cpp"] = LlamaCppBackendAdapter(llama_cpp_service)
+                logger.info("LlamaCpp backend initialized successfully")
+            except (ImportError, FileNotFoundError) as e:
+                logger.warning(f"LlamaCpp backend not available: {e}")
+            except Exception as e:
+                logger.error(f"Failed to initialize LlamaCpp backend: {e}")
+        elif "llama_cpp" in all_backends:
+            logger.warning("LlamaCpp backend not available (import failed)")
+        
     def _get_cache_key(self, text: str, backend: str, model: Optional[str] = None) -> str:
-        """Generates a cache key"""
+        """生成缓存键"""
         content = f"{backend}:{model}:{text}"
         return hashlib.md5(content.encode('utf-8')).hexdigest()
         
     def _get_from_cache(self, cache_key: str) -> Optional[EmbeddingResult]:
-        """Retrieves result from cache"""
+        """从缓存获取结果"""
         if not self.cache:
             return None
             
         cached = self.cache.get(cache_key)
-        cache_ttl = 3600  # Default cache TTL in seconds
+        cache_ttl = self.config.get("cache_ttl", 3600)
         if cached and time.time() - cached['timestamp'] < cache_ttl:
             logger.debug(f"Cache hit for key: {cache_key[:16]}...")
             return cached['result']
         return None
         
     def _save_to_cache(self, cache_key: str, result: EmbeddingResult):
-        """Saves result to cache"""
+        """保存结果到缓存"""
         if self.cache and result.success:
             self.cache[cache_key] = {
                 'result': result,
@@ -402,8 +420,8 @@ class EmbeddingService:
     async def get_embedding(self, text: str, 
                           backend: Optional[str] = None, 
                           model: Optional[str] = None) -> EmbeddingResult:
-        """Gets embedding for a single text"""
-        # Pre-check content
+        """获取单个文本的嵌入"""
+        # 内容预检查
         if not text or not text.strip():
             return EmbeddingResult(
                 success=False,
@@ -411,9 +429,9 @@ class EmbeddingService:
                 backend_used="none"
             )
         
-        primary_backend = self.provider_name
-        fallback_backends = ["ollama", "llama_cpp", "local"]  # Try all available backends as fallback
-        max_retries = 2
+        primary_backend = self.config.get("primary_backend", "llama_cpp")
+        fallback_backends = self.config.get("fallback_backends", ["ollama"])
+        max_retries = self.config.get("max_retries", 2)
         
         backends_to_try = [backend] if backend else [primary_backend] + fallback_backends
         
@@ -422,7 +440,7 @@ class EmbeddingService:
                 logger.warning(f"Backend {backend_name} not available, skipping")
                 continue
                 
-            # Check cache
+            # 检查缓存
             cache_key = self._get_cache_key(text, backend_name, model)
             cached_result = self._get_from_cache(cache_key)
             if cached_result:
@@ -440,14 +458,14 @@ class EmbeddingService:
                         return result
                     else:
                         logger.warning(f"Embedding failed with {backend_name}: {result.error_message}")
-                        break  # Do not retry on logical errors
+                        break  # 不重试逻辑错误
                         
                 except Exception as e:
                     logger.error(f"Embedding attempt {attempt + 1} failed with {backend_name}: {e}")
                     if attempt < max_retries:
-                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
+                        await asyncio.sleep(2 ** attempt)  # 指数退避
                     
-        # All backends failed
+        # 所有后端都失败
         return EmbeddingResult(
             success=False,
             error_message="All embedding backends failed",
@@ -457,13 +475,13 @@ class EmbeddingService:
     async def get_embeddings_batch(self, texts: List[str],
                                  backend: Optional[str] = None,
                                  model: Optional[str] = None) -> List[EmbeddingResult]:
-        """Gets embeddings for a batch of texts"""
+        """批量获取嵌入"""
         if not texts:
             return []
             
-        batch_size = 32  # Default batch size
+        batch_size = self.config.get("batch_size", 32)
         
-        # Process in batches
+        # 分批处理
         results = []
         for i in range(0, len(texts), batch_size):
             batch = texts[i:i + batch_size]
@@ -475,9 +493,9 @@ class EmbeddingService:
     async def _process_batch(self, texts: List[str],
                            backend: Optional[str] = None,
                            model: Optional[str] = None) -> List[EmbeddingResult]:
-        """Processes a single batch"""
-        primary_backend = self.provider_name
-        fallback_backends = ["ollama", "llama_cpp", "local"]  # Try all available backends as fallback
+        """处理单个批次"""
+        primary_backend = self.config.get("primary_backend", "llama_cpp")
+        fallback_backends = self.config.get("fallback_backends", ["ollama"])
         
         backends_to_try = [backend] if backend else [primary_backend] + fallback_backends
         
@@ -489,7 +507,7 @@ class EmbeddingService:
             
             try:
                 results = await backend.get_embeddings_batch(texts, model)
-                # Check if any results were successful
+                # 检查是否有成功的结果
                 if any(r.success for r in results):
                     logger.debug(f"Batch processing with {backend_name}: "
                                f"{sum(1 for r in results if r.success)}/{len(results)} successful")
@@ -498,219 +516,292 @@ class EmbeddingService:
                 logger.error(f"Batch processing failed with {backend_name}: {e}")
                 continue
         
-        # All backends failed, return failure results
+        # 所有后端都失败，返回失败结果
         return [EmbeddingResult(success=False, error_message="All embedding backends failed", backend_used="none") 
                for _ in texts]
     
     def clear_cache(self):
-        """Clears the cache"""
+        """清除缓存"""
         if self.cache:
             self.cache.clear()
             logger.info("Embedding cache cleared")
     
     def get_stats(self) -> Dict[str, Any]:
-        """Gets service statistics"""
+        """获取服务统计信息"""
         return {
             "config": {
-                "primary_backend": self.provider_name,
-                "cache_enabled": self.config.embedding.use_cache,
-                "provider": self.config.embedding.provider,
-                "max_input_tokens": self.config.embedding.max_input_tokens
+                "primary_backend": self.config.get("primary_backend"),
+                "fallback_backends": self.config.get("fallback_backends"),
+                "cache_enabled": self.config.get("cache_enabled"),
+                "max_retries": self.config.get("max_retries")
             },
             "backends": list(self.backends.keys()),
             "cache_size": len(self.cache) if self.cache else 0
         }
     
     async def check_dimension_compatibility(self) -> Dict[str, Any]:
-        """Check if all backend dimensions are consistent"""
-        if not self.backends:
-            return {"status": "error", "message": "No backends configured"}
-        
-        dimensions = {}
-        for name, backend in self.backends.items():
+        """检查当前模型与数据库的维度兼容性"""
+        try:
+            # 检测当前模型维度
+            test_result = await self.get_embedding("测试文本")
+            if not test_result.success:
+                return {
+                    "compatible": False,
+                    "error": f"无法获取模型嵌入: {test_result.error_message}"
+                }
+            
+            model_dimension = test_result.dimension
+            
+            # 检查数据库维度
             try:
-                # Assuming backends have a 'default_dimension' property
-                dim = backend.default_dimension
-                if dim > 0:
-                    dimensions[name] = dim
+                from simtag.config import Config
+                import sqlite3
+                import sqlite_vec
+                
+                config = Config()
+                db_path = config.vector_db_path
+                
+                if not os.path.exists(db_path):
+                    return {
+                        "compatible": True,
+                        "model_dimension": model_dimension,
+                        "db_dimension": None,
+                        "message": "数据库不存在，将自动创建"
+                    }
+                
+                conn = sqlite3.connect(db_path)
+                conn.enable_load_extension(True)
+                sqlite_vec.load(conn)
+                cursor = conn.cursor()
+                
+                # 尝试获取现有向量样本
+                cursor.execute("SELECT embedding FROM node_embeddings_vss LIMIT 1")
+                row = cursor.fetchone()
+                
+                if not row:
+                    return {
+                        "compatible": True,
+                        "model_dimension": model_dimension,
+                        "db_dimension": None,
+                        "message": "数据库为空，可以直接使用"
+                    }
+                
+                # 解析向量维度
+                embedding_data = row[0]
+                if isinstance(embedding_data, bytes):
+                    import numpy as np
+                    vector = np.frombuffer(embedding_data, dtype=np.float32)
+                    db_dimension = len(vector)
                 else:
-                    dimensions[name] = "Unknown"
-            except Exception as e:
-                dimensions[name] = f"Error: {e}"
-        
-        # Check for consistency
-        unique_dims = {d for d in dimensions.values() if isinstance(d, int) and d > 0}
-        
-        if len(unique_dims) > 1:
-            status = "warning"
-            message = f"Inconsistent embedding dimensions found: {dimensions}"
-        elif len(unique_dims) == 1:
-            status = "success"
-            message = f"All backends have a consistent dimension: {unique_dims.pop()}"
-        else:
-            status = "error"
-            message = "Could not determine dimension for any backend."
-            
-        return {"status": status, "message": message, "dimensions": dimensions} 
+                    import json
+                    vector = json.loads(embedding_data) if isinstance(embedding_data, str) else embedding_data
+                    db_dimension = len(vector)
+                
+                conn.close()
+                
+                compatible = model_dimension == db_dimension
+                return {
+                    "compatible": compatible,
+                    "model_dimension": model_dimension,
+                    "db_dimension": db_dimension,
+                    "message": "维度匹配" if compatible else f"维度不匹配: 模型({model_dimension}) vs 数据库({db_dimension})"
+                }
+                
+            except Exception as db_error:
+                return {
+                    "compatible": False,
+                    "model_dimension": model_dimension,
+                    "db_dimension": None,
+                    "error": f"数据库检查失败: {db_error}"
+                }
+                
+        except Exception as e:
+            return {
+                "compatible": False,
+                "error": f"维度兼容性检查失败: {e}"
+            }
 
-    async def get_node_embedding(self, node_data: Dict[str, Any]) -> Optional[np.ndarray]:
+    def prepare_node_text_for_embedding(self, node_data, max_total_tokens=280):
         """
-        Get the embedding vector of a node
+        为 org-supertag node 准备向量化文本
+        
+        策略：标题 + 内容前N字，充分利用 org-mode 的结构化优势
+        专门针对 LlamaCpp 的 512 token 限制进行优化
         
         Args:
-            node_data: Node data, containing content and title information
+            node_data: Node 数据字典
+            max_total_tokens: 总的最大 token 数，默认400（留出安全余量）
+            
+        Returns:
+            str: 准备好的文本
         """
-        try:
-            content = prepare_node_text_for_embedding(node_data)
-            if not content:
-                self.logger.warning(f"No content for embedding in node {node_data.get('id')}")
-                return None
-
-            result = await self.get_embedding(content)
-            if result.success and result.embedding:
-                return np.array(result.embedding, dtype=np.float32)
-            return None
-        except Exception as e:
-            self.logger.error(f"Error generating node embedding: {e}", exc_info=True)
-            return None
-
-    async def get_tag_embedding(self, tag_data: Dict[str, Any]) -> Optional[np.ndarray]:
+        text_parts = []
+        used_tokens = 0
+        
+        # Token 估算函数（粗略估算：1个中文字符≈1.5 tokens，1个英文单词≈1 token）
+        def estimate_tokens(text):
+            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
+            other_chars = len(text) - chinese_chars
+            return chinese_chars * 1.5 + other_chars * 0.3
+        
+        # 1. 优先添加标题（最重要的语义信息）
+        title = node_data.get('title', '').strip()
+        if title:
+            title_text = f"标题: {title}"
+            title_tokens = estimate_tokens(title_text)
+            if used_tokens + title_tokens < max_total_tokens:
+                text_parts.append(title_text)
+                used_tokens += title_tokens
+        
+        # 2. 添加关键上下文（如果空间允许）
+        olp = node_data.get('olp', [])
+        if olp and len(olp) > 0 and used_tokens < max_total_tokens * 0.5:  # 只在token使用率<50%时添加
+            context = " > ".join(olp[-2:])  # 只取最近的2级父标题
+            context_text = f"上下文: {context}"
+            context_tokens = estimate_tokens(context_text)
+            if used_tokens + context_tokens < max_total_tokens * 0.7:  # 上下文不超过总量的70%
+                text_parts.append(context_text)
+                used_tokens += context_tokens
+        
+        # 3. 添加标签（如果空间允许且标签数量合理）
+        tags = node_data.get('tags', [])
+        if tags and len(tags) <= 5 and used_tokens < max_total_tokens * 0.6:
+            tags_text = f"标签: {', '.join(tags[:5])}"  # 最多5个标签
+            tags_tokens = estimate_tokens(tags_text)
+            if used_tokens + tags_tokens < max_total_tokens * 0.8:  # 标签不超过总量的80%
+                text_parts.append(tags_text)
+                used_tokens += tags_tokens
+        
+        # 4. 用剩余空间添加内容
+        content = node_data.get('content', '').strip()
+        if content:
+            remaining_tokens = max_total_tokens - used_tokens - 20  # 保留20 token的安全边界
+            if remaining_tokens > 50:  # 至少要有50 token才添加内容
+                # 根据剩余token数动态计算内容字符数限制
+                max_content_chars = int(remaining_tokens / 1.2)  # 保守估算
+                truncated_content = self._smart_truncate(content, max_content_chars)
+                content_text = f"内容: {truncated_content}"
+                text_parts.append(content_text)
+        
+        final_text = " | ".join(text_parts)
+        
+        # 最终检查：确保不超过限制
+        final_tokens = estimate_tokens(final_text)
+        if final_tokens > max_total_tokens:
+            # 如果还是超过，优先保留标题和少量内容
+            title_only = node_data.get('title', '').strip()
+            content = node_data.get('content', '').strip()
+            if content:
+                remaining_chars = max_total_tokens * 0.6  # 大约60%给内容
+                truncated = self._smart_truncate(content, int(remaining_chars))
+                final_text = f"标题: {title_only} | 内容: {truncated}"
+            else:
+                final_text = f"标题: {title_only}"
+        
+        return final_text
+    
+    def _estimate_tokens(self, text):
         """
-        Get the embedding vector of a tag
+        实用的 token 数量估算
+        
+        基于实际测试优化，平衡精度和安全性
         
         Args:
-            tag_data: Tag data, containing name and fields information
+            text: 输入文本
+            
+        Returns:
+            float: 估算的 token 数量
         """
-        try:
-            # Generate embedding using tag name and field definitions
-            tag_name = tag_data.get('name') or tag_data.get('title')
-            if not tag_name:
-                self.logger.warning(f"Tag {tag_data.get('id')} has no name")
-                return None
-
-            # Build semantic representation of the tag
-            fields = tag_data.get('fields', [])
-            field_texts = []
-            for field in fields:
-                if isinstance(field, dict):
-                    field_name = field.get('name')
-                    field_type = field.get('type')
-                    if field_name and field_type:
-                        field_texts.append(f"{field_name} ({field_type})")
-
-            # Combine tag name and field information
-            content = tag_name
-            if field_texts:
-                content += ": " + ", ".join(field_texts)
-
-            result = await self.get_embedding(content)
-            if result.success and result.embedding:
-                return np.array(result.embedding, dtype=np.float32)
-            return None
-        except Exception as e:
-            self.logger.error(f"Error generating tag embedding: {e}", exc_info=True)
-            return None
-
-    def _calculate_average_node_embeddings(self, tag_id: str) -> Optional[np.ndarray]:
+        # 基于实际观察调整：中文字符约1.2 tokens，英文单词约0.8 tokens
+        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
+        other_chars = len(text) - chinese_chars
+        
+        # 更贴近实际的估算
+        estimated = chinese_chars * 1.2 + other_chars * 0.6
+        
+        # 适度的安全边界（10%）
+        return estimated * 1.1
+    
+    def _smart_truncate(self, text, max_chars):
         """
-        Calculate the semantic vector of a tag by averaging the vectors of its linked nodes
+        在句子边界智能截断文本
+        
+        Args:
+            text: 原文本
+            max_chars: 最大字符数
+            
+        Returns:
+            str: 截断后的文本
         """
-        try:
-            # Get linked nodes
-            linked_nodes = self.graph_service.get_nodes_linked_to_tag(tag_id)
-            if not linked_nodes:
-                self.logger.warning(f"Tag '{tag_id}' has no linked nodes.")
-                return None
-
-            # Collect node vectors
-            embeddings = []
-            for node in linked_nodes:
-                node_id = node.get('node_id')
-                if not node_id:
-                    continue
-                embedding = self.graph_service.get_entity_embedding_by_id(node_id)
-                if embedding is not None and embedding.size > 0:
-                    embeddings.append(embedding)
-
-            if not embeddings:
-                self.logger.warning(f"No valid embeddings found for nodes linked to tag '{tag_id}'")
-                return None
-
-            # Calculate average vector
-            return np.mean([np.array(emb) for emb in embeddings], axis=0)
-        except Exception as e:
-            self.logger.error(f"Error calculating average embeddings for tag '{tag_id}': {e}", exc_info=True)
-            return None
-
-    async def _embed_tag_by_name(self, tag_data: Dict[str, Any]) -> Optional[np.ndarray]:
-        """Helper to embed a tag by its name and fields."""
-        return await self.get_tag_embedding(tag_data)
-
-    async def batch_embed_tags(self, tag_ids: List[str], method: str = 'average') -> None:
+        if len(text) <= max_chars:
+            return text
+        
+        # 在最大长度处向前查找句子边界
+        truncated = text[:max_chars]
+        
+        # 查找句号、感叹号、问号等句子结束标记
+        sentence_endings = ['。', '！', '？', '.', '!', '?', '\n\n']
+        
+        best_cut = 0
+        for ending in sentence_endings:
+            pos = truncated.rfind(ending)
+            if pos > max_chars * 0.7:  # 确保截断位置不要太靠前
+                best_cut = max(best_cut, pos + 1)
+        
+        if best_cut > 0:
+            return text[:best_cut].strip()
+        else:
+            # 如果没找到合适的句子边界，就在单词边界截断
+            words = truncated.split()
+            return ' '.join(words[:-1]) + '...' if len(words) > 1 else truncated + '...'
+    
+    async def get_node_embedding(self, node_data, max_total_tokens=280):
         """
-        Batch process tag embeddings using a specified method, with concurrency.
+        为 org-supertag node 获取向量表示
+        
+        Args:
+            node_data: Node 数据字典  
+            max_total_tokens: 总的最大 token 数
+            
+        Returns:
+            Dict: 包含向量和 node 元数据的字典
         """
-        if not tag_ids:
-            return
-
-        self.logger.info(f"Starting batch embedding for {len(tag_ids)} tags using method: '{method}'")
-
-        async def _process_single_tag(tag_id: str):
-            try:
-                tag_data = self.graph_service.get_node_by_id(tag_id)
-                if not tag_data:
-                    self.logger.warning(f"Tag '{tag_id}' not found in database, skipping.")
-                    return
-
-                embedding = None
-                effective_method = method
-
-                if method == 'average':
-                    linked_nodes = self.graph_service.get_nodes_linked_to_tag(tag_id)
-                    if len(linked_nodes) < 3:
-                        self.logger.debug(f"Tag '{tag_id}' has < 3 nodes, falling back to 'name' method.")
-                        effective_method = 'name'
-                    else:
-                        embedding = self._calculate_average_node_embeddings(tag_id)
-
-                if effective_method == 'name':
-                    embedding = await self._embed_tag_by_name(tag_data)
-
-                if embedding is not None and embedding.size > 0:
-                    self.graph_service.upsert_entity_embedding(tag_id, embedding)
-                    self.logger.debug(f"Successfully embedded tag '{tag_id}' using method '{effective_method}'")
-                else:
-                    self.logger.warning(f"Failed to generate embedding for tag '{tag_id}' with method '{effective_method}'.")
-
-            except Exception as e:
-                self.logger.error(f"Failed to process tag '{tag_id}' in batch: {e}", exc_info=True)
-
-        # Create concurrent tasks for all tags
-        tasks = [_process_single_tag(tag_id) for tag_id in tag_ids]
-        await asyncio.gather(*tasks)
-
-        self.logger.info(f"Finished batch embedding for {len(tag_ids)} tags.") 
+        # 准备文本
+        text = self.prepare_node_text_for_embedding(node_data, max_total_tokens)
+        
+        # 获取向量
+        result = await self.get_embedding(text)
+        
+        # 构建包含 node 元数据的返回结果
+        return {
+            'embedding_result': result,
+            'node_metadata': {
+                'node_id': node_data.get('id'),
+                'node_title': node_data.get('title'),
+                'node_level': node_data.get('level'),
+                'text_used': text,
+                'text_length': len(text),
+                'estimated_tokens': self._estimate_tokens(text),
+                'max_tokens_limit': max_total_tokens,
+                'original_content_length': len(node_data.get('content', ''))
+            }
+        }
 
-    async def refresh_stale_tags(self, batch_size: int = 20, method: str = 'average'):
-        """
-        查找所有 knowledge_status='STALE' 且 type='TAG' 的节点，批量重算嵌入并将状态设为 FRESH。
-        """
-        stale_tag_ids = []
-        # 1. 查询所有 STALE 标签
-        conn = self.graph_service._get_connection()
-        cursor = conn.cursor()
-        cursor.execute("SELECT node_id FROM nodes WHERE type = 'TAG' AND knowledge_status = 'STALE'")
-        rows = cursor.fetchall()
-        stale_tag_ids = [row[0] for row in rows]
-        if not stale_tag_ids:
-            self.logger.info("No STALE tags found for refresh.")
-            return
-        self.logger.info(f"Refreshing {len(stale_tag_ids)} STALE tags...")
-        # 2. 分批处理
-        for i in range(0, len(stale_tag_ids), batch_size):
-            batch = stale_tag_ids[i:i+batch_size]
-            await self.batch_embed_tags(batch, method=method)
-            # 3. 更新状态为 FRESH
-            for tag_id in batch:
-                self.graph_service.update_node_status(tag_id, "FRESH")
-        self.logger.info("Tag embedding refresh complete.") 
\ No newline at end of file
+# 全局嵌入服务实例
+_embedding_service: Optional[EmbeddingService] = None
+
+def get_embedding_service() -> EmbeddingService:
+    """获取全局嵌入服务实例"""
+    global _embedding_service
+    if _embedding_service is None:
+        # 导入并使用config.py的配置
+        from simtag.config import Config
+        config = Config()
+        _embedding_service = EmbeddingService(config.embedding_config)
+    return _embedding_service
+
+def init_embedding_service(config_dict: Dict[str, Any]) -> EmbeddingService:
+    """初始化全局嵌入服务"""
+    global _embedding_service
+    _embedding_service = EmbeddingService(config_dict)
+    return _embedding_service 
\ No newline at end of file
diff --git a/simtag/services/llama_cpp_embedding_service.py b/simtag/services/llama_cpp_embedding_service.py
index 2ebbcb5..b146117 100755
--- a/simtag/services/llama_cpp_embedding_service.py
+++ b/simtag/services/llama_cpp_embedding_service.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 """
 LlamaCpp Embedding Service
-Provides embedding service using llama.cpp's llama-embedding command-line tool
+使用 llama.cpp 的 llama-embedding 命令行工具提供嵌入服务
 """
 
 import os
@@ -15,49 +15,24 @@ from dataclasses import dataclass
 from pathlib import Path
 import shutil
 import re
+import numpy as np
 import time
 
-# Import from the main embedding service to ensure consistency
-from .embedding_service import EmbeddingBackend, EmbeddingResult
-
 logger = logging.getLogger(__name__)
 
-def smart_truncate(text: str, max_length: int) -> str:
-    """
-    Truncates text by preserving the beginning and end, joining them with '...'.
-    This is a token-aware (by words) truncation.
-    """
-    if len(text) <= max_length:
-        return text
+@dataclass
+class EmbeddingResult:
+    """嵌入结果"""
+    success: bool
+    embedding: Optional[List[float]] = None
+    dimension: int = 0
+    error_message: Optional[str] = None
+    processing_time: float = 0.0
 
-    # Simple word-based tokenization
-    words = text.split()
-    
-    # Estimate average characters per word
-    if not words:
-        return text[:max_length]
-        
-    avg_chars_per_word = len(text) / len(words)
-    max_words = int(max_length / avg_chars_per_word)
-    
-    if max_words < 2: # Need at least one word from start and one from end
-        return text[:max_length]
-
-    # Preserve half of the words from the start, half from the end
-    keep_words_each_side = max_words // 2
-    
-    start_words = words[:keep_words_each_side]
-    end_words = words[-keep_words_each_side:]
-    
-    start_text = " ".join(start_words)
-    end_text = " ".join(end_words)
-    
-    return f"{start_text} ... {end_text}"
-
-class LlamaCppEmbeddingService(EmbeddingBackend):
+class LlamaCppEmbeddingService:
     """
-    LlamaCpp Embedding Service
-    Wraps the llama-embedding command-line tool and implements the EmbeddingBackend interface.
+    LlamaCpp 嵌入服务
+    封装 llama-embedding 命令行工具
     """
     
     def __init__(self, 
@@ -68,130 +43,79 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                  n_ctx: int = 4096,
                  config_dict: Optional[Dict[str, Any]] = None):
         """
-        Initializes the LlamaCpp Embedding Service
+        初始化 LlamaCpp 嵌入服务
         
         Args:
-            model_path: Path to the GGUF model file
-            binary_path: Path to the llama-embedding binary
-            pooling: Pooling strategy ("mean", "cls", "last", etc.)
-            n_threads: Number of threads
-            n_ctx: Context length
-            config_dict: Configuration dictionary, including options for long text processing
+            model_path: GGUF模型文件路径
+            binary_path: llama-embedding 二进制文件路径
+            pooling: 池化策略 ("mean", "cls", "last", etc.)
+            n_threads: 线程数
+            n_ctx: 上下文长度
+            config_dict: 配置字典，包含长文本处理选项
         """
         self.model_path = Path(model_path).expanduser()
         self.binary_path = binary_path
         self.pooling = pooling
         self.n_threads = n_threads or os.cpu_count()
         self.n_ctx = n_ctx
-        self._dimension: Optional[int] = None
         
-        # Configuration (removed chunking related config, using node-based preprocessing)
+        # 配置（移除了 chunking 相关配置，使用 node-based 预处理）
         self.config = config_dict or {}
         
-        # Validate model file
+        # 验证模型文件
         if not self.model_path.exists():
             raise FileNotFoundError(f"Model file not found: {self.model_path}")
         
-        # Validate binary file
+        # 验证二进制文件
         try:
             subprocess.run([self.binary_path, "--help"], 
                          capture_output=True, check=True, timeout=10)
         except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
             raise RuntimeError(f"llama-embedding binary not found or not working: {e}")
         
-        logger.info("LlamaCpp embedding service initialized:")
+        logger.info(f"LlamaCpp embedding service initialized:")
         logger.info(f"  Model: {self.model_path}")
         logger.info(f"  Binary: {self.binary_path}")
         logger.info(f"  Pooling: {self.pooling}")
         logger.info(f"  Threads: {self.n_threads}")
-        
-        # Determine the embedding dimension on initialization
-        try:
-            self._dimension = self._get_embedding_dimension()
-            logger.info(f"  Embedding Dimension: {self._dimension}")
-        except Exception as e:
-            logger.error(f"Failed to determine embedding dimension during initialization: {e}")
-            # We can either raise an exception or allow it to fail later.
-            # For robustness, we'll let it proceed and fail on the first get_embedding call.
-            pass
-    
-    @property
-    def backend_name(self) -> str:
-        return "llama_cpp"
-        
-    @property
-    def default_dimension(self) -> int:
-        if self._dimension is None:
-            # Try to determine dimension again if it failed during init
-            try:
-                self._dimension = self._get_embedding_dimension()
-            except Exception as e:
-                logger.error(f"Could not determine embedding dimension: {e}")
-                return 0 # Return 0 or a sensible default if it fails
-        return self._dimension
     
-    def _get_embedding_dimension(self) -> int:
-        """
-        Runs a test embedding to determine the model's output dimension.
-        This is a synchronous, one-off call during initialization.
-        """
-        test_text = "dimension check"
-        # Use a synchronous subprocess call for this one-time check
-        cmd = [
-            self.binary_path,
-            "--model", str(self.model_path),
-            "--pooling", self.pooling,
-            "--threads", str(self.n_threads),
-            "--ctx", str(self.n_ctx)
-        ]
-        
-        process = subprocess.run(
-            cmd, 
-            input=test_text, 
-            capture_output=True, 
-            text=True, 
-            check=True,
-            timeout=30
-        )
-        
-        output = process.stdout.strip()
-        embedding_list = json.loads(output)
-        return len(embedding_list)
+    # 移除了 _split_text_into_chunks 和 _aggregate_embeddings 方法
+    # 这些 chunking 相关的功能已被 node-based 预处理策略替代
 
     def _preprocess_text(self, text: str) -> str:
-        """Smart text preprocessing, handles issues that might cause embedding failure"""
+        """智能文本预处理，处理可能导致嵌入失败的问题"""
         if not text:
             return text
         
-        # Save original text for debugging
+        # 保存原始文本用于调试
         original_text = text
         
-        # 1. Normalize newlines and whitespace
-        # Replace multiple consecutive newlines with a single space
-        text = re.sub(r'\n{2,}', ' ', text)  # multiple newlines -> single space
-        text = re.sub(r'\n', ' ', text)      # single newline -> space
-        text = re.sub(r'\r', ' ', text)      # carriage return -> space
-        text = re.sub(r'\t', ' ', text)      # tab -> space
+        # 1. 规范化换行符和空白字符
+        # 将多个连续换行符替换为单个空格
+        text = re.sub(r'\n{2,}', ' ', text)  # 多个换行 -> 单个空格
+        text = re.sub(r'\n', ' ', text)      # 单个换行 -> 空格
+        text = re.sub(r'\r', ' ', text)      # 回车 -> 空格
+        text = re.sub(r'\t', ' ', text)      # 制表符 -> 空格
         
-        # 2. Handle repeated spaces
-        text = re.sub(r' {2,}', ' ', text)   # multiple spaces -> single space
+        # 2. 处理重复的空格
+        text = re.sub(r' {2,}', ' ', text)   # 多个空格 -> 单个空格
         
-        # 3. Remove leading/trailing whitespace
+        # 3. 移除首尾空白
         text = text.strip()
         
-        # 4. Handle overly short text - add some context
+        # 4. 处理过短的文本 - 添加一些上下文
         if len(text.strip()) < 3:
             if text.strip():
-                # For very short text, add a descriptive prefix
-                text = f"Text content: {text.strip()}"
+                # 对于很短的文本，添加描述性前缀
+                text = f"文本内容：{text.strip()}"
             
-        # 5. Handle repeated content (e.g., 'comment comment')
-        # Detect and simplify repeated patterns
-        if len(text) < 20:  # Only check short texts for repetition
-            # Check for simple repetition (e.g., "abc abc")
+        # 5. 处理重复内容（如"点评点评"）
+        # 检测重复模式并简化
+        if len(text) < 20:  # 只对短文本做重复检测
+            # 检查是否是简单重复（如："abc abc"）
             words = text.split()
             if len(words) >= 2:
-                # Check for adjacent repeated words
+                # 检查是否有相邻的重复词
                 unique_words = []
                 prev_word = None
                 for word in words:
@@ -202,19 +126,19 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                     text = ' '.join(unique_words)
                     logger.debug(f"Removed adjacent duplicates: {repr(original_text)} -> {repr(text)}")
         
-        # 6. Ensure text is not too short to cause tokenizer issues
+        # 6. 确保文本不会太短导致tokenizer问题
         if len(text.strip()) < 5:
-            text = f"This is content about '{text.strip()}'"
+            text = f"这是关于'{text.strip()}'的内容"
             logger.debug(f"Expanded short text: {repr(original_text)} -> {repr(text)}")
         
-        # Log preprocessing changes
+        # 记录预处理变化
         if text != original_text:
             logger.debug(f"Text preprocessed: {repr(original_text)} -> {repr(text)}")
         
         return text
 
     def _analyze_text(self, text: str) -> Dict[str, Any]:
-        """Analyze text features to help identify content that might cause issues"""
+        """分析文本特征，帮助识别可能导致问题的内容"""
         analysis = {
             "length": len(text),
             "stripped_length": len(text.strip()),
@@ -229,18 +153,18 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
 
     async def get_embedding(self, text: str) -> EmbeddingResult:
         """
-        Get embedding vector for a single text
+        获取单个文本的嵌入向量
         
         Args:
-            text: Input text
+            text: 输入文本
             
         Returns:
-            EmbeddingResult: Embedding result
+            EmbeddingResult: 嵌入结果
         """
         import time
         start_time = time.time()
         
-        # More strict empty text check
+        # 更严格的空文本检查
         if not text or not text.strip():
             text_analysis = self._analyze_text(text)
             logger.warning(f"Empty or whitespace-only text detected: {text_analysis['preview']}")
@@ -249,11 +173,11 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                 error_message=f"Empty or whitespace-only text provided: {text_analysis['preview']}"
             )
         
-        # Smart text preprocessing
+        # 智能文本预处理
         original_text = text
         text = self._preprocess_text(text)
         
-        # If text becomes empty after preprocessing, log and return error
+        # 如果预处理后文本变空，记录并返回错误
         if not text.strip():
             logger.warning(f"Text became empty after preprocessing: {repr(original_text)}")
             return EmbeddingResult(
@@ -261,20 +185,20 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                 error_message=f"Text became empty after preprocessing: {repr(original_text)}"
             )
         
-        # Simple length check and truncation (node-based preprocessing should have handled length)
-        # Use basic character length check as a safeguard
+        # 简单的长度检查和截断（node-based 预处理应该已经处理了长度）
+        # 使用基础的字符长度检查作为安全保障
         char_length = len(text.strip())
-        safe_char_length = int(self.n_ctx * 0.75) # Increase ratio to accommodate longer context
+        safe_char_length = int(self.n_ctx * 0.6)  # 更保守的字符长度限制
         
         if char_length > safe_char_length:
-            # If character length is still too long, perform smart truncation and warn
-            logger.warning(f"Text still too long after preprocessing ({char_length} chars, max {safe_char_length}), performing smart truncation.")
-            text = smart_truncate(text, safe_char_length)
-            logger.info(f"Smart-truncated text to {len(text)} characters: '{text[:100]}...'")
+            # 如果字符长度仍然超长，进行截断并警告
+            logger.warning(f"Text still too long after preprocessing ({char_length} chars, max {safe_char_length}), truncating. This suggests node-based preprocessing needs adjustment.")
+            text = text[:safe_char_length]
+            logger.info(f"Truncated text to {len(text)} characters")
         
-        # Try different pooling strategies to avoid llama.cpp bugs
+        # 尝试不同的池化策略来避免llama.cpp的bug
         pooling_strategies = [self.pooling, "cls", "last", "mean"]
-        # Remove duplicates and preserve order
+        # 去重并保持顺序
         pooling_strategies = list(dict.fromkeys(pooling_strategies))
         
         for i, pooling_strategy in enumerate(pooling_strategies):
@@ -283,21 +207,21 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
             result = await self._try_embedding_with_pooling(text, pooling_strategy, start_time)
             
             if result.success:
-                if i > 0:  # If not successful on the first attempt
+                if i > 0:  # 如果不是第一次尝试成功的
                     logger.info(f"Successfully generated embedding using fallback pooling strategy: {pooling_strategy}")
                 return result
             else:
-                # Check for GGML_ASSERT error
+                # 检查是否是GGML_ASSERT错误
                 if "GGML_ASSERT" in str(result.error_message) or "seq_id" in str(result.error_message):
                     text_preview = repr(text[:30]) + ("..." if len(text) > 30 else "")
                     logger.warning(f"Pooling strategy '{pooling_strategy}' failed with sequence ID issue for text {text_preview}, trying next strategy")
                     continue
                 else:
-                    # Other types of errors, possibly not a pooling strategy issue
+                    # 其他类型的错误，可能不是池化策略问题
                     logger.error(f"Non-pooling related error with strategy '{pooling_strategy}': {result.error_message}")
                     return result
         
-        # All pooling strategies failed - log detailed text analysis information
+        # 所有池化策略都失败 - 记录详细的文本分析信息
         text_analysis = self._analyze_text(text)
         
         error_details = [
@@ -320,16 +244,16 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
         )
     
     async def _try_embedding_with_pooling(self, text: str, pooling_strategy: str, start_time: float) -> EmbeddingResult:
-        """Attempt to generate embedding using the specified pooling strategy"""
+        """尝试使用指定的池化策略生成嵌入"""
         import time
         try:
-            # Use temporary file to avoid pipe issues
+            # 使用临时文件避免管道问题
             with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as tmp_file:
                 tmp_file.write(text.strip())
                 tmp_file_path = tmp_file.name
             
             try:
-                # Build command
+                # 构建命令
                 cmd = [
                     self.binary_path,
                     "-m", str(self.model_path),
@@ -337,15 +261,15 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                     "--pooling", pooling_strategy,
                     "-t", str(self.n_threads),
                     "-c", str(self.n_ctx),
-                    "-b", "1",  # Force batch size to 1 to avoid overflow
+                    "-b", "1",  # 强制批处理大小为1，避免溢出
                     "--embd-output-format", "json",
-                    "--embd-normalize", "2",  # L2 normalization
-                    "--no-warmup"  # Skip warmup
+                    "--embd-normalize", "2",  # L2标准化
+                    "--no-warmup"  # 跳过预热
                 ]
                 
                 logger.debug(f"Running command: {' '.join(cmd)}")
                 
-                # Run command
+                # 运行命令
                 result = await asyncio.create_subprocess_exec(
                     *cmd,
                     stdout=asyncio.subprocess.PIPE,
@@ -354,7 +278,7 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                 
                 stdout, stderr = await result.communicate()
                 
-                # Filter out known, harmless warning messages
+                # 过滤掉已知的、无害的警告信息
                 stderr_lines = stderr.decode().split('\n')
                 filtered_stderr = []
                 for line in stderr_lines:
@@ -379,7 +303,7 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                 filtered_stderr_text = '\n'.join(filtered_stderr).strip()
                 
                 if result.returncode != 0:
-                    # Only log filtered error messages
+                    # 只记录过滤后的错误信息
                     if filtered_stderr_text:
                         error_msg = f"llama-embedding failed with code {result.returncode}: {filtered_stderr_text}"
                     else:
@@ -391,7 +315,7 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                         processing_time=time.time() - start_time
                     )
                 
-                # Parse output
+                # 解析输出
                 output = stdout.decode().strip()
                 if not output:
                     return EmbeddingResult(
@@ -400,12 +324,12 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                         processing_time=time.time() - start_time
                     )
                 
-                # Attempt to parse JSON format output
+                # 尝试解析JSON格式输出
                 try:
-                    # Find JSON part (may be mixed with other logs)
+                    # 查找JSON部分（可能混有其他日志）
                     lines = output.split('\n')
                     
-                    # Look for the section containing JSON data
+                    # 寻找包含JSON数据的部分
                     json_text = ""
                     in_json = False
                     brace_count = 0
@@ -428,7 +352,7 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                         try:
                             json_data = json.loads(json_text)
                             
-                            # Handle OpenAI-style embedding format
+                            # 处理OpenAI风格的嵌入格式
                             if isinstance(json_data, dict) and 'data' in json_data:
                                 data_array = json_data['data']
                                 if isinstance(data_array, list) and len(data_array) > 0:
@@ -451,39 +375,39 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                                 
                         except json.JSONDecodeError as e:
                             logger.debug(f"JSON parsing failed, trying alternative parsing: {e}")
-                            # If JSON parsing fails, attempt to extract embedding array
-                            # Look for the "embedding": [list of numbers] pattern
+                            # 如果JSON解析失败，尝试提取嵌入数组
+                            # 查找 "embedding": [数字列表] 模式
                             pattern = r'"embedding":\s*\[([\d\.\-,\s]+)\]'
                             match = re.search(pattern, output, re.DOTALL)
                             if match:
                                 numbers_text = match.group(1)
-                                # Split and convert to float numbers
+                                # 分割并转换为浮点数
                                 numbers = re.findall(r'-?\d+\.?\d*', numbers_text)
                                 embedding = [float(n) for n in numbers if n]
                             else:
                                 raise ValueError("Could not extract embedding from output")
                     else:
-                        # If JSON is not found, attempt to parse raw numeric output
-                        # Find lines containing numbers
+                        # 如果没有找到JSON，尝试解析原始数字输出
+                        # 查找包含数字的行
                         number_lines = []
                         for line in lines:
                             if any(char.isdigit() or char == '.' or char == '-' for char in line):
                                 number_lines.append(line)
                         
                         if number_lines:
-                            # Combine and parse all numeric lines
+                            # 将所有数字行合并并解析
                             numbers_text = ' '.join(number_lines)
-                            # Extract float numbers
+                            # 提取浮点数
                             numbers = re.findall(r'-?\d+\.?\d*', numbers_text)
                             embedding = [float(n) for n in numbers if n]
                         else:
                             raise ValueError("No numeric data found in output")
                     
-                    # Validate embedding vector
+                    # 验证嵌入向量
                     if not embedding or not isinstance(embedding, list):
                         raise ValueError("Invalid embedding format")
                     
-                    # Check if all elements are zero (may indicate an error)
+                    # 检查是否全为零（可能表示错误）
                     if all(abs(x) < 1e-10 for x in embedding):
                         logger.warning("Generated embedding contains all zeros, this might indicate an issue")
                     
@@ -507,7 +431,7 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                     )
             
             finally:
-                # Clean up temporary file
+                # 清理临时文件
                 try:
                     os.unlink(tmp_file_path)
                 except:
@@ -522,103 +446,148 @@ class LlamaCppEmbeddingService(EmbeddingBackend):
                 processing_time=time.time() - start_time
             )
     
-    async def get_embeddings_batch(self, texts: List[str], model: Optional[str] = None) -> List[EmbeddingResult]:
+    # 移除了 _get_embedding_chunked 方法
+    # chunking 功能已被 node-based 预处理策略完全替代
+    
+    async def get_embeddings_batch(self, texts: List[str]) -> List[EmbeddingResult]:
         """
-        Get embedding vectors for a batch of texts.
-        The `model` parameter is ignored as this service is tied to a single model instance.
+        串行批量获取嵌入向量 - 针对 llama-embedding 的特性优化
+        
+        由于 llama-embedding 每次调用都需要加载模型，并发反而会增加开销
+        因此采用优化的串行处理策略
+        
+        Args:
+            texts: 文本列表
+            
+        Returns:
+            List[EmbeddingResult]: 嵌入结果列表
         """
+        logger.info(f"Processing batch of {len(texts)} texts (optimized serial)")
         start_time = time.time()
         
-        # 1. Preprocess all texts
-        processed_texts = [self._preprocess_text(text) for text in texts]
-        
-        # 2. Filter out texts that became empty after preprocessing
-        valid_texts = []
-        original_indices = []
-        for i, text in enumerate(processed_texts):
-            if text and text.strip():
-                valid_texts.append(text)
-                original_indices.append(i)
-
-        if not valid_texts:
-            return [EmbeddingResult(success=False, error_message="All texts were empty after preprocessing.")] * len(texts)
-
-        # 3. Create a temporary file to pass all valid texts to the binary
-        with tempfile.NamedTemporaryFile(mode='w+', delete=True, suffix=".txt", encoding='utf-8') as tmpfile:
-            # Each text must be on a new line
-            tmpfile.write("\n".join(valid_texts))
-            tmpfile.flush() # Ensure all data is written to disk
+        results = []
+        for i, text in enumerate(texts):
+            logger.debug(f"Processing text {i+1}/{len(texts)}")
+            result = await self.get_embedding(text)
+            results.append(result)
             
-            cmd = [
-                self.binary_path,
-                "--model", str(self.model_path),
-                "--pooling", self.pooling,
-                "--threads", str(self.n_threads),
-                "--ctx", str(self.n_ctx),
-                "--file", tmpfile.name
-            ]
+            # 提供进度反馈
+            if (i + 1) % 5 == 0 or i == len(texts) - 1:
+                elapsed = time.time() - start_time
+                rate = (i + 1) / elapsed if elapsed > 0 else 0
+                remaining = (len(texts) - i - 1) / rate if rate > 0 else 0
+                logger.info(f"Progress: {i+1}/{len(texts)} ({rate:.1f} texts/s, ~{remaining:.0f}s remaining)")
             
-            try:
-                # 4. Run the llama-embedding process asynchronously
-                process = await asyncio.create_subprocess_exec(
-                    *cmd,
-                    stdout=asyncio.subprocess.PIPE,
-                    stderr=asyncio.subprocess.PIPE
-                )
-                
-                stdout, stderr = await process.communicate()
-                
-                if process.returncode != 0:
-                    error_msg = f"Llama-embedding failed with code {process.returncode}: {stderr.decode('utf-8', 'ignore')}"
-                    logger.error(error_msg)
-                    # Return errors for all original texts
-                    return [EmbeddingResult(success=False, error_message=error_msg)] * len(texts)
-                
-                # 5. Parse the output
-                output_str = stdout.decode('utf-8').strip()
-                # The output for batch is one JSON array per line
-                lines = output_str.splitlines()
-                batch_embeddings = [json.loads(line) for line in lines]
-                
-                if len(batch_embeddings) != len(valid_texts):
-                    raise ValueError(f"Mismatch in batch embedding results. Expected {len(valid_texts)}, got {len(batch_embeddings)}.")
-
-                # 6. Reconstruct the results list in the original order
-                final_results = [EmbeddingResult(success=False, error_message="Text was empty after preprocessing.")] * len(texts)
-                processing_time = (time.time() - start_time) / len(valid_texts)
-
-                for i, embedding in enumerate(batch_embeddings):
-                    original_idx = original_indices[i]
-                    final_results[original_idx] = EmbeddingResult(
-                        success=True,
-                        embedding=embedding,
-                        dimension=len(embedding),
-                        processing_time=processing_time
-                    )
-                
-                return final_results
-
-            except Exception as e:
-                logger.error(f"Batch embedding failed: {e}", exc_info=True)
-                return [EmbeddingResult(success=False, error_message=str(e))] * len(texts)
-
+            # 如果连续失败太多，提前退出
+            if i >= 5 and all(not r.success for r in results[-5:]):
+                logger.error("Too many consecutive failures, stopping batch processing")
+                # 为剩余文本添加失败结果
+                for j in range(i+1, len(texts)):
+                    results.append(EmbeddingResult(
+                        success=False,
+                        error_message="Batch processing stopped due to consecutive failures"
+                    ))
+                break
+        
+        success_count = sum(1 for r in results if r.success)
+        total_time = time.time() - start_time
+        
+        logger.info(f"Batch processing completed: {success_count}/{len(texts)} successful in {total_time:.2f}s")
+        logger.info(f"Average time per text: {total_time/len(texts):.3f}s")
+        
+        return results
+    
     async def health_check(self) -> bool:
         """
-        Performs a health check on the service
+        健康检查
+        
+        Returns:
+            bool: 服务是否正常
         """
         try:
-            result = await self.get_embedding("health check")
+            result = await self.get_embedding("Health check test")
             return result.success
-        except Exception:
+        except Exception as e:
+            logger.error(f"Health check failed: {e}")
             return False
-
+    
     def get_model_info(self) -> Dict[str, Any]:
-        """Returns information about the model and service configuration."""
+        """
+        获取模型信息
+        
+        Returns:
+            Dict: 模型信息
+        """
         return {
             "model_path": str(self.model_path),
+            "model_exists": self.model_path.exists(),
+            "model_size_mb": round(self.model_path.stat().st_size / (1024*1024), 1) if self.model_path.exists() else 0,
             "binary_path": self.binary_path,
-            "pooling_strategy": self.pooling,
-            "threads": self.n_threads,
-            "context_length": self.n_ctx,
-            "dimension": self._dimension
-        } 
\ No newline at end of file
+            "pooling": self.pooling,
+            "n_threads": self.n_threads,
+            "n_ctx": self.n_ctx
+        }
+
+def create_qwen3_embedding_service(
+    model_path: Optional[str] = None, 
+    binary_path: Optional[str] = None,
+    config_dict: Optional[Dict[str, Any]] = None
+) -> 'LlamaCppEmbeddingService':
+    """
+    创建配置好的 Qwen3-Embedding 服务
+    
+    Args:
+        model_path: 模型文件路径，如果为None则使用默认路径
+        binary_path: llama-embedding 二进制文件路径，如果为None则使用默认
+        config_dict: 配置字典，用于获取额外的配置参数
+        
+    Returns:
+        配置好的 LlamaCppEmbeddingService 实例
+    """
+    # 如果没有提供config_dict，创建一个默认的
+    if config_dict is None:
+        config_dict = {}
+    
+    # 默认模型路径
+    if model_path is None:
+        model_path = config_dict.get(
+            "llama_cpp_model_path",
+            "~/.models/Qwen3-Embedding-0.6B-GGUF/Qwen3-Embedding-0.6B-Q8_0.gguf"
+        )
+    
+    # 展开用户路径
+    model_path = os.path.expanduser(model_path)
+    
+    # 检查模型文件是否存在
+    if not os.path.exists(model_path):
+        raise FileNotFoundError(f"Qwen3-Embedding model not found at: {model_path}")
+    
+    # 默认二进制路径
+    if binary_path is None:
+        binary_path = config_dict.get("llama_cpp_binary", "llama-embedding")
+    
+    # 检查二进制文件
+    if not shutil.which(binary_path):
+        raise FileNotFoundError(f"llama-embedding binary not found: {binary_path}")
+    
+    # 从config获取参数
+    pooling = config_dict.get("llama_cpp_pooling", "mean")
+    n_threads = config_dict.get("llama_cpp_threads")
+    n_ctx = config_dict.get("llama_cpp_max_context", 512)  # 使用较小的上下文避免批处理问题
+    
+    logger.info(f"Creating Qwen3-Embedding service with:")
+    logger.info(f"  Model: {model_path}")
+    logger.info(f"  Binary: {binary_path}")
+    logger.info(f"  Pooling: {pooling}")
+    logger.info(f"  Threads: {n_threads}")
+    logger.info(f"  Context: {n_ctx}")
+    
+    # 直接使用LlamaCppEmbeddingService构造函数
+    return LlamaCppEmbeddingService(
+        model_path=model_path,
+        binary_path=binary_path,
+        pooling=pooling,
+        n_threads=n_threads,
+        n_ctx=n_ctx,
+        config_dict=config_dict
+    ) 
\ No newline at end of file
diff --git a/simtag/services/llm_client.py b/simtag/services/llm_client.py
index 629c68a..0a743d1 100755
--- a/simtag/services/llm_client.py
+++ b/simtag/services/llm_client.py
@@ -10,363 +10,269 @@ from typing import Optional, List, Dict, Any, Union
 import requests       # NEW: Import for requests library
 import asyncio        # For running sync code in async
 import time
-from abc import ABC, abstractmethod
-from dataclasses import dataclass, field
 
 # from simtag.config import Config # LLMClient will expect an llm_config dict
-from ..config import LLMConfig
 
 logger = logging.getLogger(__name__)
 
-# =============================================================================
-# Standardized Data Structures
-# =============================================================================
-
-@dataclass
-class LLMResult:
-    """Standardized result for all LLM operations."""
-    success: bool
-    content: str = ""
-    error_message: Optional[str] = None
-    provider_name: Optional[str] = None
-    model_used: Optional[str] = None
-    response_time: Optional[float] = None
-
-# =============================================================================
-# Backend Abstraction
-# =============================================================================
-
-class LLMBackend(ABC):
-    """Abstract Base Class for all LLM provider backends."""
-    
-    @property
-    @abstractmethod
-    def provider_name(self) -> str:
-        """The name of the backend provider (e.g., 'ollama', 'openai')."""
-        pass
-    
-    @abstractmethod
-    async def generate(self,
-                     prompt: str,
-                     model: str,
-                     format_json: bool,
-                     options: Optional[Dict[str, Any]],
-                     system_prompt: Optional[str] = None,
-                     **kwargs: Any
-                    ) -> LLMResult:
-        """Abstract method for single text generation."""
-        pass
-
-    @abstractmethod
-    async def chat(self,
-                   messages: List[Dict[str, str]],
-                   model: str,
-                   format_json: bool,
-                   options: Optional[Dict[str, Any]],
-                   **kwargs: Any
-                  ) -> LLMResult:
-        """Abstract method for chat completions."""
-        pass
-
-# =============================================================================
-# Ollama Backend Implementation
-# =============================================================================
-
-class OllamaBackend(LLMBackend):
-    """LLM Backend implementation for Ollama."""
+class LLMClient:
+    """
+    LLM客户端，支持通过嵌入服务的多后端架构
+    """
     
-    def __init__(self, base_url: str, default_model: str, timeout: int):
-        self.base_url = base_url
-        self.default_model = default_model
-        self.timeout = timeout
+    def __init__(self, provider: str, config: Dict[str, Any]):
+        """
+        Initializes the LLMClient.
+
+        Args:
+            provider (str): The LLM provider.
+            config (Dict[str, Any]): Configuration dictionary for the LLM provider.
+        """
+        self.provider = provider
+        self.config = config
+        self.base_url = config.get('base_url', 'http://localhost:11434')
+        self.default_model = config.get('default_model', 'gemma3:4b')
+        self.timeout = config.get('timeout', 300)
+        
+        self._client = httpx.AsyncClient(timeout=self.timeout)
 
-    @property
-    def provider_name(self) -> str:
-        return "ollama"
+    async def check_availability(self) -> bool:
+        """Checks if the configured LLM service is available."""
+        if self.provider == 'ollama':
+            try:
+                response = await self._client.get(f"{self.base_url}/api/tags")
+                response.raise_for_status()
+                logger.info(f"Ollama service available at {self.base_url}")
+                return True
+            except httpx.RequestError as e:
+                logger.warning(f"Ollama service unavailable at {self.base_url}: {e}")
+                return False
+            except httpx.HTTPStatusError as e:
+                logger.warning(f"Ollama service returned error status at {self.base_url}: {e.response.status_code} - {e.response.text}")
+                return False
+        logger.warning(f"Availability check not implemented for provider: {self.provider}")
+        return False
 
-    async def generate(self,
-                     prompt: str,
-                     model: str,
-                     format_json: bool,
-                     options: Optional[Dict[str, Any]],
-                     system_prompt: Optional[str] = None,
-                     **kwargs: Any
-                    ) -> LLMResult:
+    async def generate(self, 
+                       prompt: str, 
+                       system_prompt: Optional[str] = None, 
+                       model: Optional[str] = None,
+                       format_json: bool = False,
+                       options: Optional[Dict[str, Any]] = None,
+                       **kwargs: Any
+                      ) -> str:
+        """
+        Generates text using the configured LLM provider.
+        """
+        target_model = model if model else self.config.get('default_model', self.default_model)
+        
+        if self.provider == 'ollama':
+            try:
+                return self._call_ollama_generate_sync(
+                    prompt=prompt,
+                    system_prompt=system_prompt,
+                    model=target_model,
+                    format_json=format_json,
+                    options=options,
+                    **kwargs
+                )
+            except Exception as e:
+                logger.error(f"Direct Ollama generate call failed: {e}")
+                return await self._call_ollama_generate_async(
+                    prompt=prompt,
+                    system_prompt=system_prompt,
+                    model=target_model,
+                    format_json=format_json,
+                    options=options,
+                    **kwargs
+                )
+        else:
+            logger.error(f"LLM provider '{self.provider}' not supported.")
+            return ""
+
+    def _call_ollama_generate_sync(self,
+                                    prompt: str,
+                                    system_prompt: Optional[str],
+                                    model: str,
+                                    format_json: bool,
+                                    options: Optional[Dict[str, Any]],
+                                    **kwargs: Any
+                                   ) -> str:
+        """
+        Synchronous version using requests.
+        """
         api_url = f"{self.base_url}/api/generate"
-        payload: Dict[str, Any] = {
+        
+        payload_dict: Dict[str, Any] = {
             "model": model,
             "prompt": prompt,
-            "stream": False, # Ensure no streaming
+            "stream": True,
+            "format": "json" if format_json else ""
         }
-        if format_json:
-            payload["format"] = "json"
+        
         if system_prompt:
-            payload["system"] = system_prompt
+            payload_dict["system"] = system_prompt
+        
         if options:
-            payload["options"] = options
-
-        logger.debug(f"Ollama payload: {json.dumps(payload)}")
-        logger.debug(f"Making Ollama generate request to {api_url} with model {model} using requests")
-        start_time = time.time()
-
-        def do_request():
-            # This function will be executed in a separate thread
-            try:
-                return requests.post(api_url, json=payload, timeout=self.timeout)
-            except requests.RequestException as e:
-                return e # Return exception to be re-raised in main thread
-
+            payload_dict["options"] = options
+        
+        logger.debug(f"Making generate request to {api_url} with model {model}")
+        
         try:
-            loop = asyncio.get_running_loop()
-            response = await loop.run_in_executor(None, do_request)
-
-            if isinstance(response, requests.RequestException):
-                raise response # Re-raise exception from the thread
-
-            response.raise_for_status()
-            response_json = response.json()
-
-            # Ollama's non-streaming response aggregates the 'response' field.
-            content = response_json.get('response', '').strip()
-
-            return LLMResult(
-                success=True,
-                content=content,
-                provider_name=self.provider_name,
-                model_used=model,
-                response_time=time.time() - start_time
+            response = requests.post(
+                api_url, 
+                json=payload_dict, 
+                headers={"Content-Type": "application/json"},
+                timeout=self.timeout,
+                stream=True
             )
-        except requests.RequestException as e:
-            msg = f"Failed to connect to Ollama service using requests: {e}"
-            logger.error(f"Ollama generate request failed for model {model}: {msg}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
-        except requests.HTTPError as e:
-            # Use e.response.text to get the body, which might have more info
-            response_text = e.response.text if e.response else "(No response text)"
-            msg = f"Ollama service returned status {e.response.status_code}: {response_text}"
-            logger.error(f"Ollama generate request failed for model {model} with status {e.response.status_code}: {response_text}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
-        except Exception as e:
-            msg = f"An unexpected error occurred: {e}"
-            logger.error(f"An unexpected error occurred during Ollama generate call for model {model}: {msg}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
-
-    async def chat(self,
-                   messages: List[Dict[str, str]],
-                   model: str,
-                   format_json: bool,
-                   options: Optional[Dict[str, Any]],
-                   **kwargs: Any
-                  ) -> LLMResult:
-        api_url = f"{self.base_url}/api/chat"
-        payload: Dict[str, Any] = {
+            
+            response.raise_for_status()
+            
+            full_response = ""
+            for line in response.iter_lines():
+                if line:
+                    try:
+                        chunk = json.loads(line.decode('utf-8'))
+                        if 'response' in chunk:
+                            full_response += chunk['response']
+                        if chunk.get('done', False):
+                            break
+                    except json.JSONDecodeError:
+                        logger.warning(f"Failed to parse JSON chunk: {line}")
+                        continue
+            
+            logger.debug(f"Successfully generated response with model {model}")
+            return full_response
+            
+        except requests.exceptions.RequestException as e:
+            logger.error(f"Ollama generate request failed for model {model}: {e}")
+            raise
+
+    async def _call_ollama_generate_async(self,
+                                        prompt: str,
+                                        system_prompt: Optional[str],
+                                        model: str,
+                                        format_json: bool,
+                                        options: Optional[Dict[str, Any]],
+                                        **kwargs: Any
+                                       ) -> str:
+        """
+        Asynchronous version using httpx.
+        """
+        api_url = f"{self.base_url}/api/generate"
+        
+        payload_dict: Dict[str, Any] = {
             "model": model,
-            "messages": messages,
-            "stream": False,
+            "prompt": prompt,
+            "stream": True,
+            "format": "json" if format_json else ""
         }
-        if format_json:
-            payload["format"] = "json"
+        
+        if system_prompt:
+            payload_dict["system"] = system_prompt
+        
         if options:
-            payload["options"] = options
-
-        logger.debug(f"Making Ollama chat request to {api_url} with model {model} using requests")
-        start_time = time.time()
-
-        def do_request():
-            try:
-                return requests.post(api_url, json=payload, timeout=self.timeout)
-            except requests.RequestException as e:
-                return e
-
+            payload_dict["options"] = options
+        
+        logger.debug(f"Making generate request to {api_url} with model {model}")
+        
         try:
-            loop = asyncio.get_running_loop()
-            response = await loop.run_in_executor(None, do_request)
-
-            if isinstance(response, requests.RequestException):
-                raise response
-
-            response.raise_for_status()
-            response_json = response.json()
-            content = response_json.get('message', {}).get('content', '').strip()
-
-            return LLMResult(
-                success=True,
-                content=content,
-                provider_name=self.provider_name,
-                model_used=model,
-                response_time=time.time() - start_time
+            response = await self._client.post(
+                api_url, 
+                json=payload_dict, 
+                headers={"Content-Type": "application/json"},
+                timeout=self.timeout
             )
-        except requests.RequestException as e:
-            msg = f"Failed to connect to Ollama service using requests: {e}"
-            logger.error(f"Ollama chat request failed for model {model}: {msg}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
-        except requests.HTTPError as e:
-            response_text = e.response.text if e.response else "(No response text)"
-            msg = f"Ollama service returned status {e.response.status_code}: {response_text}"
-            logger.error(f"Ollama chat request failed for model {model} with status {e.response.status_code}: {response_text}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
+            
+            response.raise_for_status()
+            
+            full_response = ""
+            async for line in response.aiter_lines():
+                if line:
+                    try:
+                        chunk = json.loads(line.decode('utf-8'))
+                        if 'response' in chunk:
+                            full_response += chunk['response']
+                        if chunk.get('done', False):
+                            break
+                    except json.JSONDecodeError:
+                        logger.warning(f"Failed to parse JSON chunk: {line}")
+                        continue
+            
+            logger.debug(f"Successfully generated response with model {model}")
+            return full_response
+            
+        except httpx.RequestError as e:
+            logger.error(f"Async Ollama request failed for model {model}: {e}")
+            return f"Error: Failed to connect to Ollama service at {self.base_url}"
+        except httpx.HTTPStatusError as e:
+            logger.error(f"Async Ollama request failed for model {model} with status {e.response.status_code}: {e.response.text}")
+            return f"Error: Ollama service returned status {e.response.status_code} - {e.response.text}"
         except Exception as e:
-            msg = f"An unexpected error occurred: {e}"
-            logger.error(f"An unexpected error occurred during Ollama chat call for model {model}: {msg}")
-            return LLMResult(success=False, error_message=msg, provider_name=self.provider_name, model_used=model)
-
-# =============================================================================
-# Unified LLM Client
-# =============================================================================
-
-class LLMClient:
-    """
-    Unified LLM Client that manages multiple backends.
-    """
-    
-    def __init__(self, config: Union[Dict, 'LLMConfig']):
-        if not isinstance(config, dict):
-            config = LLMConfig()
-
-        self.config = config
-        self.backends: Dict[str, LLMBackend] = {}
-        self.primary_backend: Optional[str] = None
-        self._init_backends()
-
-    def _init_backends(self):
-        """Initializes all configured LLM backends."""
-        backends = getattr(self.config, 'backends', {})
-        if "ollama" in backends:
-            ollama_config = backends["ollama"]
-            self.backends["ollama"] = OllamaBackend(
-                base_url=ollama_config.base_url,
-                default_model=ollama_config.default_model,
-                timeout=ollama_config.timeout
-            )
-            logger.info("Initialized 'ollama' LLM backend.")
-        
-        self.primary_backend = getattr(self.config, 'primary_backend', 'ollama')
-        if not self.primary_backend or self.primary_backend not in self.backends:
-            if self.backends:
-                self.primary_backend = list(self.backends.keys())[0]
-                logger.warning(f"Primary LLM backend not configured or available. Falling back to '{self.primary_backend}'.")
-            else:
-                logger.error("No LLM backends are available or configured.")
-
-    async def generate(self,
-                       prompt: str,
-                       system_prompt: Optional[str] = None,
-                       model: Optional[str] = None,
-                       format_json: bool = False,
-                       options: Optional[Dict[str, Any]] = None,
-                       use_chat_endpoint: bool = False,
-                       **kwargs: Any
-                      ) -> LLMResult:
-        if use_chat_endpoint:
-            messages = []
-            if system_prompt:
-                messages.append({"role": "system", "content": system_prompt})
-            messages.append({"role": "user", "content": prompt})
-            return await self.chat(messages=messages, model=model, format_json=format_json, options=options, **kwargs)
-
-        backend_name = self.primary_backend
-        if not backend_name or backend_name not in self.backends:
-            return LLMResult(success=False, error_message="No suitable LLM backend is available.")
-
-        llm_backend = self.backends[backend_name]
-        
-        target_model = model
-        if not target_model and isinstance(llm_backend, OllamaBackend):
-             target_model = llm_backend.default_model
-
+            logger.error(f"An unexpected error occurred during async Ollama call for model {model}: {e}")
+            return f"Error: An unexpected error occurred: {e}"
+
+    async def get_embedding(self, 
+                            text: str, 
+                            model: Optional[str] = None,
+                            **kwargs: Any
+                           ) -> List[float]:
+        """Generates an embedding for a single piece of text."""
+        target_model = model or self.config.get('embedding_config', {}).get('default_model')
         if not target_model:
-            return LLMResult(success=False, error_message="No model specified and backend has no default.", provider_name=llm_backend.provider_name)
-
-        return await llm_backend.generate(
-            prompt=prompt,
-            system_prompt=system_prompt,
-            model=target_model,
-            format_json=format_json,
-            options=options,
-            **kwargs
-        )
-    async def chat(self,
-                   messages: List[Dict[str, str]],
-                   model: Optional[str] = None,
-                   format_json: bool = False,
-                   options: Optional[Dict[str, Any]] = None,
-                   backend: Optional[str] = None,
-                   **kwargs: Any
-                  ) -> LLMResult:
-        backend_name = backend or self.primary_backend
-        if not backend_name or backend_name not in self.backends:
-            return LLMResult(success=False, error_message="No suitable LLM backend is available.")
-
-        llm_backend = self.backends[backend_name]
-        
-        # Use the backend's default model if no specific model is requested
-        target_model = model
-        if not target_model and isinstance(llm_backend, OllamaBackend): # Example of backend-specific logic
-             target_model = llm_backend.default_model
+            logger.error("No embedding model specified.")
+            return []
 
+        if self.provider == 'ollama':
+            api_url = f"{self.base_url}/api/embeddings"
+            payload = {"model": target_model, "prompt": text}
+            try:
+                response = await self._client.post(api_url, json=payload)
+                response.raise_for_status()
+                return response.json().get("embedding", [])
+            except Exception as e:
+                logger.error(f"Failed to get embedding for model {target_model}: {e}")
+                return []
+        else:
+            logger.error(f"Embedding not implemented for provider: {self.provider}")
+            return []
+
+    async def get_embeddings_batch(self,
+                                 texts: List[str],
+                                 model: Optional[str] = None,
+                                 **kwargs: Any
+                                ) -> List[List[float]]:
+        """Generates embeddings for a batch of texts sequentially."""
+        target_model = model or self.config.get('embedding_config', {}).get('default_model')
         if not target_model:
-            return LLMResult(success=False, error_message="No model specified and backend has no default.", provider_name=llm_backend.provider_name)
+            logger.error("No embedding model specified for batch operation.")
+            return [[] for _ in texts]
 
-        return await llm_backend.chat(
-            messages=messages,
-            model=target_model,
-            format_json=format_json,
-            options=options,
-            **kwargs
-        )
+        logger.info(f"Starting sequential batch embedding for {len(texts)} texts with model {target_model}.")
+        
+        embeddings = []
+        for i, text in enumerate(texts):
+            try:
+                embedding = await self.get_embedding(text, model=target_model, **kwargs)
+                embeddings.append(embedding)
+                if (i + 1) % 10 == 0:
+                    logger.info(f"Processed {i + 1}/{len(texts)} embeddings...")
+            except Exception as e:
+                logger.error(f"Failed to get embedding for text item {i}: {e}")
+                embeddings.append([])
 
-    async def check_availability(self) -> bool:
-        """Checks if the configured LLM service is available."""
-        if self.primary_backend in self.backends:
-            backend = self.backends[self.primary_backend]
-            if isinstance(backend, OllamaBackend):
-                try:
-                    async with httpx.AsyncClient(timeout=10) as client:
-                        response = await client.get(f"{backend.base_url}/api/tags")
-                        response.raise_for_status()
-                        logger.info(f"LLM service available at {backend.base_url}")
-                        return True
-                except httpx.RequestError as e:
-                    logger.warning(f"LLM service unavailable at {backend.base_url}: {e}")
-                    return False
-                except httpx.HTTPStatusError as e:
-                    logger.warning(f"LLM service returned error status at {backend.base_url}: {e.response.status_code} - {e.response.text}")
-                    return False
-        logger.warning(f"Availability check not implemented for primary backend: {self.primary_backend}")
-        return False
+        logger.info(f"Finished sequential batch embedding for {len(texts)} texts.")
+        return embeddings
 
     async def close(self):
-        """Closes the underlying HTTP client. (No-op now)"""
-        logger.info("LLMClient.close() called, but client is now managed per-request.")
-        pass
-
-    async def get_available_models(self) -> List[str]:
-        """Gets the available models from the LLM API."""
-        if self.primary_backend in self.backends:
-            backend = self.backends[self.primary_backend]
-            if isinstance(backend, OllamaBackend):
-                try:
-                    async with httpx.AsyncClient(timeout=10) as client:
-                        response = await client.get(f"{backend.base_url}/api/tags")
-                        response.raise_for_status()
-                        models_data = response.json()
-                        return [model['name'] for model in models_data.get('models', [])]
-                except Exception as e:
-                    logger.error(f"Failed to get available models from LLM: {e}")
-                    return []
-        return []
+        """Closes the underlying HTTP client."""
+        if hasattr(self, '_client') and self._client and not self._client.is_closed:
+            await self._client.aclose()
+            logger.info("LLMClient HTTP client closed.")
 
-    def get_default_model(self) -> str:
-        """Gets the default model from the config."""
-        if self.primary_backend in self.backends:
-            backends = getattr(self.config, 'backends', {})
-            backend_config = backends.get(self.primary_backend)
-            if backend_config:
-                return backend_config.default_model
-        return ''
 
-
-# Example Usage (Illustrative - requires an LLM server running for full test)
+# Example Usage (Illustrative - requires an Ollama server running for full test)
 async def main_test_llm_client():
     """测试LLMClient的功能"""
     
@@ -380,7 +286,7 @@ async def main_test_llm_client():
     client = LLMClient(provider='ollama', config=test_config)
     
     is_available = await client.check_availability()
-    print(f"LLM service available: {is_available}")
+    print(f"Ollama service available: {is_available}")
     if not is_available:
         return
 
@@ -401,6 +307,25 @@ async def main_test_llm_client():
     except json.JSONDecodeError:
         print(f"Failed to parse JSON response: {json_response_str}")
 
+    print("\n--- Testing Embedding ---")
+    embedding_text = "This is a test sentence for embeddings."
+    embedding_vector = await client.get_embedding(embedding_text)
+    print(f"Text: {embedding_text}")
+    print(f"Embedding vector (first 5 dims): {embedding_vector[:5] if embedding_vector else 'Failed'}")
+    if embedding_vector:
+        print(f"Vector dimension: {len(embedding_vector)}")
+
+    print("\n--- Testing Batch Embedding (Sequential) ---")
+    batch_texts = [
+        "The quick brown fox jumps over the lazy dog.",
+        "Hello world from the final frontier.",
+        "Batch processing is now sequential."
+    ]
+    batch_embeddings = await client.get_embeddings_batch(batch_texts)
+    print(f"Processed {len(batch_embeddings)} texts in batch.")
+    for i, vec in enumerate(batch_embeddings):
+        print(f"  - Text {i+1}: Dimension {len(vec) if vec else 'Failed'}")
+
     await client.close()
 
 if __name__ == '__main__':
diff --git a/simtag/services/log_monitor.py b/simtag/services/log_monitor.py
new file mode 100755
index 0000000..66a1a62
--- /dev/null
+++ b/simtag/services/log_monitor.py
@@ -0,0 +1,473 @@
+#!/usr/bin/env python3
+"""
+日志监控服务 - 用于实时跟踪多线程处理进度
+"""
+
+import asyncio
+import logging
+import time
+import threading
+import queue
+from typing import Dict, List, Any, Optional, Callable
+from dataclasses import dataclass, field
+from datetime import datetime
+from collections import defaultdict, deque
+import json
+
+logger = logging.getLogger(__name__)
+
+@dataclass
+class ProcessingEvent:
+    """处理事件"""
+    timestamp: float
+    event_type: str  # "task_start", "task_complete", "task_error", "progress_update", "system_info"
+    worker_id: Optional[int] = None
+    task_id: Optional[str] = None
+    message: str = ""
+    metadata: Dict[str, Any] = field(default_factory=dict)
+
+@dataclass
+class WorkerStats:
+    """工作进程统计"""
+    worker_id: int
+    pid: Optional[int] = None
+    tasks_completed: int = 0
+    tasks_failed: int = 0
+    total_processing_time: float = 0.0
+    avg_processing_time: float = 0.0
+    current_task: Optional[str] = None
+    last_activity: float = 0.0
+    status: str = "idle"  # "idle", "busy", "error", "offline"
+
+@dataclass
+class ProcessingSession:
+    """处理会话"""
+    session_id: str
+    session_type: str  # "embedding", "ner", "mixed"
+    total_tasks: int
+    completed_tasks: int = 0
+    failed_tasks: int = 0
+    start_time: float = 0.0
+    estimated_completion_time: Optional[float] = None
+    throughput: float = 0.0  # tasks per second
+    workers: Dict[int, WorkerStats] = field(default_factory=dict)
+
+class LogMonitor:
+    """日志监控器"""
+    
+    def __init__(self, max_events: int = 1000, max_sessions: int = 10):
+        self.max_events = max_events
+        self.max_sessions = max_sessions
+        
+        # 事件存储
+        self.events = deque(maxlen=max_events)
+        self.event_queue = queue.Queue()
+        
+        # 会话管理
+        self.sessions: Dict[str, ProcessingSession] = {}
+        self.current_session: Optional[str] = None
+        
+        # 统计信息
+        self.global_stats = {
+            "total_tasks_processed": 0,
+            "total_processing_time": 0.0,
+            "error_count": 0,
+            "avg_throughput": 0.0,
+            "uptime": time.time()
+        }
+        
+        # 监控线程
+        self.monitor_thread = None
+        self.stop_event = threading.Event()
+        self.is_running = False
+        
+        # 回调函数
+        self.progress_callbacks: List[Callable] = []
+        
+        logger.info("LogMonitor initialized")
+    
+    def start(self):
+        """启动监控器"""
+        if self.is_running:
+            return
+            
+        self.is_running = True
+        self.stop_event.clear()
+        
+        self.monitor_thread = threading.Thread(
+            target=self._monitor_loop,
+            daemon=True
+        )
+        self.monitor_thread.start()
+        
+        logger.info("LogMonitor started")
+    
+    def stop(self):
+        """停止监控器"""
+        if not self.is_running:
+            return
+            
+        self.is_running = False
+        self.stop_event.set()
+        
+        if self.monitor_thread:
+            self.monitor_thread.join(timeout=5)
+            self.monitor_thread = None
+        
+        logger.info("LogMonitor stopped")
+    
+    def _monitor_loop(self):
+        """监控循环"""
+        while not self.stop_event.is_set():
+            try:
+                # 处理事件队列
+                self._process_event_queue()
+                
+                # 更新统计信息
+                self._update_statistics()
+                
+                # 调用进度回调
+                self._call_progress_callbacks()
+                
+                # 清理过期数据
+                self._cleanup_expired_data()
+                
+                time.sleep(1.0)  # 每秒更新一次
+                
+            except Exception as e:
+                logger.error(f"Monitor loop error: {e}")
+                time.sleep(5.0)  # 错误后等待更长时间
+    
+    def _process_event_queue(self):
+        """处理事件队列"""
+        processed = 0
+        while not self.event_queue.empty() and processed < 100:  # 限制每次处理的事件数
+            try:
+                event = self.event_queue.get_nowait()
+                self._handle_event(event)
+                processed += 1
+            except queue.Empty:
+                break
+            except Exception as e:
+                logger.error(f"Error processing event: {e}")
+    
+    def _handle_event(self, event: ProcessingEvent):
+        """处理单个事件"""
+        self.events.append(event)
+        
+        # 更新会话统计
+        if self.current_session and self.current_session in self.sessions:
+            session = self.sessions[self.current_session]
+            
+            if event.event_type == "task_start":
+                if event.worker_id is not None:
+                    worker = session.workers.get(event.worker_id)
+                    if worker:
+                        worker.current_task = event.task_id
+                        worker.status = "busy"
+                        worker.last_activity = event.timestamp
+            
+            elif event.event_type == "task_complete":
+                session.completed_tasks += 1
+                if event.worker_id is not None:
+                    worker = session.workers.get(event.worker_id)
+                    if worker:
+                        worker.tasks_completed += 1
+                        worker.current_task = None
+                        worker.status = "idle"
+                        worker.last_activity = event.timestamp
+                        
+                        # 更新处理时间
+                        processing_time = event.metadata.get("processing_time", 0.0)
+                        worker.total_processing_time += processing_time
+                        if worker.tasks_completed > 0:
+                            worker.avg_processing_time = worker.total_processing_time / worker.tasks_completed
+            
+            elif event.event_type == "task_error":
+                session.failed_tasks += 1
+                if event.worker_id is not None:
+                    worker = session.workers.get(event.worker_id)
+                    if worker:
+                        worker.tasks_failed += 1
+                        worker.current_task = None
+                        worker.status = "error"
+                        worker.last_activity = event.timestamp
+        
+        # 更新全局统计
+        if event.event_type == "task_complete":
+            self.global_stats["total_tasks_processed"] += 1
+            processing_time = event.metadata.get("processing_time", 0.0)
+            self.global_stats["total_processing_time"] += processing_time
+        elif event.event_type == "task_error":
+            self.global_stats["error_count"] += 1
+    
+    def _update_statistics(self):
+        """更新统计信息"""
+        if not self.current_session or self.current_session not in self.sessions:
+            return
+            
+        session = self.sessions[self.current_session]
+        
+        # 计算进度
+        if session.total_tasks > 0:
+            progress = (session.completed_tasks + session.failed_tasks) / session.total_tasks
+            
+            # 估算完成时间
+            if progress > 0.1 and session.start_time > 0:
+                elapsed = time.time() - session.start_time
+                estimated_total_time = elapsed / progress
+                session.estimated_completion_time = session.start_time + estimated_total_time
+            
+            # 计算吞吐量
+            if session.start_time > 0:
+                elapsed = time.time() - session.start_time
+                if elapsed > 0:
+                    session.throughput = (session.completed_tasks + session.failed_tasks) / elapsed
+        
+        # 更新全局吞吐量
+        uptime = time.time() - self.global_stats["uptime"]
+        if uptime > 0:
+            self.global_stats["avg_throughput"] = self.global_stats["total_tasks_processed"] / uptime
+    
+    def _call_progress_callbacks(self):
+        """调用进度回调函数"""
+        if not self.current_session or not self.progress_callbacks:
+            return
+            
+        session = self.sessions.get(self.current_session)
+        if not session:
+            return
+            
+        progress_data = {
+            "session_id": self.current_session,
+            "session_type": session.session_type,
+            "progress": (session.completed_tasks + session.failed_tasks) / max(session.total_tasks, 1),
+            "completed": session.completed_tasks,
+            "failed": session.failed_tasks,
+            "total": session.total_tasks,
+            "throughput": session.throughput,
+            "estimated_completion": session.estimated_completion_time,
+            "active_workers": sum(1 for w in session.workers.values() if w.status == "busy")
+        }
+        
+        for callback in self.progress_callbacks:
+            try:
+                callback(progress_data)
+            except Exception as e:
+                logger.error(f"Progress callback error: {e}")
+    
+    def _cleanup_expired_data(self):
+        """清理过期数据"""
+        current_time = time.time()
+        
+        # 清理过期会话
+        expired_sessions = []
+        for session_id, session in self.sessions.items():
+            if session_id != self.current_session:
+                # 如果会话超过1小时没有活动，清理它
+                last_activity = max(
+                    (w.last_activity for w in session.workers.values()),
+                    default=session.start_time
+                )
+                if current_time - last_activity > 3600:  # 1小时
+                    expired_sessions.append(session_id)
+        
+        for session_id in expired_sessions:
+            del self.sessions[session_id]
+            logger.debug(f"Cleaned up expired session: {session_id}")
+    
+    # 公共接口方法
+    
+    def start_session(self, session_id: str, session_type: str, total_tasks: int, worker_count: int):
+        """开始新的处理会话"""
+        # 清理旧会话
+        if len(self.sessions) >= self.max_sessions:
+            oldest_session = min(self.sessions.keys())
+            del self.sessions[oldest_session]
+        
+        session = ProcessingSession(
+            session_id=session_id,
+            session_type=session_type,
+            total_tasks=total_tasks,
+            start_time=time.time()
+        )
+        
+        # 初始化工作进程
+        for i in range(worker_count):
+            session.workers[i] = WorkerStats(worker_id=i)
+        
+        self.sessions[session_id] = session
+        self.current_session = session_id
+        
+        # 记录事件
+        self.log_event(
+            event_type="session_start",
+            message=f"Started {session_type} session with {total_tasks} tasks and {worker_count} workers",
+            metadata={"session_id": session_id, "session_type": session_type, "total_tasks": total_tasks}
+        )
+        
+        logger.info(f"Started monitoring session: {session_id} ({session_type}, {total_tasks} tasks)")
+    
+    def end_session(self, session_id: str):
+        """结束处理会话"""
+        if session_id in self.sessions:
+            session = self.sessions[session_id]
+            duration = time.time() - session.start_time
+            
+            self.log_event(
+                event_type="session_end",
+                message=f"Session completed: {session.completed_tasks}/{session.total_tasks} successful ({duration:.2f}s)",
+                metadata={
+                    "session_id": session_id,
+                    "duration": duration,
+                    "completed": session.completed_tasks,
+                    "failed": session.failed_tasks,
+                    "total": session.total_tasks
+                }
+            )
+            
+            if self.current_session == session_id:
+                self.current_session = None
+            
+            logger.info(f"Ended monitoring session: {session_id}")
+    
+    def log_event(self, event_type: str, message: str = "", worker_id: Optional[int] = None, 
+                  task_id: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None):
+        """记录事件"""
+        event = ProcessingEvent(
+            timestamp=time.time(),
+            event_type=event_type,
+            worker_id=worker_id,
+            task_id=task_id,
+            message=message,
+            metadata=metadata or {}
+        )
+        
+        try:
+            self.event_queue.put_nowait(event)
+        except queue.Full:
+            logger.warning("Event queue is full, dropping event")
+    
+    def add_progress_callback(self, callback: Callable):
+        """添加进度回调函数"""
+        self.progress_callbacks.append(callback)
+    
+    def remove_progress_callback(self, callback: Callable):
+        """移除进度回调函数"""
+        if callback in self.progress_callbacks:
+            self.progress_callbacks.remove(callback)
+    
+    def get_current_status(self) -> Dict[str, Any]:
+        """获取当前状态"""
+        if not self.current_session or self.current_session not in self.sessions:
+            return {"status": "idle", "message": "No active session"}
+        
+        session = self.sessions[self.current_session]
+        total_processed = session.completed_tasks + session.failed_tasks
+        progress = total_processed / max(session.total_tasks, 1)
+        
+        return {
+            "status": "active",
+            "session_id": self.current_session,
+            "session_type": session.session_type,
+            "progress": progress,
+            "completed": session.completed_tasks,
+            "failed": session.failed_tasks,
+            "total": session.total_tasks,
+            "throughput": session.throughput,
+            "estimated_completion": session.estimated_completion_time,
+            "workers": {
+                worker_id: {
+                    "status": worker.status,
+                    "current_task": worker.current_task,
+                    "tasks_completed": worker.tasks_completed,
+                    "tasks_failed": worker.tasks_failed,
+                    "avg_processing_time": worker.avg_processing_time
+                }
+                for worker_id, worker in session.workers.items()
+            }
+        }
+    
+    def get_recent_events(self, count: int = 50) -> List[Dict[str, Any]]:
+        """获取最近的事件"""
+        recent_events = list(self.events)[-count:]
+        return [
+            {
+                "timestamp": event.timestamp,
+                "event_type": event.event_type,
+                "worker_id": event.worker_id,
+                "task_id": event.task_id,
+                "message": event.message,
+                "metadata": event.metadata
+            }
+            for event in recent_events
+        ]
+    
+    def get_global_stats(self) -> Dict[str, Any]:
+        """获取全局统计信息"""
+        return {
+            **self.global_stats,
+            "active_sessions": len(self.sessions),
+            "current_session": self.current_session,
+            "monitor_uptime": time.time() - self.global_stats["uptime"]
+        }
+    
+    def print_status_report(self):
+        """打印状态报告"""
+        print("\n" + "="*80)
+        print("📊 多线程处理状态报告")
+        print("="*80)
+        
+        if not self.current_session:
+            print("⏸️  当前没有活跃的处理会话")
+            return
+        
+        session = self.sessions[self.current_session]
+        total_processed = session.completed_tasks + session.failed_tasks
+        progress = total_processed / max(session.total_tasks, 1) * 100
+        
+        print(f"🎯 会话: {self.current_session} ({session.session_type})")
+        print(f"📈 进度: {total_processed}/{session.total_tasks} ({progress:.1f}%)")
+        print(f"✅ 成功: {session.completed_tasks}")
+        print(f"❌ 失败: {session.failed_tasks}")
+        print(f"🚄 吞吐量: {session.throughput:.2f} tasks/sec")
+        
+        if session.estimated_completion_time:
+            remaining = session.estimated_completion_time - time.time()
+            if remaining > 0:
+                print(f"⏰ 预计完成: {remaining/60:.1f} 分钟后")
+        
+        print(f"\n👷 工作进程状态:")
+        for worker_id, worker in session.workers.items():
+            status_icon = {"idle": "⏸️", "busy": "🔧", "error": "❌", "offline": "💤"}.get(worker.status, "❓")
+            print(f"  Worker {worker_id}: {status_icon} {worker.status}")
+            if worker.current_task:
+                print(f"    当前任务: {worker.current_task}")
+            print(f"    完成: {worker.tasks_completed}, 失败: {worker.tasks_failed}")
+            if worker.avg_processing_time > 0:
+                print(f"    平均耗时: {worker.avg_processing_time:.3f}s")
+        
+        print("="*80)
+
+# 全局监控实例
+_global_monitor: Optional[LogMonitor] = None
+
+def get_log_monitor() -> LogMonitor:
+    """获取全局日志监控实例"""
+    global _global_monitor
+    if _global_monitor is None:
+        _global_monitor = LogMonitor()
+        _global_monitor.start()
+    return _global_monitor
+
+def start_monitoring():
+    """启动全局监控"""
+    monitor = get_log_monitor()
+    monitor.start()
+
+def stop_monitoring():
+    """停止全局监控"""
+    global _global_monitor
+    if _global_monitor:
+        _global_monitor.stop()
+        _global_monitor = None 
\ No newline at end of file
diff --git a/simtag/services/memory_synthesizer.py b/simtag/services/memory_synthesizer.py
new file mode 100755
index 0000000..686e6b7
--- /dev/null
+++ b/simtag/services/memory_synthesizer.py
@@ -0,0 +1,274 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import logging
+from typing import List, Dict, Any, Optional
+import asyncio
+import dataclasses
+import uuid
+import json
+
+from simtag.config import Config
+from simtag.core.memory_engine import MemoryEngine, MemoryItem, MemoryItemType
+from simtag.services.llm_client import LLMClient
+
+logger = logging.getLogger(__name__)
+
+@dataclasses.dataclass
+class CandidateMemory:
+    """Represents a potential memory item synthesized from user interactions, awaiting user approval."""
+    candidate_id: str
+    proposed_item: MemoryItem
+    justification: str  # Explanation from the LLM on why this memory is proposed
+    confidence_score: float  # Value between 0.0 and 1.0
+    source_data: Dict[str, Any]  # The data used to generate this candidate
+
+class MemorySynthesizer:
+    """
+    Analyzes user interactions and dialogue history to synthesize new,
+    long-term memories for the system. Includes vectorization capabilities.
+    """
+    def __init__(self, config: Config, memory_engine: MemoryEngine, llm_client: LLMClient):
+        self.config = config
+        self.memory_engine = memory_engine
+        self.llm_client = llm_client
+        logger.info("MemorySynthesizer initialized.")
+        # In-memory storage for candidates before they are approved/rejected.
+        # In a larger system, this might be a database table.
+        self.candidate_store: Dict[str, CandidateMemory] = {}
+        self.synthesis_prompt_template = """
+You are a meticulous knowledge analyst reviewing a conversation between a user and an AI assistant.
+Your task is to identify and extract key pieces of information that should be saved as long-term memories for the AI.
+Focus on durable facts, user preferences, stated goals, or important conclusions.
+
+Analyze the following dialogue:
+---
+{dialogue_text}
+---
+
+Based on the dialogue, extract potential memories. For each potential memory, provide:
+1. `type`: The type of memory. Must be one of: {valid_types}.
+2. `content`: The specific information to be remembered. For a preference, this should be a dictionary. For a fact, a string.
+3. `justification`: A brief explanation of why this piece of information is important to remember.
+4. `confidence_score`: A float between 0.0 and 1.0, indicating your certainty that this is a valuable, lasting memory.
+
+Respond with a JSON object containing a single key "candidates", which is a list of these memory objects.
+If no valuable memories are found, return a JSON object with an empty "candidates" list.
+Example for a user preference: {"type": "USER_PREFERENCE", "content": {{"key": "preferred_language", "value": "Python"}}, "justification": "User explicitly stated they prefer Python for new projects.", "confidence_score": 0.95}}
+Example for a fact: {"type": "FACT", "content": "The project 'X' uses a microservices architecture.", "justification": "This is a key architectural detail mentioned by the user.", "confidence_score": 0.9}
+
+Your JSON response:
+"""
+
+    async def synthesize_from_dialogue(self, session_id: str) -> List[CandidateMemory]:
+        """
+        Analyzes a given dialogue session and proposes new memories.
+        This is the core logic for turning conversations into knowledge.
+        """
+        logger.info(f"Starting memory synthesis for dialogue session: {session_id}")
+
+        history = await self.memory_engine.get_dialogue_history(session_id)
+        if not history or not history.turns:
+            logger.info(f"No dialogue history found for session {session_id}, skipping synthesis.")
+            return []
+
+        dialogue_text = "\n".join([f"{turn.speaker.upper()}: {turn.text}" for turn in history.turns])
+        
+        valid_types = [MemoryItemType.FACT.value, MemoryItemType.USER_PREFERENCE.value]
+        
+        prompt = self.synthesis_prompt_template.format(
+            dialogue_text=dialogue_text,
+            valid_types=valid_types
+        )
+
+        try:
+            response_str = await self.llm_client.generate(prompt)
+            logger.debug(f"LLM response for memory synthesis: {response_str}")
+            
+            response_data = json.loads(response_str)
+            extracted_candidates = response_data.get("candidates", [])
+
+            if not extracted_candidates:
+                logger.info("LLM analysis found no new memory candidates.")
+                return []
+
+            new_candidates = []
+            for cand_data in extracted_candidates:
+                try:
+                    mem_type_str = cand_data.get('type')
+                    mem_type = MemoryItemType(mem_type_str)
+
+                    # Create the MemoryItem dataclass
+                    item_id = f"{mem_type.value.lower()}_{uuid.uuid4().hex[:8]}"
+                    proposed_item = MemoryItem(
+                        id=item_id,
+                        type=mem_type,
+                        content=cand_data.get('content'),
+                        metadata={'source': 'synthesis', 'session_id': session_id}
+                    )
+                    
+                    # Create the CandidateMemory dataclass
+                    candidate_id = f"cand_{uuid.uuid4().hex[:12]}"
+                    candidate = CandidateMemory(
+                        candidate_id=candidate_id,
+                        proposed_item=proposed_item,
+                        justification=cand_data.get('justification', 'No justification provided.'),
+                        confidence_score=float(cand_data.get('confidence_score', 0.5)),
+                        source_data={'session_id': session_id, 'dialogue': dialogue_text}
+                    )
+                    new_candidates.append(candidate)
+                    self.candidate_store[candidate_id] = candidate
+                    logger.info(f"Successfully created and stored candidate {candidate_id} of type {mem_type_str}.")
+
+                except (ValueError, TypeError, KeyError) as e:
+                    logger.error(f"Failed to parse a candidate from LLM response. Data: {cand_data}. Error: {e}")
+                    continue # Skip this malformed candidate
+
+            logger.info(f"Memory synthesis complete. Generated {len(new_candidates)} new candidates.")
+            return new_candidates
+
+        except json.JSONDecodeError:
+            logger.error(f"Failed to decode JSON from LLM synthesis response: {response_str}")
+            return []
+        except Exception as e:
+            logger.error(f"An unexpected error occurred during synthesis: {e}", exc_info=True)
+            return []
+
+    def get_pending_candidates(self) -> List[CandidateMemory]:
+        """Returns a list of all memory candidates awaiting review."""
+        return list(self.candidate_store.values())
+
+    def get_candidate_by_id(self, candidate_id: str) -> Optional[CandidateMemory]:
+        """Retrieves a specific candidate by its ID."""
+        return self.candidate_store.get(candidate_id)
+
+    def discard_candidate(self, candidate_id: str) -> bool:
+        """Removes a candidate from the store (user rejected)."""
+        if candidate_id in self.candidate_store:
+            del self.candidate_store[candidate_id]
+            logger.info(f"Discarded candidate memory: {candidate_id}")
+            return True
+        return False
+
+    async def vectorize_memories(self, memory_items: List[MemoryItem]) -> Dict[str, List[float]]:
+        """
+        为记忆项生成向量化表示，支持批量处理
+        
+        Args:
+            memory_items: 记忆项列表
+            
+        Returns:
+            记忆ID到向量的映射
+        """
+        if not memory_items:
+            return {}
+            
+        # 提取文本内容用于向量化
+        texts_to_vectorize = []
+        memory_ids = []
+        
+        for item in memory_items:
+            # 根据记忆类型提取合适的文本
+            if item.type == MemoryItemType.FACT:
+                text = str(item.content)
+            elif item.type == MemoryItemType.USER_PREFERENCE:
+                # 对于用户偏好，组合key和value
+                if isinstance(item.content, dict):
+                    key = item.content.get('key', '')
+                    value = item.content.get('value', '')
+                    text = f"{key}: {value}"
+                else:
+                    text = str(item.content)
+            else:
+                text = str(item.content)
+            
+            texts_to_vectorize.append(text)
+            memory_ids.append(item.id)
+        
+        try:
+            # 使用LLM客户端的批量嵌入功能
+            logger.info(f"Vectorizing {len(texts_to_vectorize)} memory items")
+            embeddings = await self.llm_client.get_embeddings_batch(
+                texts_to_vectorize,
+                use_multicore=True
+            )
+            
+            # 构建ID到向量的映射
+            memory_vectors = {}
+            for memory_id, embedding in zip(memory_ids, embeddings):
+                if embedding:  # 确保嵌入向量不为空
+                    memory_vectors[memory_id] = embedding
+                else:
+                    logger.warning(f"Empty embedding for memory {memory_id}")
+            
+            logger.info(f"Successfully vectorized {len(memory_vectors)} memory items")
+            return memory_vectors
+            
+        except Exception as e:
+            logger.error(f"Error during memory vectorization: {e}", exc_info=True)
+            return {}
+
+    async def find_similar_memories(self, 
+                                  query_text: str, 
+                                  stored_memory_vectors: Dict[str, List[float]],
+                                  top_k: int = 5) -> List[Dict[str, Any]]:
+        """
+        基于向量相似性查找相似的记忆
+        
+        Args:
+            query_text: 查询文本
+            stored_memory_vectors: 已存储的记忆向量
+            top_k: 返回最相似的k个记忆
+            
+        Returns:
+            相似记忆列表，包含ID和相似度分数
+        """
+        if not query_text or not stored_memory_vectors:
+            return []
+        
+        try:
+            # 为查询文本生成向量
+            query_embedding = await self.llm_client.get_embedding(query_text)
+            if not query_embedding:
+                logger.warning("Failed to generate embedding for query text")
+                return []
+            
+            # 计算相似度
+            similarities = []
+            for memory_id, memory_vector in stored_memory_vectors.items():
+                # 简单的余弦相似度计算
+                similarity = self._cosine_similarity(query_embedding, memory_vector)
+                similarities.append({
+                    'memory_id': memory_id,
+                    'similarity': similarity
+                })
+            
+            # 按相似度排序并返回top_k
+            similarities.sort(key=lambda x: x['similarity'], reverse=True)
+            return similarities[:top_k]
+            
+        except Exception as e:
+            logger.error(f"Error during similarity search: {e}", exc_info=True)
+            return []
+
+    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
+        """计算两个向量的余弦相似度"""
+        try:
+            import math
+            
+            # 计算点积
+            dot_product = sum(a * b for a, b in zip(vec1, vec2))
+            
+            # 计算向量长度
+            norm1 = math.sqrt(sum(a * a for a in vec1))
+            norm2 = math.sqrt(sum(a * a for a in vec2))
+            
+            # 避免除零
+            if norm1 == 0 or norm2 == 0:
+                return 0.0
+                
+            return dot_product / (norm1 * norm2)
+            
+        except Exception as e:
+            logger.error(f"Error calculating cosine similarity: {e}")
+            return 0.0 
\ No newline at end of file
diff --git a/simtag/services/ner_service.py b/simtag/services/ner_service.py
new file mode 100755
index 0000000..62e3909
--- /dev/null
+++ b/simtag/services/ner_service.py
@@ -0,0 +1,154 @@
+# simtag/services/ner_service.py
+
+import logging
+from typing import List, Dict, Any, Optional
+
+# Assuming LLMClient has an async generate method
+from simtag.services.llm_client import LLMClient
+from simtag.services.smart_ner_service import SmartNERService
+import json
+import time
+
+logger = logging.getLogger(__name__)
+
+class NERService:
+    """
+    A simplified service that acts as a facade, delegating all NER-based 
+    operations to the SmartNERService.
+    """
+    def __init__(self, llm_client: LLMClient, config: Dict[str, Any], smart_ner_service: SmartNERService):
+        """
+        Initializes the NERService.
+
+        Args:
+            llm_client: The LLM client instance.
+            config: The application configuration.
+            smart_ner_service: The high-performance NER implementation. This is a required dependency.
+        """
+        self.llm_client = llm_client
+        self.config = config
+        self.smart_ner_service = smart_ner_service
+
+        if not self.llm_client:
+            logger.error("NERService initialized without a valid LLMClient.")
+            raise ValueError("LLMClient is required for NERService.")
+
+        if not self.smart_ner_service:
+            # This check is now for robustness, but dependency injection should prevent it.
+            logger.error("NERService initialized without a valid SmartNERService.")
+            raise ValueError("SmartNERService is a required dependency for NERService.")
+        
+        logger.info("✅ NERService initialized, delegating to SmartNERService.")
+
+    async def suggest_tags_batch(self, 
+                               note_contents: List[str], 
+                               existing_tags_list: Optional[List[List[str]]] = None) -> List[List[Dict[str, Any]]]:
+        """
+        Delegates batch tag suggestion directly to SmartNERService.
+        
+        Args:
+            note_contents: A list of note contents.
+            existing_tags_list: A list of existing tag lists for each note.
+            
+        Returns:
+            A list of tag suggestion lists for each note.
+        """
+        if not note_contents:
+            return []
+        
+        logger.info(f"🧠 Delegating tag suggestion for {len(note_contents)} notes to SmartNERService.")
+        
+        try:
+            # Use the parallel version if available, otherwise fall back to serial batch.
+            if hasattr(self.smart_ner_service, 'suggest_tags_batch_parallel'):
+                logger.debug("🚀 Using parallel processing in SmartNERService.")
+                results = await self.smart_ner_service.suggest_tags_batch_parallel(
+                    note_contents=note_contents,
+                    existing_tags_lists=existing_tags_list
+                )
+            else:
+                logger.debug("🔄 Using serial batch processing in SmartNERService.")
+                # Fix: SmartNERService uses batch_generate_tags, not suggest_tags_batch
+                # We need to convert the input format to match SmartNERService expectations
+                nodes_data = []
+                for i, content in enumerate(note_contents):
+                    existing_tags = existing_tags_list[i] if existing_tags_list and i < len(existing_tags_list) else []
+                    node_data = {
+                        'id': f'temp_node_{i}',
+                        'title': '',
+                        'content': content,
+                        'file_path': '',
+                        'level': 1,
+                        'existing_tags': existing_tags
+                    }
+                    nodes_data.append(node_data)
+                
+                batch_result = await self.smart_ner_service.batch_generate_tags(nodes_data)
+                
+                # Convert BatchProcessingResult back to the expected format
+                results = []
+                for suggestion in batch_result.suggestions:
+                    node_suggestions = []
+                    for tag_data in suggestion.get('tags', []):
+                        tag_dict = {
+                            'tag_name': tag_data.get('tag_name', ''),
+                            'confidence': tag_data.get('confidence', 0.0),
+                            'reasoning': tag_data.get('reasoning', '')
+                        }
+                        node_suggestions.append(tag_dict)
+                    results.append(node_suggestions)
+            
+            # Log performance stats from the underlying service.
+            stats = self.smart_ner_service.get_performance_stats()
+            multicore_stats = ""
+            if 'multicore_sessions' in stats:
+                multicore_stats = f", multicore_sessions: {stats['multicore_sessions']}"
+            
+            logger.info(
+                f"✅ SmartNERService processing complete for {len(results)} notes. "
+                f"Avg time: {stats.get('avg_time_per_note', 0):.3f}s/note{multicore_stats}"
+            )
+            return results
+
+        except Exception as e:
+            logger.error(f"An error occurred while delegating to SmartNERService: {e}", exc_info=True)
+            # Return a list of empty lists to match the expected output format on failure.
+            return [[] for _ in note_contents]
+
+    async def discover_relationships_batch(self, 
+                                         note_contents: List[str]) -> List[List[Dict[str, Any]]]:
+        """
+        Delegates batch relationship discovery directly to SmartNERService.
+        
+        Args:
+            note_contents: A list of note contents.
+            
+        Returns:
+            A list of relationship lists for each note.
+        """
+        if not note_contents:
+            return []
+            
+        logger.info(f"🧠 Delegating relationship discovery for {len(note_contents)} notes to SmartNERService.")
+
+        try:
+            # The parallel version is preferred if available.
+            if hasattr(self.smart_ner_service, 'discover_relationships_batch_parallel'):
+                logger.debug("🚀 Using parallel relationship discovery in SmartNERService.")
+                results = await self.smart_ner_service.discover_relationships_batch_parallel(note_contents)
+            else:
+                logger.debug("🔄 Using serial relationship discovery in SmartNERService.")
+                results = await self.smart_ner_service.discover_relationships_batch(note_contents)
+            
+            stats = self.smart_ner_service.get_performance_stats()
+            logger.info(
+                f"✅ SmartNERService relationship discovery complete. "
+                f"Avg time: {stats.get('avg_time_per_note', 0):.3f}s/note"
+            )
+            return results
+        
+        except Exception as e:
+            logger.error(f"An error occurred during relationship discovery delegation: {e}", exc_info=True)
+            return [[] for _ in note_contents]
+
+ 
\ No newline at end of file
diff --git a/simtag/services/ollama.py b/simtag/services/ollama.py
new file mode 100755
index 0000000..d8ab468
--- /dev/null
+++ b/simtag/services/ollama.py
@@ -0,0 +1,148 @@
+"""
+Ollama Service Adapter - Pluggable Component
+Provides standardized interface for Ollama API interaction
+"""
+import requests
+import json
+import logging
+from typing import Optional
+
+class OllamaService:
+    """Ollama Service Abstraction Layer"""
+    
+    def __init__(self, base_url: str = "http://localhost:11434", model: str = "gemma:3b"):
+        self.base_url = base_url
+        self.model = model
+        self.logger = logging.getLogger("ollama")
+        self.available = self.check_availability()
+    
+    def check_availability(self) -> bool:
+        """Check if Ollama service is available"""
+        try:
+            resp = requests.get(f"{self.base_url}/api/tags", timeout=3)
+            return resp.status_code == 200
+        except requests.exceptions.RequestException as e:
+            self.logger.warning(f"Ollama service unavailable: {str(e)}")
+            return False
+    
+    def generate_tags(self, text: str, limit: int = 5) -> list:
+        """
+        Generate tags using Ollama
+        Return format: ["tag1", "tag2", ...]
+        """
+        if not self.available:
+            self.logger.warning("Ollama service unavailable, skipping tag generation")
+            return []
+        
+        try:
+            # Build generation prompt
+            prompt = f"""
+            Extract {limit} most relevant tags from the following text, return only comma-separated tag list:
+            
+            {text[:2000]}... [truncated]
+            """
+            
+            # Call API
+            payload = {
+                "model": self.model,
+                "prompt": prompt,
+                "options": {"temperature": 0.3, "num_predict": limit*2}
+            }
+            resp = requests.post(
+                f"{self.base_url}/api/generate", 
+                json=payload,
+                timeout=30
+            )
+            resp.raise_for_status()
+            
+            # Parse response
+            response_data = resp.json()
+            raw_tags = response_data.get("response", "").strip()
+            
+            # Process tag format
+            tags = [tag.strip() for tag in raw_tags.split(",") if tag.strip()]
+            return tags[:limit]
+            
+        except Exception as e:
+            self.logger.error(f"Tag generation failed: {str(e)}")
+            return []
+    
+    def extract_entities(self, text: str) -> list:
+        """
+        Entity recognition
+        Return format: [{"entity": "entity", "type": "type", "start": position, "end": position}]
+        """
+        if not self.available:
+            return []
+        
+        try:
+            # Entity recognition system prompt
+            system_prompt = """
+            You are an entity recognition expert, please identify the following entity types in the text:
+            PERSON - Person names
+            ORG - Organizations/Institutions
+            LOCATION - Locations
+            TECH - Technical terms
+            Return format must be a valid JSON array.
+            """
+            
+            # Call API
+            payload = {
+                "model": self.model,
+                "system": system_prompt,
+                "prompt": text[:3000],
+                "format": "json",
+                "options": {"temperature": 0.2}
+            }
+            resp = requests.post(
+                f"{self.base_url}/api/generate", 
+                json=payload,
+                timeout=180  # 增加到3分钟
+            )
+            resp.raise_for_status()
+            
+            # Parse JSON response
+            return json.loads(resp.json().get("response", "[]"))
+            
+        except Exception as e:
+            self.logger.error(f"Entity recognition failed: {str(e)}")
+            return []
+    
+    def analyze_relations(self, tag: str, related_tags: list) -> list:
+        """
+        Analyze tag relationships
+        Return format: [{"tag": "related_tag", "relation": "relation_type", "reason": "reason"}]
+        """
+        if not self.available or not related_tags:
+            return []
+        
+        try:
+            # Relationship analysis prompt
+            prompt = f"""
+            Analyze the relationship between "{tag}" and the following tags:
+            {", ".join(related_tags)}
+            
+            Relation types: contains, related, similar, opposite
+            Return format must be a valid JSON array.
+            """
+            
+            # Call API
+            payload = {
+                "model": self.model,
+                "prompt": prompt,
+                "format": "json",
+                "options": {"temperature": 0.4}
+            }
+            resp = requests.post(
+                f"{self.base_url}/api/generate", 
+                json=payload,
+                timeout=30
+            )
+            resp.raise_for_status()
+            
+            # Parse JSON response
+            return json.loads(resp.json().get("response", "[]"))
+            
+        except Exception as e:
+            self.logger.error(f"Relationship analysis failed: {str(e)}")
+            return []
diff --git a/simtag/services/rag_service.py b/simtag/services/rag_service.py
deleted file mode 100644
index 53a10b0..0000000
--- a/simtag/services/rag_service.py
+++ /dev/null
@@ -1,1108 +0,0 @@
-# simtag/services/rag_service.py
-import logging
-from typing import Dict, Any, List, Optional, Tuple
-import numpy as np
-import asyncio
-import time
-import traceback
-import json
-import re
-from dataclasses import dataclass, field
-from collections import deque
-
-from simtag.config import Config
-from simtag.core.graph_service import GraphService, NodeType
-from simtag.services.embedding_service import EmbeddingService, EmbeddingResult
-from simtag.services.llm_client import LLMClient
-from simtag.utils.unified_tag_processor import TagResult, normalize_payload
-from simtag import prompts # Import the entire prompts module
-from ..prompts import (
-    ENTITY_EXTRACTION_PROMPT,
-    QUERY_ANALYSIS_PROMPT,
-    QA_PROMPT,
-    DEFAULT_ENTITY_TYPES,
-    RELATION_INFERENCE_PROMPT,
-    ENTITY_ONLY_PROMPT,
-)
-from simtag.services.bm25_service import BM25Service
-
-logger = logging.getLogger(__name__)
-
-
-# --- Data Models ---
-@dataclass
-class Entity:
-    name: str
-    type: str
-    description: Optional[str] = None
-
-@dataclass
-class Relationship:
-    source: str
-    target: str
-    type: str
-    description: Optional[str] = None
-    properties: Dict[str, Any] = field(default_factory=dict)
-
-class RAGService:
-    """
-    Provides RAG functionalities by querying the knowledge graph.
-    """
-    def __init__(self, llm_client: LLMClient, graph_service: GraphService, embedding_service: EmbeddingService, config: Config):
-        self.llm_client = llm_client
-        self.graph_service = graph_service
-        self.embedding_service = embedding_service
-        self.config = config
-
-        # Initialize BM25 service (lightweight)
-        try:
-            self.bm25_service = BM25Service(graph_service=self.graph_service, index_path=self.config.bm25_index_path)
-        except Exception as e:
-            logger.warning(f"BM25Service initialization failed: {e}")
-            self.bm25_service = None
-            
-        # Cache for intent analysis results to reduce LLM calls
-        self._intent_cache = {}
-        self._cache_max_size = 100
-        self._cache_ttl = 3600  # 1 hour
-
-    def _parse_llm_json_output(self, llm_output: str) -> Dict[str, List[Any]]:
-        """
-        Parses the JSON output from the LLM. It robustly finds the JSON block
-        and loads it.
-        """
-        # Regex to find a JSON object, allowing for surrounding text or markdown.
-        match = re.search(r'\{.*\}', llm_output, re.DOTALL)
-        if not match:
-            logger.warning("No JSON object found in the LLM output.")
-            return {"entities": [], "relationships": [], "keywords": []}
-
-        json_str = match.group(0)
-        try:
-            data = json.loads(json_str)
-            # Ensure top-level keys exist
-            data.setdefault("entities", [])
-            data.setdefault("relationships", [])
-            data.setdefault("keywords", [])
-            return data
-        except json.JSONDecodeError:
-            logger.error(f"Failed to decode JSON from LLM output: {json_str}")
-            return {"entities": [], "relationships": [], "keywords": []}
-
-    def process_llm_output(self, llm_output: str, source_node_id: str):
-        """
-        Parses the LLM's JSON output, processes it, and adds it to the graph.
-        """
-        # 1. Parse the raw string into a structured dictionary
-        parsed_data = self._parse_llm_json_output(llm_output)
-        
-        # 2. Process entities and relationships from the parsed data
-        entities = self.process_entities(parsed_data.get("entities", []))
-        relationships = self.process_relationships(parsed_data.get("relationships", []))
-
-        # 3. Add to Graph
-        for entity in entities:
-            self.graph_service.upsert_entity({
-                'node_id': entity.name,
-                'type': NodeType.TAG.value,
-                'title': entity.name,
-                'content': entity.description,
-                'properties': {'type': entity.type}
-            })
-
-        for rel in relationships:
-            # 验证关系的必要字段
-            if not rel.source or not rel.target:
-                logger.warning(f"Skipping relationship with missing source or target: {rel}")
-                continue
-            self.graph_service.upsert_relationship(
-                source_id=rel.source,
-                target_id=rel.target,
-                type=rel.type,
-                properties=rel.properties
-            )
-        
-        # Link the source document to the newly created entities
-        for entity in entities:
-            self.graph_service.upsert_relationship(
-                source_id=source_node_id,
-                target_id=entity.name,
-                type="CONTAINS_ENTITY"
-            )
-
-    async def process_and_store_text(self, text_content: str, source_node_id: str):
-        """
-        Processes a given text to extract entities and relationships,
-        and then stores them in the knowledge graph.
-        """
-        try:
-            # 1. Extract Entities and Relationships in one go using the JSON-based prompt
-            # Manually construct the final prompt without using .format on the main template
-            final_prompt = ENTITY_EXTRACTION_PROMPT.format(
-                entity_types=DEFAULT_ENTITY_TYPES,
-                input_text=text_content,
-            )
-            llm_result = await self.llm_client.generate(final_prompt, use_chat_endpoint=False)
-            if not llm_result or not llm_result.content:
-                logger.warning("LLM failed to return content for entity extraction.")
-                return
-
-            self.process_llm_output(llm_result.content, source_node_id)
-
-        except Exception as e:
-            logger.error(f"Error during LLM-based processing for doc {source_node_id}: {e}", exc_info=True)
-
-    def process_entities(self, entity_list: List[Dict[str, Any]]) -> List[Entity]:
-        """Converts a list of entity dictionaries into Entity objects."""
-        if not isinstance(entity_list, list):
-            logger.warning(f"Invalid entity data received for processing: {entity_list}")
-            return []
-        
-        processed_entities = []
-        for item in entity_list:
-            if isinstance(item, dict) and 'name' in item and 'type' in item:
-                processed_entities.append(Entity(
-                    name=item['name'], 
-                    type=item['type'], 
-                    description=item.get('description', '')
-                ))
-            else:
-                logger.warning(f"Skipping malformed entity item: {item}")
-        return processed_entities
-
-    def process_relationships(self, relationship_list: List[Dict[str, Any]]) -> List[Relationship]:
-        """Converts a list of relationship dictionaries into Relationship objects."""
-        if not isinstance(relationship_list, list):
-            logger.warning(f"Invalid relationship data received for processing: {relationship_list}")
-            return []
-            
-        processed_relationships = []
-        for item in relationship_list:
-            if isinstance(item, dict) and 'source' in item and 'target' in item and 'type' in item:
-                # 验证 source 和 target 不为空
-                source = item['source']
-                target = item['target']
-                if not source or not target:
-                    logger.warning(f"Skipping relationship with empty source or target: {item}")
-                    continue
-                processed_relationships.append(Relationship(
-                    source=source,
-                    target=target,
-                    type=item['type'],
-                    description=item.get('description', ''),
-                    properties={'strength': item.get('strength', 0.0)}
-                ))
-            else:
-                logger.warning(f"Skipping malformed relationship item: {item}")
-        return processed_relationships
-
-    async def query(self, query_text: str, history: Optional[list] = None, is_chat: bool = True, command: Optional[str] = None) -> Dict[str, Any]:
-        """
-        Answers a query using the refactored RAG pipeline.
-        The `is_chat` flag determines whether to use the /api/chat endpoint.
-        The `command` parameter indicates if this is a custom command from the frontend.
-        """
-        history = history or []
-        
-        # Check if this is a command-based query and route accordingly
-        command_result = await self._handle_command_query(query_text, history, is_chat, command)
-        if command_result:
-            return command_result
-        
-        # Regular RAG query processing
-        return await self._process_rag_query(query_text, history, is_chat)
-
-    async def _handle_command_query(self, query_text: str, history: Optional[list], is_chat: bool, command: Optional[str] = None) -> Optional[Dict[str, Any]]:
-        """
-        Handle command-based queries with specialized logic.
-        Returns None if this is not a command query.
-        """
-        # If command is provided from frontend, use it directly
-        if command:
-            logger.info(f"Custom command detected from frontend: {command}")
-            return await self._execute_custom_command(command, query_text, history, is_chat)
-        
-        # Detect command patterns for built-in commands
-        command_patterns = {
-            'create-question': [
-                r'please list all important questions related to (.+)',
-                r'create questions for (.+)',
-                r'generate questions about (.+)'
-            ],
-            'analyze': [
-                r'analyze (.+)',
-                r'explain (.+)',
-                r'describe (.+)'
-            ],
-            'compare': [
-                r'compare (.+) and (.+)',
-                r'difference between (.+) and (.+)'
-            ]
-        }
-        
-        import re
-        for command_type, patterns in command_patterns.items():
-            for pattern in patterns:
-                match = re.search(pattern, query_text, re.IGNORECASE)
-                if match:
-                    logger.info(f"Built-in command detected: {command_type} with topic: {match.groups()}")
-                    return await self._execute_command(command_type, match.groups(), query_text, history, is_chat)
-        
-        return None
-
-    async def _execute_command(self, command_type: str, topics: tuple, original_query: str, history: Optional[list], is_chat: bool) -> Dict[str, Any]:
-        """
-        Execute a specific command with its own logic.
-        """
-        if command_type == 'create-question':
-            return await self._execute_create_question_command(topics[0], original_query, history, is_chat)
-        elif command_type == 'analyze':
-            return await self._execute_analyze_command(topics[0], original_query, history, is_chat)
-        elif command_type == 'compare':
-            return await self._execute_compare_command(topics[0], topics[1], original_query, history, is_chat)
-        
-        # Fallback to regular RAG processing
-        return None
-
-    async def _execute_create_question_command(self, topic: str, original_query: str, history: Optional[list], is_chat: bool) -> Dict[str, Any]:
-        """
-        Execute create-question command with specialized prompt.
-        """
-        # Create a specialized prompt for question generation
-        system_prompt = """You are an expert at generating insightful questions about various topics. 
-Your task is to create a comprehensive list of important questions about the given topic.
-Focus on questions that:
-1. Help understand the core concepts
-2. Explore practical applications
-3. Consider different perspectives
-4. Address common challenges or misconceptions
-5. Lead to deeper learning
-
-Provide clear, well-structured questions that would be valuable for someone learning about this topic.
-
-**IMPORTANT: Always format your response using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), and other Markdown formatting as appropriate."""
-
-        user_prompt = f"Generate a comprehensive list of important questions about: {topic}"
-
-        messages = [
-            {"role": "system", "content": system_prompt},
-            {"role": "user", "content": user_prompt}
-        ]
-
-        if is_chat:
-            chat_res = await self.llm_client.chat(messages=messages, format_json=False)
-            if chat_res.success and chat_res.content:
-                return {
-                    "answer": chat_res.content,
-                    "source_nodes": [],
-                    "note": f"Generated questions about {topic} using LLM knowledge"
-                }
-        else:
-            llm_result = await self.llm_client.generate(user_prompt, use_chat_endpoint=False)
-            if llm_result and llm_result.content:
-                return {
-                    "answer": llm_result.content,
-                    "source_nodes": [],
-                    "note": f"Generated questions about {topic} using LLM knowledge"
-                }
-        
-        return {"error": "Failed to generate questions"}
-
-    async def _execute_analyze_command(self, topic: str, original_query: str, history: Optional[list], is_chat: bool) -> Dict[str, Any]:
-        """
-        Execute analyze command with specialized prompt.
-        """
-        system_prompt = """You are an expert analyst who provides comprehensive analysis of various topics.
-Your analysis should be:
-1. Well-structured and logical
-2. Based on current knowledge and best practices
-3. Practical and actionable
-4. Balanced in perspective
-5. Clear and accessible
-
-Provide a thorough analysis that helps the user understand the topic deeply.
-
-**IMPORTANT: Always format your response using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), code blocks (```), and other Markdown formatting as appropriate."""
-
-        user_prompt = f"Provide a comprehensive analysis of: {topic}"
-
-        messages = [
-            {"role": "system", "content": system_prompt},
-            {"role": "user", "content": user_prompt}
-        ]
-
-        if is_chat:
-            chat_res = await self.llm_client.chat(messages=messages, format_json=False)
-            if chat_res.success and chat_res.content:
-                return {
-                    "answer": chat_res.content,
-                    "source_nodes": [],
-                    "note": f"Analysis of {topic} using LLM knowledge"
-                }
-        else:
-            llm_result = await self.llm_client.generate(user_prompt, use_chat_endpoint=False)
-            if llm_result and llm_result.content:
-                return {
-                    "answer": llm_result.content,
-                    "source_nodes": [],
-                    "note": f"Analysis of {topic} using LLM knowledge"
-                }
-        
-        return {"error": "Failed to analyze topic"}
-
-    async def _execute_compare_command(self, topic1: str, topic2: str, original_query: str, history: Optional[list], is_chat: bool) -> Dict[str, Any]:
-        """
-        Execute compare command with specialized prompt.
-        """
-        system_prompt = """You are an expert at comparing and contrasting different topics, technologies, or concepts.
-Your comparison should be:
-1. Structured and systematic
-2. Fair and balanced
-3. Focused on key differences and similarities
-4. Practical and actionable
-5. Based on current knowledge and best practices
-
-Provide a comprehensive comparison that helps users make informed decisions.
-
-**IMPORTANT: Always format your response using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), tables, and other Markdown formatting as appropriate."""
-
-        user_prompt = f"Compare and contrast: {topic1} vs {topic2}"
-
-        messages = [
-            {"role": "system", "content": system_prompt},
-            {"role": "user", "content": user_prompt}
-        ]
-
-        if is_chat:
-            chat_res = await self.llm_client.chat(messages=messages, format_json=False)
-            if chat_res.success and chat_res.content:
-                return {
-                    "answer": chat_res.content,
-                    "source_nodes": [],
-                    "note": f"Comparison of {topic1} vs {topic2} using LLM knowledge"
-                }
-        else:
-            llm_result = await self.llm_client.generate(user_prompt, use_chat_endpoint=False)
-            if llm_result and llm_result.content:
-                return {
-                    "answer": llm_result.content,
-                    "source_nodes": [],
-                    "note": f"Comparison of {topic1} vs {topic2} using LLM knowledge"
-                }
-        
-        return {"error": "Failed to compare topics"}
-
-    async def _execute_custom_command(self, command_name: str, query_text: str, history: Optional[list], is_chat: bool) -> Dict[str, Any]:
-        """
-        Execute a custom command from the frontend.
-        The query_text here is the final processed text (with $input replaced).
-        This method combines RAG retrieval with LLM knowledge for the best results.
-        """
-        logger.info(f"Executing custom command: {command_name} with query: {query_text}")
-        
-        # First, try to get relevant context from RAG
-        context_result = await self._get_context_for_custom_command(query_text)
-        context_text = context_result['context']
-        source_nodes = context_result['source_nodes']
-        
-        # Create a specialized prompt for custom commands that can use both context and LLM knowledge
-        system_prompt = f"""You are an AI assistant executing a custom command called "{command_name}".
-This command was defined by the user with a specific purpose and prompt template.
-
-Your task is to:
-1. Understand the intent of the custom command "{command_name}"
-2. Provide a response that fulfills the command's purpose
-3. Use both the provided context (if any) and your own knowledge
-4. If the context contains relevant information, incorporate it into your response
-5. If the context is insufficient or irrelevant, rely on your knowledge
-6. Be helpful, accurate, and relevant to the user's request
-7. **IMPORTANT: Always format your response using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), code blocks (```), and other Markdown formatting as appropriate
-
-The user's input has been processed according to the command's template and is: {query_text}"""
-
-        # Prepare the user prompt with context if available
-        if context_text and "No relevant local documents found" not in context_text:
-            user_prompt = f"""Context from local documents:
-{context_text}
-
-User request: {query_text}
-
-Please provide a comprehensive response that combines the context above with your knowledge."""
-        else:
-            user_prompt = query_text
-
-        messages = [
-            {"role": "system", "content": system_prompt},
-            {"role": "user", "content": user_prompt}
-        ]
-
-        if is_chat:
-            chat_res = await self.llm_client.chat(messages=messages, format_json=False)
-            if chat_res.success and chat_res.content:
-                return {
-                    "answer": chat_res.content,
-                    "source_nodes": source_nodes,
-                    "note": f"Executed custom command '{command_name}' with RAG + LLM knowledge"
-                }
-        else:
-            llm_result = await self.llm_client.generate(user_prompt, use_chat_endpoint=False)
-            if llm_result and llm_result.content:
-                return {
-                    "answer": llm_result.content,
-                    "source_nodes": source_nodes,
-                    "note": f"Executed custom command '{command_name}' with RAG + LLM knowledge"
-                }
-        
-        return {"error": f"Failed to execute custom command '{command_name}'"}
-
-    async def _get_context_for_custom_command(self, query_text: str) -> Dict[str, Any]:
-        """
-        Get relevant context for custom commands using RAG retrieval.
-        This is a simplified version that focuses on getting the most relevant documents.
-        Returns both context text and source nodes.
-        """
-        try:
-            # Extract keywords from the query for better retrieval
-            import re
-            words = re.findall(r'\b\w{4,}\b', query_text.lower())
-            entities_from_query = list(set(words))
-            
-            # Filter out common stop words
-            stop_words = {'what', 'how', 'when', 'where', 'why', 'who', 'which', 'this', 'that', 'these', 'those', 'and', 'or', 'but', 'with', 'for', 'from', 'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must', 'please', 'list', 'all', 'important', 'questions', 'related', 'analyze', 'explain', 'describe', 'compare', 'difference', 'between'}
-            entities_from_query = [word for word in entities_from_query if word not in stop_words]
-            
-            logger.info(f"Custom command context search - Query: '{query_text}', Keywords: {entities_from_query}")
-            
-            # Try vector search first
-            query_embedding_result = await self.embedding_service.get_embedding(query_text)
-            if not query_embedding_result.success:
-                logger.warning("Embedding failed for custom command context search")
-                return {
-                    "context": "--- Context from Notes ---\nNo relevant local documents found for this query.",
-                    "source_nodes": []
-                }
-            
-            # Get similar nodes
-            top_k = getattr(self.config, 'rag_search_top_k', 5)  # Use fewer results for custom commands
-            similar_nodes = self.graph_service.find_similar_nodes(query_embedding_result.embedding, top_k=top_k)
-            
-            # Also try entity-based search
-            similar_entities = []
-            if entities_from_query:
-                entity_query_text = " ".join(entities_from_query)
-                entity_embedding_result = await self.embedding_service.get_embedding(entity_query_text)
-                if entity_embedding_result.success:
-                    similar_entities = self.graph_service.find_similar_entities(entity_embedding_result.embedding, top_k=top_k)
-            
-            # Combine results
-            all_nodes = []
-            seen_ids = set()
-            
-            for node in similar_nodes:
-                if node['node_id'] not in seen_ids:
-                    all_nodes.append(node)
-                    seen_ids.add(node['node_id'])
-            
-            for entity in similar_entities:
-                if entity['node_id'] not in seen_ids:
-                    all_nodes.append(entity)
-                    seen_ids.add(entity['node_id'])
-            
-            # If no vector results, try text search
-            if not all_nodes:
-                text_results = self.graph_service.search_nodes_by_title_content(query_text, limit=top_k)
-                if text_results:
-                    all_nodes.extend(text_results)
-                else:
-                    # Try searching with extracted keywords
-                    for ent in entities_from_query:
-                        if len(ent) >= 3:
-                            ent_results = self.graph_service.search_nodes_by_title_content(ent, limit=2)
-                            if ent_results:
-                                all_nodes.extend(ent_results)
-                            if len(all_nodes) >= top_k:
-                                break
-            
-            if not all_nodes:
-                return {
-                    "context": "--- Context from Notes ---\nNo relevant local documents found for this query.",
-                    "source_nodes": []
-                }
-            
-            # Prepare source nodes for frontend
-            source_nodes = []
-            for node in all_nodes[:top_k]:
-                source_nodes.append({
-                    "id": node['node_id'],
-                    "title": node.get('title', node.get('name', '')),
-                    "snippet": ((node.get('content') or '') if node.get('content') else (node.get('title', node.get('name',''))) )[:200]
-                })
-            
-            # Synthesize context
-            context_parts = ["--- Context from Notes ---"]
-            for node in all_nodes[:top_k]:  # Limit to top_k results
-                title = node.get('title', node.get('name', ''))
-                content = node.get('content', '')
-                if not content and title:
-                    content = title
-                snippet = (content or '')[:300]  # Longer snippets for custom commands
-                context_parts.append(f"- {title}\n  {snippet}")
-            
-            context_text = "\n".join(context_parts)
-            logger.info(f"Custom command found {len(all_nodes)} relevant documents")
-            return {
-                "context": context_text,
-                "source_nodes": source_nodes
-            }
-            
-        except Exception as e:
-            logger.error(f"Error getting context for custom command: {e}")
-            return {
-                "context": "--- Context from Notes ---\nError retrieving local documents.",
-                "source_nodes": []
-            }
-
-    async def _process_rag_query(self, query_text: str, history: Optional[list] = None, is_chat: bool = True) -> Dict[str, Any]:
-        """
-        Process regular RAG queries (non-command queries).
-        """
-        # 1. Smart keyword extraction without LLM call
-        # Skip expensive LLM analysis for simple queries
-        entities_from_query = []
-        
-        # Basic keyword extraction from query text
-        import re
-        # Extract potential keywords (words longer than 3 characters)
-        words = re.findall(r'\b\w{4,}\b', query_text.lower())
-        entities_from_query = list(set(words))  # Remove duplicates
-        
-        # Filter out common stop words
-        stop_words = {'what', 'how', 'when', 'where', 'why', 'who', 'which', 'this', 'that', 'these', 'those', 'and', 'or', 'but', 'with', 'for', 'from', 'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must'}
-        entities_from_query = [word for word in entities_from_query if word not in stop_words]
-
-        # Add debug logging for keyword extraction
-        logger.info(f"RAG Query Debug - Original query: '{query_text}'")
-        logger.info(f"RAG Query Debug - Extracted keywords: {entities_from_query}")
-
-        # 2. Embed the query AND the extracted entities
-        query_embedding_result = await self.embedding_service.get_embedding(query_text)
-        if not query_embedding_result.success:
-            logger.warning("Embedding failed; falling back to direct LLM answer without RAG context.")
-            llm_fallback = await self.llm_client.generate(query_text, use_chat_endpoint=True)
-            if llm_fallback.success:
-                return {
-                    "answer": llm_fallback.content,
-                    "source_nodes": [],
-                    "note": "Answer generated without vector search due to embedding failure"
-                }
-            else:
-                return {"error": llm_fallback.error_message or "LLM fallback failed"}
-
-        # 3. Multi-pronged search
-        top_k = getattr(self.config, 'rag_search_top_k', 10)  # 使用默认值
-        # a) Vector search on the query
-        similar_nodes = self.graph_service.find_similar_nodes(query_embedding_result.embedding, top_k=top_k)
-        logger.info(f"RAG Query Debug - Vector search results count: {len(similar_nodes)}")
-        
-        # b) Vector search on extracted entities
-        similar_entities = []
-        if entities_from_query:
-            entity_query_text = " ".join(entities_from_query)
-            entity_embedding_result = await self.embedding_service.get_embedding(entity_query_text)
-            if entity_embedding_result.success:
-                similar_entities = self.graph_service.find_similar_entities(entity_embedding_result.embedding, top_k=top_k)
-                logger.info(f"RAG Query Debug - Entity search results count: {len(similar_entities)}")
-
-        # Combine initial results to get starting points for traversal
-        initial_node_ids = {node['node_id'] for node in similar_nodes}
-        initial_node_ids.update(entity['node_id'] for entity in similar_entities)
-        
-        if not initial_node_ids:
-            # Text-search fallback when vector search yields nothing.
-            # 1) try query string
-            context_nodes = []  # ensure variable exists
-            text_results = self.graph_service.search_nodes_by_title_content(query_text, limit=top_k)
-            if text_results:
-                initial_node_ids.update(node['node_id'] for node in text_results)
-                context_nodes.extend(text_results)
-                logger.info(f"RAG Query Debug - Text search results count: {len(text_results)}")
-            # 2) try each extracted entity separately to capture keyword hits
-            if (not initial_node_ids) and entities_from_query:
-                for ent in entities_from_query:
-                    # Skip very short tokens to avoid noise
-                    if len(ent) < 3:
-                        continue
-                    ent_results = self.graph_service.search_nodes_by_title_content(ent, limit=3)
-                    if ent_results:
-                        initial_node_ids.update(n['node_id'] for n in ent_results)
-                        # ensure list exists
-                        if context_nodes is None:
-                            context_nodes = []
-                        context_nodes.extend(ent_results)
-                        logger.info(f"RAG Query Debug - Entity '{ent}' text search results count: {len(ent_results)}")
-                    if len(initial_node_ids) >= top_k:
-                        break
-         
-        if not initial_node_ids:
-            logger.info(f"RAG Query Debug - No initial nodes found, falling back to chat")
-            return await self._chat_fallback(history, query_text)
-        
-        # 4. Traversal: Expand from initial nodes to build a rich context
-        context_nodes = []
-        max_hops = getattr(self.config, 'rag_traversal_max_hops', 2)  
-        
-        for node_id in initial_node_ids:
-            # get nodes
-            node = self.graph_service.get_node_by_id(node_id)
-            if node:
-                context_nodes.append(node)
-                # get neighbors
-                neighbors = self.graph_service.get_neighbors(node_id)
-                context_nodes.extend(neighbors[:5])  # limit neighbors to 5
-        
-        logger.debug(f"Context nodes count: {len(context_nodes)}, ids: {[n['node_id'] for n in context_nodes]}")
-
-        # 5. Synthesize context from the final node list
-        context_text = self._synthesize_context_from_nodes(context_nodes)
-
-        # 6. Generate a response
-        if is_chat:
-            # Use the centralized prompt from prompts.py
-            prompt = prompts.MINIRAG_RAG_RESPONSE_PROMPT.format(context=context_text, query=query_text)
-            # For chat models, the prompt becomes the user message, and we can have a simpler system message.
-            messages = [
-                {"role": "system", "content": "You are a helpful AI assistant. **IMPORTANT: Always format your responses using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), code blocks (```), and other Markdown formatting as appropriate."},
-                {"role": "user", "content": prompt}
-            ]
-
-            chat_res = await self.llm_client.chat(messages=messages, format_json=False)
-            if chat_res.success and chat_res.content:
-                return {
-                    "answer": chat_res.content,
-                    "source_nodes": [
-                        {"id": n['node_id'],
-                         "title": n.get('title', n.get('name', '')),
-                         "snippet": ((n.get('content') or '') if n.get('content') else (n.get('title', n.get('name',''))) )[:200]}
-                        for n in context_nodes]
-                }
-            # If chat failed, fallback
-            return await self._chat_fallback(history, query_text)
-        else:
-            prompt = QA_PROMPT.format(context=context_text, question=query_text)
-            llm_result = await self.llm_client.generate(prompt, use_chat_endpoint=False)
-            if llm_result and llm_result.content:
-                return {
-                    "answer": llm_result.content,
-                    "source_nodes": [
-                        {"id": n['node_id'],
-                         "title": n.get('title', n.get('name', '')),
-                         "snippet": ((n.get('content') or '') if n.get('content') else (n.get('title', n.get('name',''))) )[:200]}
-                        for n in context_nodes]
-                }
-            return await self._chat_fallback(history, query_text)
-
-    async def _chat_fallback(self, history: list, query_text: str) -> Dict[str, Any]:
-        """Fallback to pure chat with provided history when RAG retrieval fails."""
-        messages = []
-        for item in reversed(history):
-            # 支持多种格式的历史项：dict、plist 转 dict、字符串
-            if isinstance(item, (list, tuple)):
-                try:
-                    item = dict(item)
-                except Exception:
-                    pass
-            if isinstance(item, dict):
-                role = item.get('role') or item.get(':role')
-                content = item.get('content') or item.get(':content')
-                if role and content:
-                    messages.append({"role": str(role), "content": str(content)})
-            elif isinstance(item, str):
-                # 若无法得知角色，默认视为 user 历史输入
-                if item.strip():
-                    messages.append({"role": "user", "content": item.strip()})
-            else:
-                # 其他类型忽略或记录
-                logger.debug(f"Skipped unsupported history item type: {type(item)}")
-        
-        # Add system message to ensure Markdown formatting
-        system_message = {
-            "role": "system", 
-            "content": "You are a helpful AI assistant. **IMPORTANT: Always format your responses using Markdown syntax** - use headers (# ##), lists (- *), bold (**text**), italic (*text*), code blocks (```), and other Markdown formatting as appropriate."
-        }
-        messages.insert(0, system_message)
-        messages.append({"role": "user", "content": query_text})
-
-        chat_result = await self.llm_client.chat(messages=messages, use_chat_endpoint=True)
-        if chat_result.success:
-            return {"answer": chat_result.content, "source_nodes": []}
-        return {"answer": "I could not find any relevant information."}
-
-    def _synthesize_context_from_nodes(self, nodes: List[Dict]) -> str:
-        """Creates a text context from a list of nodes."""
-        context_parts = []
-        seen_ids = set()
-        context_parts.append("--- Context from Notes ---")
-        for node in nodes:
-            node_id = node.get('node_id')
-            if node_id in seen_ids:
-                continue
-            seen_ids.add(node_id)
-            title = node.get('title', node.get('name', ''))
-            content = node.get('content', '')
-            if not content and title:
-                content = title
-            snippet = (content or '')[:200]
-            context_parts.append(f"- {title}\n  {snippet}")
-        return "\n".join(context_parts)
-
-    # ==============================================================================
-    # Phase 2: High-Precision RAG/Query Methods (inspired by MiniRAG)
-    # ==============================================================================
-
-    async def _analyze_intent_and_get_entities(self, query_text: str, is_chat: bool = False) -> Dict[str, Any]:
-        """
-        Analyzes the user's query to understand intent and extract key entities.
-        Implements caching to reduce duplicate LLM calls while preserving intent analysis.
-        """
-        logger.debug(f"Analyzing intent for query: '{query_text}'")
-        
-        # Check cache first
-        cache_key = f"{query_text}_{is_chat}"
-        current_time = time.time()
-        
-        # Clean expired cache entries
-        expired_keys = [
-            key for key, (timestamp, _) in self._intent_cache.items()
-            if current_time - timestamp > self._cache_ttl
-        ]
-        for key in expired_keys:
-            del self._intent_cache[key]
-        
-        # Check if we have a cached result
-        if cache_key in self._intent_cache:
-            cached_timestamp, cached_result = self._intent_cache[cache_key]
-            if current_time - cached_timestamp <= self._cache_ttl:
-                logger.debug(f"Using cached intent analysis for query: '{query_text[:50]}...'")
-                return cached_result
-        
-        try:
-            # For the answer type pool, we can use the defined NodeType enums
-            # In a more advanced system, this could be dynamic.
-            answer_type_pool = {nt.value: [nt.value] for nt in NodeType}
-
-            prompt = prompts.MINIRAG_QUERY_TO_KEYWORDS_PROMPT.format(
-                query=query_text,
-                answer_type_pool=json.dumps(answer_type_pool, indent=2)
-            )
-
-            llm_result = await self.llm_client.generate(prompt, use_chat_endpoint=is_chat)
-            response_json_str = llm_result.content
-            
-            # The LLM might return a markdown code block, so we need to clean it
-            if response_json_str.strip().startswith("```json"):
-                response_json_str = response_json_str.strip()[7:-3].strip()
-
-            intent_data = json.loads(response_json_str)
-            logger.info(f"Intent analysis complete: {intent_data}")
-            
-            # Cache the result
-            if len(self._intent_cache) >= self._cache_max_size:
-                # Remove oldest entry when cache is full
-                oldest_key = min(self._intent_cache.keys(), key=lambda k: self._intent_cache[k][0])
-                del self._intent_cache[oldest_key]
-            
-            self._intent_cache[cache_key] = (current_time, intent_data)
-            
-            return intent_data
-
-        except Exception as e:
-            logger.error(f"Failed to analyze query intent: {e}", exc_info=True)
-            # Fallback to a simple keyword extraction if intent analysis fails
-            fallback_result = {
-                "answer_type_keywords": [],
-                "entities_from_query": query_text.split() 
-            }
-            
-            # Cache the fallback result too to avoid repeated failures
-            if len(self._intent_cache) < self._cache_max_size:
-                self._intent_cache[cache_key] = (current_time, fallback_result)
-            
-            return fallback_result
-
-    async def _search_and_score_paths(
-        self,
-        query_text: str,
-        entities_from_query: List[str],
-        answer_type_keywords: List[str]
-    ) -> Dict[str, Any]:
-        """Hybrid vector + graph retrieval. Returns subgraph with scored nodes."""
-        cfg = self.config
-        neighbor_depth = getattr(cfg, 'rag_neighbor_depth', 2)
-        ratio = getattr(cfg, 'rag_vector_graph_ratio', 0.5)
-        top_k_total = getattr(cfg, 'rag_vector_results', 10)
-        vec_k = max(1, int(top_k_total * ratio))
-
-        bm25_k = getattr(cfg, 'rag_bm25_top_k', 10)
-        bm25_weight = getattr(cfg, 'rag_bm25_weight', 0.5)
-
-        logger.debug(
-            f"Hybrid search: query='{query_text[:60]}…', entities={entities_from_query}, vec_k={vec_k}, depth={neighbor_depth}")
-
-        # 1. Build start nodes from explicit entities
-        start_node_ids: List[str] = []
-        for ent in entities_from_query:
-            node_d = self.graph_service.get_node_by_id(ent)
-            if node_d:
-                start_node_ids.append(node_d['node_id'])
-
-        # 2. Vector search to get similar TAG/ENTITY nodes
-        vec_node_map: Dict[str, Dict] = {}
-        try:
-            emb_res: EmbeddingResult = await self.embedding_service.get_embedding(query_text)
-            if emb_res.success:
-                similar_entities = self.graph_service.find_similar_entities(emb_res.embedding, top_k=vec_k)
-                for idx, ent in enumerate(similar_entities):
-                    ent_id = ent['node_id']
-                    ent['distance_rank'] = idx  # keep rank
-                    vec_node_map[ent_id] = ent
-                    start_node_ids.append(ent_id)  # treat similar tags as start nodes too
-        except Exception as e:
-            logger.warning(f"Vector search failed: {e}")
-
-        # 2.5 BM25 keyword search for additional candidate nodes
-        bm25_node_map: Dict[str, Dict] = {}
-        max_bm25_raw = 0.0
-        if self.bm25_service:
-            try:
-                bm25_results = self.bm25_service.query(query_text, top_k=bm25_k)
-                for node_id, raw_score in bm25_results:
-                    max_bm25_raw = max(max_bm25_raw, raw_score)
-                    node_d = self.graph_service.get_node_by_id(node_id)
-                    if node_d:
-                        bm25_node_map[node_id] = {**node_d, "bm25_raw": raw_score}
-                        start_node_ids.append(node_id)
-            except Exception as e:
-                logger.warning(f"BM25 retrieval failed: {e}")
-
-        # Deduplicate start ids
-        start_node_ids = list(dict.fromkeys(start_node_ids))
-        if not start_node_ids:
-            logger.warning("No start nodes found, returning empty subgraph")
-            return {"nodes": {}, "relations": {}}
-
-        # 3. Graph expansion (BFS)
-        graph_nodes = self._bfs_expand_neighbors(start_node_ids, max_depth=neighbor_depth)
-
-        # Merge vector nodes into graph_nodes (may add extra metadata)
-        for nid, ndata in vec_node_map.items():
-            if nid not in graph_nodes:
-                graph_nodes[nid] = ndata
-            else:
-                graph_nodes[nid].update(ndata)
-
-        # Merge bm25 nodes
-        for nid, ndata in bm25_node_map.items():
-            # Normalize bm25 score now that max is known
-            raw = ndata.get("bm25_raw", 0.0)
-            if max_bm25_raw > 0:
-                ndata["bm25_score"] = raw / max_bm25_raw
-            else:
-                ndata["bm25_score"] = 0.0
-            if nid not in graph_nodes:
-                graph_nodes[nid] = ndata
-            else:
-                graph_nodes[nid].update(ndata)
-
-        # 4. Score nodes (vector + bm25 + heuristics)
-        scored_nodes: Dict[str, Dict] = {}
-        def _score(node: Dict) -> float:
-            score = 0.0
-            # Vector similarity: 1 - distance (if present)
-            dist = node.get('distance')
-            if dist is not None:
-                score += max(0.0, 1.0 - float(dist)) * 2.0
-            # BM25 score (already normalized 0~1)
-            bm25_s = node.get('bm25_score')
-            if bm25_s is not None:
-                score += bm25_s * bm25_weight
-            # Start nodes boost
-            if node['node_id'] in start_node_ids:
-                score += 0.5
-            # Type keyword boost
-            if answer_type_keywords and node.get('type') in answer_type_keywords:
-                score += 0.5
-            return score
-
-        for nid, node in graph_nodes.items():
-            node['relevance_score'] = _score(node)
-            scored_nodes[nid] = node
-
-        # TODO: relations scoring (future)
-        subgraph = {"nodes": scored_nodes, "relations": {}}
-        logger.info(f"Hybrid search complete. Nodes: {len(scored_nodes)}")
-        return subgraph
-
-    def _build_context_from_subgraph(self, subgraph: Dict[str, Any], top_k: int = 5):
-        """
-        根据打分后的子图构建用于 LLM 的上下文文本，同时生成前端需要的 source_nodes 列表。
-
-        Args:
-            subgraph: 包含节点与关系的字典，节点已带有 `relevance_score` 字段。
-            top_k: 需纳入上下文的最高得分节点数量。
-
-        Returns:
-            Tuple[str, List[Dict]]: (context_str, source_nodes)
-        """
-        nodes = list(subgraph.get("nodes", {}).values())
-        relations = list(subgraph.get("relations", {}).values())
-
-        if not nodes:
-            return "No relevant information found in the knowledge graph.", []
-
-        # 1. 依据相关度排序
-        nodes.sort(key=lambda x: x.get('relevance_score', 0.0), reverse=True)
-
-        # 2. 取前 top_k，并按标题去重（忽略大小写）
-        seen_titles: set = set()
-        top_nodes: list = []
-        for node in nodes:
-            title = (node.get('title') or node.get('name') or '').strip()
-            if not title:
-                continue
-            title_lc = title.lower()
-            if title_lc in seen_titles:
-                continue
-            seen_titles.add(title_lc)
-            top_nodes.append(node)
-            if len(top_nodes) >= top_k:
-                break
-
-        top_node_ids = {n['node_id'] for n in top_nodes}
-
-        # 3. 构建上下文文本
-        context_parts: list = ["--- Relevant Concepts and Entities ---"]
-        source_nodes: list = []
-        snippet_len = getattr(self.config, 'rag_snippet_len', 200)
-
-        for node in top_nodes:
-            title = node.get('title') or node.get('name') or 'Untitled Node'
-            score = node.get('relevance_score', 0.0)
-            snippet = self._get_snippet(node.get('content') or title, limit=snippet_len)
-
-            context_parts.append(f"- {title} (ID: {node['node_id']}, Type: {node.get('type')}, Score: {score:.2f})")
-            if snippet:
-                context_parts.append(f"  - Snippet: {snippet}")
-
-            source_nodes.append({
-                "id": node['node_id'],
-                "title": title,
-                "snippet": snippet
-            })
-
-        # 4. 关系信息
-        context_parts.append("\n--- Identified Relationships ---")
-        included_relations_count = 0
-        for rel in relations:
-            if rel.get('source_id') in top_node_ids and rel.get('target_id') in top_node_ids:
-                source_title = subgraph['nodes'].get(rel['source_id'], {}).get('title', 'Unknown')
-                target_title = subgraph['nodes'].get(rel['target_id'], {}).get('title', 'Unknown')
-                rel_type = rel.get('type', 'RELATED_TO')
-                context_parts.append(f"- ({source_title}) --[{rel_type}]--> ({target_title})")
-                included_relations_count += 1
-
-        if included_relations_count == 0:
-            context_parts.append("No direct relationships found among the top entities.")
-
-        context_str = "\n".join(context_parts)
-        return context_str, source_nodes
-
-    # ==============================================================================
-    # Phase 3: High-Quality Data Ingestion & Autotagging
-    # ==============================================================================
-
-    
-
-
-
-    
-
-    async def infer_relations(self, text: str, entities: List[str]) -> List[Relationship]:
-        """Infer relationships between provided entities within the given text."""
-        if not entities:
-            return []
-        try:
-            ent_list = ", ".join(entities)
-            prompt = RELATION_INFERENCE_PROMPT.format(entities=ent_list, input_text=text)
-            llm_res = await self.llm_client.generate(prompt, use_chat_endpoint=False)
-            if not llm_res or not llm_res.content:
-                logger.warning("LLM returned empty result for relation inference.")
-                return []
-            parsed = self._parse_llm_json_output_for_relations(llm_res.content)
-            return parsed
-        except Exception as e:
-            logger.error(f"infer_relations failed: {e}", exc_info=True)
-            return []
-
-    def _parse_llm_json_output_for_relations(self, llm_output: str) -> List[Relationship]:
-        """Parse JSON object with relationships key from LLM."""
-        try:
-            match = re.search(r'\{.*\}', llm_output, re.DOTALL)
-            if not match:
-                return []
-            data = json.loads(match.group(0))
-            rels = data.get("relationships", [])
-            parsed = []
-            for item in rels:
-                if all(k in item for k in ("source", "target", "type")):
-                    parsed.append(Relationship(source=item["source"], target=item["target"], type=item["type"]))
-            return parsed
-        except Exception as e:
-            logger.error("Failed to parse relation JSON: %s", e)
-            return []
-
-
-
-    # --- Utility Helpers ---
-    def _get_snippet(self, text: str, limit: int = 200) -> str:
-        """Return a clean snippet up to <limit> chars, ending at nearest sentence boundary."""
-        if not text:
-            return ""
-        text = text.strip().replace("\n", " ")
-        if len(text) <= limit:
-            return text
-        cut = text[:limit]
-        # Try to cut at last period
-        last_period = cut.rfind('.')
-        if last_period > limit * 0.5:
-            return cut[:last_period+1]
-        # Else cut at space
-        last_space = cut.rfind(' ')
-        if last_space > 0:
-            return cut[:last_space] + '…'
-        return cut + '…'
-
-    def _bfs_expand_neighbors(self, start_ids: List[str], max_depth: int = 2, relation_whitelist: Optional[List[str]] = None, per_node_limit: int = 5) -> Dict[str, Dict]:
-        """Breadth-first traverse neighbors up to depth, return node dicts."""
-        relation_whitelist = relation_whitelist or ["HAS_TAG", "REF_TO"]
-        visited = set(start_ids)
-        queue = deque([(nid, 0) for nid in start_ids])
-        nodes: Dict[str, Dict] = {}
-        while queue:
-            node_id, depth = queue.popleft()
-            node = self.graph_service.get_node_by_id(node_id)
-            if node:
-                nodes[node_id] = node
-            if depth >= max_depth:
-                continue
-            neighbors = self.graph_service.get_neighbors(node_id)
-            cnt = 0
-            for nb in neighbors:
-                if cnt >= per_node_limit:
-                    break
-                nb_id = nb.get('node_id')
-                rel_type = None  # placeholder, GraphService doesn't return relation type here
-                if nb_id and nb_id not in visited and (not relation_whitelist or rel_type in relation_whitelist):
-                    visited.add(nb_id)
-                    queue.append((nb_id, depth+1))
-                    cnt += 1
-        return nodes
diff --git a/simtag/services/smart_ner_service.py b/simtag/services/smart_ner_service.py
new file mode 100755
index 0000000..17f4ec6
--- /dev/null
+++ b/simtag/services/smart_ner_service.py
@@ -0,0 +1,463 @@
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import asyncio
+import logging
+import time
+from concurrent.futures import ThreadPoolExecutor
+from typing import Dict, List, Any, Optional, Callable, Awaitable, Tuple
+from dataclasses import dataclass
+
+from simtag.prompts import create_prompt, ORG_MODE_TAG_EXTRACTION_PROMPT, DEFAULT_ENTITY_TYPES
+from simtag.utils.unified_tag_processor import UnifiedTagProcessor, TagResult, NoteResult
+
+logger = logging.getLogger(__name__)
+
+@dataclass
+class BatchProcessingResult:
+    """批量处理结果"""
+    generated_count: int
+    failed_count: int
+    failed_nodes: List[str]
+    processing_time: float
+    suggestions: List[Dict[str, Any]]
+
+@dataclass
+class NodeProcessingTask:
+    """节点处理任务"""
+    node_id: str
+    title: str
+    content: str
+    file_path: str
+    level: int
+    existing_tags: List[str]
+    retry_count: int = 0
+
+class SmartNERService:
+    """
+    A specialized service for high-quality Named Entity Recognition (NER)
+    tailored for Org-mode auto-tagging.
+
+    This service uses a dedicated, fine-tuned prompt to extract relevant
+    nouns (people, places, concepts) and formats them according to
+    Org-mode's conventions (e.g., using underscores).
+    
+    Features:
+    - Single node processing
+    - Batch processing with multi-threading
+    - Two-stage confidence evaluation
+    - Progress tracking and error handling
+    """
+
+    def __init__(self,
+                 llm_async_callable: Callable[[str], Awaitable[str]],
+                 config: Optional[Dict[str, Any]] = None):
+        """
+        Initializes the SmartNERService.
+
+        Args:
+            llm_async_callable: An asynchronous function that takes a prompt
+                                string and returns the LLM completion string.
+            config: Optional configuration dictionary.
+        """
+        self.llm_async_callable = llm_async_callable
+        self.config = config if config else {}
+        self.entity_types = self.config.get('entity_types', DEFAULT_ENTITY_TYPES)
+        self.unified_processor = UnifiedTagProcessor()
+        
+        # 批量处理配置
+        self.max_workers = self.config.get('max_workers', 2)  # 2线程
+        self.max_retries = self.config.get('max_retries', 3)  # 重试3次
+        self.confidence_threshold_first = self.config.get('confidence_threshold_first', 0.3)  # 第一轮阈值
+        self.confidence_threshold_final = self.config.get('confidence_threshold_final', 0.6)  # 最终阈值
+        self.batch_size = self.config.get('batch_size', 10)  # 批处理大小
+        
+        # 进度回调
+        self.progress_callback = None
+        
+        logger.info(f"SmartNERService initialized with {self.max_workers} workers, "
+                   f"confidence thresholds: {self.confidence_threshold_first}/{self.confidence_threshold_final}")
+
+    def set_progress_callback(self, callback: Optional[Callable[[Dict[str, Any]], None]]):
+        """设置进度回调函数"""
+        self.progress_callback = callback
+
+    async def get_tags_for_content(
+        self,
+        content: str,
+        node_id: str,
+        existing_tags: Optional[List[str]] = None
+    ) -> List[TagResult]:
+        """
+        Extracts high-quality, Org-mode formatted tags from text content.
+
+        Args:
+            content: The text content of the org-mode node.
+            node_id: The unique identifier for the org-mode node.
+            existing_tags: A list of tags already associated with the node.
+
+        Returns:
+            A list of TagResult objects.
+        """
+        logger.debug(f"SmartNERService: Starting tag extraction for node_id: {node_id}")
+
+        if not content.strip():
+            logger.debug("Content is empty, skipping extraction.")
+            return []
+
+        # 1. Create the specialized prompt for Org-mode
+        try:
+            prompt = create_prompt(
+                contents=[content],
+                entity_types=self.entity_types,
+                existing_tags_lists=[existing_tags or []],
+                prompt_template=ORG_MODE_TAG_EXTRACTION_PROMPT
+            )
+        except Exception as e:
+            logger.error(f"Error creating prompt for node {node_id}: {e}", exc_info=True)
+            return []
+
+        # 2. Call the LLM
+        try:
+            llm_response = await self.llm_async_callable(prompt)
+        except Exception as e:
+            logger.error(f"LLM call failed in SmartNERService for node {node_id}: {e}")
+            return []
+
+        if not llm_response:
+            logger.warning(f"LLM returned an empty response for node {node_id}.")
+            return []
+
+        logger.debug(f"LLM raw response: {llm_response}")
+
+        # 3. Parse the result using the unified processor
+        note_results: List[NoteResult] = self.unified_processor.process_llm_response(
+            response_str=llm_response,
+            note_ids=[node_id]
+        )
+
+        if not note_results or note_results[0].error:
+            logger.error(f"Failed to parse LLM response for node {node_id}: {note_results[0].error if note_results else 'No result'}")
+            return []
+
+        tags = note_results[0].tags
+        logger.info(f"SmartNERService extracted {len(tags)} tags from node {node_id}.")
+        return tags
+
+    async def batch_generate_tags(self, nodes_data: List[Dict[str, Any]]) -> BatchProcessingResult:
+        """
+        批量生成标签建议
+        
+        Args:
+            nodes_data: 节点数据列表，每个元素包含：
+                - id: 节点ID
+                - title: 节点标题
+                - content: 节点内容
+                - file_path: 文件路径
+                - level: 节点层级
+                - existing_tags: 现有标签列表
+        
+        Returns:
+            BatchProcessingResult: 批量处理结果
+        """
+        start_time = time.time()
+        logger.info(f"开始批量处理 {len(nodes_data)} 个节点")
+        
+        # 转换为处理任务
+        tasks = []
+        for node_data in nodes_data:
+            task = NodeProcessingTask(
+                node_id=node_data.get('id', ''),
+                title=node_data.get('title', ''),
+                content=node_data.get('content', ''),
+                file_path=node_data.get('file_path', ''),
+                level=node_data.get('level', 1),
+                existing_tags=node_data.get('existing_tags', [])
+            )
+            tasks.append(task)
+        
+        # 执行批量处理
+        result = await self._process_tasks_in_batches(tasks)
+        
+        processing_time = time.time() - start_time
+        result.processing_time = processing_time
+        
+        logger.info(f"批量处理完成: 生成 {result.generated_count} 个建议, "
+                   f"失败 {result.failed_count} 个节点, 耗时 {processing_time:.2f}s")
+        
+        return result
+
+    async def _process_tasks_in_batches(self, tasks: List[NodeProcessingTask]) -> BatchProcessingResult:
+        """分批处理任务"""
+        total_tasks = len(tasks)
+        generated_count = 0
+        failed_nodes = []
+        all_suggestions = []
+        
+        # 分批处理
+        for i in range(0, total_tasks, self.batch_size):
+            batch = tasks[i:i + self.batch_size]
+            batch_results = await self._process_batch_with_threading(batch, i, total_tasks)
+            
+            generated_count += batch_results['generated_count']
+            failed_nodes.extend(batch_results['failed_nodes'])
+            all_suggestions.extend(batch_results['suggestions'])
+            
+            # 更新进度
+            if self.progress_callback:
+                progress_data = {
+                    'current': min(i + self.batch_size, total_tasks),
+                    'total': total_tasks,
+                    'current_batch_size': len(batch),
+                    'generated_in_batch': batch_results['generated_count'],
+                    'failed_in_batch': len(batch_results['failed_nodes'])
+                }
+                try:
+                    self.progress_callback(progress_data)
+                except Exception as e:
+                    logger.warning(f"进度回调失败: {e}")
+        
+        return BatchProcessingResult(
+            generated_count=generated_count,
+            failed_count=len(failed_nodes),
+            failed_nodes=failed_nodes,
+            processing_time=0.0,  # 将在上层设置
+            suggestions=all_suggestions
+        )
+
+    async def _process_batch_with_threading(self, batch: List[NodeProcessingTask], 
+                                          batch_start_idx: int, total_tasks: int) -> Dict[str, Any]:
+        """使用多线程处理一批任务"""
+        logger.debug(f"处理批次 {batch_start_idx//self.batch_size + 1}, 包含 {len(batch)} 个任务")
+        
+        # 使用线程池执行批处理
+        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
+            # 创建异步任务
+            futures = []
+            for task in batch:
+                future = asyncio.get_event_loop().run_in_executor(
+                    executor, 
+                    self._process_single_task_sync, 
+                    task
+                )
+                futures.append(future)
+            
+            # 等待所有任务完成
+            results = await asyncio.gather(*futures, return_exceptions=True)
+        
+        # 统计结果
+        generated_count = 0
+        failed_nodes = []
+        suggestions = []
+        
+        for i, result in enumerate(results):
+            task = batch[i]
+            if isinstance(result, Exception):
+                logger.error(f"任务处理异常 {task.node_id}: {result}")
+                failed_nodes.append(task.node_id)
+            elif result is None:
+                logger.warning(f"任务处理失败 {task.node_id}: 返回None")
+                failed_nodes.append(task.node_id)
+            else:
+                # 成功处理
+                task_suggestions = result.get('suggestions', [])
+                if task_suggestions:
+                    generated_count += len(task_suggestions)
+                    suggestions.extend(task_suggestions)
+                else:
+                    logger.debug(f"节点 {task.node_id} 未生成标签建议")
+        
+        # 按节点组织建议
+        suggestions_by_node = {}
+        for suggestion in suggestions:
+            node_id = suggestion.get('node_id')
+            if node_id:
+                if node_id not in suggestions_by_node:
+                    suggestions_by_node[node_id] = []
+                suggestions_by_node[node_id].append(suggestion)
+        
+        # 转换为Elisp期望的格式
+        organized_suggestions = []
+        for node_id, node_suggestions in suggestions_by_node.items():
+            organized_suggestions.append({
+                'node_id': node_id,
+                'suggestions': node_suggestions
+            })
+        
+        return {
+            'generated_count': generated_count,
+            'failed_nodes': failed_nodes,
+            'suggestions': organized_suggestions
+        }
+
+    def _process_single_task_sync(self, task: NodeProcessingTask) -> Optional[Dict[str, Any]]:
+        """同步处理单个任务（在线程池中运行）"""
+        try:
+            # 在新的事件循环中运行异步处理
+            loop = asyncio.new_event_loop()
+            asyncio.set_event_loop(loop)
+            try:
+                return loop.run_until_complete(self._process_single_task_async(task))
+            finally:
+                loop.close()
+        except Exception as e:
+            logger.error(f"同步任务处理失败 {task.node_id}: {e}", exc_info=True)
+            return None
+
+    async def _process_single_task_async(self, task: NodeProcessingTask) -> Optional[Dict[str, Any]]:
+        """异步处理单个任务"""
+        max_retries = self.max_retries
+        last_error = None
+        
+        for attempt in range(max_retries):
+            try:
+                # 第一阶段：生成候选标签
+                candidate_tags = await self._generate_candidate_tags(task)
+                if not candidate_tags:
+                    logger.debug(f"节点 {task.node_id} 未生成候选标签")
+                    return {'suggestions': []}
+                
+                # 第二阶段：置信度评估
+                evaluated_tags = await self._evaluate_tag_confidence(task, candidate_tags)
+                
+                # 过滤低置信度标签
+                final_tags = [tag for tag in evaluated_tags 
+                             if tag.confidence >= self.confidence_threshold_final]
+                
+                if not final_tags:
+                    logger.debug(f"节点 {task.node_id} 所有标签置信度过低")
+                    return {'suggestions': []}
+                
+                # 转换为建议格式
+                suggestions = []
+                for tag in final_tags:
+                    suggestion = {
+                        'node_id': task.node_id,
+                        'tag_name': tag.tag_name,
+                        'confidence': tag.confidence,
+                        'reasoning': tag.reasoning,
+                        'source': 'smart_ner_batch',
+                        'created_at': time.time()
+                    }
+                    suggestions.append(suggestion)
+                
+                logger.debug(f"节点 {task.node_id} 生成 {len(suggestions)} 个标签建议")
+                return {'suggestions': suggestions}
+                
+            except Exception as e:
+                last_error = e
+                logger.warning(f"处理节点 {task.node_id} 失败 (尝试 {attempt + 1}/{max_retries}): {e}")
+                if attempt < max_retries - 1:
+                    await asyncio.sleep(1)  # 重试前等待
+        
+        logger.error(f"节点 {task.node_id} 处理最终失败: {last_error}")
+        return None
+
+    async def _generate_candidate_tags(self, task: NodeProcessingTask) -> List[TagResult]:
+        """第一阶段：生成候选标签"""
+        content = f"{task.title}\n\n{task.content}".strip()
+        if not content:
+            return []
+        
+        # 使用现有的标签提取逻辑
+        tags = await self.get_tags_for_content(
+            content=content,
+            node_id=task.node_id,
+            existing_tags=task.existing_tags
+        )
+        
+        # 第一轮过滤：只保留置信度较高的候选标签
+        candidate_tags = [tag for tag in tags 
+                         if tag.confidence >= self.confidence_threshold_first]
+        
+        logger.debug(f"节点 {task.node_id} 生成 {len(candidate_tags)} 个候选标签")
+        return candidate_tags
+
+    async def _evaluate_tag_confidence(self, task: NodeProcessingTask, 
+                                     candidate_tags: List[TagResult]) -> List[TagResult]:
+        """第二阶段：评估标签置信度"""
+        if not candidate_tags:
+            return []
+        
+        # 构建置信度评估提示
+        evaluation_prompt = self._create_confidence_evaluation_prompt(task, candidate_tags)
+        
+        try:
+            # 调用LLM进行置信度评估
+            llm_response = await self.llm_async_callable(evaluation_prompt)
+            
+            # 解析评估结果
+            evaluated_tags = self._parse_confidence_evaluation(candidate_tags, llm_response)
+            
+            logger.debug(f"节点 {task.node_id} 完成置信度评估，{len(evaluated_tags)} 个标签")
+            return evaluated_tags
+            
+        except Exception as e:
+            logger.warning(f"置信度评估失败 {task.node_id}: {e}")
+            # 评估失败时返回原始标签
+            return candidate_tags
+
+    def _create_confidence_evaluation_prompt(self, task: NodeProcessingTask, 
+                                           candidate_tags: List[TagResult]) -> str:
+        """创建置信度评估提示"""
+        content = f"{task.title}\n\n{task.content}".strip()
+        tag_list = [f"- {tag.tag_name}: {tag.reasoning}" for tag in candidate_tags]
+        
+        prompt = f"""
+请评估以下标签对于给定内容的相关性和质量，为每个标签提供0-1之间的置信度分数。
+
+内容:
+{content}
+
+候选标签:
+{chr(10).join(tag_list)}
+
+现有标签: {', '.join(str(tag) for tag in task.existing_tags) if task.existing_tags else '无'}
+
+评估标准:
+1. 相关性：标签是否准确描述内容主题
+2. 特异性：标签是否足够具体，避免过于宽泛
+3. 实用性：标签是否有助于知识组织和检索
+4. 避重复：避免与现有标签重复或过于相似
+
+请以JSON格式返回评估结果:
+[
+  {{"tag_name": "标签名", "confidence": 0.8, "reasoning": "评估理由"}},
+  ...
+]
+"""
+        return prompt.strip()
+
+    def _parse_confidence_evaluation(self, candidate_tags: List[TagResult], 
+                                   llm_response: str) -> List[TagResult]:
+        """解析置信度评估结果"""
+        try:
+            # 使用统一处理器解析响应
+            note_results = self.unified_processor.process_llm_response(
+                response_str=llm_response,
+                note_ids=['evaluation']
+            )
+            
+            if not note_results or note_results[0].error:
+                logger.warning("置信度评估解析失败，使用原始标签")
+                return candidate_tags
+            
+            evaluated_tags = note_results[0].tags
+            
+            # 将评估结果映射回原始标签
+            tag_map = {tag.tag_name: tag for tag in candidate_tags}
+            final_tags = []
+            
+            for eval_tag in evaluated_tags:
+                if eval_tag.tag_name in tag_map:
+                    original_tag = tag_map[eval_tag.tag_name]
+                    # 更新置信度和推理
+                    original_tag.confidence = eval_tag.confidence
+                    original_tag.reasoning = eval_tag.reasoning or original_tag.reasoning
+                    final_tags.append(original_tag)
+            
+            return final_tags
+            
+        except Exception as e:
+            logger.warning(f"置信度评估解析异常: {e}")
+            return candidate_tags 
\ No newline at end of file
diff --git a/simtag/services/user_interface.py b/simtag/services/user_interface.py
new file mode 100755
index 0000000..3f9b0dc
--- /dev/null
+++ b/simtag/services/user_interface.py
@@ -0,0 +1,251 @@
+"""
+User interface service layer
+
+This module provides a user-friendly interface, responsible for converting between the UUID technical layer and the user display layer.
+The user sees titles and content snippets, while the system still uses UUIDs for precision.
+"""
+
+import logging
+from typing import Dict, List, Any, Optional, Tuple
+import re
+from ..core.graph_service import GraphService
+
+logger = logging.getLogger(__name__)
+
+class UserFriendlyNode:
+    """User-friendly node representation"""
+    def __init__(self, uuid: str, title: str, content_snippet: str, 
+                 file_path: str = "", score: float = 0.0, metadata: Dict[str, Any] = None):
+        self.uuid = uuid
+        self.title = title
+        self.content_snippet = content_snippet
+        self.file_path = file_path
+        self.score = score
+        self.metadata = metadata or {}
+    
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert to dictionary format, for EPC transmission"""
+        return {
+            "title": self.title,
+            "snippet": self.content_snippet,
+            "file_path": self.file_path,
+            "score": self.score,
+            "uuid": self.uuid,  # UUID retained but not main display information
+            "metadata": self.metadata
+        }
+
+class UserInterfaceService:
+    """User interface service, providing user-friendly interaction methods"""
+    
+    def __init__(self, graph_service: GraphService):
+        self.graph_service = graph_service
+        logger.info("UserInterfaceService initialized")
+    
+    def _generate_content_snippet(self, content: str, max_length: int = 100) -> str:
+        """Generate content snippet for display"""
+        if not content:
+            return "No content"
+        
+        # Clean content: remove extra whitespace and newlines
+        cleaned = re.sub(r'\s+', ' ', content.strip())
+        
+        if len(cleaned) <= max_length:
+            return cleaned
+        
+        # Truncate at word boundary
+        truncated = cleaned[:max_length]
+        last_space = truncated.rfind(' ')
+        if last_space > max_length * 0.7:  # If space position is reasonable
+            truncated = truncated[:last_space]
+        
+        return truncated + "..."
+    
+    def _enhance_node_with_metadata(self, node_uuid: str, base_score: float = 0.0) -> Optional[UserFriendlyNode]:
+        """Enhance node information with metadata"""
+        try:
+            # Get node details
+            nodes_details = self.graph_service.get_nodes_by_ids([node_uuid])
+            if not nodes_details:
+                logger.warning(f"Node metadata not found: {node_uuid}")
+                return UserFriendlyNode(
+                    uuid=node_uuid,
+                    title=f"Node {node_uuid[:8]}...",
+                    content_snippet="Metadata unavailable",
+                    score=base_score
+                )
+            
+            node_detail = nodes_details[0]
+            title = node_detail.get('title', '') or f"未命名节点 {node_uuid[:8]}..."
+            content = node_detail.get("content", "")
+            content_snippet = self._generate_content_snippet(content, max_length=100)
+            
+            return UserFriendlyNode(
+                uuid=node_uuid,
+                title=title,
+                content_snippet=content_snippet,
+                score=base_score,
+                metadata={
+                    'document_date': node_detail.get('document_date'),
+                    'node_id': node_uuid,
+                    'content_length': len(content)
+                }
+            )
+            
+        except Exception as e:
+            logger.error(f"Error enhancing node metadata {node_uuid}: {e}")
+            return UserFriendlyNode(
+                uuid=node_uuid,
+                title=f"Node {node_uuid[:8]}...",
+                content_snippet="Error processing",
+                score=base_score
+            )
+    
+    def convert_similar_nodes_to_user_friendly(
+        self, 
+        uuid_results: List[Tuple[str, float]], 
+        include_content: bool = True
+    ) -> List[Dict[str, Any]]:
+        """
+        Convert UUID-based similar node results to user-friendly format
+        
+        Args:
+            uuid_results: [(node_uuid, similarity_score), ...] format results
+            include_content: Whether to include content snippet
+            
+        Returns:
+            User-friendly node list
+        """
+        user_friendly_results = []
+        
+        for node_uuid, score in uuid_results:
+            try:
+                friendly_node = self._enhance_node_with_metadata(node_uuid, score)
+                if friendly_node:
+                    user_friendly_results.append(friendly_node.to_dict())
+                else:
+                    # fallback: basic information
+                    user_friendly_results.append({
+                        "title": f"Node {node_uuid[:8]}...",
+                        "snippet": "Information unavailable",
+                        "uuid": node_uuid,
+                        "score": score,
+                        "metadata": {}
+                    })
+            except Exception as e:
+                logger.error(f"Error converting node {node_uuid}: {e}")
+                continue
+        
+        return user_friendly_results
+    
+    def search_nodes_by_title_or_content(
+        self, 
+        query: str, 
+        top_k: int = 10,
+        fuzzy_match: bool = True
+    ) -> List[Dict[str, Any]]:
+        """
+        Search nodes by title or content (user-friendly interface)
+        
+        Args:
+            query: Search query (can be title snippet or content keyword)
+            top_k: Number of results to return
+            fuzzy_match: Whether to enable fuzzy matching
+            
+        Returns:
+            User-friendly search results
+        """
+        try:
+            logger.info(f"User-friendly search: '{query}', top_k={top_k}")
+            
+            # Use storage layer's search functionality
+            search_results = self.graph_service.search_nodes_by_title_content(query, limit=top_k)
+            
+            # Convert to user-friendly format
+            user_friendly_results = []
+            for result in search_results:
+                content_snippet = self._generate_content_snippet(result.get('content', ''), max_length=100)
+                
+                friendly_result = {
+                    "title": result.get('title', f"Node {result['node_id'][:8]}..."),
+                    "snippet": content_snippet,
+                    "uuid": result['node_id'],
+                    "score": 1.0,  # Text search gives fixed score
+                    "file_path": "",  # If available, can be obtained from metadata
+                    "metadata": {
+                        'document_date': result.get('document_date'),
+                        'node_id': result['node_id'],
+                        'search_type': 'title_content',
+                        'content_length': len(result.get('content', ''))
+                    }
+                }
+                user_friendly_results.append(friendly_result)
+            
+            logger.info(f"Search '{query}' found {len(user_friendly_results)} results")
+            return user_friendly_results
+            
+        except Exception as e:
+            logger.error(f"Error searching nodes: {e}")
+            return []
+    
+    def get_node_context_by_uuid(self, node_uuid: str) -> Optional[Dict[str, Any]]:
+        """
+        Get complete context information for a node by UUID
+        
+        Args:
+            node_uuid: Node UUID
+            
+        Returns:
+            Dictionary containing node details and related context
+        """
+        try:
+            friendly_node = self._enhance_node_with_metadata(node_uuid)
+            if not friendly_node:
+                return None
+            
+            # TODO: Add related nodes, tags, etc. context information
+            context = friendly_node.to_dict()
+            context.update({
+                "related_nodes": [],  # Related nodes
+                "tags": [],          # Related tags
+                "references": [],    # Reference relationships
+            })
+            
+            return context
+            
+        except Exception as e:
+            logger.error(f"Error getting node context {node_uuid}: {e}")
+            return None
+    
+    def resolve_user_input_to_uuid(self, user_input: str) -> Optional[str]:
+        """
+        Parse user input (title snippet, content keyword, etc.) into specific node UUID
+        
+        Args:
+            user_input: User input query
+            
+        Returns:
+            Most matching node UUID, or None if not found
+        """
+        try:
+            # Check if already UUID format
+            if self._is_uuid_format(user_input):
+                return user_input
+            
+            # Try to find the most matching node by title search
+            search_results = self.graph_service.search_nodes_by_title_content(user_input, limit=1)
+            if search_results:
+                best_match = search_results[0]
+                logger.info(f"Parsed user input '{user_input}' to UUID: {best_match['node_id']}")
+                return best_match['node_id']
+            
+            logger.info(f"Cannot parse user input to UUID: '{user_input}'")
+            return None
+            
+        except Exception as e:
+            logger.error(f"Error parsing user input: {e}")
+            return None
+    
+    def _is_uuid_format(self, text: str) -> bool:
+        """Check if text is in UUID format"""
+        uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
+        return bool(re.match(uuid_pattern, text.lower())) 
\ No newline at end of file
diff --git a/simtag/simtag_bridge.py b/simtag/simtag_bridge.py
old mode 100644
new mode 100755
index ddacfb7..9f29987
--- a/simtag/simtag_bridge.py
+++ b/simtag/simtag_bridge.py
@@ -7,120 +7,77 @@ import logging
 import traceback
 import threading
 import argparse
+import sexpdata
 import asyncio
-import time
-import json
 
 from epc.server import ThreadingEPCServer
+from epc.client import EPCClient
+
+from dependency_injector.wiring import inject, Provide
+from simtag.container import AppContainer
+from simtag.module.sync_handler import SyncHandler
+from simtag.module.query_handler import QueryHandler
+from simtag.module.diagnostics_handler import DiagnosticsHandler
+from simtag.module.autotag_handler import AutotagHandler
+from simtag.module.resonance_handler import ResonanceHandler
+from simtag.config import Config
+from typing import Dict, List, Any, Optional
 
-# --- Project Setup ---
-# Add project root to sys.path to allow for absolute imports
+from simtag.utils.unified_tag_processor import normalize_payload
+
+# --- Logging Setup ---
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
 PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
 if PROJECT_ROOT not in sys.path:
     sys.path.insert(0, PROJECT_ROOT)
 
-from simtag.context import context
-from simtag.utils.unified_tag_processor import normalize_payload
-from typing import Dict, Any
-
-# --- Logging ---
-# Note: Full logging to file is initialized inside SimTagBridge
-# after the data_directory is known.
 logger = logging.getLogger("simtag_bridge")
 
+def setup_simtag_logging(data_dir):
+    """Sets up logging for the SimTag bridge."""
+    log_file = os.path.join(data_dir, 'simtag_bridge.log')
+    package_logger = logging.getLogger("simtag")
+    package_logger.setLevel(logging.DEBUG)
+    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    if package_logger.hasHandlers():
+        package_logger.handlers.clear()
+    stream_handler = logging.StreamHandler(sys.stderr)
+    stream_handler.setFormatter(formatter)
+    package_logger.addHandler(stream_handler)
+    file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
+    file_handler.setFormatter(formatter)
+    package_logger.addHandler(file_handler)
+    logging.getLogger("simtag.bridge_setup").info(f"SimTag package logging initialized. Log file: {log_file}")
+    return package_logger
+
 class SimTagBridge:
-    def __init__(self, port_file: str, data_directory: str, server_port: int, server_host: str = '127.0.0.1'):
-        # --- Logging Setup ---
-        log_file = os.path.join(data_directory, 'simtag_bridge.log')
-        if not os.path.exists(data_directory):
-            os.makedirs(data_directory)
-            
-        package_logger = logging.getLogger("simtag")
-        package_logger.setLevel(logging.DEBUG)
-        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-        
-        if package_logger.hasHandlers():
-            package_logger.handlers.clear()
-            
-        stream_handler = logging.StreamHandler(sys.stderr)
-        stream_handler.setFormatter(formatter)
-        package_logger.addHandler(stream_handler)
-        
-        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
-        file_handler.setFormatter(formatter)
-        package_logger.addHandler(file_handler)
-        
-        logging.getLogger("simtag.bridge_setup").info(f"SimTag package logging initialized. Log file: {log_file}")
-        
-        # --- Core Initialization ---
-        self.server_host = server_host
-        self.port = server_port
-        self.port_file = port_file
-        self.shutdown_event = threading.Event()
-        
-        # --- Request Tracking ---
-        self.running_tasks = {}  # request_id -> task_info
-        self.task_lock = threading.Lock()
-        self.request_counter = 0
+    def __init__(self, container: AppContainer, emacs_epc_port: int, server_host: str = '127.0.0.1'):
+        logger.info(f"Initializing SimTagBridge. Emacs EPC port: {emacs_epc_port}")
         
-        # --- Component Setup ---
-        self._init_event_loop()
-        
-        # 1. Initialize the EPC Server
-        self.server = ThreadingEPCServer((self.server_host, self.port), log_traceback=True)
+        self.container = container
+        self.emacs_client: Optional[EPCClient] = None
+        self._init_emacs_client(emacs_epc_port)
+
+        self.container.config.emacs_client.from_value(self.emacs_client)
+
+        self.server = ThreadingEPCServer((server_host, 0), log_traceback=True)
         self.server.allow_reuse_address = True
         
-        # 2. Initialize all application services via the global context
-        # This must be done *before* registering methods that use these services.
-        context.initialize(port=self.port, emacs_client=None) # emacs_client can be set later if needed
-        
-        # 3. Register methods with the server
         self._register_methods()
-        
-        # 4. Start the server in a separate thread
-        self.server_thread = threading.Thread(target=self._run_server, daemon=True)
+        logger.info("Registered SimTagBridge instance methods with EPC server.")
+
+        self.server_thread = threading.Thread(target=self.server.serve_forever)
+        self.server_thread.daemon = True
         self.server_thread.start()
-        
-        # 5. Write the actual port number to the port file for Emacs to read
-        actual_port = self.server.server_address[1]
-        logger.info(f"SimTagBridge EPC server listening on port: {actual_port}. Writing to port file: {self.port_file}")
-        try:
-            with open(self.port_file, 'w') as f:
-                f.write(str(actual_port))
-            logger.info("Successfully wrote port to file.")
-        except IOError as e:
-            logger.error(f"FATAL: Could not write to port file {self.port_file}. Error: {e}", exc_info=True)
-            self.shutdown()
-
-    def _init_event_loop(self):
-        """Initializes the asyncio event loop and starts it in a separate thread."""
-        self.loop = asyncio.new_event_loop()
-        self.loop_thread = threading.Thread(target=self._run_event_loop, daemon=True)
-        self.loop_thread.start()
-        logger.info("Dedicated asyncio event loop thread started.")
-
-    def _run_server(self):
-        """Runs the EPC server loop until a shutdown is requested."""
-        logger.info("EPC server thread started.")
-        self.server.serve_forever()
-        logger.info("EPC server thread has shut down.")
-
-    def _run_event_loop(self):
-        """Runs the asyncio event loop and logs its lifecycle."""
-        asyncio.set_event_loop(self.loop)
-        logger.info("Event loop thread started and running.")
-        try:
-            self.loop.run_forever()
-        except Exception as e:
-            logger.error(f"""Event loop thread crashed with an exception: {e}
-{traceback.format_exc()}""")
-        finally:
-            self.loop.close()
-            logger.info("Event loop has been closed and thread is terminating.")
+        logger.info("SimTagBridge EPC server started in a new thread.")
+
+        server_port = self.server.server_address[1]
+        logger.info(f"SimTagBridge EPC server listening on port: {server_port}. Informing Emacs.")
+        self._eval_in_emacs('org-supertag-bridge--handle-python-server-ready-signal', server_port)
+        logger.info("Informed Emacs about the SimTagBridge EPC server port.")
 
     def _register_methods(self):
-        """Register all available EPC methods by referencing handlers from the global context."""
+        """Register all available EPC methods."""
         logger.info("Registering EPC methods...")
         
         # Core
@@ -129,370 +86,449 @@ class SimTagBridge:
         self.server.register_function(self.cleanup, 'cleanup')
 
         # Diagnostics
-        self.server.register_function(self.get_status, 'diagnostics/get_status')
-        self.server.register_function(self.get_config, 'diagnostics/get_config')
-        self.server.register_function(self.check_imports, 'diagnostics/check_imports')
-        self.server.register_function(self.test_embedding_retrieval, 'diagnostics/test_embedding_retrieval')
-        self.server.register_function(self.get_processing_status, 'diagnostics/get_processing_status')
-        self.server.register_function(self.print_processing_report, 'diagnostics/print_processing_report')
-        self.server.register_function(self.get_available_models, 'diagnostics/get_available_models')
-        self.server.register_function(self.get_default_model, 'diagnostics/get_default_model')
+        self.server.register_function(self.get_status, 'get_status')
+        self.server.register_function(self.get_config, 'get_config')
+        self.server.register_function(self.check_imports, 'check_imports')
+        self.server.register_function(self.test_embedding_retrieval, 'test_embedding_retrieval')
+        self.server.register_function(self.get_processing_status, 'get_processing_status')
+        self.server.register_function(self.print_processing_report, 'print_processing_report')
 
         # Sync
-        self.server.register_function(self.bulk_process_snapshot, 'sync/bulk_process')
+        self.server.register_function(self.sync_library, 'sync_library')
+        self.server.register_function(self.bulk_process_snapshot, 'bulk_process_snapshot')
         
         # Query
-        self.server.register_function(context.query_handler.get_similar_nodes, 'query/get_similar_nodes')
+        self.server.register_function(self.get_similar_nodes, 'get_similar_nodes')
+        self.server.register_function(self.find_similar_nodes_friendly, 'find_similar_nodes_friendly')
+        self.server.register_function(self.search_nodes_by_content, 'search_nodes_by_content')
+        self.server.register_function(self.get_node_context_friendly, 'get_node_context_friendly')
         
         # Autotag
-        self.server.register_function(self.batch_generate_tags, 'autotag/batch_generate_tags')
-        self.server.register_function(self.generate_single_node_tags, 'autotag/generate_single_node_tags')
-         
-        # RAG Assistant
-        self.server.register_function(self.query, 'rag/query')
-        
-        # AI Commands (dedicated interfaces for chat commands)
-        self.server.register_function(self.suggest_tags, 'ai/suggest-tags')
-        self.server.register_function(self.find_connections, 'ai/find-connections')
-        self.server.register_function(self.expand_content, 'ai/expand')
-
-        # Embedding maintenance
-        self.server.register_function(self.refresh_stale_tags, 'embedding/refresh_stale_tags')
+        self.server.register_function(self.extract_entities_for_tagging, 'extract_entities_for_tagging')
+        self.server.register_function(self.force_extract_entities_for_tagging, 'force_extract_entities_for_tagging')
+        self.server.register_function(self.batch_extract_entities_for_tagging, 'batch_extract_entities_for_tagging')
+        self.server.register_function(self.rag_extract_tag_patterns, 'rag_extract_tag_patterns')
+        self.server.register_function(self.get_tag_similarity_suggestions, 'get_tag_similarity_suggestions')
         
-        # Reasoning
-        # New KnowledgeHandler cycle (replaces old reasoning/run_cycle)
-        self.server.register_function(self.run_knowledge_cycle, 'knowledge/run_cycle')
-        # Maintain backward compatibility with older Emacs versions
-        self.server.register_function(self.run_inference_cycle, 'reasoning/run_cycle')
-        # New queue status interface
-        self.server.register_function(self.get_knowledge_queue_status, 'knowledge/get_queue_status')
-        # Backward compatibility
-        self.server.register_function(self.get_reasoning_queue_status, 'reasoning/get_queue_status')
-
-        # Feedback
-        self.server.register_function(self.submit_feedback, 'feedback/submit')
-        self.server.register_function(self.debug_payload, 'debug/payload')
+        # Tag Governance
+        self.server.register_function(self.sync_tag_event, 'sync_tag_event')
+        self.server.register_function(self.update_tag_status, 'update_tag_status')
+        self.server.register_function(self.update_tag_rules, 'update_tag_rules')
+        self.server.register_function(self.update_relation_type, 'update_relation_type')
+        self.server.register_function(self.update_relation_rules, 'update_relation_rules')
         
-        # Smart Companion
-        self.server.register_function(self.analyze_tag_context, 'smart_companion/analyze_tag_context')
+        # Batch Tag Processing
+        self.server.register_function(self.batch_generate_tags, 'batch_generate_tags')
+        self.server.register_function(self.generate_single_node_tags, 'generate_single_node_tags')
         
-        # Task Management
-        self.server.register_function(self._get_running_tasks, 'diagnostics/get_running_tasks')
+        # Proactive Features
+        self.server.register_function(self.proactive_get_resonance, 'proactive_get_resonance')
         
         logger.info("All EPC methods registered successfully.")
 
-    def echo(self, message):
-        """Echoes back the received message."""
-        logger.info(f"Received echo request: {message}")
-        return message
-
-    def ping(self):
-        """A simple method to check if the server is alive."""
-        logger.info("Received ping.")
-        return "pong"
-
-    def cleanup(self):
-        """Shuts down the bridge and its resources gracefully."""
-        logger.info("Received shutdown request. Cleaning up resources...")
-        self.shutdown_event.set()
-        if self.loop.is_running():
-            asyncio.run_coroutine_threadsafe(self.final_async_cleanup(), self.loop)
-        logger.info("Graceful shutdown complete.")
-
-    async def final_async_cleanup(self):
-        tasks = [t for t in asyncio.all_tasks(loop=self.loop) if t is not asyncio.current_task()]
-        if tasks:
-            logger.info(f"Cancelling {len(tasks)} outstanding async tasks...")
-            for task in tasks:
-                task.cancel()
-            await asyncio.gather(*tasks, return_exceptions=True)
-            logger.info("All outstanding async tasks cancelled.")
-        self.loop.stop()
-
-    # --- Method Implementations ---
-    # All methods now delegate to the handlers stored in the global `context`.
-
-    def get_status(self):
-        return context.diagnostics_handler.get_status()
-
-    def get_config(self):
-        return context.diagnostics_handler.get_config()
-
-    def check_imports(self):
-        return context.diagnostics_handler.check_imports()
-
-    def test_embedding_retrieval(self, text: str):
-        return self._run_async(context.diagnostics_handler.test_embedding_retrieval(text))
-
-    def get_processing_status(self):
-        return self._run_async(context.diagnostics_handler.get_processing_status())
-
-    def print_processing_report(self):
-        return self._run_async(context.diagnostics_handler.print_processing_report())
-
-
-
-    def get_available_models(self):
-        return self._run_async(context.diagnostics_handler.get_available_models())
-
-    def get_default_model(self):
-        return self._run_async(context.diagnostics_handler.get_default_model())
-
-    
+    def _init_emacs_client(self, emacs_server_port):
+        if self.emacs_client is None:
+            try:
+                self.emacs_client = EPCClient(("127.0.0.1", emacs_server_port), log_traceback=True)
+            except Exception as e:
+                logger.error(f"Error initializing EPC client: {e}\n{traceback.format_exc()}")
 
-    def bulk_process_snapshot(self, *args) -> Dict[str, Any]:
+    def _close_emacs_client(self):
+        if self.emacs_client is not None:
+            try:
+                self.emacs_client.close()
+                self.emacs_client = None
+            except Exception as e:
+                logger.error(f"Error closing EPC client: {e}\n{traceback.format_exc()}")
+
+    def _eval_in_emacs(self, method_name_str, *args):
+        if self.emacs_client is None:
+            logger.error("EPC client to Emacs is not initialized. Cannot eval in Emacs.")
+            return None
         try:
-            logger.info("Bridge: Received call for bulk_process_snapshot.")
-            payload = normalize_payload(args)
-            logger.info(f"Bridge: Payload normalized. Processing {len(payload.get('entities', []))} entities.")
-
-            # --- Ad-hoc Data Transformation (Temporary) ---
-            # Standardize link types from Elisp symbols to Python strings
-            if 'links' in payload and isinstance(payload['links'], list):
-                for link in payload['links']:
-                    # Elisp sends links as alists, which EPC converts to Python lists of lists.
-                    # e.g., [['type', '.', ':node-tag'], ['from', '.', 'id1'], ...]
-                    type_found = False
-                    for i, pair in enumerate(link):
-                        if isinstance(pair, (list, tuple)) and len(pair) > 0 and pair[0] == 'type':
-                            # Check for Elisp symbol `:'node-tag`
-                            if len(pair) > 2 and pair[2] == "':node-tag":
-                                link[i] = ['type', '.', '"HAS_TAG"']
-                                type_found = True
-                                logger.debug(f"Transformed link type for link from {link[1][2]}")
+            method_symbol = sexpdata.Symbol(method_name_str)
+            processed_args = [sexpdata.Quoted(arg) if isinstance(arg, str) else arg for arg in args]
+            s_expression_parts = [method_symbol] + processed_args
+            sexp_to_eval = sexpdata.dumps(s_expression_parts)
+            self.emacs_client.call("eval-in-emacs", [sexp_to_eval])
+        except Exception as e:
+            logger.error(f"Error evaluating in Emacs: {e}\n{traceback.format_exc()}")
+    
+    def _parse_elisp_to_dict(self, elisp_data: Any) -> Any:
+        """
+        Recursively converts complex Elisp-style data structures from EPC into
+        standard Python dictionaries and lists. It specifically handles:
+        1.  The special EPC serialization for a LIST of hash-tables:
+            `[Symbol('#s'), <hash_list1>, Symbol('#s'), <hash_list2>, ...]`
+        2.  s-expressions for single hash-tables: `{'#s': [Symbol('hash-table'), ...]}`
+        3.  Standard alists and plists.
+        """
+        # Case 1: A list, which could be various Elisp structures
+        if isinstance(elisp_data, list):
+            # Subcase 1.1: EPC serialization of a list of hash-tables
+            if len(elisp_data) >= 2 and elisp_data[0] == sexpdata.Symbol('#s'):
+                logger.debug("Parsing EPC-serialized list of hash-tables.")
+                nodes = []
+                it = iter(elisp_data)
+                try:
+                    while True:
+                        s_symbol = next(it)
+                        if s_symbol != sexpdata.Symbol('#s'):
+                            logger.warning(f"Expected '#s' symbol, but got {s_symbol}. List may be malformed.")
+                            break
+                        
+                        hash_content_list = next(it)
+                        reconstructed_hash_dict = {'#s': hash_content_list}
+                        nodes.append(self._parse_elisp_to_dict(reconstructed_hash_dict))
+                except StopIteration:
+                    pass # Consumed all items
+                return nodes
+
+            # Subcase 1.2: Standard alist (dotted or simple)
+            is_alist_dotted = all(isinstance(i, list) and len(i) == 3 and isinstance(i[1], sexpdata.Symbol) and i[1].value() == '.' for i in elisp_data)
+            if is_alist_dotted:
+                logger.debug("Parsing Elisp alist (dotted).")
+                return {item[0]: self._parse_elisp_to_dict(item[2]) for item in elisp_data}
+            
+            is_alist_simple = all(isinstance(i, list) and len(i) == 2 for i in elisp_data)
+            if is_alist_simple:
+                logger.debug("Parsing Elisp alist (simple).")
+                return {item[0]: self._parse_elisp_to_dict(item[1]) for item in elisp_data}
+            
+            # Subcase 1.3: Fallback for a regular list of other items
+            return [self._parse_elisp_to_dict(item) for item in elisp_data]
+
+        # Case 2: s-expression for a SINGLE Elisp hash-table
+        if isinstance(elisp_data, dict) and '#s' in elisp_data:
+            s_exp = elisp_data['#s']
+            if isinstance(s_exp, list) and s_exp and s_exp[0] == sexpdata.Symbol('hash-table'):
+                logger.debug("Parsing single Elisp hash-table s-expression.")
+                res_dct = {}
+                try:
+                    data_symbol = sexpdata.Symbol('data')
+                    data_index = s_exp.index(data_symbol)
+                    kv_list = s_exp[data_index + 1]
+                    
+                    it = iter(kv_list)
+                    while True:
+                        try:
+                            key = next(it)
+                            value = next(it)
+                            key_str = key.value() if isinstance(key, sexpdata.Symbol) else str(key)
+                            res_dct[key_str] = self._parse_elisp_to_dict(value)
+                        except StopIteration:
                             break
-                    if not type_found:
-                        # If no type specified, assume it's a node-tag link for now
-                        pass # Let the handler deal with it, or insert a default.
+                    return res_dct
+                except (ValueError, IndexError, StopIteration, TypeError) as e:
+                    logger.error(f"Failed to parse hash-table s-expression content: {e}", exc_info=True)
+                    return {}
+            return elisp_data
+
+        # Case 3: A standard Python dictionary
+        if isinstance(elisp_data, dict):
+            return {self._parse_elisp_to_dict(k): self._parse_elisp_to_dict(v) for k, v in elisp_data.items()}
 
-            # The coroutine to be executed
-            coro = context.sync_handler.bulk_process_snapshot(payload)
+        # Case 4: A primitive value or a Symbol
+        if isinstance(elisp_data, sexpdata.Symbol):
+            return elisp_data.value()
 
-            # Run the async task and get the result
-            result = self._run_async(coro)
+        return elisp_data
 
-            logger.info(f"Bridge: Async task finished. Result before returning to Emacs: {result}")
+    def echo(self, message):
+        logger.info(f"Received message from Emacs to echo: {message}")
+        return message
 
-            # Ensure we always return a dictionary to Emacs
-            if not isinstance(result, dict):
-                logger.error(f"Bridge: Result is not a dictionary (type: {type(result)}). Returning error state.")
-                return {"status": "error", "message": "Internal bridge error: result was not a dictionary."}
+    def ping(self):
+        return "pong"
 
-            return result
+    def cleanup(self):
+        logger.info("Closing EPC server and client connection.")
+        self._close_emacs_client()
+        if self.server:
+            self.server.shutdown()
+
+    @inject
+    def sync_library(self, db_file: str, db_snapshot_json_str: str, handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        return handler.sync_library_from_snapshot_json(db_file, db_snapshot_json_str)
+
+    @inject
+    def get_similar_nodes(self, query_input: str, top_k: int = 10, handler: QueryHandler = Provide[AppContainer.query_handler]):
+        return handler.get_similar_nodes(query_input, top_k)
+
+    @inject
+    def find_similar_nodes_friendly(self, query_input: str, top_k: int = 10, handler: QueryHandler = Provide[AppContainer.query_handler]):
+        return handler.find_similar_nodes_friendly(query_input, top_k)
+
+    @inject
+    def search_nodes_by_content(self, search_query: str, top_k: int = 10, fuzzy_match: bool = True, handler: QueryHandler = Provide[AppContainer.query_handler]):
+        return handler.search_nodes_by_content(search_query, top_k, fuzzy_match)
+
+    @inject
+    def get_node_context_friendly(self, node_identifier: str, handler: QueryHandler = Provide[AppContainer.query_handler]):
+        return handler.get_node_context_friendly(node_identifier)
+
+    @inject
+    def get_status(self, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]):
+        return handler.get_status()
+
+    @inject
+    def get_config(self, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]):
+        return handler.get_config()
+
+    @inject
+    def check_imports(self, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]):
+        return handler.check_imports()
+        
+    def _run_async(self, coro):
+        """Helper to run a coroutine from a synchronous context."""
+        try:
+            loop = asyncio.get_running_loop()
+        except RuntimeError:  # No running loop in this thread
+            loop = asyncio.new_event_loop()
+            asyncio.set_event_loop(loop)
+        
+        return loop.run_until_complete(coro)
 
+    @inject
+    def proactive_get_resonance(self, payload: Dict[str, Any], handler: ResonanceHandler = Provide[AppContainer.resonance_handler]) -> Dict[str, Any]:
+        logger.debug(f"proactive_get_resonance received payload for node: {payload.get('node_id')}")
+        
+        # This handler method is async
+        coro = handler.get_resonance(payload)
+        return self._run_async(coro)
+
+    @inject
+    def test_embedding_retrieval(self, text: str, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]):
+        return handler.test_embedding_retrieval(text)
+
+    @inject
+    def bulk_process_snapshot(self, *args, handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        """
+        Processes a snapshot of nodes and links from Elisp.
+        Uses *args to handle flexible payload structures from EPC.
+        """
+        logger.debug(f"bulk_process_snapshot received {len(args)} raw args.")
+
+        # Defensive coding: The entire payload is expected from Elisp.
+        # EPC seems to be unpacking our list, so we treat all args as part of the payload.
+        # The contract from Elisp is (list HASH-TABLE), which becomes the single argument.
+        if not args:
+            logger.error("bulk_process_snapshot received no arguments.")
+            return {"error": "No data received"}
+
+        # The payload from Elisp is the first element of the args tuple.
+        raw_payload = args[0]
+        
+        try:
+            # The handler expects the raw payload as passed from Elisp.
+            return handler.bulk_process_snapshot(raw_payload)
         except Exception as e:
-            logger.error(f"Bridge: Unhandled exception in bulk_process_snapshot: {e}", exc_info=True)
-            return {"status": "error", "message": f"Unhandled bridge exception: {e}"}
-
-    # --- Placeholder Governance Methods ---
-    # def sync_tag_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
-    #     return self._run_async(context.sync_handler.sync_tag_event(event_data))
-    # def update_tag_status(self, tag_data: Dict[str, Any]) -> Dict[str, Any]:
-    #     return self._run_async(context.sync_handler.update_tag_status(tag_data))
-    # def update_tag_rules(self, tag_data: Dict[str, Any]) -> Dict[str, Any]:
-    #     return self._run_async(context.sync_handler.update_tag_rules(tag_data))
-    # def update_relation_type(self, relation_data: Dict[str, Any]) -> Dict[str, Any]:
-    #     return self._run_async(context.sync_handler.update_relation_type(relation_data))
-    # def update_relation_rules(self, relation_data: Dict[str, Any]) -> Dict[str, Any]:
-    #     return context.sync_handler.update_relation_rules(relation_data)
-
-    # ------------------------------------------------------------------
-    # AI Commands (Dedicated interfaces)
-    # ------------------------------------------------------------------
-
-    def suggest_tags(self, *args):
-        """Generate intelligent tag suggestions for content."""
-        return self._run_async(context.ai_handler.suggest_tags(*args), method_name="ai/suggest-tags", args=args)
-
-    def find_connections(self, *args):
-        """Find knowledge connections for a given tag."""
-        return self._run_async(context.ai_handler.find_connections(*args), method_name="ai/find-connections", args=args)
-
-    def expand_content(self, *args):
-        """Expand and elaborate on given content or topic."""
-        return self._run_async(context.ai_handler.expand_content(*args), timeout=240, method_name="ai/expand", args=args)
-
-    def batch_generate_tags(self, *args):
+            logger.error(f"Error in bulk_process_snapshot: {e}\\n{traceback.format_exc()}")
+            return {"error": str(e), "traceback": traceback.format_exc()}
+
+    @inject
+    def get_processing_status(self, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]) -> Dict[str, Any]:
+        return handler.get_processing_status()
+
+    @inject
+    def print_processing_report(self, handler: DiagnosticsHandler = Provide[AppContainer.diagnostics_handler]) -> Dict[str, Any]:
+        return handler.print_processing_report()
+
+    @inject
+    def extract_entities_for_tagging(self, payload_list: List[Dict[str, Any]], handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        # Pass the raw payload_list directly to AutotagHandler, which now handles Elisp alist format
+        return handler.extract_entities_for_tagging(payload_list)
+
+    @inject
+    def force_extract_entities_for_tagging(self, payload_list: List[Dict[str, Any]], handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        # Pass the raw payload_list directly to AutotagHandler, which now handles Elisp alist format
+        return handler.force_extract_entities_for_tagging(payload_list)
+
+    @inject
+    def batch_extract_entities_for_tagging(self, payload_list: List[Dict[str, Any]], handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        # Pass the raw payload_list directly to AutotagHandler, which now handles Elisp alist format
+        return handler.batch_extract_entities_for_tagging(payload_list)
+
+    @inject
+    def rag_extract_tag_patterns(self, payload_list: List[Dict[str, Any]], handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        # Pass the raw payload_list directly to AutotagHandler, which now handles Elisp alist format
+        return handler.rag_extract_tag_patterns(payload_list)
+
+    @inject
+    def get_tag_similarity_suggestions(self, payload_list: List[Dict[str, Any]], handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        # Pass the raw payload_list directly to AutotagHandler, which now handles Elisp alist format
+        return handler.get_tag_similarity_suggestions(payload_list)
+
+    @inject
+    def sync_tag_event(self, event_data: Dict[str, Any], handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        """
+        Handles various tag-related events from Elisp.
+        'event_type': 'add', 'remove', 'rename', 'merge'
+        'data': { ... event specific data ... }
+        """
+        event_type = event_data.get('event_type')
+        data = event_data.get('data')
+        
+        # Here you can call different methods on SyncHandler based on event_type
+        # e.g., handler.add_tag(data), handler.remove_tag(data), etc.
+        logger.info(f"Received tag event: {event_type} with data: {data}")
+        return {"status": "received", "event": event_type}
+
+    @inject
+    def update_tag_status(self, tag_data: Dict[str, Any], handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        """
+        Updates the status of a tag (e.g., 'active', 'deprecated').
+        'tag_name': name of the tag
+        'status': new status
+        """
+        tag_name = tag_data.get('tag_name')
+        status = tag_data.get('status')
+        # Call handler.update_tag_status(tag_name, status)
+        logger.info(f"Updating tag '{tag_name}' to status '{status}'")
+        return {"status": "updated", "tag_name": tag_name, "new_status": status}
+
+    @inject
+    def update_tag_rules(self, tag_data: Dict[str, Any], handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        """
+        Updates inference or application rules for a tag.
+        'tag_name': name of the tag
+        'rules': { ... new rules ... }
+        """
+        tag_name = tag_data.get('tag_name')
+        rules = tag_data.get('rules')
+        # Call handler.update_tag_rules(tag_name, rules)
+        logger.info(f"Updating rules for tag '{tag_name}': {rules}")
+        return {"status": "rules_updated", "tag_name": tag_name}
+
+    @inject
+    def update_relation_type(self, relation_data: Dict[str, Any], handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        """
+        Updates a relationship type (e.g., rename, add properties).
+        'type_name': name of the relation type
+        'updates': { ... changes to apply ... }
+        """
+        type_name = relation_data.get('type_name')
+        updates = relation_data.get('updates')
+        # Call handler.update_relation_type(type_name, updates)
+        logger.info(f"Updating relation type '{type_name}': {updates}")
+        return {"status": "relation_type_updated", "type_name": type_name}
+
+    @inject
+    def update_relation_rules(self, relation_data: Dict[str, Any], handler: SyncHandler = Provide[AppContainer.sync_handler]) -> Dict[str, Any]:
+        logger.debug(f"update_relation_rules received data for relation: {relation_data.get('type')}")
+        return handler.update_relation_rules(relation_data)
+
+    @inject
+    def batch_generate_tags(self, *args, handler: AutotagHandler = Provide[AppContainer.autotag_handler]):
+        """
+        Processes a batch of items to generate tags by running the async handler.
+        """
+        logger.debug(f"Bridge batch_generate_tags received {len(args)} arguments")
         try:
-            payload = normalize_payload(args)
-            coro = context.autotag_handler.batch_generate_tags(payload)
+            coro = handler.batch_generate_tags(*args)
             return self._run_async(coro)
         except Exception as e:
-            logger.error(f"""Error in BATCH_GENERATE_TAGS bridge call: {e}\n{traceback.format_exc()}""")
-            return {"error": "batch_generate_tags_failed", "message": str(e)}
-
-    def generate_single_node_tags(self, *args) -> Dict[str, Any]:
+            logger.error(f"Error running async batch_generate_tags: {e}", exc_info=True)
+            return {"error": "async_execution_error", "message": str(e)}
+
+    @inject
+    def generate_single_node_tags(self, *args, handler: AutotagHandler = Provide[AppContainer.autotag_handler]) -> Dict[str, Any]:
+        """
+        Processes a single node to generate tags by running the async handler.
+        """
+        logger.debug(f"Bridge received generate_single_node_tags, forwarding to async handler.")
         try:
-            payload = normalize_payload(args)
-            coro = context.autotag_handler.suggest_tags_for_single_node_elisp(payload)
+            coro = handler.suggest_tags_for_single_node_elisp(*args)
             return self._run_async(coro)
         except Exception as e:
-            logger.error(f"""Error calling async generate_single_node_tags handler: {e}\n{traceback.format_exc()}""")
+            logger.error(f"Error calling async generate_single_node_tags handler: {e}", exc_info=True)
             return {"status": "error", "message": str(e)}
 
-    
-
-    # ------------------------------------------------------------------
-    # RAG Query (New unified chat endpoint)
-    # ------------------------------------------------------------------
-
-    def query(self, *args):
-        """Forward generic chat/query requests to RAGHandler."""
-        return self._run_async(context.rag_handler.query(*args), method_name="rag/query", args=args)
-        
-    # ------------------------------------------------------------------
-    # Knowledge processing cycles
-    # ------------------------------------------------------------------
-
-    def run_knowledge_cycle(self, limit: int = 5):
-        """Preferred new entry point for background knowledge extraction."""
-        return self._run_async(context.knowledge_handler.run_extraction_cycle(limit))
-
-    # Backward-compat alias
-    def run_inference_cycle(self, limit: int = 5):
-        return self.run_knowledge_cycle(limit)
+def main():
+    parser = argparse.ArgumentParser(description="Org SuperTag EPC Bridge Server (Python)")
+    parser.add_argument("--emacs-epc-port", type=int, required=True, help="EPC port of the Emacs server")
+    parser.add_argument("--data-directory", type=str, required=True, help="Data directory for org-supertag")
+    parser.add_argument("--config-file", type=str, help="Path to the TOML configuration file")
+    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
 
-    def get_reasoning_queue_status(self):
-        return self._run_async(context.knowledge_handler.get_queue_status())
+    args = parser.parse_args()
 
-    def get_knowledge_queue_status(self):
-        return self._run_async(context.knowledge_handler.get_queue_status())
+    # --- Config Loading ---
+    config = Config()
+    if args.config_file:
+        try:
+            config = Config.from_toml(args.config_file)
+            print(f"Configuration loaded from {args.config_file}", file=sys.stderr)
+        except FileNotFoundError:
+            print(f"Warning: Config file not found at {args.config_file}. Using default settings.", file=sys.stderr)
+        except Exception as e:
+            print(f"Error loading config file: {e}. Using default settings.", file=sys.stderr)
 
-    def submit_feedback(self, *args):
-        return self._run_async(context.feedback_handler.submit_feedback(*args))
-        
-    def debug_payload(self, *args):
-        logger.info(f"--- DEBUG PAYLOAD ---\nType: {type(args)}\nContent: {args}\n--- END DEBUG ---")
-        return args
-
-    # ------------------------------------------------------------------
-    # Smart Companion Methods
-    # ------------------------------------------------------------------
-
-    def analyze_tag_context(self, *args):
-        """Analyze tag context for smart companion suggestions."""
-        return self._run_async(context.smart_companion_handler.analyze_tag_context(*args))
-
-    def refresh_stale_tags(self, batch_size: int = 20):
-        """RPC: Refresh embeddings for all STALE tags (knowledge_status='STALE')."""
-        logger.info(f"RPC call: embedding/refresh_stale_tags (batch_size={batch_size})")
-        async def _run():
-            await context.embedding_service.refresh_stale_tags(batch_size=batch_size)
-            return {"status": "success"}
-        # Run in event loop and wait for result synchronously for EPC
-        future = asyncio.run_coroutine_threadsafe(_run(), self.loop)
-        return future.result()
-
-    def _generate_request_id(self, method_name: str) -> str:
-        """Generate unique request ID for tracking."""
-        with self.task_lock:
-            self.request_counter += 1
-            return f"{method_name}_{int(time.time())}_{self.request_counter}"
+    # Override config with command line arguments
+    if args.data_directory:
+        config.data_directory = args.data_directory
+    if args.debug:
+        config.debug = args.debug
     
-    def _register_task(self, request_id: str, method_name: str, args: Any) -> None:
-        """Register a running task."""
-        with self.task_lock:
-            self.running_tasks[request_id] = {
-                "method": method_name,
-                "start_time": time.time(),
-                "args": str(args)[:200],  # Truncate for logging
-                "status": "running"
-            }
-            logger.debug(f"Task registered: {request_id} - {method_name}")
-    
-    def _unregister_task(self, request_id: str) -> None:
-        """Unregister a completed task."""
-        with self.task_lock:
-            if request_id in self.running_tasks:
-                task_info = self.running_tasks.pop(request_id)
-                duration = time.time() - task_info["start_time"]
-                logger.debug(f"Task completed: {request_id} - duration: {duration:.2f}s")
-    
-    def _get_running_tasks(self) -> Dict[str, Any]:
-        """Get current running tasks."""
-        with self.task_lock:
-            return dict(self.running_tasks)
-    
-    def _run_async(self, coro, timeout=300, method_name="unknown", args=None):
-        if self.shutdown_event.is_set():
-            logger.warning("Shutdown in progress. Rejecting new async task.")
-            return {"error": "shutdown_in_progress"}
-        if not self.loop_thread.is_alive() or not self.loop.is_running():
-            logger.error("CRITICAL: Event loop is not running. Cannot schedule task.")
-            return {"error": "event_loop_dead"}
+    # --- Logging Setup ---
+    logger = setup_simtag_logging(config.data_directory)
+
+    # --- Dependency Injection Container Setup ---
+    try:
+        container = AppContainer()
+        container.config.from_dict(config.to_dict())
+        container.wire(modules=[__name__])
+        logger.info("Dependency injection container wired successfully.")
+    except Exception as e:
+        logger.error(f"Failed to wire dependency injection container: {e}\n{traceback.format_exc()}")
+        sys.exit(1)
+
+    # --- Start EPC Server ---
+    try:
+        bridge = SimTagBridge(container, args.emacs_epc_port)
+        logger.info(f"SimTagBridge initialized. Python backend is ready.")
         
-        request_id = self._generate_request_id(method_name)
-        self._register_task(request_id, method_name, args)
+        # Keep the main thread alive for the server thread
+        while True:
+            # Use a mechanism that allows for graceful shutdown if needed
+            # For now, just sleep.
+            import time
+            time.sleep(1)
+
+    except KeyboardInterrupt:
+        logger.info("Shutting down SimTagBridge due to KeyboardInterrupt.")
+    except Exception as e:
+        logger.error(f"An unexpected error occurred: {e}\n{traceback.format_exc()}")
+    finally:
+        if 'bridge' in locals() and bridge:
+            bridge.cleanup()
+        logger.info("SimTagBridge has been shut down.")
+
+if __name__ == "__main__":
+    # To enable profiled run:
+    # Set environment variable: `export ENABLE_PROFILING=1`
+    # Then run the script. The profile `simtag_bridge.prof` will be saved on exit.
+    if os.environ.get("ENABLE_PROFILING"):
+        import cProfile
+        import atexit
         
-        async def tracked_coro():
-            try:
-                result = await coro
-                return {"result": result, "request_id": request_id}
-            except asyncio.CancelledError:
-                logger.warning(f"Task cancelled: {request_id}")
-                raise
-            finally:
-                self._unregister_task(request_id)
+        print("Profiling is enabled. Profile will be saved to 'simtag_bridge.prof'.", file=sys.stderr)
         
-        future = asyncio.run_coroutine_threadsafe(tracked_coro(), self.loop)
-        try:
-            response = future.result(timeout=timeout)
-            if isinstance(response, dict) and "request_id" in response:
-                return response["result"]
-            return response
-        except asyncio.TimeoutError:
-            logger.error(f"Async task timed out: {request_id}")
-            self._unregister_task(request_id)
-            return {"error": "timeout", "request_id": request_id}
-        except Exception as e:
-            logger.error(f"Error running async task {request_id}: {e}\n{traceback.format_exc()}")
-            self._unregister_task(request_id)
-            return {"error": "async_execution_error", "message": str(e), "request_id": request_id}
-
-def main():
-    parser = argparse.ArgumentParser(description="SimTag EPC Bridge for Emacs.")
-    parser.add_argument("--port-file", type=str, required=True, help="File to write the dynamic port number to.")
-    parser.add_argument("--data-directory", type=str, required=True, help="Path to the org-supertag data directory.")
-    parser.add_argument("--port", type=int, default=0, help="Port for the Python EPC server. 0 means dynamic.")
-    parser.add_argument("--profile", action='store_true', help="Enable profiling.")
-    args = parser.parse_args()
-
-    # Initialize the bridge. This will also initialize the context and all services.
-    bridge = SimTagBridge(
-        port_file=args.port_file,
-        data_directory=args.data_directory,
-        server_port=args.port
-    )
-
-    # Profiling setup
-    profiler = None
-    if args.profile:
-        import cProfile
         profiler = cProfile.Profile()
         
         def save_profile():
-            profile_dir = os.path.join(args.data_directory, "profiles")
-            if not os.path.exists(profile_dir):
-                os.makedirs(profile_dir)
-            timestamp = time.strftime("%Y%m%d-%H%M%S")
-            profile_file = os.path.join(profile_dir, f"simtag_bridge_{timestamp}.prof")
-            profiler.dump_stats(profile_file)
-            logging.getLogger("simtag_bridge").info(f"Profiling data saved to {profile_file}")
+            profiler.dump_stats("simtag_bridge.prof")
+            print("Profiling data saved.", file=sys.stderr)
+
+        atexit.register(save_profile)
 
         def profiled_run():
             try:
                 profiler.enable()
-                bridge.shutdown_event.wait()
+                main()
             finally:
                 profiler.disable()
-                save_profile()
         
-        profiled_thread = threading.Thread(target=profiled_run, daemon=True)
-        profiled_thread.start()
+        profiled_run()
     else:
-        bridge.shutdown_event.wait()
-
-    logging.getLogger("simtag_bridge").info("Shutdown event received. Exiting main thread.")
-
-if __name__ == '__main__':
-    main() 
+        main() 
\ No newline at end of file
diff --git a/simtag/tag_generator.py b/simtag/tag_generator.py
new file mode 100644
index 0000000..58ef0cd
--- /dev/null
+++ b/simtag/tag_generator.py
@@ -0,0 +1,286 @@
+"""
+Tag Generator Module
+Responsible for generating tag suggestions from text content
+"""
+
+import re
+import logging
+from typing import List, Dict, Any, Optional
+import traceback
+
+# Global singleton instance
+_generator_instance = None
+
+def suggest_tags(text: str, limit: int = 5) -> List[str]:
+    """Extract tags from text.
+    
+    Args:
+        text: The text to analyze
+        limit: The maximum number of tags to return (default 5)
+        
+    Returns:
+        A list of tags
+    """
+    global _generator_instance
+    
+    if _generator_instance is None:
+        _generator_instance = TagGenerator(None)
+    
+    return _generator_instance.suggest_tags(text)  # Don't pass limit parameter
+
+class TagGenerator:
+    """Tag generator class"""
+
+    def __init__(self, ollama_bridge):
+        """Initialize tag generator
+        
+        Args:
+            ollama_bridge: LLM interface object
+        """
+        self.logger = logging.getLogger("simtag.tag_generator")
+        self.ollama = ollama_bridge
+        
+    def suggest_tags(self, text: str, limit: int = 5) -> List[str]:
+        """Generate tag suggestions"""
+        try:
+            self.logger.debug(f"Starting tag generation, text length: {len(text)}")
+            
+            if not text or len(text.strip()) == 0:
+                self.logger.warning("The input text is empty or only contains whitespace")
+                return []
+            
+            try:
+                if isinstance(text, str):
+                    text_bytes = text.encode('utf-8')
+                    text = text_bytes.decode('utf-8')
+            except UnicodeError as e:
+                self.logger.warning(f"Text encoding issue: {e}")
+                try:
+                    if isinstance(text, str):
+                        text = text.encode('utf-8', errors='replace').decode('utf-8')
+                except Exception as e:
+                    self.logger.error(f"I can't fix the text encoding: {e}")
+                    return []
+            
+            preview = text[:100] + "..." if len(text) > 100 else text
+            self.logger.debug(f"Preview: {preview}")
+            
+            if not self.ollama:
+                self.logger.error("Ollama client not initialized")
+                return []
+
+            cleaned_text = text.strip()
+            if len(cleaned_text) < 10:
+                self.logger.warning(f"The text is too short: '{cleaned_text}'")
+                if cleaned_text:
+                    return [cleaned_text]
+                return []
+
+            prompt = f"""Extract 5 significant tags from the following text:
+
+TEXT START
+{cleaned_text}
+TEXT END
+
+Return ONLY a comma-separated list of tags, with no explanations or other text.
+Example format: tag1, tag2, tag3, tag4, tag5"""
+
+            # Add system variable definition
+            system = """You are a tag generation expert. Your task is to generate relevant tags for the given text.
+Guidelines:
+1. Each tag should be concise and accurate
+2. Tags should reflect the main topics and concepts in the text
+3. Return ONLY a comma-separated list of tags
+4. Do not include any explanations or other text in your response"""
+
+            # Try to call Ollama directly
+            self.logger.debug("Preparing to call Ollama API...")
+            
+            # Check Ollama status and add additional tests
+            try:
+                # Test using ollama command line directly
+                import subprocess
+                result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
+                self.logger.debug(f"Ollama command line test: {result.stdout.strip()}")
+                
+                # Check API availability
+                import requests
+                test_response = requests.get("http://127.0.0.1:11434/api/tags")
+                self.logger.debug(f"Ollama API test: {test_response.status_code}")
+                
+                # Check object status
+                if hasattr(self.ollama, 'status'):
+                    status = self.ollama.status()
+                    self.logger.debug(f"Ollama status: {status}")
+            except Exception as e:
+                self.logger.error(f"Failed to check Ollama status: {e}")
+            
+            # Call Ollama
+            try:
+                self.logger.debug("Calling Ollama to generate tags")
+                
+                # Log request details
+                self.logger.debug(f"System prompt: {system}")
+                # Avoid logging long prompts, only log first 200 and last 100 characters
+                if len(prompt) > 300:
+                    self.logger.debug(f"User prompt (truncated): {prompt[:200]}...{prompt[-100:]}")
+                else:
+                    self.logger.debug(f"User prompt: {prompt}")
+                self.logger.info(f"User prompt length: {len(prompt)} characters")
+                
+                # Verify if text is actually included in prompt
+                text_in_prompt = "TEXT START" in prompt and "TEXT END" in prompt
+                self.logger.debug(f"Text correctly included in prompt: {text_in_prompt}")
+                
+                # Call Ollama and log details
+                self.logger.debug(f"Starting Ollama.run(), prompt length: {len(prompt)}")
+                response = self.ollama.run(prompt, system=system)
+                self.logger.debug(f"Ollama.run() completed, response length: {len(response) if response else 0}")
+                
+                # Log raw response
+                raw_response = response if response else "No response"
+                if len(raw_response) > 200:
+                    self.logger.debug(f"Ollama raw response (truncated): {raw_response[:200]}...")
+                else:
+                    self.logger.debug(f"Ollama raw response: '{raw_response}'")
+                self.logger.info(f"Received Ollama response, length: {len(raw_response)}")
+                
+                # Identify and handle special response cases
+                lower_response = raw_response.lower() if raw_response else ""
+                special_phrases = [
+                    "please provide", "i need", "please give", 
+                    "the text is empty", "no text provided", "i don't see any text",
+                    "cannot generate", "unable to generate"
+                ]
+                
+                is_error_response = any(phrase in lower_response for phrase in special_phrases)
+                if is_error_response:
+                    self.logger.warning(f"Ollama returned error response: '{raw_response}'")
+                    
+                    # Try again with clearer instructions
+                    self.logger.info("Attempting with backup prompt...")
+                    
+                    # Backup prompt is simpler and more direct
+                    backup_prompt = f"Generate 5 tags for this text: {cleaned_text[:1000]}"
+                    self.logger.debug(f"Backup prompt: {backup_prompt[:200]}...")
+                    
+                    try:
+                        backup_response = self.ollama.run(backup_prompt, system=system)
+                        if backup_response and not any(phrase in backup_response.lower() for phrase in special_phrases):
+                            self.logger.info(f"Backup request successful: '{backup_response}'")
+                            response = backup_response
+                        else:
+                            self.logger.warning("Backup request also failed")
+                            return []
+                    except Exception as e:
+                        self.logger.error(f"Backup request failed: {e}")
+                        return []
+                    
+                # If still no valid response, return empty list
+                if not response:
+                    self.logger.warning("Ollama returned empty response")
+                    return []
+                
+                # Enhanced tag extraction logic
+                # First try direct comma separation
+                raw_tags = [tag.strip() for tag in response.split(',')]
+                
+                # If only one element, response format might be incorrect
+                if len(raw_tags) <= 1:
+                    self.logger.warning("Response format may be incorrect, trying alternative splitting methods")
+                    
+                    # First try splitting by newlines
+                    line_tags = []
+                    for line in response.split('\n'):
+                        line = line.strip()
+                        # Skip empty lines and obvious non-tag lines
+                        if not line or len(line) > 100:
+                            continue
+                        # Check if it's a list item (starts with number or hyphen)
+                        if line.startswith(('-', '*', '1.', '2.', '3.', '4.', '5.')):
+                            # Remove list markers
+                            line = line.lstrip('-*0123456789. ')
+                        # If line has commas, might be multiple tags
+                        if ',' in line:
+                            line_tags.extend([t.strip() for t in line.split(',')])
+                        else:
+                            line_tags.append(line)
+                    
+                    if line_tags:
+                        self.logger.debug(f"Extracted {len(line_tags)} tags using newline splitting")
+                        raw_tags = line_tags
+                    else:
+                        # If newline splitting also failed, try more aggressive splitting
+                        # Look for possible tag patterns like quoted content or content after colons
+                        potential_tags = re.findall(r'"([^"]+)"|\'([^\']+)\'|:\s*([^,\n]+)', response)
+                        
+                        extracted_tags = []
+                        for tag_tuple in potential_tags:
+                            # findall returns tuples, get non-empty value
+                            tag = next((t for t in tag_tuple if t), None)
+                            if tag:
+                                extracted_tags.append(tag.strip())
+                        
+                        if extracted_tags:
+                            self.logger.debug(f"Extracted {len(extracted_tags)} tags using regex")
+                            raw_tags = extracted_tags
+                        elif raw_tags[0]:  # If there's only one non-empty element
+                            # Use the original single element as the only tag
+                            self.logger.debug(f"Using original response as single tag: {raw_tags[0]}")
+                        else:
+                            self.logger.warning("Unable to extract tags from response")
+                            return []
+                
+                self.logger.debug(f"Raw tag list ({len(raw_tags)}): {raw_tags}")
+                
+                # Clean and filter tags
+                valid_tags = []
+                for tag in raw_tags:
+                    # Skip obviously invalid tags
+                    if not tag or len(tag) > 50:
+                        self.logger.debug(f"Skipping invalid tag: '{tag}'")
+                        continue
+                        
+                    # Normalize tag (lowercase, remove extra spaces and punctuation)
+                    clean_tag = tag.strip().lower()
+                    # Remove quotes and brackets
+                    clean_tag = re.sub(r'[\'"`\(\)\[\]\{\}]', '', clean_tag)
+                    # Remove leading numbers and dots
+                    clean_tag = re.sub(r'^[\d\.\-\s]+', '', clean_tag)
+                    # Remove backslash characters
+                    clean_tag = clean_tag.replace('\\', '')
+                    # Replace internal spaces with underscores
+                    clean_tag = re.sub(r'\s+', '_', clean_tag)
+                    # No longer truncate tag length
+                    
+                    if clean_tag and clean_tag not in valid_tags:
+                        valid_tags.append(clean_tag)
+                    else:
+                        self.logger.debug(f"Ignoring duplicate or empty tag: '{tag}'")
+                
+                # Ensure we have tags
+                if not valid_tags:
+                    self.logger.warning("No valid tags extracted")
+                    # If unable to extract tags, try using first line as tag
+                    first_line = response.split('\n')[0].strip()
+                    if first_line and len(first_line) <= 50:
+                        self.logger.info(f"Using first line of response as tag: '{first_line}'")
+                        valid_tags = [first_line.lower()]
+                    else:
+                        return []
+                
+                if limit and valid_tags:
+                    valid_tags = valid_tags[:limit]
+                    
+                self.logger.info(f"Final valid tags ({len(valid_tags)}): {valid_tags}")
+                return valid_tags
+                
+            except Exception as e:
+                self.logger.error(f"Ollama call failed: {e}")
+                self.logger.error(traceback.format_exc())
+                return []
+                
+        except Exception as e:
+            self.logger.error(f"Tag generation process failed: {e}")
+            self.logger.error(traceback.format_exc())
+            return []  # Return empty list instead of raising exception
\ No newline at end of file
diff --git a/simtag/tag_relation_analyzer.py b/simtag/tag_relation_analyzer.py
new file mode 100644
index 0000000..f76a89e
--- /dev/null
+++ b/simtag/tag_relation_analyzer.py
@@ -0,0 +1,138 @@
+"""
+SimTag tag relation analysis module
+Analyzes semantic relationships between tags
+"""
+
+import logging
+from typing import List, Dict, Any, Optional
+
+# Predefined relation types
+RELATIONS = {
+    "CONTRAST": "Comparison or contrast relationship",
+    "RELATE": "General association relationship",
+    "INFLUENCE": "Influence relationship",
+    "CONTAIN": "Containment relationship (parent)",
+    "BELONG": "Subordinate relationship (child)",
+    "PARALLEL": "Parallel relationship",
+    "DEPENDENCY": "Dependency relationship",
+    "CAUSE": "Causal relationship (cause)",
+    "EFFECT": "Causal relationship (effect)",
+    "COOCCURRENCE": "Co-occurrence relationship"
+}
+
+class TagRelationAnalyzer:
+    """Tag relation analyzer"""
+    
+    def __init__(self, ollama_bridge: Any = None):
+        """Initialize tag relation analyzer
+        
+        Args:
+            ollama_bridge: Ollama bridge object for relation analysis
+        """
+        self.logger = logging.getLogger("simtag.tag_relation")
+        self.ollama = ollama_bridge
+        
+    def analyze_relations(self, tag: str, tags: List[str]) -> List[Dict[str, Any]]:
+        """Analyze relationships between tags
+        
+        Args:
+            tag: Target tag
+            tags: List of tags to analyze
+            
+        Returns:
+            List of tag relationships, each containing:
+                - tag: Related tag
+                - relation: Relation type
+                - reason: Relation explanation
+        """
+        if not self.ollama:
+            self.logger.error("No Ollama instance provided, cannot analyze tag relations")
+            return []
+            
+        system = """You are a tag relationship analyzer. Your task is to determine the relationship between two tags.
+
+Available relationship types:
+CONTRAST   - Tags represent contrasting or comparable concepts
+RELATE     - Tags have a general association
+INFLUENCE  - First tag has significant impact on second tag
+CONTAIN    - First tag is a broader category that includes second tag
+BELONG     - First tag is a subset or member of second tag
+PARALLEL   - Tags represent similar-level concepts
+DEPENDENCY - First tag requires or depends on second tag
+CAUSE      - First tag leads to or causes second tag
+EFFECT     - First tag is a result of second tag
+COOCCURRENCE - Tags commonly appear together
+
+Response format requirements:
+1. Use EXACTLY this format (including newline):
+   RELATION: <TYPE>
+   REASON: <brief explanation>
+2. Choose only ONE relationship type from the list above
+3. Provide a clear, concise reason (1-2 sentences)
+4. Use technical language when appropriate
+5. Be specific about the relationship direction
+
+Example response:
+RELATION: BELONG
+REASON: Python is a specific programming language, making it a subset of programming.
+
+DO NOT include any other text or explanations."""
+
+        results = []
+        for related_tag in tags:
+            prompt = f"""How is "{related_tag}" related to "{tag}"?
+
+Choose ONE relationship type and explain why.
+Use EXACTLY this format (including newline):
+RELATION: <TYPE>
+REASON: <explanation>"""
+
+            try:
+                response = self.ollama.run(prompt, system)
+                response = response.strip()
+                
+                # Validate response format
+                if not ('\nREASON:' in response and response.startswith('RELATION:')):
+                    self.logger.warning(f"Invalid response format for tag '{related_tag}', retrying...")
+                    response = self.ollama.run(prompt, system)
+                    if not ('\nREASON:' in response and response.startswith('RELATION:')):
+                        self.logger.warning(f"Still invalid format after retry, skipping tag '{related_tag}'")
+                        continue
+                
+                # Parse response
+                lines = response.strip().split('\n')
+                relation_line = next(line for line in lines if line.startswith('RELATION:'))
+                reason_line = next(line for line in lines if line.startswith('REASON:'))
+                
+                relation = relation_line.split(':', 1)[1].strip().upper()
+                reason = reason_line.split(':', 1)[1].strip()
+                
+                # Validate relation type
+                if relation not in RELATIONS:
+                    self.logger.warning(f"Invalid relation type '{relation}' for tag '{related_tag}'")
+                    continue
+                
+                results.append({
+                    'tag': related_tag,
+                    'relation': relation.lower(),
+                    'reason': reason
+                })
+                
+            except Exception as e:
+                self.logger.error(f"Error parsing response for tag '{related_tag}': {e}")
+                self.logger.debug(f"Raw response:\n{response}")
+                continue
+                
+        return results
+
+# Global singleton instance
+_analyzer_instance = None
+
+def analyze_tag_relations(tag: str, tags: List[str]) -> List[Dict[str, Any]]:
+    """Global function for analyzing tag relations, called by EPC server"""
+    global _analyzer_instance
+    
+    if _analyzer_instance is None:
+        _analyzer_instance = TagRelationAnalyzer()
+        
+    return _analyzer_instance.analyze_relations(tag, tags)
\ No newline at end of file
diff --git a/simtag/tag_vectors.py b/simtag/tag_vectors.py
new file mode 100644
index 0000000..a707222
--- /dev/null
+++ b/simtag/tag_vectors.py
@@ -0,0 +1,870 @@
+"""
+SimTag tag vector processing module
+Provides functions for generating, storing, and querying tag vectors
+"""
+
+import os
+import json
+import logging
+import traceback
+import re
+import time
+from datetime import datetime
+from typing import List, Dict, Any, Tuple, Optional
+import numpy as np
+from sentence_transformers import SentenceTransformer
+import torch
+
+class TagVectorEngine:
+    """Tag vector engine class"""
+    
+    def __init__(self, vector_file: str = None):
+        """Initializes the tag vector engine
+        
+        Args:
+            vector_file: Path to the vector file
+        """
+        self.logger = logging.getLogger("simtag.tag_vectors")
+        self.vector_file = vector_file
+        self.tag_vectors = {}  # Tag vector dictionary
+        self.is_initialized = False
+        self.model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+        self._model = None
+        
+        if vector_file and os.path.exists(vector_file):
+            self.load_vectors(vector_file)
+            
+    @property
+    def model(self):
+        """Lazily loads the model"""
+        if self._model is None:
+            self.logger.info(f"loading model: {self.model_name}")
+            self._model = SentenceTransformer(
+                self.model_name,
+                cache_folder=os.path.join(os.path.dirname(__file__), 'models')
+            )
+            # Set device
+            device = self._get_device()
+            if device.type != 'cpu':
+                self._model = self._model.to(device)
+                self.logger.info(f"enabled {device.type.upper()} acceleration")
+                
+        return self._model
+        
+    def _get_device(self):
+        """Gets the best available device"""
+        if torch.cuda.is_available():
+            return torch.device('cuda')
+        elif torch.backends.mps.is_available():
+            return torch.device('mps')
+        return torch.device('cpu')
+        
+    def status(self) -> Dict[str, Any]:
+        """Gets the engine status
+        
+        Returns:
+            Status information dictionary
+        """
+        return {
+            "initialized": self.is_initialized,
+            "vector_file": self.vector_file,
+            "vector_count": len(self.tag_vectors),
+            "file_exists": os.path.exists(self.vector_file) if self.vector_file else False,
+            "file_size": os.path.getsize(self.vector_file) if self.vector_file and os.path.exists(self.vector_file) else 0
+        }
+            
+    def load_vectors(self, vector_file: str) -> bool:
+        """Loads tag vectors
+        
+        Args:
+            vector_file: Path to the vector file
+            
+        Returns:
+            True if loaded successfully, False otherwise
+        """
+        try:
+            if not os.path.exists(vector_file):
+                self.logger.error(f"vector file not found: {vector_file}")
+                return False
+                
+            self.logger.info(f"loading vector file: {vector_file}")
+            with open(vector_file, 'r') as f:
+                data = json.load(f)
+                
+            if not isinstance(data, dict) or 'tags' not in data:
+                self.logger.error(f"invalid vector file format")
+                return False
+                
+            # Update vectors
+            self.tag_vectors = {
+                tag_id: np.array(info['vector']) 
+                for tag_id, info in data['tags'].items()
+            }
+            
+            self.vector_file = vector_file
+            self.is_initialized = True
+            self.logger.info(f"successfully loaded {len(self.tag_vectors)} tag vectors")
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"error loading vector file: {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+            
+    def find_similar(self, tag_name: str, top_k: int = 5) -> List[Tuple[str, float]]:
+        """Finds tags similar to the given tag
+        
+        Args:
+            tag_name: Tag name
+            top_k: Number of results to return
+            
+        Returns:
+            List of similar tags, each element is (tag_name, similarity_score)
+        """
+        self.logger.info(f"finding similar tags: tag={tag_name}, top_k={top_k}")
+        
+        # Check vector file
+        if not self.is_initialized:
+            if not self.vector_file or not os.path.exists(self.vector_file):
+                self.logger.error("vector file not specified or not found")
+                return []
+            if not self.load_vectors(self.vector_file):
+                self.logger.error("unable to load vector file")
+                return []
+        
+        # Check tag vector dictionary
+        if not self.tag_vectors:
+            self.logger.error("no available tag vectors")
+            return []
+            
+        # Get target tag vector
+        if tag_name not in self.tag_vectors:
+            try:
+                self.logger.info(f"tag '{tag_name}' not in vector library, generating vector...")
+                target_vector = self.model.encode(tag_name)
+                self.logger.info(f"vector generated successfully, dimension: {target_vector.shape}")
+            except Exception as e:
+                self.logger.error(f"error generating vector: {e}")
+                return []
+        else:
+            target_vector = self.tag_vectors[tag_name]
+            self.logger.info(f"tag '{tag_name}' vector already exists")
+            
+        # Calculate similarity
+        start_time = time.time()
+        similarities = []
+        self.logger.info(f"calculating similarity with {len(self.tag_vectors)} tags...")
+        
+        for other_tag, other_vector in self.tag_vectors.items():
+            if other_tag != tag_name:
+                try:
+                    # Calculate cosine similarity
+                    sim = self._compute_similarity(target_vector, other_vector)
+                    similarities.append((other_tag, sim))
+                except Exception as e:
+                    self.logger.error(f"error calculating similarity with tag '{other_tag}': {e}")
+                    continue
+        
+        # Sort by similarity
+        similarities.sort(key=lambda x: x[1], reverse=True)
+        
+        # Return top_k results
+        results = similarities[:top_k]
+        
+        elapsed = time.time() - start_time
+        self.logger.info(f"similarity calculation completed, time: {elapsed:.2f} seconds, found {len(results)} similar tags")
+        
+        return results
+        
+    def _compute_similarity(self, vec1, vec2) -> float:
+        """Computes the similarity between two vectors
+        
+        Args:
+            vec1: The first vector
+            vec2: The second vector
+            
+        Returns:
+            Similarity score
+        """
+        try:
+            # Ensure vectors are numpy arrays
+            if not isinstance(vec1, np.ndarray):
+                vec1 = np.array(vec1)
+            if not isinstance(vec2, np.ndarray):
+                vec2 = np.array(vec2)
+            
+            # Ensure vectors are 2D
+            if len(vec1.shape) == 1:
+                vec1 = vec1.reshape(1, -1)
+            if len(vec2.shape) == 1:
+                vec2 = vec2.reshape(1, -1)
+            
+            # Calculate cosine similarity
+            sim = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
+            return float(sim[0][0])  # Ensure to return Python native float
+        except Exception as e:
+            self.logger.error(f"error calculating similarity: {e}")
+            self.logger.error(traceback.format_exc())
+            raise
+            
+    def test_engine(self, test_text: str) -> np.ndarray:
+        """Tests if the engine is working correctly
+        
+        Args:
+            test_text: Test text
+            
+        Returns:
+            Generated vector
+            
+        Raises:
+            Exception: If the engine is not working correctly
+        """
+        self.logger.info("testing engine functionality...")
+        
+        # Ensure the model is loaded
+        if not self.model:
+            raise RuntimeError("text similarity model not loaded")
+            
+        try:
+            # Generate vector
+            vector = self.model.encode(test_text)
+            
+            # Verify vector
+            if not isinstance(vector, np.ndarray):
+                raise TypeError(f"vector type error: {type(vector)}")
+                
+            if vector.shape[0] != 384:  # MiniLM-L6 dimension
+                raise ValueError(f"vector dimension error: {vector.shape}")
+                
+            self.logger.info("engine test successful")
+            return vector.tolist()  # Convert to list for serialization
+            
+        except Exception as e:
+            self.logger.error(f"engine test failed: {e}")
+            self.logger.error(traceback.format_exc())
+            raise
+
+    def initialize(self, db_file: str, vector_file: str, tag_data: List[Dict] = None) -> Dict[str, Any]:
+        """Initializes the tag library
+        
+        Args:
+            db_file: Path to the database file
+            vector_file: Output path for the vector file
+            tag_data: List of tag data
+            
+        Returns:
+            Initialization result information
+        """
+        try:
+            # Ensure parameter types are correct
+            if not isinstance(db_file, str):
+                self.logger.error(f"db_file 参数类型错误: {type(db_file)}")
+                return {
+                    "status": "error",
+                    "message": f"数据库文件路径必须是字符串，而不是 {type(db_file)}",
+                    "result": None
+                }
+                
+            if not isinstance(vector_file, str):
+                self.logger.error(f"vector_file 参数类型错误: {type(vector_file)}")
+                return {
+                    "status": "error",
+                    "message": f"vector file path must be a string, not {type(vector_file)}",
+                    "result": None
+                }
+            
+            # Log initialization parameters
+            self.logger.info(f"initializing tag library:")
+            self.logger.info(f"- database file: {db_file}")
+            self.logger.info(f"- vector file: {vector_file}")
+            self.logger.info(f"- tag data: {len(tag_data) if tag_data else 'none'}")
+            
+            self.vector_file = vector_file
+            
+            # Test the engine first
+            try:
+                test_vector = self.test_engine("test sentence for initialization")
+                self.logger.info("engine test successful")
+            except Exception as e:
+                return {
+                    "status": "error",
+                    "message": f"engine test failed: {e}",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+
+            # First ensure the model is loaded successfully
+            if not self.model:
+                return {
+                    "status": "error",
+                    "message": "unable to load text similarity model",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+            
+            # Test if the model can work normally
+            try:
+                test_text = "test sentence for model verification"
+                test_vector = self.model.encode(test_text)
+                if not isinstance(test_vector, np.ndarray) or test_vector.shape[0] != 384:  # MiniLM-L6 dimension
+                    raise ValueError(f"model output vector dimension error: {test_vector.shape}")
+            except Exception as e:
+                self.logger.error(f"model functionality test failed: {e}")
+                return {
+                    "status": "error",
+                    "message": f"model functionality test failed: {e}",
+                    "result": {
+                        "vector_file": vector_file,
+                        "db_file": db_file,
+                        "model": None
+                    }
+                }
+
+            # Ensure the output directory exists
+            os.makedirs(os.path.dirname(vector_file), exist_ok=True)
+            
+            # Parse tag data
+            tag_info = {}
+            if tag_data:
+                for tag_dict in tag_data:
+                    # Handle tag data in different formats
+                    if isinstance(tag_dict, dict):
+                        tag_data_dict = tag_dict
+                    elif isinstance(tag_dict, list):
+                        # Convert list to dictionary
+                        tag_data_dict = {}
+                        for item in tag_dict:
+                            if isinstance(item, (list, tuple)) and len(item) == 2:
+                                key, value = item
+                                # Handle Symbol('.') and other special cases
+                                if hasattr(value, '__name__') and value.__name__ == '.':
+                                    # Skip Symbol('.') entries
+                                    continue
+                                if isinstance(value, list) and len(value) == 1:
+                                    value = value[0]
+                                tag_data_dict[key] = value
+                            elif isinstance(item, (list, tuple)) and len(item) == 3:
+                                # Handle triplet format like ['id', Symbol('.'), 'c_maker']
+                                key, symbol, value = item
+                                if hasattr(symbol, '__name__') and symbol.__name__ == '.':
+                                    # This is a valid triplet with Symbol('.') separator
+                                    tag_data_dict[key] = value
+                                else:
+                                    self.logger.warning(f"Unknown triplet format: {item}")
+                        
+                        # If we couldn't convert the list, skip it
+                        if not tag_data_dict:
+                            self.logger.warning(f"skipping invalid tag data: {tag_dict}")
+                            continue
+                    else:
+                        self.logger.warning(f"skipping invalid tag data: {tag_dict}")
+                        continue
+                        
+                    # Extract tag ID
+                    tag_id = tag_data_dict.get('id') or tag_data_dict.get('name')
+                    if not tag_id:
+                        continue
+                        
+                    # If tag_id is a list, take the first element
+                    if isinstance(tag_id, list):
+                        tag_id = tag_id[0]
+                        
+                    # Process fields
+                    fields = self._process_fields(tag_data_dict.get('fields', []))
+                    
+                    # Process behaviors
+                    behaviors = self._process_behaviors(tag_data_dict.get('behaviors', []))
+                        
+                    # Process relations
+                    relations = self._process_relations(tag_data_dict.get('relations', []))
+                        
+                    # Store tag information
+                    tag_info[tag_id] = {
+                        'name': tag_id,
+                        'type': 'tag',
+                        'fields': fields,
+                        'behaviors': behaviors,
+                        'relations': relations,
+                    }
+            else:
+                # Parse from database file
+                tag_info = self.parse_supertag_db(db_file)
+            
+            self.logger.info(f"processed {len(tag_info)} tags")
+            
+            if not tag_info:
+                self.logger.error("no valid tag data found")
+                return {"status": "error", "message": "no valid tag data found"}
+            
+            # Generate tag vectors
+            tag_vectors = {}
+            for tag_id, info in tag_info.items():
+                try:
+                    # Generate vector
+                    tag_vector = self.model.encode(tag_id)
+                    tag_vectors[tag_id] = tag_vector
+                except Exception as e:
+                    self.logger.error(f"error generating tag '{tag_id}' vector: {e}")
+                    continue
+            
+            # Build cache data
+            cache_data = {
+                'tags': {
+                    tag_id: {
+                        'name': info['name'],
+                        'vector': tag_vectors[tag_id].tolist() if tag_id in tag_vectors else [],
+                        'info': info
+                    }
+                    for tag_id, info in tag_info.items() if tag_id in tag_vectors
+                },
+                'metadata': {
+                    'total_tags': len(tag_vectors),
+                    'vector_dim': 384,  # MiniLM-L6 dimension
+                    'created_at': datetime.now().isoformat(),
+                    'model_name': 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+                }
+            }
+            
+            # Save to file
+            with open(vector_file, 'w') as f:
+                json.dump(cache_data, f, indent=2)
+                
+            self.logger.info(f"tag library initialized, saved to {vector_file}")
+            self.logger.info(f"file size: {os.path.getsize(vector_file)} bytes")
+            
+            # Update status
+            self.tag_vectors = {
+                tag_id: np.array(vector) for tag_id, vector in tag_vectors.items()
+            }
+            self.is_initialized = True
+            
+            return {
+                "status": "success",
+                "result": {
+                    "vector_file": vector_file,
+                    "db_file": db_file,
+                    "model": 'sentence-transformers/paraphrase-MiniLM-L6-v2',
+                    "tag_count": len(tag_vectors),
+                    "file_size": os.path.getsize(vector_file)
+                }
+            }
+            
+        except Exception as e:
+            self.logger.error(f"error initializing tag library: {e}")
+            self.logger.error(traceback.format_exc())
+            return {
+                "status": "error", 
+                "message": str(e),
+                "result": {
+                    "vector_file": vector_file,
+                    "db_file": db_file,
+                    "model": 'sentence-transformers/paraphrase-MiniLM-L6-v2'
+                }
+            }
+    
+    def _process_fields(self, raw_fields):
+        """Processes field data"""
+        fields = []
+        if raw_fields:
+            for field in raw_fields:
+                if isinstance(field, dict):
+                    fields.append(field)
+                elif isinstance(field, (list, tuple)):
+                    field_dict = {}
+                    for i in range(0, len(field), 2):
+                        if i + 1 < len(field):
+                            key = field[i]
+                            value = field[i + 1]
+                            if key == 'name':
+                                field_dict['name'] = value
+                            elif key == 'type':
+                                field_dict['type'] = value
+                            elif key == 'description':
+                                field_dict['description'] = value
+                            elif key == 'options':
+                                if isinstance(value, list):
+                                    field_dict['options'] = value
+                    if field_dict:
+                        fields.append(field_dict)
+        return fields
+    
+    def _process_behaviors(self, raw_behaviors):
+        """Processes behavior data"""
+        behaviors = []
+        if isinstance(raw_behaviors, dict):
+            behaviors = list(raw_behaviors.keys())
+        elif isinstance(raw_behaviors, list):
+            behaviors = [b for b in raw_behaviors if b]
+        return behaviors
+    
+    def _process_relations(self, raw_relations):
+        """Processes relation data"""
+        relations = []
+        if isinstance(raw_relations, list):
+            relations = [r for r in raw_relations if r]
+        return relations
+            
+    def parse_supertag_db(self, db_file_path: str) -> Dict[str, Dict]:
+        """Parses the supertag-db.el file to extract tag information
+        
+        Args:
+            db_file_path: Path to the database file
+            
+        Returns:
+            Dictionary of tag information
+        """
+        self.logger.info(f"parsing database file: {db_file_path}")
+        tag_info = {}
+        
+        try:
+            with open(db_file_path) as f:
+                content = f.read()
+                
+            # Extract tag definitions
+            tag_pattern = r'\(ht-set!\s+org-supertag-db--object\s+"([^"]+)"\s+\'(\(:type\s+:tag.*?\))\)'
+            for match in re.finditer(tag_pattern, content, re.DOTALL):
+                tag_id = match.group(1)
+                tag_props = match.group(2)
+                
+                # Skip metadata
+                if tag_id == "metadata":
+                    continue
+                    
+                # Extract field definitions
+                fields = []
+                fields_match = re.search(r':fields\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if fields_match and fields_match.group(1) != 'nil':
+                    field_str = fields_match.group(1)
+                    # Parse field list
+                    field_pattern = r'\(([^)]+)\)'
+                    for field_match in re.finditer(field_pattern, field_str):
+                        field_def = field_match.group(1)
+                        field_parts = field_def.strip().split()
+                        if len(field_parts) >= 2:
+                            field = {
+                                'name': field_parts[0].strip(':'),
+                                'type': field_parts[1].strip(':')
+                            }
+                            if len(field_parts) > 2:
+                                field['description'] = ' '.join(field_parts[2:]).strip('"')
+                            fields.append(field)
+                
+                # Extract behavior definitions
+                behaviors = []
+                behaviors_match = re.search(r':behaviors\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if behaviors_match and behaviors_match.group(1) != 'nil':
+                    behavior_str = behaviors_match.group(1).strip('()')
+                    behaviors = [b.strip('"') for b in behavior_str.split()]
+                
+                # Extract relation definitions
+                relations = []
+                relations_match = re.search(r':relations\s+(\(.*?\)|nil)(?=\s+:|$)', tag_props, re.DOTALL)
+                if relations_match and relations_match.group(1) != 'nil':
+                    relation_str = relations_match.group(1).strip('()')
+                    relations = [r.strip('"') for r in relation_str.split()]
+                
+                # Store tag information
+                tag_info[tag_id] = {
+                    'name': tag_id,
+                    'type': 'tag',
+                    'fields': fields,
+                    'behaviors': behaviors,
+                    'relations': relations
+                }
+            
+            self.logger.info(f"found {len(tag_info)} tag definitions")
+            return tag_info
+            
+        except Exception as e:
+            self.logger.error(f"error parsing database file: {e}")
+            self.logger.error(traceback.format_exc())
+            return {}
+
+    def add_tag(self, tag_id: str, tag_name: str = None) -> bool:
+        """Add a new tag to the vector library
+        
+        Args:
+            tag_id: Tag ID
+            tag_name: Tag name (if different from ID)
+            
+        Returns:
+            True if added successfully, False otherwise
+        """
+        try:
+            # Use tag_name if provided, otherwise use tag_id
+            text_to_encode = tag_name or tag_id
+            
+            self.logger.info(f"Adding tag vector: {tag_id} -> '{text_to_encode}'")
+            
+            # Generate vector
+            tag_vector = self.model.encode(text_to_encode)
+            
+            # Add to memory
+            self.tag_vectors[tag_id] = tag_vector
+            
+            # Save to file
+            self._save_vectors()
+            
+            self.logger.info(f"Successfully added tag: {tag_id}")
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"Error adding tag '{tag_id}': {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+    
+    def update_tag(self, tag_id: str, tag_name: str = None) -> bool:
+        """Update an existing tag in the vector library
+        
+        Args:
+            tag_id: Tag ID
+            tag_name: Tag name (if different from ID)
+            
+        Returns:
+            True if updated successfully, False otherwise
+        """
+        try:
+            # Use tag_name if provided, otherwise use tag_id
+            text_to_encode = tag_name or tag_id
+            
+            self.logger.info(f"Updating tag vector: {tag_id} -> '{text_to_encode}'")
+            
+            # Generate new vector
+            tag_vector = self.model.encode(text_to_encode)
+            
+            # Update in memory
+            self.tag_vectors[tag_id] = tag_vector
+            
+            # Save to file
+            self._save_vectors()
+            
+            self.logger.info(f"Successfully updated tag: {tag_id}")
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"Error updating tag '{tag_id}': {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+    
+    def remove_tag(self, tag_id: str) -> bool:
+        """Remove a tag from the vector library
+        
+        Args:
+            tag_id: Tag ID to remove
+            
+        Returns:
+            True if removed successfully, False otherwise
+        """
+        try:
+            self.logger.info(f"Removing tag vector: {tag_id}")
+            
+            # Remove from memory
+            if tag_id in self.tag_vectors:
+                del self.tag_vectors[tag_id]
+                
+                # Save to file
+                self._save_vectors()
+                
+                self.logger.info(f"Successfully removed tag: {tag_id}")
+                return True
+            else:
+                self.logger.warning(f"Tag '{tag_id}' not found in vector library")
+                return False
+                
+        except Exception as e:
+            self.logger.error(f"Error removing tag '{tag_id}': {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+    
+    def _save_vectors(self) -> bool:
+        """Save current vectors to file
+        
+        Returns:
+            True if saved successfully, False otherwise
+        """
+        try:
+            if not self.vector_file:
+                self.logger.error("No vector file specified")
+                return False
+            
+            # Build cache data
+            cache_data = {
+                'tags': {
+                    tag_id: {
+                        'name': tag_id,
+                        'vector': vector.tolist(),
+                        'info': {'name': tag_id, 'type': 'tag'}
+                    }
+                    for tag_id, vector in self.tag_vectors.items()
+                },
+                'metadata': {
+                    'total_tags': len(self.tag_vectors),
+                    'vector_dim': 384,  # MiniLM-L6 dimension
+                    'updated_at': datetime.now().isoformat(),
+                    'model_name': self.model_name
+                }
+            }
+            
+            # Ensure directory exists
+            os.makedirs(os.path.dirname(self.vector_file), exist_ok=True)
+            
+            # Save to file
+            with open(self.vector_file, 'w') as f:
+                json.dump(cache_data, f, indent=2)
+                
+            self.logger.info(f"Vectors saved to {self.vector_file}")
+            return True
+            
+        except Exception as e:
+            self.logger.error(f"Error saving vectors: {e}")
+            self.logger.error(traceback.format_exc())
+            return False
+
+    def sync_from_tags(self, tag_data: List[Dict[str, str]]) -> Dict[str, Any]:
+        """Synchronize the vector library with the provided tag data
+        
+        Args:
+            tag_data: List of dictionaries, each containing tag information
+                     Expected format: [{"id": tag_id, "name": tag_name}, ...]
+                     
+        Returns:
+            Dictionary with synchronization results
+        """
+        try:
+            self.logger.info(f"Starting synchronization with {len(tag_data)} tags")
+            
+            # Parse tag data
+            processed_tags = {}
+            for tag_dict in tag_data:
+                # Handle tag data in different formats  
+                if isinstance(tag_dict, dict):
+                    tag_data_dict = tag_dict
+                elif isinstance(tag_dict, list):
+                    # Convert list to dictionary
+                    tag_data_dict = {}
+                    for item in tag_dict:
+                        if isinstance(item, (list, tuple)) and len(item) == 2:
+                            key, value = item
+                            # Handle Symbol('.') and other special cases
+                            if hasattr(value, '__name__') and value.__name__ == '.':
+                                # Skip Symbol('.') entries
+                                continue
+                            if isinstance(value, list) and len(value) == 1:
+                                value = value[0]
+                            tag_data_dict[key] = value
+                        elif isinstance(item, (list, tuple)) and len(item) == 3:
+                            # Handle triplet format like ['id', Symbol('.'), 'c_maker']
+                            key, symbol, value = item
+                            if hasattr(symbol, '__name__') and symbol.__name__ == '.':
+                                # This is a valid triplet with Symbol('.') separator
+                                tag_data_dict[key] = value
+                            else:
+                                self.logger.warning(f"Unknown triplet format: {item}")
+                    
+                    # If we couldn't convert the list, skip it
+                    if not tag_data_dict:
+                        self.logger.warning(f"Skipping invalid tag data: {tag_dict}")
+                        continue
+                        
+                    tag_dict = tag_data_dict
+                else:
+                    self.logger.warning(f"Skipping invalid tag data: {tag_dict}")
+                    continue
+                
+                # Extract tag ID and name
+                tag_id = tag_dict.get('id') or tag_dict.get('name')
+                tag_name = tag_dict.get('name') or tag_id
+                
+                if not tag_id:
+                    self.logger.warning(f"Skipping tag without ID: {tag_dict}")
+                    continue
+                
+                # If tag_id is a list, take the first element
+                if isinstance(tag_id, list):
+                    tag_id = tag_id[0]
+                if isinstance(tag_name, list):
+                    tag_name = tag_name[0]
+                
+                processed_tags[tag_id] = tag_name
+            
+            self.logger.info(f"Processed {len(processed_tags)} valid tags")
+            
+            if not processed_tags:
+                self.logger.warning("No valid tags to synchronize")
+                return {
+                    "status": "success",
+                    "added": 0,
+                    "updated": 0,
+                    "removed": 0,
+                    "total": 0
+                }
+            
+            # Track statistics
+            added_count = 0
+            updated_count = 0
+            
+            # Process each tag
+            for tag_id, tag_name in processed_tags.items():
+                try:
+                    if tag_id in self.tag_vectors:
+                        # Update existing tag
+                        if self.update_tag(tag_id, tag_name):
+                            updated_count += 1
+                            self.logger.debug(f"Updated tag: {tag_id}")
+                    else:
+                        # Add new tag
+                        if self.add_tag(tag_id, tag_name):
+                            added_count += 1
+                            self.logger.debug(f"Added tag: {tag_id}")
+                        
+                except Exception as e:
+                    self.logger.error(f"Error processing tag '{tag_id}': {e}")
+                    continue
+            
+            # Remove tags that are no longer in the sync data
+            current_tags = set(self.tag_vectors.keys())
+            new_tags = set(processed_tags.keys())
+            tags_to_remove = current_tags - new_tags
+            
+            removed_count = 0
+            for tag_id in tags_to_remove:
+                try:
+                    if self.remove_tag(tag_id):
+                        removed_count += 1
+                        self.logger.debug(f"Removed tag: {tag_id}")
+                except Exception as e:
+                    self.logger.error(f"Error removing tag '{tag_id}': {e}")
+                    continue
+            
+            # Final save (in case individual operations didn't save)
+            if added_count > 0 or updated_count > 0 or removed_count > 0:
+                self._save_vectors()
+            
+            result = {
+                "status": "success",
+                "added": added_count,
+                "updated": updated_count,
+                "removed": removed_count,
+                "total": len(self.tag_vectors)
+            }
+            
+            self.logger.info(f"Synchronization completed: {result}")
+            return result
+            
+        except Exception as e:
+            self.logger.error(f"Error during synchronization: {e}")
+            self.logger.error(traceback.format_exc())
+            return {
+                "status": "error",
+                "message": str(e),
+                "added": 0,
+                "updated": 0,
+                "removed": 0,
+                "total": len(self.tag_vectors) if hasattr(self, 'tag_vectors') else 0
+            }
\ No newline at end of file
diff --git a/simtag/unlock_database.py b/simtag/unlock_database.py
new file mode 100755
index 0000000..ab903af
--- /dev/null
+++ b/simtag/unlock_database.py
@@ -0,0 +1,172 @@
+#!/usr/bin/env python3
+"""
+Database Unlock Utility for SimTag
+
+This script helps unlock a SQLite database that may be locked due to:
+- Crashed processes that didn't properly close connections
+- WAL files left behind from previous sessions
+- Multiple processes accessing the same database
+
+Usage:
+    python unlock_database.py [database_path]
+
+If no database path is provided, it will look for the default database
+in the data directory.
+"""
+
+import os
+import sys
+import sqlite3
+import argparse
+from pathlib import Path
+
+
+def find_default_database():
+    """Find the default database path."""
+    # Look for common database locations
+    possible_paths = [
+        "../data/org_supertag.db",
+        "data/org_supertag.db", 
+        os.path.expanduser("~/.emacs.d/org-supertag/org_supertag.db"),
+        os.path.expanduser("~/Documents/emacs/package/org-supertag/data/org_supertag.db")
+    ]
+    
+    for path in possible_paths:
+        if os.path.exists(path):
+            return path
+    
+    return None
+
+
+def check_database_status(db_path):
+    """Check if database is accessible."""
+    try:
+        conn = sqlite3.connect(db_path, timeout=5.0)
+        conn.execute("SELECT 1").fetchone()
+        conn.close()
+        return True, "Database is accessible"
+    except sqlite3.OperationalError as e:
+        return False, str(e)
+    except Exception as e:
+        return False, f"Unexpected error: {e}"
+
+
+def force_unlock_database(db_path):
+    """Force unlock database by removing WAL and SHM files."""
+    print(f"Attempting to unlock database: {db_path}")
+    
+    # Check if database file exists
+    if not os.path.exists(db_path):
+        print(f"❌ Database file not found: {db_path}")
+        return False
+    
+    # Check current status
+    accessible, status = check_database_status(db_path)
+    if accessible:
+        print(f"✅ Database is already accessible: {status}")
+        return True
+    
+    print(f"⚠️  Database issue detected: {status}")
+    
+    # Look for WAL and SHM files
+    wal_file = db_path + "-wal"
+    shm_file = db_path + "-shm"
+    
+    files_removed = []
+    
+    try:
+        if os.path.exists(wal_file):
+            os.remove(wal_file)
+            files_removed.append("WAL")
+            print(f"🗑️  Removed WAL file: {wal_file}")
+            
+        if os.path.exists(shm_file):
+            os.remove(shm_file)
+            files_removed.append("SHM")
+            print(f"🗑️  Removed SHM file: {shm_file}")
+            
+        if not files_removed:
+            print("ℹ️  No WAL/SHM files found to remove")
+            
+        # Test database access again
+        accessible, status = check_database_status(db_path)
+        if accessible:
+            print(f"✅ Database unlocked successfully: {status}")
+            return True
+        else:
+            print(f"❌ Database still not accessible: {status}")
+            return False
+            
+    except Exception as e:
+        print(f"❌ Error during unlock process: {e}")
+        return False
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Unlock SQLite database for SimTag",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+    python unlock_database.py
+    python unlock_database.py /path/to/database.db
+    python unlock_database.py --check-only /path/to/database.db
+        """
+    )
+    
+    parser.add_argument(
+        'database_path', 
+        nargs='?', 
+        help='Path to the SQLite database file'
+    )
+    
+    parser.add_argument(
+        '--check-only', 
+        action='store_true',
+        help='Only check database status, do not attempt to unlock'
+    )
+    
+    args = parser.parse_args()
+    
+    # Determine database path
+    db_path = args.database_path
+    if not db_path:
+        db_path = find_default_database()
+        if not db_path:
+            print("❌ No database path provided and no default database found.")
+            print("Please specify the database path:")
+            print("    python unlock_database.py /path/to/database.db")
+            return 1
+        else:
+            print(f"ℹ️  Using default database: {db_path}")
+    
+    # Check if database exists
+    if not os.path.exists(db_path):
+        print(f"❌ Database file not found: {db_path}")
+        return 1
+    
+    print(f"🔍 Checking database: {db_path}")
+    
+    # Check current status
+    accessible, status = check_database_status(db_path)
+    print(f"Status: {status}")
+    
+    if args.check_only:
+        return 0 if accessible else 1
+    
+    if accessible:
+        print("✅ Database is already accessible, no action needed.")
+        return 0
+    
+    # Attempt to unlock
+    if "database is locked" in status.lower():
+        success = force_unlock_database(db_path)
+        return 0 if success else 1
+    else:
+        print(f"❌ Database issue is not lock-related: {status}")
+        print("This tool can only help with database lock issues.")
+        return 1
+
+
+if __name__ == "__main__":
+    sys.exit(main()) 
\ No newline at end of file
diff --git a/simtag/utils/__init__.py b/simtag/utils/__init__.py
old mode 100755
new mode 100644
index 48e8283..87d1b42
--- a/simtag/utils/__init__.py
+++ b/simtag/utils/__init__.py
@@ -1,39 +1,77 @@
 """
+<<<<<<< HEAD
+SimTag Toolkit
+Provides logging and serialization utilities
+"""
+
+from .logging import setup_logging, get_logger
+from .serialization import (
+    to_serializable,
+    serialize_to_json,
+    save_to_json_file,
+    load_from_json_file,
+    normalize_response
+)
+
+__all__ = [
+    'setup_logging',
+    'get_logger',
+    'to_serializable',
+    'serialize_to_json',
+    'save_to_json_file',
+    'load_from_json_file',
+    'normalize_response'
+]
+
+"""SimTag Toolkit Module"""
+=======
 SimTag Utils Package
 ===================
 
-General utility package providing cross-component functionality
+通用工具包，提供跨组件使用的通用功能
 """
 
-# Export unified tag processor related classes
+# 导出统一标签处理器相关类
 from .unified_tag_processor import (
+    UnifiedTagProcessor,
     TagResult,
     NoteResult,
     BatchResult
 )
 
-# Export unified Prompt management
+# 导出统一 Prompt 管理
 from ..prompts import (
+    create_prompt,
+    log_prompt_usage,
     DEFAULT_ENTITY_TYPES
 )
 
-# Export other existing utilities
+# 导出其他现有工具
 try:
     from .utils import *
 except ImportError:
     pass
 
+try:
+    from .logging import *
+except ImportError:
+    pass
+
 try:
     from .serialization import *
 except ImportError:
     pass
 
 __all__ = [
-    # Unified tag processor
+    # 统一标签处理器
+    'UnifiedTagProcessor',
     'TagResult', 
     'NoteResult',
     'BatchResult',
     
-    # Unified Prompt management
+    # 统一 Prompt 管理
+    'create_prompt',
+    'log_prompt_usage',
     'DEFAULT_ENTITY_TYPES',
-] 
\ No newline at end of file
+] 
+>>>>>>> a6a9089 (opt org-supertag-background-sync.el, make a correct incremental embedding workflow.)
diff --git a/simtag/utils/__pycache__/__init__.cpython-313.pyc b/simtag/utils/__pycache__/__init__.cpython-313.pyc
new file mode 100644
index 0000000..e23ab95
Binary files /dev/null and b/simtag/utils/__pycache__/__init__.cpython-313.pyc differ
diff --git a/simtag/utils/__pycache__/logging.cpython-313.pyc b/simtag/utils/__pycache__/logging.cpython-313.pyc
new file mode 100644
index 0000000..a302507
Binary files /dev/null and b/simtag/utils/__pycache__/logging.cpython-313.pyc differ
diff --git a/simtag/utils/__pycache__/serialization.cpython-313.pyc b/simtag/utils/__pycache__/serialization.cpython-313.pyc
new file mode 100644
index 0000000..db1b6ef
Binary files /dev/null and b/simtag/utils/__pycache__/serialization.cpython-313.pyc differ
diff --git a/simtag/utils/dimension_manager.py b/simtag/utils/dimension_manager.py
new file mode 100755
index 0000000..20ff1cc
--- /dev/null
+++ b/simtag/utils/dimension_manager.py
@@ -0,0 +1,352 @@
+#!/usr/bin/env python3
+"""
+Vector Dimension Management Tool
+Automatically detects and resolves vector dimension mismatch issues
+"""
+
+import sqlite3
+import sqlite_vec
+import os
+import asyncio
+import shutil
+from typing import Dict, Any, Optional, List, Tuple
+from simtag.config import Config
+from simtag.services.embedding_service import get_embedding_service
+
+class DimensionManager:
+    """Vector Dimension Manager"""
+    
+    def __init__(self):
+        self.config = Config()
+        self.db_path = self.config.vector_db_path
+        self.embedding_service = get_embedding_service()
+    
+    def get_database_info(self) -> Dict[str, Any]:
+        """Get database information"""
+        if not os.path.exists(self.db_path):
+            return {"exists": False, "error": "Database file does not exist"}
+        
+        try:
+            # Enable extension loading
+            conn = sqlite3.connect(self.db_path)
+            conn.enable_load_extension(True)
+            sqlite_vec.load(conn)
+            cursor = conn.cursor()
+            
+            info = {"exists": True, "tables": {}}
+            
+            # Check vector tables
+            for table_name in ["node_embeddings_vss", "tag_embeddings_vss"]:
+                try:
+                    # Get table structure information
+                    cursor.execute(f"PRAGMA table_info({table_name})")
+                    table_info = cursor.fetchall()
+                    
+                    if table_info:
+                        # Get data count
+                        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
+                        count = cursor.fetchone()[0]
+                        
+                        # Try to get a sample vector to detect dimension
+                        dimension = None
+                        if count > 0:
+                            cursor.execute(f"SELECT embedding FROM {table_name} LIMIT 1")
+                            row = cursor.fetchone()
+                            if row and row[0]:
+                                if isinstance(row[0], (bytes, str)):
+                                    try:
+                                        import json
+                                        import numpy as np
+                                        if isinstance(row[0], bytes):
+                                            # sqlite-vec C extension returns bytes
+                                            vector = np.frombuffer(row[0], dtype=np.float32)
+                                        else:
+                                            # JSON string format
+                                            vector = json.loads(row[0])
+                                        dimension = len(vector)
+                                    except:
+                                        dimension = "unknown format"
+                        
+                        info["tables"][table_name] = {
+                            "exists": True,
+                            "count": count,
+                            "dimension": dimension,
+                            "structure": table_info
+                        }
+                    else:
+                        info["tables"][table_name] = {"exists": False}
+                        
+                except Exception as e:
+                    info["tables"][table_name] = {"exists": False, "error": str(e)}
+            
+            conn.close()
+            return info
+            
+        except Exception as e:
+            return {"exists": True, "error": f"Cannot access database: {e}"}
+    
+    async def detect_model_dimension(self) -> Optional[int]:
+        """Detect current embedding model dimension"""
+        try:
+            print("🔍 Detecting current embedding model dimension...")
+            test_text = "Test text"
+            result = await self.embedding_service.get_embedding(test_text)
+            
+            if result.success and result.embedding:
+                dimension = len(result.embedding)
+                print(f"✅ Current model dimension: {dimension}")
+                return dimension
+            else:
+                print(f"❌ Cannot get embedding vector: {result.error_message}")
+                return None
+                
+        except Exception as e:
+            print(f"❌ Failed to detect model dimension: {e}")
+            return None
+    
+    def check_dimension_compatibility(self) -> Dict[str, Any]:
+        """Check dimension compatibility"""
+        print("🔍 Checking vector dimension compatibility...")
+        
+        db_info = self.get_database_info()
+        
+        if not db_info.get("exists"):
+            return {
+                "compatible": False,
+                "issue": "database_missing",
+                "message": "Database file does not exist",
+                "action": "create_database"
+            }
+        
+        if "error" in db_info:
+            return {
+                "compatible": False,
+                "issue": "database_error",
+                "message": db_info["error"],
+                "action": "fix_database"
+            }
+        
+        # Check dimensions of each table
+        dimensions = {}
+        for table_name, table_info in db_info.get("tables", {}).items():
+            if table_info.get("exists") and table_info.get("dimension"):
+                dimensions[table_name] = table_info["dimension"]
+        
+        if not dimensions:
+            return {
+                "compatible": True,
+                "issue": "no_data",
+                "message": "Database is empty, can be used directly",
+                "action": "none"
+            }
+        
+        # Check if dimensions are consistent
+        unique_dimensions = set(d for d in dimensions.values() if isinstance(d, int))
+        
+        if len(unique_dimensions) > 1:
+            return {
+                "compatible": False,
+                "issue": "inconsistent_dimensions",
+                "message": f"Table dimensions are inconsistent: {dimensions}",
+                "action": "recreate_tables",
+                "dimensions": dimensions
+            }
+        
+        return {
+            "compatible": True,
+            "current_dimension": list(unique_dimensions)[0] if unique_dimensions else None,
+            "tables": dimensions,
+            "message": "Dimension check passed"
+        }
+    
+    async def auto_fix_dimension_mismatch(self, target_dimension: Optional[int] = None) -> bool:
+        """Automatically fix dimension mismatch"""
+        print("🔧 Starting automatic dimension mismatch fix...")
+        
+        # If target dimension is not specified, detect current model dimension
+        if target_dimension is None:
+            target_dimension = await self.detect_model_dimension()
+            if target_dimension is None:
+                print("❌ Cannot detect model dimension, fix failed")
+                return False
+        
+        # Check compatibility
+        compatibility = self.check_dimension_compatibility()
+        
+        if compatibility["compatible"]:
+            current_dim = compatibility.get("current_dimension")
+            if current_dim and current_dim != target_dimension:
+                print(f"⚠️ Database dimension({current_dim}) does not match model dimension({target_dimension})")
+            else:
+                print("✅ Dimensions are already compatible, no fix needed")
+                return True
+        
+        # Execute fix
+        return await self._recreate_tables_with_dimension(target_dimension)
+    
+    async def _recreate_tables_with_dimension(self, dimension: int) -> bool:
+        """Recreate tables to support specified dimension"""
+        try:
+            print(f"🔧 Recreating tables to support {dimension}-dimensional vectors...")
+            
+            # Backup database
+            backup_path = f"{self.db_path}.backup_{int(asyncio.get_event_loop().time())}"
+            shutil.copy2(self.db_path, backup_path)
+            print(f"✅ Database backed up to: {backup_path}")
+            
+            # Connect to database
+            conn = sqlite3.connect(self.db_path)
+            conn.enable_load_extension(True)
+            sqlite_vec.load(conn)
+            cursor = conn.cursor()
+            
+            # Check existing data
+            existing_data = {}
+            for table_name in ["node_embeddings_vss", "tag_embeddings_vss"]:
+                try:
+                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
+                    count = cursor.fetchone()[0]
+                    existing_data[table_name] = count
+                except:
+                    existing_data[table_name] = 0
+            
+            total_existing = sum(existing_data.values())
+            if total_existing > 0:
+                print(f"⚠️ Warning: Will delete {total_existing} existing vector data entries")
+                response = input("Confirm to continue? (y/N): ")
+                if response.lower() != 'y':
+                    print("❌ Operation cancelled by user")
+                    return False
+            
+            # Delete old tables
+            cursor.execute("DROP TABLE IF EXISTS node_embeddings_vss")
+            cursor.execute("DROP TABLE IF EXISTS tag_embeddings_vss")
+            print("🗑️ Old vector tables deleted")
+            
+            # Create new tables
+            cursor.execute(f"""
+            CREATE VIRTUAL TABLE node_embeddings_vss USING vec0(
+                embedding FLOAT[{dimension}],
+                node_id_ref TEXT HIDDEN
+            )
+            """)
+            
+            cursor.execute(f"""
+            CREATE VIRTUAL TABLE tag_embeddings_vss USING vec0(
+                embedding FLOAT[{dimension}],
+                tag_id_ref TEXT HIDDEN
+            )
+            """)
+            
+            conn.commit()
+            conn.close()
+            
+            print(f"✅ Successfully created {dimension}-dimensional vector tables")
+            return True
+            
+        except Exception as e:
+            print(f"❌ Failed to recreate tables: {e}")
+            return False
+    
+    def get_recovery_suggestions(self, compatibility_result: Dict[str, Any]) -> List[str]:
+        """Get recovery suggestions"""
+        suggestions = []
+        
+        issue = compatibility_result.get("issue")
+        
+        if issue == "database_missing":
+            suggestions.extend([
+                "🔧 Create new vector database",
+                "📝 Run org-supertag initialization command",
+                "🔄 Regenerate all embedding vectors"
+            ])
+        
+        elif issue == "database_error":
+            suggestions.extend([
+                "🔧 Check sqlite-vec extension installation",
+                "📁 Check database file permissions",
+                "🔄 Reinstall sqlite-vec package"
+            ])
+        
+        elif issue == "inconsistent_dimensions":
+            suggestions.extend([
+                "🔧 Run automatic dimension repair tool",
+                "🗑️ Clean and recreate vector tables",
+                "🔄 Regenerate all embedding vectors",
+                "💾 Ensure consistent embedding model usage"
+            ])
+        
+        else:
+            suggestions.append("✅ Configuration is normal")
+        
+        return suggestions
+
+async def main():
+    """Main function"""
+    print("=" * 60)
+    print("🔧 Vector Dimension Management Tool")
+    print("=" * 60)
+    
+    manager = DimensionManager()
+    
+    # 1. Check current status
+    print("🔍 Checking current status...")
+    compatibility = manager.check_dimension_compatibility()
+    
+    print(f"\n📊 Compatibility check results:")
+    print(f"   Status: {'✅ Compatible' if compatibility['compatible'] else '❌ Incompatible'}")
+    print(f"   Message: {compatibility['message']}")
+    
+    if not compatibility["compatible"]:
+        print(f"   Issue type: {compatibility.get('issue', 'Unknown')}")
+    
+    # 2. Detect model dimension
+    model_dimension = await manager.detect_model_dimension()
+    
+    if model_dimension:
+        current_db_dim = compatibility.get("current_dimension")
+        if current_db_dim and current_db_dim != model_dimension:
+            print(f"⚠️ Dimension mismatch: Database({current_db_dim}) vs Model({model_dimension})")
+    
+    # 3. Get suggestions
+    suggestions = manager.get_recovery_suggestions(compatibility)
+    print(f"\n💡 Suggested actions:")
+    for suggestion in suggestions:
+        print(f"   {suggestion}")
+    
+    # 4. Interactive repair
+    if not compatibility["compatible"] or (model_dimension and compatibility.get("current_dimension") != model_dimension):
+        print(f"\n🔧 Repair options:")
+        print("   1. Auto-fix dimension mismatch")
+        print("   2. Show detailed information only")
+        print("   3. Exit")
+        
+        choice = input("\nSelect option (1-3): ").strip()
+        
+        if choice == "1":
+            success = await manager.auto_fix_dimension_mismatch(model_dimension)
+            if success:
+                print("✅ Dimension repair completed!")
+                print("📝 Suggestion: Regenerate all embedding vectors")
+                print("💡 Run: M-x org-supertag-sync-all")
+            else:
+                print("❌ Dimension repair failed")
+        
+        elif choice == "2":
+            # Show detailed information
+            db_info = manager.get_database_info()
+            print(f"\n📋 Detailed database information:")
+            for table_name, table_info in db_info.get("tables", {}).items():
+                print(f"   {table_name}:")
+                print(f"     Exists: {table_info.get('exists', False)}")
+                print(f"     Count: {table_info.get('count', 0)}")
+                print(f"     Dimension: {table_info.get('dimension', 'Unknown')}")
+        
+        else:
+            print("👋 Exiting")
+    
+    else:
+        print("✅ Current configuration is normal, no repair needed")
+
+if __name__ == "__main__":
+    asyncio.run(main())
\ No newline at end of file
diff --git a/simtag/utils/logging.py b/simtag/utils/logging.py
new file mode 100644
index 0000000..fa5854d
--- /dev/null
+++ b/simtag/utils/logging.py
@@ -0,0 +1,52 @@
+"""
+SimTag Logging Utility Module
+Provides unified logging functionality
+"""
+
+import os
+import sys
+import logging
+from typing import Dict, Any, Optional
+
+def setup_logging(log_file: Optional[str], log_level: int = logging.INFO):
+    """Set up logging configuration
+    
+    Args:
+        log_file: Log file path, if None, only output to console
+        log_level: Log level
+    """
+    # Basic configuration
+    logging.basicConfig(
+        level=log_level,
+        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+        datefmt='%Y-%m-%d %H:%M:%S'
+    )
+    
+    # If a log file is specified, add a file handler
+    if log_file:
+        # Ensure the log directory exists
+        log_dir = os.path.dirname(log_file)
+        if log_dir and not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+            
+        # Create a file handler
+        file_handler = logging.FileHandler(log_file)
+        file_handler.setLevel(log_level)
+        file_handler.setFormatter(logging.Formatter(
+            '%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+            '%Y-%m-%d %H:%M:%S'
+        ))
+        
+        # Add to the root logger
+        logging.getLogger().addHandler(file_handler)
+
+def get_logger(name: str) -> logging.Logger:
+    """Get a logger with the specified name
+    
+    Args:
+        name: Logger name
+        
+    Returns:
+        Named logger
+    """
+    return logging.getLogger(name) 
\ No newline at end of file
diff --git a/simtag/utils/serialization.py b/simtag/utils/serialization.py
old mode 100755
new mode 100644
index cf59492..66936d4
--- a/simtag/utils/serialization.py
+++ b/simtag/utils/serialization.py
@@ -5,7 +5,7 @@ Provides data serialization and deserialization functionality
 
 import json
 import numpy as np
-from typing import Any, Dict
+from typing import Any, Dict, List, Union
 from datetime import datetime
 
 def to_serializable(obj: Any) -> Any:
diff --git a/simtag/utils/text_processing.py b/simtag/utils/text_processing.py
deleted file mode 100644
index 40bf2ec..0000000
--- a/simtag/utils/text_processing.py
+++ /dev/null
@@ -1,202 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-import logging
-from typing import Dict, Any
-
-logger = logging.getLogger(__name__)
-
-def prepare_node_text_for_embedding(node_data: Dict[str, Any], max_total_tokens: int = 450) -> str:
-    """
-    Prepare vectorization text for org-supertag node
-    
-    Strategy: Title + first N characters of content, fully utilizing org-mode's structural advantages
-    Specifically optimized for LlamaCpp's 512 token limit
-    
-    Args:
-        node_data: Node data dictionary
-        max_total_tokens: Maximum total tokens, default 280 (leaving safety margin)
-        
-    Returns:
-        str: Prepared text
-    """
-    text_parts = []
-    used_tokens = 0
-    
-    # 1. Prioritize adding title (most important semantic information)
-    title = (node_data.get('title') or node_data.get(':title') or node_data.get('name') or '').strip()
-    if title:
-        title_text = f"Title: {title}"
-        title_tokens = _estimate_tokens(title_text)
-        if used_tokens + title_tokens < max_total_tokens:
-            text_parts.append(title_text)
-            used_tokens += title_tokens
-    
-    # 2. Add key context (if space allows)
-    olp = node_data.get('olp') or node_data.get(':olp', [])
-    if olp and len(olp) > 0 and used_tokens < max_total_tokens * 0.5:  # Only add when token usage < 50%
-        context = " > ".join(olp[-2:])  # Only take the last 2 levels of parent titles
-        context_text = f"Context: {context}"
-        context_tokens = _estimate_tokens(context_text)
-        if used_tokens + context_tokens < max_total_tokens * 0.7:  # Context not exceeding 70% of total
-            text_parts.append(context_text)
-            used_tokens += context_tokens
-    
-    # 3. Add tags (if space allows and tag count is reasonable)
-    tags = node_data.get('tags') or node_data.get(':tags', [])
-    if tags and len(tags) <= 5 and used_tokens < max_total_tokens * 0.6:
-        tags_text = f"Tags: {', '.join(tags[:5])}"  # Maximum 5 tags
-        tags_tokens = _estimate_tokens(tags_text)
-        if used_tokens + tags_tokens < max_total_tokens * 0.8:  # Tags not exceeding 80% of total
-            text_parts.append(tags_text)
-            used_tokens += tags_tokens
-    
-    # 4. Add content with remaining space
-    content = (node_data.get('content') or node_data.get(':content') or '').strip()
-    if content:
-        remaining_tokens = max_total_tokens - used_tokens - 20  # Reserve 20 token safety boundary
-        if remaining_tokens > 50:  # Need at least 50 tokens to add content
-            # Dynamically calculate content character limit based on remaining tokens
-            max_content_chars = int(remaining_tokens / 1.2)  # Conservative estimate
-            truncated_content = _smart_truncate(content, max_content_chars)
-            content_text = f"Content: {truncated_content}"
-            text_parts.append(content_text)
-    
-    final_text = " | ".join(text_parts)
-    
-    # Final check: ensure not exceeding limit
-    final_tokens = _estimate_tokens(final_text)
-    if final_tokens > max_total_tokens:
-        # If still exceeding, prioritize title and small amount of content
-        title_only = (node_data.get('title') or node_data.get(':title') or node_data.get('name') or '').strip()
-        content = (node_data.get('content') or node_data.get(':content') or '').strip()
-        if content:
-            remaining_chars = max_total_tokens * 0.6  # About 60% for content
-            truncated = _smart_truncate(content, int(remaining_chars))
-            final_text = f"Title: {title_only} | Content: {truncated}"
-        else:
-            final_text = f"Title: {title_only}"
-    
-    return final_text
-
-def _estimate_tokens(text: str) -> float:
-    """
-    Practical token count estimation
-    
-    Optimized based on actual testing, balancing accuracy and safety
-    
-    Args:
-        text: Input text
-        
-    Returns:
-        float: Estimated token count
-    """
-    # Adjusted based on actual observations: Chinese characters ~1.2 tokens, English words ~0.8 tokens
-    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
-    other_chars = len(text) - chinese_chars
-    
-    # More realistic estimation
-    estimated = chinese_chars * 1.5 + other_chars * 0.8
-    
-    # Moderate safety boundary (10%)
-    return estimated * 1.1
-
-def _smart_truncate(text: str, max_chars: int) -> str:
-    """
-    Intelligently truncate text at sentence boundaries
-    
-    Args:
-        text: Original text
-        max_chars: Maximum character count
-        
-    Returns:
-        str: Truncated text
-    """
-    if len(text) <= max_chars:
-        return text
-    
-    # Look forward from maximum length to find sentence boundaries
-    truncated = text[:max_chars]
-    
-    # Look for sentence ending markers like periods, exclamation marks, question marks
-    sentence_endings = ['。', '！', '？', '.', '!', '?', '\n\n']
-    
-    best_cut = 0
-    for ending in sentence_endings:
-        pos = truncated.rfind(ending)
-        if pos > max_chars * 0.7:  # Ensure truncation position is not too far forward
-            best_cut = max(best_cut, pos + 1)
-    
-    if best_cut > 0:
-        return text[:best_cut].strip()
-    else:
-        # If no suitable sentence boundary found, truncate at word boundary
-        return text[:max_chars]
-
-def generate_semantic_id(node_data: Dict[str, Any]) -> str:
-    """
-    Generate semantic identifier for node, facilitating RAG retrieval
-    
-    Format: uuid:title_summary or uuid:content_summary
-    This maintains uniqueness while providing semantic information
-    
-    Args:
-        node_data: Node data dictionary
-        
-    Returns:
-        str: Semantic identifier
-    """
-    # Get original UUID
-    node_id = node_data.get('node_id') or node_data.get('id', '')
-    
-    # Get summary of title or content
-    title = (node_data.get('title') or node_data.get(':title') or node_data.get('name') or '').strip()
-    content = (node_data.get('content') or node_data.get(':content') or '').strip()
-    
-    # Generate summary text
-    summary = ""
-    if title:
-        # Use title as summary (length limited)
-        summary = _smart_truncate(title, 50)
-    elif content:
-        # If no title, use first 50 characters of content as summary
-        summary = _smart_truncate(content, 50)
-    
-    # Clean special characters from summary to ensure it can be used as identifier
-    if summary:
-        summary = summary.replace('\n', ' ').replace('\r', ' ').replace('|', '-')
-        summary = ''.join(c for c in summary if c.isprintable()).strip()
-    
-    # Generate composite identifier
-    if summary:
-        return f"{node_id}:{summary}"
-    else:
-        return node_id
-
-def extract_uuid_from_semantic_id(semantic_id: str) -> str:
-    """
-    Extract original UUID from semantic identifier
-    
-    Args:
-        semantic_id: Semantic identifier
-        
-    Returns:
-        str: Original UUID
-    """
-    if ':' in semantic_id:
-        return semantic_id.split(':', 1)[0]
-    return semantic_id
-
-def extract_summary_from_semantic_id(semantic_id: str) -> str:
-    """
-    Extract summary part from semantic identifier
-    
-    Args:
-        semantic_id: Semantic identifier
-        
-    Returns:
-        str: Summary part
-    """
-    if ':' in semantic_id:
-        return semantic_id.split(':', 1)[1]
-    return ""
\ No newline at end of file
diff --git a/simtag/utils/unified_tag_processor.py b/simtag/utils/unified_tag_processor.py
index 98d2e68..c014c9c 100755
--- a/simtag/utils/unified_tag_processor.py
+++ b/simtag/utils/unified_tag_processor.py
@@ -1,93 +1,10 @@
 #!/usr/bin/env python3
 """
-Unify data Processer
+统一标签处理器
 ===============
 
-Data Contract Specification
-==========================================
-
-### 1. Elisp 端数据准备规范
-
-Elisp 端必须按照以下规范准备数据，确保数据结构符合 sync_handler.py 的期望：
-
-#### 1.1 实体 (Entity) 数据格式
-```elisp
-;; 每个实体必须包含以下字段：
-'((id . "entity-id")           ; 必需：实体唯一标识
-  (type . "node")              ; 必需：实体类型 ("node" | "tag")
-  (title . "实体标题")         ; 可选：实体标题
-  (content . "实体内容")       ; 可选：实体内容
-  (properties . properties-alist) ; 可选：属性列表
-  (file_path . "/path/to/file")    ; 可选：文件路径
-  (pos . 123)                      ; 可选：位置信息
-  ;; ... 其他字段
-  )
-```
-
-#### 1.2 快照 (Snapshot) 数据格式
-```elisp
-;; 完整的同步快照数据：
-'((entities . (entity1 entity2 ...))     ; 要更新的实体列表
-  (links . (link1 link2 ...))           ; 要更新的链接列表
-  (ids_to_delete . ("id1" "id2" ...))   ; 要删除的实体ID列表
-  )
-```
-
-#### 1.3 链接 (Link) 数据格式
-```elisp
-;; 链接数据：
-'((source . "source-id")       ; 必需：源实体ID
-  (target . "target-id")       ; 必需：目标实体ID
-  (type . "REF_TO")           ; 可选：链接类型
-  ;; ... 其他属性
-  )
-```
-
-### 2. Python 端处理规范
-
-#### 2.1 normalize_payload 函数职责
-- 接收 Elisp 传输的数据 (通过 EPC 协议)
-- 将 Elisp alist 转换为 Python dict
-- 处理 Symbol 类型和特殊值 (t, nil)
-- 递归处理嵌套结构
-
-#### 2.2 期望的输出格式
-```python
-{
-    "entities": [
-        {
-            "id": "entity-id",
-            "type": "node",
-            "title": "实体标题",
-            "content": "实体内容",
-            "properties": {...},
-            # ... 其他字段
-        },
-        # ... 更多实体
-    ],
-    "links": [
-        {
-            "source": "source-id",
-            "target": "target-id",
-            "type": "REF_TO",
-            # ... 其他属性
-        },
-        # ... 更多链接
-    ],
-    "ids_to_delete": ["id1", "id2", ...]
-}
-```
-
-### 3. 数据验证规则
-
-#### 3.1 必需字段验证
-- 实体必须有 'id' 和 'type' 字段
-- 链接必须有 'source' 和 'target' 字段
-
-#### 3.2 数据类型验证
-- ID 字段必须是字符串
-- type 字段必须是 "node" 或 "tag"
-- properties 必须是字典或能转换为字典的结构
+本模块负责统一所有 Elisp <-> Python 数据传输的格式和转换逻辑。
+所有跨语言数据交互都必须遵循此文件中定义的约定。
 
 数据传输约定 (Elisp -> Python)
 ---------------------------------
@@ -96,7 +13,7 @@ Elisp 端必须按照以下规范准备数据，确保数据结构符合 sync_ha
 
 1.  **Elisp 端**:
     - 所有数据必须被构建成一个 **关联列表 (alist)**，例如 
-      `'(("entities" . entities-list) ("links" . links-list))`。
+      `'(("nodes" . nodes-list) ("config" . config-list))`。
       这是最可靠的序列化格式，必须取代 hash-table。
     - 在调用 `org-supertag-bridge` 的任何函数时，此 alist 必须被包裹在一个
       **列表中**，例如 `(list payload-alist)`。
@@ -129,7 +46,7 @@ import json
 import logging
 import re
 from typing import List, Dict, Any, Optional, Union
-from dataclasses import dataclass, field
+from dataclasses import dataclass, asdict, field
 from sexpdata import Symbol 
 
 logger = logging.getLogger(__name__)
@@ -176,329 +93,312 @@ class TagRelationData:
     rel_rules: Dict[str, Any] = field(default_factory=dict)
     rel_history: List[Dict[str, Any]] = field(default_factory=list)
 
-def _is_alist(data: list) -> bool:
-    """
-    Deterministically checks if a list has the structure of an Elisp alist.
-    An alist is a list of lists/tuples.
-    """
-    if not data:
-        return True
-    return all(isinstance(item, (list, tuple)) for item in data)
+# ====== 最终的、健壮的数据转换逻辑 (V7) ======
 
 def _parse_elisp_data(data: Any) -> Any:
     """
     Recursively and robustly parses Elisp data structures into Python equivalents.
+    V9: The truly, absolutely final version. Handles string-based keywords.
     """
-    if not isinstance(data, (list, tuple)):
+    # Base case: data is not a list.
+    if not isinstance(data, list):
         if isinstance(data, Symbol):
             val = data.value()
             if val == 't': return True
             if val == 'nil': return None
-            # Handle keywords by stripping the leading ':'
             if val.startswith(':'):
                 return val[1:]
             return val
+        
+        # THIS IS THE FINAL FIX: Handle strings that are formatted like keywords.
+        if isinstance(data, str) and data.startswith(':'):
+            return data[1:]
+        
         return data
 
+    # Recursive case: data is a list.
     if not data:
         return []
-
-    # Check if it's an alist
+    
+    # Special case: Emacs time list format
+    # Emacs time lists are typically [high low microsecs picosecs] or [high low microsecs]
+    if (len(data) in [3, 4] and 
+        all(isinstance(x, (int, float)) for x in data) and
+        len([x for x in data if isinstance(x, int) and x >= 0]) == len(data)):
+        # This looks like an Emacs time list, convert to ISO string
+        # For now, just return a string representation
+        # In the future, we could convert to actual datetime
+        return f"emacs-time:{data}"
+
+    # Case 1: #s-prefixed structure for objects or empty hash-tables
+    if isinstance(data[0], Symbol) and data[0].value() == '#s':
+        # Handles #s() -> {}
+        if len(data) == 2 and isinstance(data[1], list) and not data[1]:
+            return {}
+        # Handles #s(...) -> list of objects
+        return [_parse_elisp_data(item) for item in data if not (isinstance(item, Symbol) and item.value() == '#s')]
+
+    # Case 2: A hash-table representation
+    if isinstance(data[0], Symbol) and data[0].value() == 'hash-table':
+        return _parse_hash_table_list(data)
+
+    # Case 3: An association list (alist) - THIS IS THE KEY FIX
     if _is_alist(data):
-        # This check is to differentiate a list of alists (like nodes) from a single alist
-        if any(isinstance(item[0], (list, tuple)) for item in data if item):
-             return [_parse_elisp_data(item) for item in data]
-        
         result_dict = {}
         for item in data:
-            if not item: continue
+            if not item: continue # Skip empty lists in the alist
             
             key = _parse_elisp_data(item[0])
+            value = None
             
-            # Handle dotted pair `(key . val)`
+            # Check for dotted pair `(key . val)`
             if len(item) == 3 and isinstance(item[1], Symbol) and item[1].value() == '.':
                 value = _parse_elisp_data(item[2])
             else:
+                # Handle `(key val_part_1 val_part_2 ...)`
                 value_parts = item[1:]
-                value = _parse_elisp_data(value_parts[0]) if len(value_parts) == 1 else _parse_elisp_data(value_parts)
+                if len(value_parts) == 1:
+                    value = _parse_elisp_data(value_parts[0])
+                else:
+                    # Recursively parse the entire tail of the list as the value
+                    value = _parse_elisp_data(value_parts)
             
-            result_dict[key] = value
+            if isinstance(key, (str, int, float, bool, type(None))):
+                result_dict[key] = value
+            else:
+                logger.warning(f"Skipping unhashable key of type {type(key)} in alist: {key}")
         return result_dict
 
-    # Otherwise, it's a plain list
+    # Case 4: A plain list of items
     return [_parse_elisp_data(item) for item in data]
 
-def parse_llm_json_response(response_text: str) -> Optional[Union[Dict, List]]:
+def _is_alist(data: list) -> bool:
     """
-    Robustly parses a JSON response from an LLM, handling potential markdown fences
-    and other text noise by extracting the first valid JSON object or array.
-
-    Args:
-        response_text: The raw string output from the LLM.
-
-    Returns:
-        A parsed Python dictionary or list if successful, otherwise None.
+    Deterministically checks if a list has the structure of an Elisp alist.
+    An alist is a list of lists.
     """
-    if not response_text or not isinstance(response_text, str):
-        return None
-
-    try:
-        # Regex to find a JSON object {...} or array [...]
-        # It's important to try to find the most specific match first.
-        # A common failure is a JSON object wrapped in ```json ... ```
-        match = re.search(r'```json\s*(\{.*\}|\[.*\])\s*```', response_text, re.DOTALL)
-        if match:
-            json_str = match.group(1)
-        else:
-            # If not in a marked code block, find the first JSON object or array
-            match = re.search(r'(\{.*\}|\[.*\])', response_text, re.DOTALL)
-            if not match:
-                logger.error(f"No JSON object or array found in LLM response: {response_text}")
-                return None
-            json_str = match.group(0)
-
-        return json.loads(json_str)
-
-    except json.JSONDecodeError:
-        logger.error(f"Failed to decode extracted LLM JSON response: {json_str}")
-        return None
-    except Exception as e:
-        logger.error(f"An unexpected error occurred while parsing LLM response: {e}")
-        return None
+    if not data:
+        return True
+    return all(isinstance(item, list) for item in data)
 
+def _parse_hash_table_list(data_list: list) -> dict:
+    """Helper to parse the [Symbol('hash-table'), ..., [k,v,...]] structure."""
+    if len(data_list) < 4: return {}
 
-def validate_entity_data(entity: Dict[str, Any]) -> bool:
-    """
-    Validate entity data
-    
-    Args:
-        entity: Entity data dictionary
-        
-    Returns:
-        bool: Validation passed
-    """
-    # Required fields check
-    if not entity.get('id'):
-        logger.error(f"Entity missing required 'id' field: {entity}")
-        return False
-    
-    if not entity.get('type'):
-        logger.error(f"Entity missing required 'type' field: {entity}")
-        return False
-    
-    # Type validation
-    if entity.get('type') not in ['node', 'tag']:
-        logger.error(f"Invalid entity type '{entity.get('type')}', must be 'node' or 'tag': {entity}")
-        return False
-    
-    # ID type check
-    if not isinstance(entity.get('id'), str):
-        logger.error(f"Entity 'id' must be a string, got {type(entity.get('id'))}: {entity}")
-        return False
-    
-    return True
+    kv_list = data_list[-1]
+    if not isinstance(kv_list, list): return {}
 
-def validate_link_data(link: Dict[str, Any]) -> bool:
-    """
-    Validate link data
-    
-    Args:
-        link: Link data dictionary
+    h_table = {}
+    i = 0
+    while i < len(kv_list):
+        # 1. Parse the key
+        key = _parse_elisp_data(kv_list[i])
+        i += 1
         
-    Returns:
-        bool: Validation passed
-    """
-    # Required fields check
-    if not link.get('source'):
-        logger.error(f"Link missing required 'source' field: {link}")
-        return False
-    
-    if not link.get('target'):
-        logger.error(f"Link missing required 'target' field: {link}")
-        return False
-    
-    # Type check
-    if not isinstance(link.get('source'), str):
-        logger.error(f"Link 'source' must be a string, got {type(link.get('source'))}: {link}")
-        return False
-    
-    if not isinstance(link.get('target'), str):
-        logger.error(f"Link 'target' must be a string, got {type(link.get('target'))}: {link}")
-        return False
-    
-    return True
-
-def validate_snapshot_data(data: Dict[str, Any]) -> Dict[str, Any]:
-    """
-    Validate and clean snapshot data.
-    
-    Args:
-        data: Snapshot data dictionary
+        if i >= len(kv_list): break
+
+        # 2. Parse the value, which may be multi-part (e.g., #s <obj>)
+        val_item = kv_list[i]
+        if isinstance(val_item, Symbol) and val_item.value() == '#s':
+            if i + 1 < len(kv_list):
+                value = _parse_elisp_data([val_item, kv_list[i+1]])
+                i += 2
+            else: 
+                value = {} # Dangling #s is an empty hash-table
+                i += 1
+        else:
+            value = _parse_elisp_data(val_item)
+            i += 1
         
-    Returns:
-        Dict: Validated data, invalid data will be filtered out
-    """
-    validated_data = {
-        'entities': [],
-        'links': [],
-        'ids_to_delete': []
-    }
-    
-    # Validate entity data
-    entities = data.get('entities', [])
-    if not isinstance(entities, list):
-        logger.error(f"'entities' must be a list, got {type(entities)}")
-        entities = []
-    
-    for entity in entities:
-        if validate_entity_data(entity):
-            validated_data['entities'].append(entity)
-    
-    # Validate link data
-    links = data.get('links', [])
-    if not isinstance(links, list):
-        logger.error(f"'links' must be a list, got {type(links)}")
-        links = []
-    
-    for link in links:
-        if validate_link_data(link):
-            validated_data['links'].append(link)
-    
-    # Validate delete ID list
-    ids_to_delete = data.get('ids_to_delete', [])
-    if not isinstance(ids_to_delete, list):
-        logger.error(f"'ids_to_delete' must be a list, got {type(ids_to_delete)}")
-        ids_to_delete = []
-    
-    valid_ids = []
-    for id_val in ids_to_delete:
-        if isinstance(id_val, str) and id_val.strip():
-            valid_ids.append(id_val.strip())
+        if isinstance(key, (str, int, float, bool, type(None))):
+            h_table[key] = value
         else:
-            logger.warning(f"Invalid ID in ids_to_delete: {id_val}")
-    
-    validated_data['ids_to_delete'] = valid_ids
-    
-    logger.info(f"Validation completed: {len(validated_data['entities'])} entities, "
-                f"{len(validated_data['links'])} links, {len(validated_data['ids_to_delete'])} deletions")
-    
-    return validated_data
+            logger.warning(f"Skipping unhashable key of type {type(key)} in hash-table: {key}")
+            
+    return h_table
 
 def normalize_payload(payload: Any) -> Dict:
     """
     The single, robust entry point for normalizing any payload from Elisp.
-    It recursively unwraps tuples and lists until it finds the first
-    dictionary-like structure (an alist), then converts it to a dict.
-    
-    Enhanced with data validation and error handling.
+    It defensively handles payloads that may have had their outer list wrapper
+    stripped by the EPC layer.
     """
     logger.debug(f"normalize_payload received raw payload of type: {type(payload)}")
 
-    current_data = payload
-    # Elisp often wraps data in a single-element list or tuple
-    while isinstance(current_data, (list, tuple)) and len(current_data) == 1:
-        current_data = current_data[0]
-        logger.debug(f"Unwrapped payload, current type: {type(current_data)}")
-
-    # After unwrapping, we should have the core alist (as a list/tuple of lists/tuples)
-    if isinstance(current_data, (list, tuple)):
-        parsed_dict = _parse_elisp_data(current_data)
-        if isinstance(parsed_dict, dict):
-            logger.debug(f"Successfully parsed payload to dict: {list(parsed_dict.keys())}")
-            # Validate and clean data
-            validated_data = validate_snapshot_data(parsed_dict)
-            logger.debug("Data validation completed successfully")
-            # === Preserve additional top-level keys that are not part of the standard snapshot schema ===
-            for k, v in parsed_dict.items():
-                if k not in validated_data:
-                    validated_data[k] = v
-            return validated_data
-        else:
-            logger.error(f"Parsed data is not a dict, but {type(parsed_dict)}. Returning empty dict.")
-            return {'entities': [], 'links': [], 'ids_to_delete': []}
-    
-    if isinstance(current_data, dict):
-        # Validate and clean data
-        validated_data = validate_snapshot_data(current_data)
-        # === Preserve additional top-level keys that are not part of the standard snapshot schema ===
-        for k, v in current_data.items():
-            if k not in validated_data:
-                validated_data[k] = v
-        return validated_data
-
-    logger.error(f"Could not normalize payload. Final type was {type(current_data)}. Returning empty dict.")
-    return {'entities': [], 'links': [], 'ids_to_delete': []}
-
-# ====== UnifiedTagProcessor (restored) ======
-class UnifiedTagProcessor:
-    """Provides helper methods to clean and parse LLM tag-extraction responses into
-    structured TagResult / NoteResult objects. This class was accidentally
-    removed; restoring it ensures downstream imports function correctly."""
+    elisp_data = None
+    # The contract is that Elisp sends `(list alist)`.
+    # After EPC, this should arrive as `[alist]`.
+    if isinstance(payload, list) and len(payload) == 1:
+        # This is the ideal, correctly wrapped payload.
+        elisp_data = payload[0]
+        logger.debug(f"Payload matches contract. Extracted Elisp data structure of type: {type(elisp_data)}")
+    else:
+        # This is likely a bare alist whose wrapper was stripped by EPC.
+        # We assume the entire payload is the data structure to parse.
+        logger.warning(f"Payload wrapper missing. Assuming entire payload of type {type(payload)} is the data structure.")
+        elisp_data = payload
+
+    # --- END TEMPORARY DEBUG LOG ---
 
-    # ------------------------------------------------------------
-    # Public helpers
-    # ------------------------------------------------------------
+    try:
+        # Parse the extracted data using our robust parser.
+        parsed_data = _parse_elisp_data(elisp_data)
+        
+        if not isinstance(parsed_data, dict):
+            # This check is a safeguard. If the top-level data from elisp was not
+            # a dictionary-like structure (alist/hash-table), we wrap it to ensure
+            # handlers always receive a dictionary.
+            logger.warning(f"Parser returned a non-dictionary type ({type(parsed_data)}). Wrapping it in a default dictionary under key 'data'.")
+            return {'data': parsed_data}
+
+        logger.debug("Successfully parsed Elisp data into a Python dictionary.")
+        return parsed_data
+    except Exception as e:
+        logger.error(f"Failed to parse Elisp data structure: {e}", exc_info=True)
+        return {"error": "parsing_failed", "message": str(e)}
+
+# ====== 旧的、复杂的数据转换逻辑 (将被删除) ======
+# All old functions like _convert_simple_sexp, _convert_sexp_to_dict, 
+# normalize_autotag_payload etc. are now obsolete and removed.
+
+class UnifiedTagProcessor:
+    """
+    统一的标签处理器 - 标准化LLM响应的解析。
+    """
+    
     def clean_llm_response(self, response_str: str) -> str:
-        """Extract the first valid JSON object/array from the LLM response."""
+        """
+        Cleans the LLM response string to extract only the valid JSON part.
+        It finds the first occurrence of a JSON array or object.
+        """
+        # Regex to find a JSON object {...} or array [...]
         json_match = re.search(r'(\{.*\}|\[.*\])', response_str, re.DOTALL)
+        
         if json_match:
-            return json_match.group(0)
-
-        # Fallback – strip markdown fences
-        cleaned = response_str.strip()
-        if cleaned.startswith('```json'):
-            cleaned = cleaned[7:].strip()
-        elif cleaned.startswith('```'):
-            cleaned = cleaned[3:].strip()
-        if cleaned.endswith('```'):
-            cleaned = cleaned[:-3].strip()
-        return cleaned
+            json_str = json_match.group(0)
+            logger.debug(f"Extracted JSON string from LLM response: {json_str[:300]}...")
+            return json_str
+        else:
+            logger.warning("No JSON object or array found in the LLM response.")
+            # Fallback: try to remove markdown backticks as a last resort
+            cleaned = response_str.strip()
+            if cleaned.startswith('```json'):
+                cleaned = cleaned[7:].strip()
+            elif cleaned.startswith('```'):
+                cleaned = cleaned[3:].strip()
+            if cleaned.endswith('```'):
+                cleaned = cleaned[:-3].strip()
+            return cleaned
 
     def process_llm_response(self, response_str: str, note_ids: List[str]) -> List[NoteResult]:
-        """Parse LLM output string and return a list of NoteResult objects."""
-        results: List[NoteResult] = []
+        """
+        处理 LLM 响应并返回标准化的 NoteResult 列表。
+        
+        Args:
+            response_str: LLM 的原始响应字符串
+            note_ids: 对应的笔记ID列表
+            
+        Returns:
+            List[NoteResult]: 处理结果列表
+        """
+        import json
+        
+        results = []
+        
         try:
+            # Clean the response to get only the JSON part
             json_string = self.clean_llm_response(response_str)
-            parsed = json.loads(json_string)
-        except Exception as e:
-            logger.error(f"Failed to parse LLM response: {e}")
-            return [NoteResult(note_id=nid, tags=[], error=str(e)) for nid in note_ids]
-
-        # Helper to convert raw list/dict to TagResult list
-        def _to_tag_results(raw) -> List[TagResult]:
-            tag_list: List[TagResult] = []
-            if not isinstance(raw, list):
-                raw = [raw]
-            for item in raw:
-                try:
-                    if isinstance(item, dict):
-                        tag_name = item.get('tag_name') or item.get('name') or ''
-                        conf = float(item.get('confidence', 0.0))
-                        reasoning = item.get('reasoning', item.get('reason', ''))
-                    else:
-                        tag_name = str(item)
-                        conf = 0.5
-                        reasoning = 'Extracted as simple string'
-                    if tag_name:
-                        tag_list.append(TagResult(tag_name=tag_name, confidence=conf, reasoning=reasoning, source='llm'))
-                except Exception as ex:
-                    logger.warning(f"Failed to parse tag item {item}: {ex}")
-            return tag_list
-
-        try:
-            if isinstance(parsed, list):
-                tags = _to_tag_results(parsed)
-                results.append(NoteResult(note_id=note_ids[0] if note_ids else 'unknown', tags=tags))
-            elif isinstance(parsed, dict):
-                raw_tags = parsed.get('tags', parsed)
-                tags = _to_tag_results(raw_tags)
-                results.append(NoteResult(note_id=note_ids[0] if note_ids else 'unknown', tags=tags))
+            
+            try:
+                # Attempt to parse the cleaned JSON string
+                parsed_data = json.loads(json_string)
+            except json.JSONDecodeError as e:
+                logger.error(f"Failed to parse extracted JSON response: {e}. JSON string was: '{json_string}'")
+                # Create error results for all note_ids
+                return [NoteResult(note_id=note_id, tags=[], error=f"JSON parse error: {e}") 
+                        for note_id in note_ids]
+            
+            # Process the parsed data
+            if isinstance(parsed_data, list):
+                # Response is a list of tags
+                tags = self._parse_tag_list(parsed_data)
+                note_id = note_ids[0] if note_ids else "unknown"
+                results.append(NoteResult(note_id=note_id, tags=tags))
+                
+            elif isinstance(parsed_data, dict):
+                # Response is a dictionary, might contain tags
+                if 'tags' in parsed_data and isinstance(parsed_data['tags'], list):
+                    tags = self._parse_tag_list(parsed_data['tags'])
+                else:
+                    # Try to parse the whole dict as a single tag
+                    tags = self._parse_tag_list([parsed_data])
+                
+                note_id = note_ids[0] if note_ids else "unknown"
+                results.append(NoteResult(note_id=note_id, tags=tags))
             else:
-                logger.warning(f"Unexpected JSON root type: {type(parsed)}")
-        except Exception as err:
-            logger.error(f"Error constructing NoteResult: {err}")
-
-        # Ensure one result per note_id
+                logger.warning(f"Unexpected JSON format after parsing: {type(parsed_data)}")
+                return [NoteResult(note_id=note_id, tags=[], error="Unexpected JSON format") 
+                        for note_id in note_ids]
+                
+        except Exception as e:
+            logger.error(f"Error processing LLM response: {e}", exc_info=True)
+            return [NoteResult(note_id=note_id, tags=[], error=str(e)) for note_id in note_ids]
+        
+        # 如果结果数量不匹配 note_ids 数量，补齐空结果
         while len(results) < len(note_ids):
             results.append(NoteResult(note_id=note_ids[len(results)], tags=[]))
-        return results
\ No newline at end of file
+            
+        return results
+    
+    def _parse_tag_list(self, tag_data: List[Dict[str, Any]]) -> List[TagResult]:
+        """
+        解析标签数据列表为 TagResult 对象列表。
+        
+        Args:
+            tag_data: 标签数据列表，每个元素是包含标签信息的字典
+            
+        Returns:
+            List[TagResult]: 解析后的标签结果列表
+        """
+        tags = []
+        
+        if not isinstance(tag_data, list):
+            logger.warning(f"Expected list for tag_data, got {type(tag_data)}")
+            return tags
+        
+        for item in tag_data:
+            try:
+                if isinstance(item, dict):
+                    # 标准格式：{"tag_name": "...", "confidence": ..., "reasoning": "..."}
+                    tag_name = item.get('tag_name', item.get('name', ''))
+                    confidence = float(item.get('confidence', 0.0))
+                    reasoning = item.get('reasoning', item.get('reason', ''))
+                    
+                    if tag_name:
+                        tag_result = TagResult(
+                            tag_name=tag_name,
+                            confidence=confidence,
+                            reasoning=reasoning,
+                            source="llm"
+                        )
+                        tags.append(tag_result)
+                    else:
+                        logger.warning(f"Tag item missing name: {item}")
+                        
+                elif isinstance(item, str):
+                    # 简单字符串格式
+                    tag_result = TagResult(
+                        tag_name=item,
+                        confidence=0.5,  # 默认置信度
+                        reasoning="Extracted as simple string",
+                        source="llm"
+                    )
+                    tags.append(tag_result)
+                else:
+                    logger.warning(f"Unexpected tag item format: {type(item)}")
+                    
+            except Exception as e:
+                logger.error(f"Error parsing tag item {item}: {e}")
+                continue
+        
+        return tags
\ No newline at end of file
diff --git a/simtag/utils/utils.py b/simtag/utils/utils.py
index 6dd3b8d..9f07a13 100755
--- a/simtag/utils/utils.py
+++ b/simtag/utils/utils.py
@@ -19,7 +19,7 @@
 # You should have received a copy of the GNU General Public License
 # along with this program.  If not, see <http://www.gnu.org/licenses/>.
 import functools
-from typing import Optional, Dict, Any
+from typing import Optional
 from urllib.parse import urlparse
 
 import sexpdata
@@ -27,8 +27,6 @@ import logging
 import pathlib
 import platform
 import sys
-import json
-import re
 
 from epc.client import EPCClient
 
@@ -232,41 +230,3 @@ def get_os_name():
 def parse_json_content(content):
     return json_parser.loads(content)
 
-
-def extract_json_from_response(text: str) -> Optional[Dict[str, Any]]:
-    """
-    Robustly extracts a JSON object from a string, which may contain markdown code blocks or other text.
-
-    Args:
-        text: The string containing the JSON object.
-
-    Returns:
-        The parsed JSON object as a dictionary, or None if no valid JSON object can be found.
-    """
-    if not text:
-        return None
-
-    # 1. Look for a markdown JSON block
-    match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
-    if match:
-        json_str = match.group(1)
-        try:
-            return json.loads(json_str)
-        except json.JSONDecodeError as e:
-            logger.warning(f"Failed to parse JSON from markdown block: {e}")
-            # Fall through to try other methods
-
-    # 2. Look for a raw JSON object that spans the entire string (or is embedded)
-    # This is a bit more aggressive. We look for the first '{' and the last '}'
-    try:
-        start = text.find('{')
-        end = text.rfind('}')
-        if start != -1 and end != -1 and end > start:
-            json_str = text[start:end+1]
-            return json.loads(json_str)
-    except json.JSONDecodeError as e:
-        logger.warning(f"Failed to parse JSON from raw string slice: {e}")
-
-    logger.error(f"Could not find any valid JSON in the response text: {text}")
-    return None
-
diff --git a/simtag_epc.py b/simtag_epc.py
new file mode 100755
index 0000000..c7ceb83
--- /dev/null
+++ b/simtag_epc.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python3
+"""
+SimTag EPC Server - Provides tag similarity, entity extraction, and tag generation as a resident service
+Uses EPC (Emacs RPC) to communicate with Emacs
+"""
+
+import os
+import sys
+import argparse
+
+# Add current directory to Python path
+script_dir = os.path.dirname(os.path.realpath(__file__))
+if script_dir not in sys.path:
+    sys.path.insert(0, script_dir)
+
+# Try to import the SimTag module
+try:
+    from simtag.config import Config
+    from simtag.epc_server import main as server_main
+except ImportError as e:
+    # If the import fails, try to display a more friendly error message
+    print(f"Failed to import the SimTag module: {e}")
+    print("Please make sure all dependencies are installed:")
+    print("uv pip install epc sentence-transformers torch numpy requests")
+    sys.exit(1)
+
+def main():
+    """Main function"""
+    try:
+        # Set environment variable to indicate EPC mode
+        os.environ["SIMTAG_EPC_MODE"] = "1"
+        
+        # Parse command line arguments
+        parser = argparse.ArgumentParser(description='SimTag EPC Server')
+        parser.add_argument('--vector-file', help='Path to the vector file')
+        parser.add_argument('--db-file', help='Path to the database file')
+        parser.add_argument('--model', help='Model name')
+        parser.add_argument('--debug', action='store_true', help='Enable debug mode')
+        parser.add_argument('--log-file', help='Path to the log file')
+        parser.add_argument('--host', default='127.0.0.1', help='Server address')
+        parser.add_argument('--port', type=int, default=0, help='Server port')
+        args = parser.parse_args()
+
+        # Create a configuration object
+        config = Config(
+            vector_file=args.vector_file,
+            db_file=args.db_file,
+            model_name=args.model,
+            debug=args.debug,
+            log_file=args.log_file,
+            host=args.host,
+            port=args.port
+        )
+        
+        # Make sure all output is flushed
+        sys.stdout.flush()
+        sys.stderr.flush()
+        
+        # Call the server main function
+        server_main(config)
+        
+    except Exception as e:
+        # Make sure error messages are written to stderr
+        print(f"Failed to start the server: {e}", file=sys.stderr, flush=True)
+        traceback.print_exc(file=sys.stderr)
+        sys.exit(1)
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/test-org-supertag-sim-advanced.el b/test-org-supertag-sim-advanced.el
new file mode 100644
index 0000000..5be65e9
--- /dev/null
+++ b/test-org-supertag-sim-advanced.el
@@ -0,0 +1,468 @@
+;;; org-supertag-sim-advanced.el --- Advanced EPC operations for org-supertag
+
+;; Copyright (C) 2023 
+
+;; Author: 
+;; Keywords: org-mode, nlp, epc
+
+;; This file is not part of GNU Emacs.
+
+;;; Commentary:
+;; 
+;; This module provides advanced operations for org-supertag using the EPC
+;; communication mechanism defined in org-supertag-sim-epc.el.
+;;
+;; The main functions are:
+;; - Interactive entity extraction from text
+;; - Batch processing of entities from multiple files
+;; - Interactive tag suggestion helpers
+;; - Similarity-based tag navigation
+;;
+
+;;; Code:
+
+(require 'org-supertag-sim-epc)
+(require 'org-supertag-db)
+(require 'org)
+(require 'cl-lib)
+
+(defgroup org-supertag-sim-advanced nil
+  "Advanced NLP features for Org Supertag."
+  :group 'org-supertag)
+
+(defcustom org-supertag-sim-suggest-threshold 0.6
+  "Threshold for tag similarity suggestions (0.0-1.0).
+Higher values require greater similarity."
+  :type 'float
+  :group 'org-supertag-sim-advanced)
+
+(defcustom org-supertag-sim-max-suggestions 5
+  "Maximum number of tag suggestions to display."
+  :type 'integer
+  :group 'org-supertag-sim-advanced)
+
+(defcustom org-supertag-sim-batch-size 10
+  "Number of files to process in each batch for batch operations."
+  :type 'integer
+  :group 'org-supertag-sim-advanced)
+
+(defcustom org-supertag-sim-disable-live-suggestions nil
+  "When non-nil, disable real-time tag suggestions."
+  :type 'boolean
+  :group 'org-supertag-sim-advanced)
+
+;;;###autoload
+(defun org-supertag-sim-extract-from-region (begin end)
+  "Extract entities from selected region and provide tag suggestions.
+BEGIN and END are the start and end positions of the region."
+  (interactive "r")
+  (unless (region-active-p)
+    (user-error "No region selected"))
+  
+  (let ((text (buffer-substring-no-properties begin end)))
+    (message "Analyzing selected region...")
+    
+    ;; Use EPC asynchronous processing
+    (org-supertag-sim-epc-extract-entities-async 
+     text
+     (lambda (entities)
+       ;; Entity extraction callback
+       (when entities
+         (let ((buf (get-buffer-create "*org-supertag-entities*"))
+               (entity-count (length entities)))
+           (with-current-buffer buf
+             (erase-buffer)
+             (insert (format "Extracted %d entities from selected text:\n\n" entity-count))
+             
+             ;; Display extracted entities
+             (dolist (entity entities)
+               (let ((entity-text (cdr (assoc 'entity entity)))
+                     (entity-type (cdr (assoc 'type entity))))
+                 (insert (format "- %s (%s)\n" entity-text entity-type))))
+             
+             ;; If there are entities, provide tag suggestions
+             (when entities
+               (insert "\nGenerating tag suggestions based on extracted entities...\n")
+               
+               ;; Extract entity text
+               (let ((entity-texts (mapcar (lambda (e) (cdr (assoc 'entity e))) entities)))
+                 ;; Connect all entity texts
+                 (let ((combined-text (mapconcat 'identity entity-texts " ")))
+                   ;; Asynchronously get tag suggestions
+                   (org-supertag-sim-epc-get-tag-suggestions-async 
+                    combined-text
+                    org-supertag-sim-max-suggestions
+                    (lambda (suggestions)
+                      ;; Tag suggestion callback
+                      (with-current-buffer buf
+                        (goto-char (point-max))
+                        (insert "\nTag suggestions:\n\n")
+                        
+                        ;; Display suggested tags
+                        (if suggestions
+                            (dolist (suggestion suggestions)
+                              (let ((tag-name (cdr (assoc 'tag suggestion)))
+                                    (score (cdr (assoc 'score suggestion))))
+                                (insert (format "- %s (%.2f)\n" tag-name score))))
+                          (insert "No related tag suggestions found\n"))
+                        
+                        ;; Add apply suggestion button
+                        (insert "\n")
+                        (dolist (suggestion suggestions)
+                          (let ((tag-name (cdr (assoc 'tag suggestion))))
+                            (insert-button (format "[Apply tag: %s] " tag-name)
+                                          'action `(lambda (_)
+                                                    (org-supertag-sim-advanced--apply-tag ,tag-name))
+                                          'follow-link t
+                                          'help-echo (format "Apply tag '%s' to current entry" tag-name))
+                          (insert " ")))
+                        
+                        ;; Add create all tags button
+                        (when suggestions
+                          (insert "\n\n")
+                          (insert-button "[Apply all suggestions]"
+                                        'action `(lambda (_)
+                                                  (org-supertag-sim-advanced--apply-all-tags ',suggestions))
+                                        'follow-link t
+                                        'help-echo "Apply all suggested tags to current entry"))))))))))
+           
+           ;; Display result buffer
+           (switch-to-buffer-other-window buf))))))
+
+(defun org-supertag-sim-advanced--apply-tag (tag-name)
+  "Apply a tag to the current Org entry.
+TAG-NAME is the name of the tag to apply."
+  (when (and tag-name (org-supertag-db-find-by-name tag-name :tag))
+    ;; If tag exists, apply directly
+    (message "Applying tag: %s" tag-name)
+    (org-back-to-heading t)
+    (let ((tag-id (org-supertag-db-find-by-name tag-name :tag)))
+      (when tag-id
+        (org-supertag-add `(:tag ,tag-id)))))
+  
+  ;; If tag does not exist, prompt to create
+  (when (and tag-name (not (org-supertag-db-find-by-name tag-name :tag)))
+    (when (yes-or-no-p (format "Tag '%s' does not exist, create it? " tag-name))
+      (let ((tag-id (org-supertag-upsert `(:name ,tag-name :type :tag))))
+        (when tag-id
+          (message "Created and applied tag: %s" tag-name)
+          (org-back-to-heading t)
+          (org-supertag-add `(:tag ,tag-id)))))))
+
+(defun org-supertag-sim-advanced--apply-all-tags (suggestions)
+  "Apply all suggested tags to the current Org entry.
+SUGGESTIONS is the list of tag suggestions."
+  (org-back-to-heading t)
+  (let ((applied-count 0))
+    (dolist (suggestion suggestions)
+      (let* ((tag-name (cdr (assoc 'tag suggestion)))
+             (tag-id (org-supertag-db-find-by-name tag-name :tag)))
+        
+        ;; If tag does not exist, create it
+        (unless tag-id
+          (setq tag-id (org-supertag-upsert `(:name ,tag-name :type :tag))))
+        
+        ;; Apply the tag
+        (when tag-id
+          (org-supertag-add `(:tag ,tag-id))
+          (cl-incf applied-count))))
+    
+    (message "Applied %d tags" applied-count)))
+
+;;;###autoload
+(defun org-supertag-sim-suggest-for-buffer ()
+  "Generate tag suggestions for the current buffer content."
+  (interactive)
+  (save-excursion
+    (let ((text (buffer-substring-no-properties (point-min) (point-max))))
+      (message "Analyzing buffer content...")
+      
+      ;; Asynchronously get tag suggestions
+      (org-supertag-sim-epc-get-tag-suggestions-async 
+       text
+       org-supertag-sim-max-suggestions
+       (lambda (suggestions)
+         ;; Create an interactive selection interface
+         (if suggestions
+             (let* ((choices (mapcar (lambda (s) 
+                                      (cons (format "%s (%.2f)" 
+                                                    (cdr (assoc 'tag s)) 
+                                                    (cdr (assoc 'score s)))
+                                            s))
+                                    suggestions))
+                    (selection (completing-read "Select tag to apply: " choices nil t)))
+               (when selection
+                 (let* ((selected-suggestion (cdr (assoc selection choices)))
+                        (tag-name (cdr (assoc 'tag selected-suggestion))))
+                   (org-supertag-sim-advanced--apply-tag tag-name))))
+           (message "No related tag suggestions found")))))))
+
+;;;###autoload
+(defun org-supertag-sim-extract-from-current-heading ()
+  "Extract entities from the current Org heading and provide tag suggestions."
+  (interactive)
+  (save-excursion
+    (org-back-to-heading t)
+    (let* ((heading-element (org-element-at-point))
+           (heading-text (org-element-property :title heading-element))
+           (section-end (or (org-element_property :contents-end heading-element)
+                            (save-excursion 
+                              (org-end-of-subtree t) 
+                              (point))))
+           (content-start (org-element-property :contents-begin heading-element))
+           (content (when content-start
+                      (buffer-substring-no-properties content-start section-end)))
+           (full-text (concat heading-text " " (or content ""))))
+      
+      (message "Analyzing current heading content...")
+      
+      ;; Use EPC asynchronous processing
+      (org-supertag-sim-epc-extract-entities-async 
+       full-text
+       (lambda (entities)
+         ;; Entity extraction callback
+         (when entities
+           (let ((entity-texts (mapcar (lambda (e) (cdr (assoc 'entity e))) entities)))
+             ;; Connect all entity texts
+             (let ((combined-text (mapconcat 'identity entity-texts " ")))
+               ;; Asynchronously get tag suggestions
+               (org-supertag-sim-epc-get-tag-suggestions-async 
+                combined-text
+                org-supertag-sim-max-suggestions
+                (lambda (suggestions)
+                  ;; Create an interactive selection interface
+                  (if suggestions
+                      (let* ((choices (mapcar (lambda (s) 
+                                               (cons (format "%s (%.2f)" 
+                                                             (cdr (assoc 'tag s)) 
+                                                             (cdr (assoc 'score s)))
+                                                     s))
+                                             suggestions))
+                             (selection (completing-read "Select tag to apply: " choices nil t)))
+                        (when selection
+                          (let* ((selected-suggestion (cdr (assoc selection choices)))
+                                 (tag-name (cdr (assoc 'tag selected-suggestion))))
+                            (org-supertag-sim-advanced--apply-tag tag-name))))
+                    (message "No related tag suggestions found"))))))))))))
+
+;;;###autoload
+(defun org-supertag-sim-find-similar-to-tag (tag-id)
+  "Find tags similar to the specified tag and provide navigation options.
+TAG-ID is the tag ID to find similar tags."
+  (interactive (list (org-supertag-sim-advanced--select-tag)))
+  
+  (when tag-id
+    (let* ((tag-props (org-supertag-db-get tag-id))
+           (tag-name (plist-get tag-props :name)))
+      (message "Finding tags similar to '%s'..." tag-name)
+      
+      ;; Asynchronously find similar tags
+      (org-supertag-sim-epc-find-similar-async
+       tag-name
+       org-supertag-sim-max-suggestions
+       (lambda (similar-tags)
+         (if similar-tags
+             (let* ((choices (mapcar (lambda (s)
+                                      (cons (format "%s (%.2f)" 
+                                                    (cdr (assoc 'tag s)) 
+                                                    (cdr (assoc 'score s)))
+                                            s))
+                                    similar-tags))
+                    (selection (completing-read "Select similar tag to navigate to: " choices nil t)))
+               (when selection
+                 (let* ((selected-tag (cdr (assoc selection choices)))
+                        (tag-name (cdr (assoc 'tag selected-tag)))
+                        (tag-id (org-supertag-db-find-by-name tag-name :tag)))
+                   (when tag-id
+                     (org-supertag-find tag-id)))))
+           (message "No similar tags found for '%s'" tag-name)))))))
+
+(defun org-supertag-sim-advanced--select-tag ()
+  "Select a tag and return its ID."
+  (let* ((tags (org-supertag-db-find-by-type :tag))
+         (tag-names (mapcar (lambda (id)
+                             (let ((props (org-supertag-db-get id)))
+                               (cons (plist-get props :name) id)))
+                           tags))
+         (selection (completing-read "Select tag: " tag-names nil t)))
+    (cdr (assoc selection tag-names))))
+
+;;;###autoload
+(defun org-supertag-sim-batch-process-org-files (org-files)
+  "Batch process multiple Org files, extract entities and generate tag suggestions for each entry.
+ORG-FILES is a list of Org file paths."
+  (interactive (list (directory-files-recursively 
+                      (read-directory-name "Select Org file directory: ") 
+                      "\\.org$")))
+  
+  (when org-files
+    (let ((total-files (length org-files))
+          (processed-files 0)
+          (report-buffer (get-buffer-create "*SimTag Batch Processing*"))
+          (batch-timer nil))
+      
+      ;; Initialize the report buffer
+      (with-current-buffer report-buffer
+        (erase-buffer)
+        (insert "SimTag Batch Processing Report\n")
+        (insert "===================\n\n")
+        (insert (format "Total files: %d\n\n" total-files))
+        (insert "Processing...\n\n"))
+      
+      ;; Display the report buffer
+      (display-buffer report-buffer)
+      
+      ;; Define the batch processing function
+      (cl-labels 
+          ((process-batch 
+            (remaining-files)
+            (if (null remaining-files)
+                ;; All files processed
+                (with-current-buffer report-buffer
+                  (goto-char (point-max))
+                  (insert "\nProcessing completed!\n")
+                  (insert (format "Processed %d files\n" processed-files)))
+              
+              ;; Get the current batch of files
+              (let* ((current-batch (cl-subseq remaining-files 0 
+                                               (min org-supertag-sim-batch-size 
+                                                    (length remaining-files))))
+                     (next-batch (cl-subseq remaining-files 
+                                            (min org-supertag-sim-batch-size 
+                                                 (length remaining-files)))))
+                
+                ;; Process the current batch of files
+                (process-file (car current-batch) 
+                              (cdr current-batch) 
+                              next-batch)))))
+        
+        ;; Define the single file processing function
+        (process-file 
+         (file remaining-files-in-batch next-batch)
+         (if (null file)
+             ;; Current batch processed, continue with the next batch
+             (setq batch-timer 
+                   (run-with-timer 0.5 nil #'process-batch next-batch))
+           
+           ;; Process the single file
+           (with-current-buffer report-buffer
+             (goto-char (point-max))
+             (insert (format "Processing file: %s\n" file)))
+           
+           (with-temp-buffer
+             (insert-file-contents file)
+             (org-mode)
+             (setq processed-files (1+ processed-files))
+             
+             ;; Process each heading in the file
+             (org-map-entries
+              (lambda ()
+                (let* ((heading-element (org-element-at-point))
+                       (heading-text (org-element-property :title heading-element))
+                       (section-end (or (org-element_property :contents-end heading-element)
+                                        (save-excursion 
+                                          (org-end-of-subtree t) 
+                                          (point))))
+                       (content-start (org-element-property :contents-begin heading-element))
+                       (content (when content-start
+                                  (buffer-substring-no-properties content-start section-end)))
+                       (full-text (concat heading-text " " (or content ""))))
+                  
+                  ;; Record the processed heading
+                  (with-current-buffer report-buffer
+                    (goto-char (point-max))
+                    (insert (format "  - Heading: %s\n" heading-text)))
+                  
+                  ;; Asynchronously extract entities
+                  (org-supertag-sim-epc-extract-entities-async 
+                   full-text
+                   (lambda (entities)
+                     (when entities
+                       ;; Record the extracted entities
+                       (with-current-buffer report-buffer
+                         (goto-char (point-max))
+                         (insert (format "    - Extracted %d entities\n" (length entities))))
+                       
+                       ;; Combine entity texts and get tag suggestions
+                       (let ((entity-texts (mapcar (lambda (e) 
+                                                    (cdr (assoc 'entity e))) 
+                                                  entities)))
+                         (when entity-texts
+                           (let ((combined-text (mapconcat 'identity entity-texts " ")))
+                             ;; Asynchronously get tag suggestions
+                             (org-supertag-sim-epc-get-tag-suggestions-async 
+                              combined-text
+                              org-supertag-sim-max-suggestions
+                              (lambda (suggestions)
+                                (when suggestions
+                                  ;; Record the tag suggestions
+                                  (with-current-buffer report-buffer
+                                    (goto-char (point-max))
+                                    (insert (format "    - Generated %d tag suggestions\n" 
+                                                    (length suggestions)))
+                                    (dolist (suggestion suggestions)
+                                      (insert (format "      * %s (%.2f)\n" 
+                                                      (cdr (assoc 'tag suggestion))
+                                                      (cdr (assoc 'score suggestion)))))))))))))))))))
+             
+             ;; Current file processed, continue with the next file in the batch
+             (process-file (car remaining-files-in-batch)
+                           (cdr remaining-files-in-batch)
+                           next-batch))))
+        
+        ;; Start processing the first batch of files
+        (process-batch org-files))))
+
+;; Provide a hook function for automatic tag suggestions
+(defvar org-supertag-sim-suggestion-timer nil
+  "Timer for delayed tag suggestions.")
+
+(defvar org-supertag-sim-last-content ""
+  "Last analyzed content, used to avoid duplicate analysis.")
+
+(defun org-supertag-sim-advanced--suggest-on-idle ()
+  "Provide tag suggestions for the current heading when idle."
+  (unless org-supertag-sim-disable-live-suggestions
+    (when (and (eq major-mode 'org-mode)
+               (not (minibufferp)))
+      ;; Cancel the previous timer
+      (when org-supertag-sim-suggestion-timer
+        (cancel-timer org-supertag-sim-suggestion-timer))
+      
+      ;; Set a new timer
+      (setq org-supertag-sim-suggestion-timer
+            (run-with-idle-timer 
+             2 nil
+             (lambda ()
+               (when (and (eq major-mode 'org-mode)
+                          (not (minibufferp)))
+                 (save-excursion
+                   (when (org-at-heading-p)
+                     (let* ((heading-element (org-element-at-point))
+                            (heading-text (org-element-property :title heading-element))
+                            (full-text heading-text))
+                       (unless (string= full-text org-supertag-sim-last-content)
+                         (setq org-supertag-sim-last-content full-text)
+                         (org-supertag-sim-epc-get-tag-suggestions-async 
+                          full-text
+                          (lambda (suggestions)
+                            (when (and suggestions 
+                                       (> (length suggestions) 0))
+                              (message "Tag suggestions: %s" 
+                                       (mapconcat 
+                                        (lambda (s) 
+                                          (format "%s" (cdr (assoc 'tag s))))
+                                        suggestions ", "))))))))))))))))
+(add-hook 'post-command-hook 'org-supertag-sim-advanced--suggest-on-idle)
+
+;;;###autoload
+(defun org-supertag-sim-toggle-live-suggestions ()
+  "Toggle real-time tag suggestions."
+  (interactive)
+  (setq org-supertag-sim-disable-live-suggestions 
+        (not org-supertag-sim-disable-live-suggestions))
+  (message "Real-time tag suggestions are now %s" 
+           (if org-supertag-sim-disable-live-suggestions "disabled" "enabled")))
+
+(provide 'org-supertag-sim-advanced)
+;;; org-supertag-sim-advanced.el ends here 
diff --git a/test/diagnose_ollama.py b/test/diagnose_ollama.py
new file mode 100755
index 0000000..d770dd0
--- /dev/null
+++ b/test/diagnose_ollama.py
@@ -0,0 +1,368 @@
+#!/usr/bin/env python3
+"""
+Ollama服务诊断工具
+用于检查Ollama服务状态、模型可用性和性能
+"""
+
+import asyncio
+import requests
+import time
+import json
+import logging
+from typing import Dict, List, Optional, Any
+from dataclasses import dataclass
+
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+@dataclass
+class ModelInfo:
+    name: str
+    size: int
+    digest: str
+    modified_at: str
+
+@dataclass
+class DiagnosticResult:
+    service_available: bool
+    models: List[ModelInfo]
+    model_test_results: Dict[str, Any]
+    performance_metrics: Dict[str, float]
+    recommendations: List[str]
+
+class OllamaDiagnostic:
+    """Ollama诊断器"""
+    
+    def __init__(self, base_url: str = "http://localhost:11434"):
+        self.base_url = base_url
+        
+    def check_service_availability(self) -> bool:
+        """检查Ollama服务是否可用"""
+        try:
+            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
+            response.raise_for_status()
+            logger.info("✅ Ollama service is available")
+            return True
+        except Exception as e:
+            logger.error(f"❌ Ollama service unavailable: {e}")
+            return False
+    
+    def get_models(self) -> List[ModelInfo]:
+        """获取可用模型列表"""
+        try:
+            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
+            response.raise_for_status()
+            data = response.json()
+            
+            models = []
+            for model in data.get("models", []):
+                models.append(ModelInfo(
+                    name=model.get("name", ""),
+                    size=model.get("size", 0),
+                    digest=model.get("digest", ""),
+                    modified_at=model.get("modified_at", "")
+                ))
+            
+            logger.info(f"📦 Found {len(models)} models")
+            for model in models:
+                size_mb = model.size / (1024 * 1024)
+                logger.info(f"  - {model.name}: {size_mb:.1f}MB")
+            
+            return models
+        except Exception as e:
+            logger.error(f"❌ Failed to get models: {e}")
+            return []
+    
+    async def test_model_generation(self, model_name: str, timeout: int = 60) -> Dict[str, Any]:
+        """测试模型生成能力"""
+        test_prompt = "Hello, how are you today?"
+        
+        try:
+            start_time = time.time()
+            
+            # 使用requests进行同步调用，在线程中执行
+            def make_request():
+                payload = {
+                    "model": model_name,
+                    "prompt": test_prompt,
+                    "stream": False,
+                    "options": {"temperature": 0.1}
+                }
+                response = requests.post(
+                    f"{self.base_url}/api/generate",
+                    json=payload,
+                    timeout=timeout
+                )
+                response.raise_for_status()
+                return response.json()
+            
+            # 在线程中执行同步请求
+            response_data = await asyncio.to_thread(make_request)
+            
+            end_time = time.time()
+            response_time = end_time - start_time
+            
+            response_text = response_data.get("response", "")
+            
+            result = {
+                "success": True,
+                "response_time": response_time,
+                "response_length": len(response_text),
+                "response_preview": response_text[:100] + "..." if len(response_text) > 100 else response_text,
+                "tokens_per_second": len(response_text.split()) / response_time if response_time > 0 else 0
+            }
+            
+            logger.info(f"✅ {model_name} test successful: {response_time:.2f}s, {result['tokens_per_second']:.1f} tokens/s")
+            return result
+            
+        except asyncio.TimeoutError:
+            logger.error(f"⏰ {model_name} test timeout ({timeout}s)")
+            return {"success": False, "error": "timeout"}
+        except Exception as e:
+            logger.error(f"❌ {model_name} test failed: {e}")
+            return {"success": False, "error": str(e)}
+    
+    async def test_embedding_model(self, model_name: str, timeout: int = 60) -> Dict[str, Any]:
+        """测试嵌入模型"""
+        test_text = "This is a test sentence for embedding generation."
+        
+        try:
+            start_time = time.time()
+            
+            def make_request():
+                payload = {
+                    "model": model_name,
+                    "prompt": test_text
+                }
+                response = requests.post(
+                    f"{self.base_url}/api/embeddings",
+                    json=payload,
+                    timeout=timeout
+                )
+                response.raise_for_status()
+                return response.json()
+            
+            response_data = await asyncio.to_thread(make_request)
+            
+            end_time = time.time()
+            response_time = end_time - start_time
+            
+            embedding = response_data.get("embedding", [])
+            
+            result = {
+                "success": True,
+                "response_time": response_time,
+                "embedding_dimension": len(embedding),
+                "sample_values": embedding[:5] if len(embedding) >= 5 else embedding
+            }
+            
+            logger.info(f"✅ {model_name} embedding test successful: {response_time:.2f}s, dim={len(embedding)}")
+            return result
+            
+        except Exception as e:
+            logger.error(f"❌ {model_name} embedding test failed: {e}")
+            return {"success": False, "error": str(e)}
+    
+    def get_system_info(self) -> Dict[str, Any]:
+        """获取系统信息"""
+        try:
+            # 尝试获取Ollama版本信息
+            response = requests.get(f"{self.base_url}/api/version", timeout=10)
+            if response.status_code == 200:
+                version_info = response.json()
+            else:
+                version_info = {"version": "unknown"}
+            
+            # 检查资源使用情况（基本信息）
+            import psutil
+            cpu_percent = psutil.cpu_percent(interval=1)
+            memory = psutil.virtual_memory()
+            disk = psutil.disk_usage('/')
+            
+            return {
+                "ollama_version": version_info.get("version", "unknown"),
+                "cpu_usage": cpu_percent,
+                "memory_usage": memory.percent,
+                "memory_available": memory.available / (1024**3),  # GB
+                "disk_usage": disk.percent,
+                "disk_free": disk.free / (1024**3)  # GB
+            }
+        except Exception as e:
+            logger.warning(f"Failed to get system info: {e}")
+            return {}
+    
+    async def run_full_diagnostic(self) -> DiagnosticResult:
+        """运行完整诊断"""
+        logger.info("🔍 Starting Ollama diagnostic...")
+        
+        # 1. 检查服务可用性
+        service_available = self.check_service_availability()
+        
+        if not service_available:
+            return DiagnosticResult(
+                service_available=False,
+                models=[],
+                model_test_results={},
+                performance_metrics={},
+                recommendations=["Ollama service is not running. Please start it with 'ollama serve'"]
+            )
+        
+        # 2. 获取模型列表
+        models = self.get_models()
+        
+        # 3. 测试模型
+        model_test_results = {}
+        
+        # 测试生成模型
+        generation_models = [m.name for m in models if not any(embed_name in m.name.lower() 
+                           for embed_name in ['embed', 'embedding', 'nomic-embed'])]
+        
+        if generation_models:
+            test_model = generation_models[0]  # 测试第一个生成模型
+            logger.info(f"🧪 Testing generation model: {test_model}")
+            model_test_results[test_model] = await self.test_model_generation(test_model)
+        
+        # 测试嵌入模型
+        embedding_models = [m.name for m in models if any(embed_name in m.name.lower() 
+                          for embed_name in ['embed', 'embedding', 'nomic-embed'])]
+        
+        if embedding_models:
+            test_embed_model = embedding_models[0]  # 测试第一个嵌入模型
+            logger.info(f"🧪 Testing embedding model: {test_embed_model}")
+            model_test_results[f"{test_embed_model}_embedding"] = await self.test_embedding_model(test_embed_model)
+        
+        # 4. 性能指标
+        performance_metrics = {}
+        system_info = self.get_system_info()
+        
+        if system_info:
+            performance_metrics.update(system_info)
+        
+        # 5. 生成建议
+        recommendations = self._generate_recommendations(models, model_test_results, system_info)
+        
+        return DiagnosticResult(
+            service_available=service_available,
+            models=models,
+            model_test_results=model_test_results,
+            performance_metrics=performance_metrics,
+            recommendations=recommendations
+        )
+    
+    def _generate_recommendations(self, models: List[ModelInfo], 
+                                test_results: Dict[str, Any], 
+                                system_info: Dict[str, Any]) -> List[str]:
+        """生成建议"""
+        recommendations = []
+        
+        # 检查模型数量
+        if not models:
+            recommendations.append("No models found. Install models with 'ollama pull <model_name>'")
+        
+        # 检查嵌入模型
+        embedding_models = [m for m in models if 'embed' in m.name.lower()]
+        if not embedding_models:
+            recommendations.append("No embedding models found. Consider installing 'ollama pull nomic-embed-text'")
+        
+        # 检查小模型用于实体提取
+        small_models = [m for m in models if any(size in m.name.lower() 
+                       for size in ['1b', '2b', '0.5b', 'mini'])]
+        if not small_models:
+            recommendations.append("Consider installing smaller models for faster entity extraction: 'ollama pull qwen2.5:1.5b'")
+        
+        # 检查性能
+        for model_name, result in test_results.items():
+            if result.get("success"):
+                response_time = result.get("response_time", 0)
+                if response_time > 30:
+                    recommendations.append(f"Model {model_name} is slow ({response_time:.1f}s). Consider using a smaller model.")
+        
+        # 检查系统资源
+        if system_info:
+            memory_usage = system_info.get("memory_usage", 0)
+            cpu_usage = system_info.get("cpu_usage", 0)
+            
+            if memory_usage > 90:
+                recommendations.append("High memory usage detected. Consider closing other applications.")
+            
+            if cpu_usage > 90:
+                recommendations.append("High CPU usage detected. System may be under load.")
+        
+        if not recommendations:
+            recommendations.append("System appears to be functioning well!")
+        
+        return recommendations
+
+def print_diagnostic_report(result: DiagnosticResult):
+    """打印诊断报告"""
+    print("\n" + "="*60)
+    print("🔍 OLLAMA DIAGNOSTIC REPORT")
+    print("="*60)
+    
+    # 服务状态
+    status_emoji = "✅" if result.service_available else "❌"
+    print(f"{status_emoji} Service Status: {'Available' if result.service_available else 'Unavailable'}")
+    
+    if not result.service_available:
+        print("\n❌ Cannot proceed with further diagnostics.")
+        print("\n💡 RECOMMENDATIONS:")
+        for rec in result.recommendations:
+            print(f"  • {rec}")
+        print("="*60)
+        return
+    
+    # 模型信息
+    print(f"\n📦 MODELS ({len(result.models)} total):")
+    if result.models:
+        for model in result.models:
+            size_gb = model.size / (1024**3)
+            print(f"  • {model.name}: {size_gb:.2f}GB")
+    else:
+        print("  No models found")
+    
+    # 测试结果
+    print(f"\n🧪 MODEL TESTS:")
+    if result.model_test_results:
+        for model_name, test_result in result.model_test_results.items():
+            if test_result.get("success"):
+                response_time = test_result.get("response_time", 0)
+                tokens_per_sec = test_result.get("tokens_per_second", 0)
+                if "embedding" in model_name:
+                    dim = test_result.get("embedding_dimension", 0)
+                    print(f"  ✅ {model_name}: {response_time:.2f}s (dim={dim})")
+                else:
+                    print(f"  ✅ {model_name}: {response_time:.2f}s ({tokens_per_sec:.1f} tokens/s)")
+            else:
+                error = test_result.get("error", "unknown")
+                print(f"  ❌ {model_name}: {error}")
+    else:
+        print("  No tests performed")
+    
+    # 性能指标
+    if result.performance_metrics:
+        print(f"\n📊 PERFORMANCE:")
+        metrics = result.performance_metrics
+        if "cpu_usage" in metrics:
+            print(f"  CPU Usage: {metrics['cpu_usage']:.1f}%")
+        if "memory_usage" in metrics:
+            print(f"  Memory Usage: {metrics['memory_usage']:.1f}%")
+        if "memory_available" in metrics:
+            print(f"  Memory Available: {metrics['memory_available']:.1f}GB")
+        if "ollama_version" in metrics:
+            print(f"  Ollama Version: {metrics['ollama_version']}")
+    
+    # 建议
+    print(f"\n💡 RECOMMENDATIONS:")
+    for rec in result.recommendations:
+        print(f"  • {rec}")
+    
+    print("="*60)
+
+async def main():
+    """主函数"""
+    diagnostic = OllamaDiagnostic()
+    result = await diagnostic.run_full_diagnostic()
+    print_diagnostic_report(result)
+
+if __name__ == "__main__":
+    asyncio.run(main()) 
\ No newline at end of file
diff --git a/test/local_test_sqlite_vec_read_type.py b/test/local_test_sqlite_vec_read_type.py
new file mode 100755
index 0000000..a6f5951
--- /dev/null
+++ b/test/local_test_sqlite_vec_read_type.py
@@ -0,0 +1,103 @@
+import sqlite3
+import json
+import numpy as np
+import os
+
+DB_PATH = "test_sqlite_vec_type.db"
+VEC_DIM = 3 # Example dimension
+
+def main():
+    if os.path.exists(DB_PATH):
+        os.remove(DB_PATH)
+
+    conn = sqlite3.connect(DB_PATH)
+    conn.enable_load_extension(True)
+    
+    try:
+        import sqlite_vec
+        sqlite_vec.load(conn)
+        print(f"sqlite-vec loaded, version: {conn.execute('SELECT vec_version()').fetchone()[0]}")
+    except Exception as e:
+        print(f"Failed to load sqlite-vec: {e}")
+        conn.close()
+        return
+
+    cursor = conn.cursor()
+
+    # Create virtual table
+    try:
+        cursor.execute(f"""
+        CREATE VIRTUAL TABLE IF NOT EXISTS test_float_vec USING vec0(
+            embedding FLOAT[{VEC_DIM}]
+        )
+        """)
+        print(f"Virtual table 'test_float_vec' created with dimension {VEC_DIM}.")
+    except Exception as e:
+        print(f"Failed to create virtual table: {e}")
+        conn.close()
+        return
+
+    # Insert data as JSON string
+    test_vector_list = [0.1, 0.2, 0.3]
+    if len(test_vector_list) != VEC_DIM:
+        print(f"Error: test_vector_list length {len(test_vector_list)} does not match VEC_DIM {VEC_DIM}")
+        conn.close()
+        return
+        
+    test_vector_json = json.dumps(test_vector_list)
+    
+    print(f"\nAttempting to INSERT JSON string: '{test_vector_json}' (type: {type(test_vector_json)})")
+    try:
+        cursor.execute("INSERT INTO test_float_vec (embedding) VALUES (?)", (test_vector_json,))
+        conn.commit()
+        inserted_rowid = cursor.lastrowid
+        print(f"Successfully inserted vector. Row ID: {inserted_rowid}")
+    except Exception as e:
+        print(f"Failed to insert JSON string: {e}")
+        conn.close()
+        return
+
+    # Select the data
+    print(f"\nAttempting to SELECT the inserted vector (rowid {inserted_rowid})...")
+    try:
+        cursor.execute("SELECT embedding FROM test_float_vec WHERE rowid = ?", (inserted_rowid,))
+        row = cursor.fetchone()
+        if row:
+            retrieved_data = row[0]
+            print(f"Successfully retrieved data.")
+            print(f"Retrieved data: {retrieved_data!r}")
+            print(f"Type of retrieved data: {type(retrieved_data)}")
+
+            if isinstance(retrieved_data, str):
+                print("Conclusion: sqlite-vec FLOAT column returned a STRING (JSON).")
+                # Try parsing
+                try:
+                    parsed_list = json.loads(retrieved_data)
+                    print(f"Parsed JSON string: {parsed_list}")
+                except Exception as e_parse:
+                    print(f"Failed to parse retrieved string as JSON: {e_parse}")
+            elif isinstance(retrieved_data, bytes):
+                print("Conclusion: sqlite-vec FLOAT column returned BYTES.")
+                # Try parsing
+                try:
+                    parsed_array = np.frombuffer(retrieved_data, dtype=np.float32)
+                    print(f"Parsed bytes using np.frombuffer: {parsed_array.tolist()}")
+                    if len(parsed_array) == VEC_DIM:
+                        print(f"Length matches VEC_DIM ({VEC_DIM}).")
+                    else:
+                         print(f"WARNING: Length {len(parsed_array)} does NOT match VEC_DIM ({VEC_DIM}).")
+                except Exception as e_parse_bytes:
+                    print(f"Failed to parse retrieved bytes using np.frombuffer: {e_parse_bytes}")
+            else:
+                print(f"Conclusion: sqlite-vec FLOAT column returned an unexpected type: {type(retrieved_data)}")
+
+        else:
+            print("Failed to retrieve the inserted row.")
+    except Exception as e:
+        print(f"Failed to select data: {e}")
+
+    conn.close()
+    # os.remove(DB_PATH) # Clean up
+
+if __name__ == '__main__':
+    main() 
\ No newline at end of file
diff --git a/test/minirag/.gitignore b/test/minirag/.gitignore
new file mode 100755
index 0000000..3142f5a
--- /dev/null
+++ b/test/minirag/.gitignore
@@ -0,0 +1,28 @@
+__pycache__
+*.egg-info
+dickens/
+book.txt
+lightrag-dev/
+.idea/
+dist/
+env/
+local_neo4jWorkDir/
+neo4jWorkDir/
+ignore_this.txt
+.venv/
+*.ignore.*
+.ruff_cache/
+gui/
+*.log
+.vscode
+inputs
+rag_storage
+.env
+venv/
+examples/input/
+examples/output/
+.DS_Store
+#Remove config.ini from repo
+*.ini
+build/
+minirag-venv/
diff --git a/test/minirag/.pre-commit-config.yaml b/test/minirag/.pre-commit-config.yaml
new file mode 100755
index 0000000..169a7cc
--- /dev/null
+++ b/test/minirag/.pre-commit-config.yaml
@@ -0,0 +1,22 @@
+repos:
+  - repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: v5.0.0
+    hooks:
+      - id: trailing-whitespace
+      - id: end-of-file-fixer
+      - id: requirements-txt-fixer
+
+
+  - repo: https://github.com/astral-sh/ruff-pre-commit
+    rev: v0.6.4
+    hooks:
+      - id: ruff-format
+      - id: ruff
+        args: [--fix, --ignore=E402]
+
+
+  - repo: https://github.com/mgedmin/check-manifest
+    rev: "0.49"
+    hooks:
+      - id: check-manifest
+        stages: [manual]
diff --git a/test/minirag/Dockerfile b/test/minirag/Dockerfile
new file mode 100755
index 0000000..1401d5e
--- /dev/null
+++ b/test/minirag/Dockerfile
@@ -0,0 +1,49 @@
+# Build stage
+FROM python:3.11-slim as builder
+
+WORKDIR /app
+
+# Install build dependencies
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    build-essential \
+    curl \
+    pkg-config \
+    libssl-dev \
+    && curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y \
+    && . "$HOME/.cargo/env" \
+    && rustup default stable \
+    && rm -rf /var/lib/apt/lists/*
+
+ENV PATH="/root/.cargo/bin:${PATH}"
+
+# Copy only requirements files first to leverage Docker cache
+COPY requirements.txt .
+COPY minirag/api/requirements.txt ./minirag/api/
+
+# Install dependencies
+RUN pip install --user --no-cache-dir -r requirements.txt
+RUN pip install --user --no-cache-dir -r minirag/api/requirements.txt
+
+# Final stage
+FROM python:3.11-slim
+
+WORKDIR /app
+
+# Copy only necessary files from builder
+COPY --from=builder /root/.local /root/.local
+COPY ./minirag ./minirag
+COPY setup.py .
+COPY .env .
+
+RUN pip install .
+# Make sure scripts in .local are usable
+ENV PATH=/root/.local/bin:$PATH
+
+# Create necessary directories
+RUN mkdir -p /app/data/rag_storage /app/data/inputs
+
+# Expose the default port
+EXPOSE 9621
+
+# Set entrypoint
+ENTRYPOINT ["python", "-m", "minirag.api.minirag_server"]
diff --git a/test/minirag/LICENSE b/test/minirag/LICENSE
new file mode 100755
index 0000000..61fc28b
--- /dev/null
+++ b/test/minirag/LICENSE
@@ -0,0 +1,21 @@
+MIT License
+
+Copyright (c) 2024 Gustavo Ye
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/test/minirag/MANIFEST.in b/test/minirag/MANIFEST.in
new file mode 100755
index 0000000..b7a7b62
--- /dev/null
+++ b/test/minirag/MANIFEST.in
@@ -0,0 +1,3 @@
+include README.md
+include requirements.txt
+include minirag/api/requirements.txt" > MANIFEST.in
\ No newline at end of file
diff --git a/test/minirag/README.md b/test/minirag/README.md
new file mode 100755
index 0000000..7b2903b
--- /dev/null
+++ b/test/minirag/README.md
@@ -0,0 +1,170 @@
+# MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation
+
+![MiniRAG](https://files.mdnice.com/user/87760/ff711e74-c382-4432-bec2-e6f2aa787df1.jpg)
+
+
+The Code Repository: **MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation**
+<br />
+
+[Tianyu Fan](https://tianyufan0504.github.io/), [Jingyuan Wang](), [Xubin Ren](https://ren-xubin.github.io/), [Chao Huang](https://sites.google.com/view/chaoh)* (*Correspondence)<br />
+</div>
+
+<a href='https://arxiv.org/abs/2501.06713'><img src='https://img.shields.io/badge/arXiv-2501.06713-b31b1b'>
+
+
+## 🌍 README Translations
+
+[中文说明](./README_CN.md) | [日本語](./README_JA.md)
+
+
+
+## 🎉 News
+- [x] [2025.02.27]🎯📢Now you can use `pip install minirag-hku` to run our code!
+- [x] [2025.02.14]🎯📢Now MiniRAG supports 10+ heterogeneous graph databases, including Neo4j, PostgreSQL, TiDB, etc. Happy valentine's day!🌹🌹🌹
+- [x] [2025.02.05]🎯📢Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG) understanding extremely long-context videos.
+- [x] [2025.02.01]🎯📢Now MiniRAG supports API&Docker deployment. see [This](./minirag/api/README.md) for more details.
+
+## TLDR
+MiniRAG is an extremely simple retrieval-augmented generation framework that enables small models to achieve good RAG performance through heterogeneous graph indexing and lightweight topology-enhanced retrieval.
+
+## Abstract
+The growing demand for efficient and lightweight Retrieval-Augmented Generation (RAG) systems has highlighted significant challenges when deploying Small Language Models (SLMs) in existing RAG frameworks. Current approaches face severe performance degradation due to SLMs' limited semantic understanding and text processing capabilities, creating barriers for widespread adoption in resource-constrained scenarios. To address these fundamental limitations, we present **MiniRAG**, a novel RAG system designed for extreme simplicity and efficiency. **MiniRAG** introduces two key technical innovations: (1) a semantic-aware heterogeneous graph indexing mechanism that combines text chunks and named entities in a unified structure, reducing reliance on complex semantic understanding, and (2) a lightweight topology-enhanced retrieval approach that leverages graph structures for efficient knowledge discovery without requiring advanced language capabilities. Our extensive experiments demonstrate that **MiniRAG** achieves comparable performance to LLM-based methods even when using SLMs while requiring only 25\% of the storage space. Additionally, we contribute a comprehensive benchmark dataset LiHua-World for evaluating lightweight RAG systems under realistic on-device scenarios with complex queries.
+
+## MiniRAG Framework
+
+![MiniRAG](https://files.mdnice.com/user/87760/02baba85-fa69-4223-ac22-914fef7120ae.jpg)
+
+MiniRAG employs a streamlined workflow built on the key components: heterogeneous graph indexing and lightweight graph-based knowledge retrieval. This architecture addresses the unique challenges faced by on-device RAG systems, optimizing for both efficiency and effectiveness.
+
+
+## Install
+
+* Install from source (Recommend)
+
+```bash
+cd MiniRAG
+pip install -e .
+```
+* Install from PyPI (Our code is based on [LightRAG](https://github.com/HKUDS/LightRAG), so you can install it directly)
+
+```bash
+pip install lightrag-hku
+```
+
+## Quick Start
+* All the code can be found in the `./reproduce`.
+* Download the dataset you need.
+* Put the dataset in the `./dataset` directory.
+* Note: We have already put the LiHua-World dataset in `./dataset/LiHua-World/data/` as `LiHuaWorld.zip`. If you want to use other dataset, you can put it in the `./dataset/xxx`.
+
+
+Then use the following bash command to index the dataset:
+```bash
+python ./reproduce/Step_0_index.py
+python ./reproduce/Step_1_QA.py
+```
+
+Or, use the code in `./main.py` to initialize MiniRAG.
+
+
+### Overall Performance Table
+| Model | NaiveRAG | | GraphRAG | | LightRAG | | **MiniRAG** | |
+|-------|----------|----------|-----------|----------|-----------|----------|----------|----------|
+| | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ |
+| LiHua-World | | | | | | | | |
+| Phi-3.5-mini-instruct | 41.22% | 23.20% | / | / | 39.81% | 25.39% | **53.29%** | 23.35% |
+| GLM-Edge-1.5B-Chat | 42.79% | 24.76% | / | / | 35.74% | 25.86% | **52.51%** | 25.71% |
+| Qwen2.5-3B-Instruct | 43.73% | 24.14% | / | / | 39.18% | 28.68% | **48.75%** | 26.02% |
+| MiniCPM3-4B | 43.42% | 17.08% | / | / | 35.42% | 21.94% | **51.25%** | 21.79% |
+| gpt-4o-mini | 46.55% | 19.12% | 35.27% | 37.77% | **56.90%** | 20.85% | 54.08% | 19.44% |
+| MultiHop-RAG | | | | | | | | |
+| Phi-3.5-mini-instruct | 42.72% | 31.34% | / | / | 27.03% | 11.78% | **49.96%** | 28.44% |
+| GLM-Edge-1.5B-Chat | 44.44% | 24.26% | / | / | / | / | **51.41%** | 23.44% |
+| Qwen2.5-3B-Instruct | 39.48% | 31.69% | / | / | 21.91% | 13.73% | **48.55%** | 33.10% |
+| MiniCPM3-4B | 39.24% | 31.42% | / | / | 19.48% | 10.41% | **47.77%** | 26.88% |
+| gpt-4o-mini | 53.60% | 27.19% | 60.92% | 16.86% | 64.91% | 19.37% | **68.43%** | 19.41% |
+
+
+In the table, / means the method struggles to generate effective responses.
+
+## Reproduce
+All the code can be found in the `./reproduce` directory.
+
+## Code Structure
+
+```python
+├── dataset
+│   └── LiHua-World
+│       ├── README.md
+│       ├── README_CN.md
+│       ├── data
+│       │   ├── LiHuaWorld.zip
+│       └── qa
+│           ├── query_set.csv
+│           └── query_set.json
+├── minirag
+│   ├── kg
+│   │   ├── __init__.py
+│   │   ├── neo4j_impl.py
+│   │   └── oracle_impl.py
+│   ├── __init__.py
+│   ├── base.py
+│   ├── exceptions.py
+│   ├── llm.py
+│   ├── minirag.py
+│   ├── operate.py
+│   ├── prompt.py
+│   ├── storage.py
+│   └── utils.py
+├── reproduce
+│   ├── Step_0_index.py
+│   └── Step_1_QA.py
+├── LICENSE
+├── main.py
+├── README.md
+├── README_CN.md
+├── requirements.txt
+├── setup.py
+```
+
+## Dataset: LiHua-World
+
+![LiHuaWorld](https://files.mdnice.com/user/87760/39923168-2267-4caf-b715-7f28764549de.jpg)
+
+LiHua-World is a dataset specifically designed for on-device RAG scenarios, containing one year of chat records from a virtual user named LiHua. The dataset includes three types of questions: single-hop, multi-hop, and summary, with each question paired with manually annotated answers and supporting documents. For more details, please refer to the [README of LiHua-World](./dataset/LiHua-World/README.md) dataset.
+
+## Star History
+
+<a href="https://star-history.com/#HKUDS/MiniRAG&Date">
+ <picture>
+   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date&theme=dark" />
+   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+ </picture>
+</a>
+
+## Contribution
+
+Thank you to all our contributors!
+
+<a href="https://github.com/HKUDS/MiniRAG/graphs/contributors">
+  <img src="https://contrib.rocks/image?repo=HKUDS/MiniRAG" />
+</a>
+
+
+## Acknowledgements
+You may refer to related work that serves as foundations for our framework and code repository,
+[nano-graphrag](https://github.com/gusye1234/nano-graphrag) and [LightRAG](https://github.com/HKUDS/LightRAG). Thanks for their wonderful works.
+
+## 🌟Citation
+
+```python
+@article{fan2025minirag,
+  title={MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation},
+  author={Fan, Tianyu and Wang, Jingyuan and Ren, Xubin and Huang, Chao},
+  journal={arXiv preprint arXiv:2501.06713},
+  year={2025}
+}
+```
+
+**Thank you for your interest in our work!**
diff --git a/test/minirag/README_CN.md b/test/minirag/README_CN.md
new file mode 100755
index 0000000..8842a59
--- /dev/null
+++ b/test/minirag/README_CN.md
@@ -0,0 +1,163 @@
+# MiniRAG: 迈向极简检索增强生成
+
+![MiniRAG](https://files.mdnice.com/user/87760/ff711e74-c382-4432-bec2-e6f2aa787df1.jpg)
+
+
+本仓库是论文: **MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation** 的代码仓库。
+
+<br />
+
+[Tianyu Fan](https://tianyufan0504.github.io/), [Jingyuan Wang](), [Xubin Ren](https://ren-xubin.github.io/), [Chao Huang](https://sites.google.com/view/chaoh)* (*Correspondence)<br />
+</div>
+
+
+<a href='https://arxiv.org/abs/2501.06713'><img src='https://img.shields.io/badge/arXiv-2501.06713-b31b1b'>
+
+
+## 🎉 News
+- [x] [2025.02.27]🎯📢现在您可以使用 `pip install minirag-hku` 来运行我们的代码！
+- [x] [2025.02.14]🎯📢现在MiniRAG支持包括Neo4j、PostgreSQL、TiDB等在内的10多种异构图数据库。情人节快乐！🌹🌹🌹
+- [x] [2025.02.05]🎯📢我们的团队发布了[VideoRAG](https://github.com/HKUDS/VideoRAG)，能够理解极长上下文视频。
+- [x] [2025.02.01]🎯📢现在MiniRAG支持API和Docker部署。更多详情请参见[这里](./minirag/api/README.md)。
+
+## TLDR
+MiniRAG 是一个极简的检索增强生成框架，它通过异质图索引和轻量级的拓扑增强检索，让小模型也能取得很好的RAG效果。
+
+## Abstract
+对高效且轻量级的检索增强生成（RAG）系统日益增长的需求，凸显了在现有RAG框架中部署小型语言模型（SLMs）时所面临的重大挑战。由于SLMs在语义理解和文本处理能力上的局限性，当前方法面临严重的性能下降问题，这在资源受限的场景中阻碍了其广泛应用。为了应对这些根本性限制，我们提出了**MiniRAG**，这是一种专为极简和高效而设计的新型RAG系统。MiniRAG引入了两项关键技术创新：（1）一种语义感知的异构图索引机制，将文本块和命名实体结合在一个统一结构中，减少了对复杂语义理解的依赖；（2）一种轻量级的拓扑增强检索方法，利用图结构实现高效的知识发现，而无需高级语言能力。我们的大量实验表明，MiniRAG在使用SLMs时，性能与基于LLM的方法相当，同时仅需25%的存储空间。此外，我们还贡献了一个全面的基准数据集LiHua-World，用于评估轻量级RAG系统在现实设备场景下处理复杂查询的能力。
+
+## MiniRAG 框架
+
+![MiniRAG](https://files.mdnice.com/user/87760/02baba85-fa69-4223-ac22-914fef7120ae.jpg)
+
+MiniRAG 采用基于两个关键组件构建的精简工作流程:异构图索引和轻量级的基于图的知识检索。这种架构解决了设备端 RAG 系统面临的独特挑战,在效率和效果之间实现了优化。
+
+## 安装
+
+* 从源码安装（推荐）
+
+```bash
+cd MiniRAG
+pip install -e .
+```
+* 从 PyPI 安装（我们的代码基于 [LightRAG](https://github.com/HKUDS/LightRAG)，因此可以直接安装）
+
+```bash
+pip install lightrag-hku
+```
+
+## 快速开始
+* 所有复现代码可以在 `./reproduce` 目录下找到。
+* 下载您需要的知识库数据集。
+* 将数据集放入 `./dataset` 目录下。
+* Note：我们已经将 LiHua-World 数据集以 `LiHuaWorld.zip` 的形式放在了 `./dataset/LiHua-World/data/` 目录下。如果您想使用其他数据集，可以将其放在 `./dataset/xxx` 目录下。
+
+
+然后使用以下命令对数据集进行索引：
+```bash
+python ./reproduce/Step_0_index.py
+python ./reproduce/Step_1_QA.py
+```
+
+或者，使用 `./main.py` 中的代码初始化 MiniRAG。
+
+
+### 整体性能表
+| Model | NaiveRAG | | GraphRAG | | LightRAG | | **MiniRAG** | |
+|-------|----------|----------|-----------|----------|-----------|----------|----------|----------|
+| | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ |
+| LiHua-World | | | | | | | | |
+| Phi-3.5-mini-instruct | 41.22% | 23.20% | / | / | 39.81% | 25.39% | **53.29%** | 23.35% |
+| GLM-Edge-1.5B-Chat | 42.79% | 24.76% | / | / | 35.74% | 25.86% | **52.51%** | 25.71% |
+| Qwen2.5-3B-Instruct | 43.73% | 24.14% | / | / | 39.18% | 28.68% | **48.75%** | 26.02% |
+| MiniCPM3-4B | 43.42% | 17.08% | / | / | 35.42% | 21.94% | **51.25%** | 21.79% |
+| gpt-4o-mini | 46.55% | 19.12% | 35.27% | 37.77% | **56.90%** | 20.85% | 54.08% | 19.44% |
+| MultiHop-RAG | | | | | | | | |
+| Phi-3.5-mini-instruct | 42.72% | 31.34% | / | / | 27.03% | 11.78% | **49.96%** | 28.44% |
+| GLM-Edge-1.5B-Chat | 44.44% | 24.26% | / | / | / | / | **51.41%** | 23.44% |
+| Qwen2.5-3B-Instruct | 39.48% | 31.69% | / | / | 21.91% | 13.73% | **48.55%** | 33.10% |
+| MiniCPM3-4B | 39.24% | 31.42% | / | / | 19.48% | 10.41% | **47.77%** | 26.88% |
+| gpt-4o-mini | 53.60% | 27.19% | 60.92% | 16.86% | 64.91% | 19.37% | **68.43%** | 19.41% |
+
+表中，/ 表示该方法难以生成有效响应。
+
+## 复现
+所有代码可以在 `./reproduce` 目录下找到。
+
+## 代码结构
+
+```python
+├── dataset
+│   └── LiHua-World
+│       ├── README.md
+│       ├── README_CN.md
+│       ├── data
+│       │   ├── LiHuaWorld.zip
+│       └── qa
+│           ├── query_set.csv
+│           └── query_set.json
+├── minirag
+│   ├── kg
+│   │   ├── __init__.py
+│   │   ├── neo4j_impl.py
+│   │   └── oracle_impl.py
+│   ├── __init__.py
+│   ├── base.py
+│   ├── llm.py
+│   ├── minirag.py
+│   ├── operate.py
+│   ├── prompt.py
+│   ├── storage.py
+│   └── utils.py
+├── reproduce
+│   ├── Step_0_index.py
+│   └── Step_1_QA.py
+├── LICENSE
+├── main.py
+├── README.md
+├── README_CN.md
+├── requirements.txt
+├── setup.py
+```
+
+## 数据集: LiHua-World
+
+![LiHuaWorld](https://files.mdnice.com/user/87760/39923168-2267-4caf-b715-7f28764549de.jpg)
+
+LiHua-World 是一个专门为本地 RAG 场景设计的数据集，包含了一个名为 LiHua 的虚拟用户一年内的聊天记录。该数据集包含三种类型的问题：单跳、多跳和总结性问题，每个问题都配有人工标注的答案和支持文档。更多细节请参考 [LiHua-World 数据集的 README](./dataset/LiHua-World/README_CN.md)。
+
+
+## Star History
+
+<a href="https://star-history.com/#HKUDS/MiniRAG&Date">
+ <picture>
+   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date&theme=dark" />
+   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+ </picture>
+</a>
+
+## Contribution
+
+感谢MiniRAG项目的所有贡献者！
+
+<a href="https://github.com/HKUDS/MiniRAG/graphs/contributors">
+  <img src="https://contrib.rocks/image?repo=HKUDS/MiniRAG" />
+</a>
+
+
+## 致谢
+你可以参考以下相关工作，它们为我们的框架和代码库奠定了基础：[nano-graphrag](https://github.com/gusye1234/nano-graphrag) 和 [LightRAG](https://github.com/HKUDS/LightRAG)。感谢他们的出色工作。
+
+## 🌟引用
+
+```python
+@article{fan2025minirag,
+  title={MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation},
+  author={Fan, Tianyu and Wang, Jingyuan and Ren, Xubin and Huang, Chao},
+  journal={arXiv preprint arXiv:2501.06713},
+  year={2025}
+}
+```
+
+**感谢您对我们工作的关注！**
diff --git a/test/minirag/README_JA.md b/test/minirag/README_JA.md
new file mode 100755
index 0000000..c7c2b60
--- /dev/null
+++ b/test/minirag/README_JA.md
@@ -0,0 +1,166 @@
+# MiniRAG: 極めてシンプルな検索強化生成に向けて
+
+![MiniRAG](https://files.mdnice.com/user/87760/ff711e74-c382-4432-bec2-e6f2aa787df1.jpg)
+
+
+コードリポジトリ: **MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation**
+<br />
+
+[Tianyu Fan](https://tianyufan0504.github.io/), [Jingyuan Wang](), [Xubin Ren](https://ren-xubin.github.io/), [Chao Huang](https://sites.google.com/view/chaoh)* (*Correspondence)<br />
+</div>
+
+<a href='https://arxiv.org/abs/2501.06713'><img src='https://img.shields.io/badge/arXiv-2501.06713-b31b1b'>
+
+
+## 🌍 READMEの翻訳
+
+[English](./README.md) | [中文](./README_CN.md)
+
+## 🎉 News
+- [x] [2025.02.27]🎯📢`pip install minirag-hku`を使用して私たちのコードを実行できるようになりました！
+- [x] [2025.02.14]🎯📢MiniRAGがNeo4j、PostgreSQL、TiDBなど10以上の異種グラフデータベースをサポートするようになりました。バレンタインデーおめでとう！🌹🌹🌹
+- [x] [2025.02.05]🎯📢私たちのチームは、非常に長いコンテキストの動画を理解するVideoRAGをリリースしました。
+- [x] [2025.02.01]🎯📢MiniRAGがAPI&Dockerデプロイメントをサポートするようになりました。詳細はこちらをご覧ください。
+
+## TLDR
+MiniRAGは、異種グラフインデックスと軽量なトポロジー強化検索を通じて、小さなモデルでも優れたRAGパフォーマンスを実現する極めてシンプルな検索強化生成フレームワークです。
+
+## 概要
+効率的で軽量な検索強化生成（RAG）システムの需要が高まる中、既存のRAGフレームワークに小型言語モデル（SLM）を導入する際の重大な課題が浮き彫りになっています。現在のアプローチは、SLMの限られた意味理解とテキスト処理能力のために深刻な性能低下に直面しており、リソースが限られたシナリオでの広範な採用に障害をもたらしています。これらの根本的な制限に対処するために、私たちは極めてシンプルで効率的な新しいRAGシステムである**MiniRAG**を提案します。**MiniRAG**は、2つの重要な技術革新を導入しています：（1）テキストチャンクと名前付きエンティティを統一構造に組み合わせる意味認識異種グラフインデックスメカニズム、これにより複雑な意味理解への依存を減らします。（2）高度な言語能力を必要とせずにグラフ構造を活用して効率的な知識発見を実現する軽量なトポロジー強化検索アプローチ。私たちの広範な実験は、**MiniRAG**がSLMを使用してもLLMベースの方法と同等の性能を達成しながら、ストレージスペースの25％しか必要としないことを示しています。さらに、複雑なクエリを持つ現実的なオンデバイスシナリオで軽量RAGシステムを評価するための包括的なベンチマークデータセットLiHua-Worldも提供します。
+
+## MiniRAGフレームワーク
+
+![MiniRAG](https://files.mdnice.com/user/87760/02baba85-fa69-4223-ac22-914fef7120ae.jpg)
+
+MiniRAGは、異種グラフインデックスと軽量なグラフベースの知識検索という主要なコンポーネントに基づいて構築された簡素化されたワークフローを採用しています。このアーキテクチャは、オンデバイスRAGシステムが直面する独自の課題に対処し、効率と効果の両方を最適化します。
+
+
+## インストール
+
+* ソースからインストール（推奨）
+
+```bash
+cd MiniRAG
+pip install -e .
+```
+* PyPIからインストール（私たちのコードは[LightRAG](https://github.com/HKUDS/LightRAG)に基づいているため、直接インストールできます）
+
+```bash
+pip install lightrag-hku
+```
+
+## クイックスタート
+* すべてのコードは`./reproduce`にあります。
+* 必要なデータセットをダウンロードします。
+* データセットを`./dataset`ディレクトリに配置します。
+* 注：LiHua-Worldデータセットは`./dataset/LiHua-World/data/`に`LiHuaWorld.zip`として既に配置されています。他のデータセットを使用したい場合は、`./dataset/xxx`に配置できます。
+
+
+次に、以下のbashコマンドを使用してデータセットをインデックスします：
+```bash
+python ./reproduce/Step_0_index.py
+python ./reproduce/Step_1_QA.py
+```
+
+または、`./main.py`のコードを使用してMiniRAGを初期化します。
+
+
+### 全体のパフォーマンステーブル
+| モデル | NaiveRAG | | GraphRAG | | LightRAG | | **MiniRAG** | |
+|-------|----------|----------|-----------|----------|-----------|----------|----------|----------|
+| | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ | acc↑ | err↓ |
+| LiHua-World | | | | | | | | |
+| Phi-3.5-mini-instruct | 41.22% | 23.20% | / | / | 39.81% | 25.39% | **53.29%** | 23.35% |
+| GLM-Edge-1.5B-Chat | 42.79% | 24.76% | / | / | 35.74% | 25.86% | **52.51%** | 25.71% |
+| Qwen2.5-3B-Instruct | 43.73% | 24.14% | / | / | 39.18% | 28.68% | **48.75%** | 26.02% |
+| MiniCPM3-4B | 43.42% | 17.08% | / | / | 35.42% | 21.94% | **51.25%** | 21.79% |
+| gpt-4o-mini | 46.55% | 19.12% | 35.27% | 37.77% | **56.90%** | 20.85% | 54.08% | 19.44% |
+| MultiHop-RAG | | | | | | | | |
+| Phi-3.5-mini-instruct | 42.72% | 31.34% | / | / | 27.03% | 11.78% | **49.96%** | 28.44% |
+| GLM-Edge-1.5B-Chat | 44.44% | 24.26% | / | / | / | / | **51.41%** | 23.44% |
+| Qwen2.5-3B-Instruct | 39.48% | 31.69% | / | / | 21.91% | 13.73% | **48.55%** | 33.10% |
+| MiniCPM3-4B | 39.24% | 31.42% | / | / | 19.48% | 10.41% | **47.77%** | 26.88% |
+| gpt-4o-mini | 53.60% | 27.19% | 60.92% | 16.86% | 64.91% | 19.37% | **68.43%** | 19.41% |
+
+
+表中、/はその方法が効果的な応答を生成するのに苦労していることを意味します。
+
+## 再現
+すべてのコードは`./reproduce`ディレクトリにあります。
+
+## コード構造
+
+```python
+├── dataset
+│   └── LiHua-World
+│       ├── README.md
+│       ├── README_CN.md
+│       ├── data
+│       │   ├── LiHuaWorld.zip
+│       └── qa
+│           ├── query_set.csv
+│           └── query_set.json
+├── minirag
+│   ├── kg
+│   │   ├── __init__.py
+│   │   ├── neo4j_impl.py
+│   │   └── oracle_impl.py
+│   ├── __init__.py
+│   ├── base.py
+│   ├── llm.py
+│   ├── minirag.py
+│   ├── operate.py
+│   ├── prompt.py
+│   ├── storage.py
+│   └── utils.py
+├── reproduce
+│   ├── Step_0_index.py
+│   └── Step_1_QA.py
+├── LICENSE
+├── main.py
+├── README.md
+├── README_CN.md
+├── requirements.txt
+├── setup.py
+```
+
+## データセット: LiHua-World
+
+![LiHuaWorld](https://files.mdnice.com/user/87760/39923168-2267-4caf-b715-7f28764549de.jpg)
+
+LiHua-Worldは、仮想ユーザーLiHuaの1年間のチャット記録を含む、オンデバイスRAGシナリオ専用に設計されたデータセットです。このデータセットには、シングルホップ、マルチホップ、およびサマリーの3種類の質問が含まれており、各質問には手動で注釈が付けられた回答とサポート文書がペアになっています。詳細については、[LiHua-WorldデータセットのREADME](./dataset/LiHua-World/README.md)を参照してください。
+
+
+## Star History
+
+<a href="https://star-history.com/#HKUDS/MiniRAG&Date">
+ <picture>
+   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date&theme=dark" />
+   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/MiniRAG&type=Date" />
+ </picture>
+</a>
+
+## Contribution
+
+MiniRAGプロジェクトのすべての貢献者に感謝します！
+
+<a href="https://github.com/HKUDS/MiniRAG/graphs/contributors">
+  <img src="https://contrib.rocks/image?repo=HKUDS/MiniRAG" />
+</a>
+
+## 謝辞
+私たちのフレームワークとコードリポジトリの基礎となる関連作業については、[nano-graphrag](https://github.com/gusye1234/nano-graphrag)および[LightRAG](https://github.com/HKUDS/LightRAG)を参照してください。素晴らしい仕事に感謝します。
+
+## 🌟引用
+
+```python
+@article{fan2025minirag,
+  title={MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation},
+  author={Fan, Tianyu and Wang, Jingyuan and Ren, Xubin and Huang, Chao},
+  journal={arXiv preprint arXiv:2501.06713},
+  year={2025}
+}
+```
+
+**私たちの仕事に興味を持っていただき、ありがとうございます！**
diff --git a/test/minirag/assets/logo.png b/test/minirag/assets/logo.png
new file mode 100755
index 0000000..63425bc
Binary files /dev/null and b/test/minirag/assets/logo.png differ
diff --git a/test/minirag/dataset/LiHua-World/README.md b/test/minirag/dataset/LiHua-World/README.md
new file mode 100755
index 0000000..9d18fcb
--- /dev/null
+++ b/test/minirag/dataset/LiHua-World/README.md
@@ -0,0 +1,50 @@
+# LiHua-World Dataset
+
+![LiHuaWorld](https://files.mdnice.com/user/87760/39923168-2267-4caf-b715-7f28764549de.jpg)
+
+[中文说明](./README_CN.md)
+
+
+LiHua-World is a dataset specifically designed for local RAG (Retrieval-Augmented Generation) scenarios. It contains one year's worth of chat records from a virtual user named LiHua.
+
+## Dataset Features
+
+- Includes three types of questions:
+  - Single-hop
+  - Multi-hop
+  - Summary
+- Each question is accompanied by manually annotated answers and supporting documents
+- The chat records cover various aspects of daily life, including:
+  - Social interactions
+  - Fitness training
+  - Entertainment activities
+  - Life affairs
+  - ...
+## Dataset Structure
+
+The dataset mainly consists of the following parts:
+
+### 1. Original Chat Records (/data)
+- Chat messages organized in chronological order
+- Each message contains:
+  - Timestamp
+  - Sender
+  - Message content
+  - Message type
+
+### 2. Q&A Data (/qa)
+- query_set.csv: Contains questions, standard answers, and evidence
+- query_set.json: JSON format version of the CSV file
+
+### 3. Metadata
+- User information
+- Time range: January 2026 to December 2026
+- List of conversation participants
+
+## Usage Instructions
+
+Step 1. Unzip the `LiHuaWorld.zip` file in the `./data` directory to obtain the original chat records.
+
+Step 2. Use all the chat records in the `./data` directory as the knowledge base.
+
+Step 3. Use `query_set.csv` or `query_set.json` in the `./qa` directory as the question set to conduct RAG testing.
diff --git a/test/minirag/dataset/LiHua-World/README_CN.md b/test/minirag/dataset/LiHua-World/README_CN.md
new file mode 100755
index 0000000..58e5693
--- /dev/null
+++ b/test/minirag/dataset/LiHua-World/README_CN.md
@@ -0,0 +1,49 @@
+# LiHua-World 数据集
+
+![LiHuaWorld](https://files.mdnice.com/user/87760/39923168-2267-4caf-b715-7f28764549de.jpg)
+
+LiHua-World 是一个专门为本地 RAG (检索增强生成)场景设计的数据集。该数据集包含了一个名为 LiHua 的虚拟用户一年内的聊天记录。
+
+## 数据集特点
+
+- 包含三种类型的问题:
+  - 单跳问题 (Single-hop)
+  - 多跳问题 (Multi-hop)
+  - 总结性问题 (Summary)
+- 每个问题都配有人工标注的答案和支持文档
+- 聊天记录涵盖了日常生活的多个方面,包括:
+  - 社交互动
+  - 健身训练
+  - 娱乐活动
+  - 生活事务
+  - ...
+
+## 数据集结构
+
+数据集主要包含以下部分:
+
+### 1. 原始聊天记录 (./data)
+- 按时间顺序组织的聊天消息
+- 每条消息包含:
+  - 时间戳
+  - 发送者
+  - 消息内容
+  - 消息类型
+为了方便组织，每个文件夹包含的是一周的聊天记录。
+
+### 2. 问答数据 (/qa)
+- query_set.csv: 包含问题、标准答案和证据
+- query_set.json: CSV文件的JSON格式版本
+
+### 3. 元数据
+- 用户信息
+- 时间范围: 2026年1月至12月
+- 对话参与者列表
+
+## 使用说明
+
+Step1. 在./data文件夹下，解压LiHuaWorld.zip，获取原始聊天记录。
+
+Step2. 使用./data下的所有聊天记录作为知识库。
+
+Step3. 使用./qa下的query_set.csv或是query_set.json作为问题，进行RAG测试。
diff --git a/test/minirag/dataset/LiHua-World/data/LiHuaWorld.zip b/test/minirag/dataset/LiHua-World/data/LiHuaWorld.zip
new file mode 100755
index 0000000..2378abe
Binary files /dev/null and b/test/minirag/dataset/LiHua-World/data/LiHuaWorld.zip differ
diff --git a/test/minirag/dataset/LiHua-World/evaluation.ipynb b/test/minirag/dataset/LiHua-World/evaluation.ipynb
new file mode 100755
index 0000000..2ea635e
--- /dev/null
+++ b/test/minirag/dataset/LiHua-World/evaluation.ipynb
@@ -0,0 +1,214 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from huggingface_hub import login\n",
+    "import os\n",
+    "import sys\n",
+    "import csv\n",
+    "from tqdm import trange\n",
+    "from transformers import AutoModel,AutoTokenizer\n",
+    "FILE_PATH = './QA_results_GT.csv'\n",
+    "os.environ[\"OPENAI_API_KEY\"] = AAA"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "ANA_FILE_PATH = './mthp_output.csv'\n",
+    "\n",
+    "naiveanswer_LIST = []\n",
+    "lightraganswer_LIST = []\n",
+    "minianswer_LIST = []\n",
+    "QUESTION_LIST = []\n",
+    "GA_LIST = []\n",
+    "filelength = 0\n",
+    "with open(ANA_FILE_PATH, mode='r', encoding='utf-8') as question_file:\n",
+    "    reader = csv.DictReader(question_file)\n",
+    "    for row in reader:\n",
+    "        QUESTION_LIST.append(row['Question'])\n",
+    "        GA_LIST.append(row['Gold Answer'])\n",
+    "        naiveanswer_LIST.append(row['naive'])\n",
+    "        lightraganswer_LIST.append(row['lightrag'])\n",
+    "        minianswer_LIST.append(row['minirag'])\n",
+    "        filelength = filelength+1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "PROMPT = \"\"\"\n",
+    "Now, I'll give you a question, a gold answer to this question, and three answers provided by different students.\n",
+    "\n",
+    "Determine the answer according to the following rules:\n",
+    "If the answer is correct, get 1 point.\n",
+    "If the answer is irrelevant to the question, it will receive 0 points.\n",
+    "If the answer is incorrect, get -1 point.\n",
+    "\n",
+    "Return your answer in JSON mode.\n",
+    "\n",
+    "For example:\n",
+    "\n",
+    "Question:\n",
+    "When does Li Hua arrive to the city?\n",
+    "\n",
+    "Gold Answer:\n",
+    "20260105\n",
+    "\n",
+    "Answer1: LiHua arrived on the afternoon of January 5th\n",
+    "Answer2: Sorry, there is no information about LiHua's arrival in the information you provided\n",
+    "Answer3: There is no accurate answer in the information you provided, but according to the first information found, LiHua arrived on April 17th\n",
+    "\n",
+    "output:\n",
+    "{{\n",
+    "\"Score1\": 1,\n",
+    "\"Score2\": 0,\n",
+    "\"Score3\": -1,\n",
+    "}}\n",
+    "\n",
+    "\n",
+    "\n",
+    "Real data:\n",
+    "\n",
+    "Question:\n",
+    "{question}\n",
+    "Gold Answer:\n",
+    "{ga}\n",
+    "\n",
+    "Answer1: {naive}\n",
+    "Answer2: {light}\n",
+    "Answer3: {mini}\n",
+    "\n",
+    "output:\n",
+    "\n",
+    "\"\"\""
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#deepseek\n",
+    "from openai import OpenAI\n",
+    "chatbot = OpenAI(api_key=My_deepseek_key, base_url=\"https://api.deepseek.com\")\n",
+    "\n",
+    "chat_list = []\n",
+    "for i in range(filelength):\n",
+    "    p = PROMPT.format(question = QUESTION_LIST[i], ga = GA_LIST[i], naive = naiveanswer_LIST[i], light = lightraganswer_LIST[i], mini = minianswer_LIST[i])\n",
+    "    chat_completion = chatbot.chat.completions.create(\n",
+    "        messages=[\n",
+    "            {\n",
+    "                \"role\": \"system\",\n",
+    "                \"content\":p,\n",
+    "            },\n",
+    "            \n",
+    "\n",
+    "        ],\n",
+    "        model=\"deepseek-chat\",\n",
+    "        stream = False\n",
+    "    )\n",
+    "    chat_list.append(chat_completion.choices[0].message.content.strip())"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#openai\n",
+    "from openai import OpenAI\n",
+    "from tqdm import trange\n",
+    "chatbot = OpenAI()\n",
+    "chat_list = []\n",
+    "for i in trange(filelength):\n",
+    "    p = PROMPT.format(question = QUESTION_LIST[i], ga = GA_LIST[i], naive = naiveanswer_LIST[i], light = lightraganswer_LIST[i], mini = minianswer_LIST[i])\n",
+    "    chat_completion = chatbot.chat.completions.create(\n",
+    "        messages=[\n",
+    "            {\n",
+    "                \"role\": \"system\",\n",
+    "                \"content\":p,\n",
+    "            },\n",
+    "        ],\n",
+    "        model=\"gpt-4o\",\n",
+    "    )\n",
+    "    chat_list.append(chat_completion.choices[0].message.content.strip())\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import json\n",
+    "import json_repair\n",
+    "chat_score_list = []    \n",
+    "for chat in chat_list:\n",
+    "    try:\n",
+    "        data = json_repair.loads(chat.strip('```json').strip('```'))\n",
+    "        chat_score_list.append(data)\n",
+    "    except:\n",
+    "        chat_score_list.append(0)\n",
+    "        print('Error in chat:', chat)\n",
+    "\n",
+    "all_score1 = [data['Score1'] for data in chat_score_list]\n",
+    "all_score2 = [data['Score2'] for data in chat_score_list]\n",
+    "all_score3 = [data['Score3'] for data in chat_score_list]\n",
+    "\n",
+    "all_score1_1 = all_score1.count(1)\n",
+    "all_score1_0 = all_score1.count(0)\n",
+    "all_score1_neg = all_score1.count(-1)\n",
+    "\n",
+    "all_score2_1 = all_score2.count(1)\n",
+    "all_score2_0 = all_score2.count(0)\n",
+    "all_score2_neg = all_score2.count(-1)\n",
+    "\n",
+    "all_score3_1 = all_score3.count(1)\n",
+    "all_score3_0 = all_score3.count(0)\n",
+    "all_score3_neg = all_score3.count(-1)\n",
+    "\n",
+    "all = len(all_score1)\n",
+    "print(all_score1_1, all_score1_0, all_score1_neg)\n",
+    "print(all_score2_1, all_score2_0, all_score2_neg)\n",
+    "print(all_score3_1, all_score3_0, all_score3_neg)\n",
+    "\n",
+    "print(f\"Score1 1: {all_score1_1 / all * 100:.2f}\\%, Score1 0: {all_score1_0 / all * 100:.2f}\\%, Score1 -1: {all_score1_neg / all * 100:.2f}\\%\")    \n",
+    "print(f\"Score2 1: {all_score2_1 / all * 100:.2f}\\%, Score2 0: {all_score2_0 / all * 100:.2f}\\%, Score2 -1: {all_score2_neg / all * 100:.2f}\\%\")\n",
+    "print(f\"Score3 1: {all_score3_1 / all * 100:.2f}\\%, Score3 0: {all_score3_0 / all * 100:.2f}\\%, Score3 -1: {all_score3_neg / all * 100:.2f}\\%\")"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Tianyu_agent",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "name": "python",
+   "version": "3.9.19"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/test/minirag/dataset/LiHua-World/qa/query_set.csv b/test/minirag/dataset/LiHua-World/qa/query_set.csv
new file mode 100755
index 0000000..4997314
--- /dev/null
+++ b/test/minirag/dataset/LiHua-World/qa/query_set.csv
@@ -0,0 +1,638 @@
+Question,Gold Answer,Evidence,Type
+Did Adam Smith send a message to Li Hua about the upcoming building maintenance schedule before the administrators announced a temporary change in the construction schedule due to weather conditions?,Yes,20260121_10:00<and>20260701_10:00,Multi
+"Did Wolfgang ask Li Hua about watching ""Star Wars: A New Hope"" after he asked Li Hua about going to see ""Overwatch 3""?",Yes,20260121_13:00<and>20261009_17:00,Multi
+Did Li Hua agree to go out for dinner after Wolfgang first asked him if he wanted to go out for dinner?,Yes,20260123_17:00<and>20260930_16:00,Multi
+Did Li Hua send a message to Jennifer thanking her for the new training schedule before he requested a change in his training schedule for Thursday?,Yes,20260204_15:00<and>20260204_16:00<and>20260211_19:00,Multi
+Did Li Hua ask Jennifer for advice on how to prevent muscle soreness after an intense workout session before he told her that he feels soreness in his arm muscles after the workout this week?,Yes,20260206_16:00<and>20260811_11:00<and>20261008_14:00<and>20261211_11:00,Multi
+Did Li Hua send a message to Jennifer asking for her opinion on protein supplements before he consulted her about his daily protein powder consumption?,Yes,20260214_16:00<and>20261120_20:00,Multi
+"Did Yuriko ask Li Hua for help with her studio's homepage before she booked a seat at the ""Central Perk"" cafe?",Yes,20260223_15:00<and>20260225_15:00,Multi
+Did Li Hua discuss his progress with the fitness plan before he shared a blog post about his recent fitness achievements?,Yes,20260305_17:00<and>20260325_19:00<and>20260610_16:00<and>20260630_18:00<and>20260708_14:00<and>20260817_12:15<and>20261022_22:00<and>20261202_14:00,Multi
+Did Li Hua send a message to Jennifer asking if he can turn the Thursday class to Friday after he requested a change in his training schedule for Thursday?,Yes,20260211_19:00<and>20260309_12:00,Multi
+Did Li Hua ask Yuriko to play music together before Wolfgang proposed to pause playing musical instruments?,Yes,20260318_15:10<and>20260416_21:00,Multi
+"Did Wolfgang Schulz recommend the band learns ""Viva la Vida"" by Coldplay after he and Li Hua discussed what song to play this Sunday?",Yes,20260318_15:30<and>20260625_19:00,Multi
+Did Wolfgang's promotion announcement occur before he invited Li Hua for dinner on 20260430?,Yes,20260428_18:30<and>20260930_16:00,Multi
+Did Turalyon announce the construction updates and feedback from residents after Illidan Stormrage complained about the construction noise?,Yes,20260526_15:40<and>20260527_10:00<and>20260528_15:00,Multi
+Did Hailey announce the new line of high-protein breads before inviting Li Hua to the special bakery event?,Yes,20260611_12:45<and>20260622_13:30,Multi
+Did Chae tell Li Hua that taking a warm shower before sleeping can improve the sleep quality before sharing the neuroscience article with her?,No,20260711_11:00<and>20260926_10:30,Multi
+Did Li Hua ask Jennifer Moore for book recommendations on fitness nutrition before she announced the special guest speaker at the gym?,No,20260713_19:30<and>20260817_12:15<and>20260831_19:00,Multi
+Did Jennifer remind Li Hua about proper nutrition and hydration before Jake shared his tips for staying hydrated during the match?,Yes,20260804_14:00<and>20261001_18:00,Multi
+Did the group members talk about their favorite characters in the TV series Game of Thrones after Emily started a vote on the most hateable character?,Yes,20260902_16:00<and>20260915_12:00,Multi
+Question: Did Jennifer remind Li Hua to consume enough protein after the workout before she shared tips with the group members on common mistakes to avoid after an intense workout?,Answer: Yes,20260919_10:00<and>20261101_11:00,Multi
+Did Li Hua ask Thane about his opinion on The Last of Us before he asked about Sekiro: Shadows Die Twice?,No,20261001_20:00<and>20261228_14:00,Multi
+Did Jake Watson and Li Hua discuss the classic matches between FC Barcelona and FC Bayern Munich before the group members discussed the classic matches between FC Barcelona and Real Madrid?,Yes,20261005_10:05<and>20261026_16:00,Multi
+Did the group members debate about the best football manager in the Premier League history after they debated if Pep Guardiola is the greatest soccer manager in football history?,Yes,20261006_10:00<and>20261110_10:00,Multi
+Did the discussion about Jaime Lannister's character occur after the discussion about Cersei Lannister's character?,Yes,20261010_10:10<and>20261026_20:00,Multi
+Did Wolfgang ask Li Hua if she wants to have pizza for dinner after work today before he wondered if she wanted to have Sichuan hot pot for dinner tonight?,No,20260930_16:00<and>20261015_15:00,Multi
+Did Jennifer challenge Li Hua to do 60 pull-ups in a training session after she challenged him to do 100 pushups?,Yes,20261104_18:00<and>20261129_19:00,Multi
+Did Jake share common knowledge about offside in soccer with Li Hua before he passed practical techniques to Li Hua on how to avoid offside for a forward?,Yes,20261105_15:00<and>20261130_11:30,Multi
+Did Wolfgang arrive in Hong Kong after he informed Li Hua about his upcoming trip?,Yes,20261219_19:00<and>20261223_23:00,Multi
+"What time does Li Hua watch the movie ""Overwatch 3""?",20260122,20260122_17:00<and>20260121_13:00,Multi
+"Who does Li Hua go to watch the movie ""Overwatch 3"" with?",Wolfgang,20260122_17:00<and>20260121_13:00,Multi
+Has Wolfgang ever been to Hong Kong?,Yes,20261219_19:00<and>20261220_20:00<and>20261221_12:00<and>20261228_10:00,Multi
+Who knows about Wolfgang going to Hong Kong?,LiHua & Chae & Yuriko,20261219_19:00<and>20261220_20:00<and>20261221_12:00<and>20261228_10:00,Multi
+Who wished Li Hua a happy Lunar New Year?,Adam Smith & Jennifer Moore & Wolfgang Schulz,20260119_11:30<and>20260119_14:30<and>20260119_09:30,Multi
+Who introduced the bread delivery service and recommend Alice for the delivery?,HaileyJohnson,20260318_15:00<and>20260329_13:00,Multi
+What is the opportunity that makes Wolfgang and Yuriko acquaitances?,LiHua introduce them to each other by saying that they can play music together every Sunday,20260318_15:10<and>20260319_16:00,Multi
+What was the content of the first-ever delivery from Hailey to LiHua and what was LiHua's opinion about it?,a fresh sourdough loaf and a bottle of milk and LiHua praises Hailey's bread and milk,20260317_08:00<and>20260318_15:00,Multi
+What opportunity did LiHua create for Chae to meet Wolfgang and Yuriko?,LiHua introduced Chae to Wolfgang and Yuriko during the band's gathering on Sunday evening,20260425_21:00<and>20260425_23:30,Multi
+What special offerings did Hailey have for her backery shop in the month of May?,a special Mother's Day bakery promotion & a special summer promotion on ice cream & a free baking class at the end of May & banana durian cheesecake,20260508_08:00<and>20260514_14:00<and>20260527_16:00<and>20260531_20:00,Multi
+What feedbacks does Hailey ask from LiHua in July?,feedback on the bread delivery service & customer feedback on a new line of artisanal donuts,20260710_08:30<and>20260712_16:00,Multi
+How long does it take in total from LiHua planning on getting the air-conditioner to the air-conditioner been installed?,about 27 days,20260716_10:00<and>20260812_11:00,Multi
+Did it take more than 3 weeks from LiHua planning on getting the air-conditioner to the air-conditioner been actually installed?,Yes,20260716_10:00<and>20260812_11:00,Multi
+Did it take more than a week from Adam asking LiHua about the ideal installation date to Adam reminding LiHua about the contractor team installing air-conditioner at 18:00?,Yes,20260803_13:00<and>20260812_11:00,Multi
+Who does LiHua want to invite to the photo exhibition and who goes with him (during August)?,Wolfgang,20260801_19:00<and>20260805_16:00,Multi
+Is the time interval between LiHua asking JakeWatson to help him with dribbling skills and Li Hua asking the group about classic must-watch UCL matches more than 2 days (restrict your search within August)?,Yes,20260819_10:00<and>20260821_15:00,Multi
+Is the time interval more than 3 days between LiHua asking Adam to help him install a curtain on the basement window and Adam asking LiHua to measure the size of the window?,Yes,20260921_16:00<and>20260928_10:00,Multi
+Is the time interval more than 7 days between Adam asking LiHua to measure the size of the window and Adam informing Li Hua that he has booked the curtain of the right size?,Yes,20260928_10:00<and>20261007_12:00,Multi
+Is the time interval more than 3 days between LiHua confirming that he has received the curtain and Adam asking LiHua if the curtain is all good?,Yes,20261012_10:00<and>20261019_20:00,Multi
+Is the time interval more than 3 days between LiHua first asking Adam if he can buy a small fridge for the basement and Adam asking LiHua about the size of the fridge?,Yes,20261110_11:00<and>20261116_16:00,Multi
+Is the time interval more than 7 days between Adam asking LiHua about the size of the fridge and Adam informing LiHua that the fridge will be delivered at 4pm next day?,Yes,20261116_16:00<and>20261123_23:00,Multi
+Wolfgang suddenly becomes very concerned about good body shape and healthy food choices in December. What are the two conversations he had with LiHua in December that reflect this?,20261202_14:00 & 20261209_19:00,20261202_14:00<and>20261209_19:00,Multi
+Did Li Hua agree to have dinner with Wolfgang after he told Wolfgang about the lunch arrangement?,Yes,20260105_11:00<and>20260930_16:00,Multi
+Did Li Hua ask Wolfgang Schulz for a recommendation on a gym or fitness center before asking Jennifer Moore for book recommendations on fitness nutrition?,Yes,20260111_08:00<and>20260831_19:00,Multi
+Did Li Hua ask Wolfgang Schulz if he wants to go to the gym together before Jennifer reminded Li Hua to participate in the gym's membership feedback activity?,Yes,20260112_10:00<and>20260605_11:00,Multi
+Did Li Hua send a message to Wolfgang Schulz saying that he has prepared all the delicious food for tonight's Chinese Lunar New Year before Wolfgang sent a message to Li Hua wishing him a happy Lunar New Year?,Yes,20260113_11:00<and>20260118_12:00<and>20260119_11:30<and>20260119_14:30<and>20260119_09:30,Multi
+Did Li Hua provide feedback to Jennifer Moore on his new meal plan before he asked her for advice on a healthy meal plan?,No,20260115_16:45<and>20260122_15:00,Multi
+Did Li Hua's complaint about the customer who modifies their requirements occur before Wolfgang comforted him?,No,20260123_17:30<and>20260131_14:00,Multi
+Did Adam Smith send Li Hua a reminder about the upcoming rent due date before Li Hua sent a message about having already transferred the rent on 20260301?,Yes,20260127_20:30<and>20260227_18:30<and>20260301_10:00<and>20260330_18:00<and>20260331_17:00<and>20260429_17:00<and>20260429_18:00,Multi
+Did Li Hua share a blog post about his recent fitness achievements after Jennifer sent him a motivational message?,Yes,20260520_18:00<and>20260606_09:00<and>20260817_12:15<and>20261022_22:00<and>20261202_14:00,Multi
+Did Li Hua send a follow-up message to Jennifer before she asked him about his latest sleeping schedule?,Yes,20260205_13:00<and>20260725_10:00,Multi
+Did Li Hua ask Adam Smith about placing potted plants in the basement before he asked about decorating the basement?,No,20260219_20:00<and>20261214_14:00,Multi
+Did Li Hua ask Wolfgang for advice on renovating the basement before he invited Adam Smith to check the progress of the basement renovation?,Yes,20260219_20:10<and>20260223_17:00<and>20260707_16:00,Multi
+Did Li Hua tell Wolfgang about making a new friend before Yuriko reminded the group members to meet for the music festival?,Yes,20260302_18:00<and>20260318_14:27<and>20261212_12:00,Multi
+"Did Yuriko tell Li Hua about booking a seat at the ""Central Perk"" cafe before Li Hua sent her a message to confirm the details of their next meeting?",Yes,20260225_15:00<and>20260303_09:30,Multi
+What time does Li Hua check in with Adam about moving in?,5:30 PM,20260105_14:00,Single
+When was the first time Li Hua had dinner with Wolfgang this year?,20260108,20260108_11:00,Single
+Where was the first time Li Hua had dinner with Wolfgang this year?,the cozy café downtown,20260108_11:00,Single
+What time is Li Hua's lunch with Wolfgang Schulz at the cozy café downtown?,20260108,20260108_11:00,Single
+What is the Wi-Fi password at Li Hua's house?,Family123,20260106_09:00,Single
+What does Adam say about having friends over?,having friends over occasionally is fine,20260106_09:00,Single
+What house rule does Adam mention?,keep noise to a minimum during late hours,20260106_09:00,Single
+What does Li Hua report to Adam on January 6th?,the water tab in the apartment is broken,20260106_13:00,Single
+When does Adam confirm the plumber will arrive?,tomorrow at 10 AM,20260106_15:00,Single
+What does Li Hua ask Adam about the door hinge?,a small repair,20260107_15:00,Single
+What is the name of the gym that Wolfgang recommended LiHua to go to?,FitZone,20260111_08:00,Single
+When does Li Hua ask Jennifer Moore about adjusting the protein in her meal plan?,20260122,20260122_15:00<and>20260122_15:00,Multi
+"What time does Li Hua watch the movie ""Overwatch 3""?",20260122,20260122_17:00<and>20260121_13:00,Multi
+"Who does Li Hua go to watch the movie ""Overwatch 3"" with?",Wolfgang,20260122_17:00<and>20260121_13:00,Multi
+When does Li Hua plan to celebrate Chinese Lunar New Year?,20260118,20260113_11:00,Single
+What does Li Hua plan to celebrate Chinese Lunar New Year?,dumplings,20260113_11:00,Single
+Who does Li Hua plan to celebrate Chinese Lunar New Year with?,Wolfgang,20260113_11:00,Single
+Li Hua accidentally broke a light fixture once. What did Adam say about the light fixture?,I'll arrange for a professional to take a look,20260108_19:00,Single
+Li Hua has a difficult client. When did he solve that client's project?,20260209,20260123_17:30<and>20260209_21:00,Multi
+When did Li Hua express frustration about a client changing requirements?,20260131_14:00,20260131_14:00,Single
+Who suggested Li Hua keep a clear log of all the changes requested by the client?,Wolfgang Schulz,20260131_14:00,Single
+What did Wolfgang Schulz suggest Li Hua to do when Li Hua face a difficult client?,keep a clear log of all the changes requested by the client,20260131_14:00,Single
+What does Li Hua plan to do to clear their head?,go for a walk and find a new café,20260131_14:00,Single
+What is the name of the café Wolfgang Schulz recommends to Li Hua?,The Java Spot,20260131_14:00,Single
+Can Li Hua hang his artworks on the wall?,Yes,20260201_20:00,Single
+What does Adam Smith suggest Li Hua use to hang artwork without damaging the walls?,use removable hooks,20260201_20:00,Single
+How many times does Li Hua train per week now?,2,20260204_15:00 ,Single
+When did Li Hua change the training plan?,20260204,20260204_15:00,Single
+What days does Li Hua takes for the training sessions?,Tuesdays and Thursdays,20260204_15:00,Single
+When did Li Hua ask Jennifer Moore for tips on dealing with muscle soreness?,20260206_16:00,20260206_16:00,Single
+What are some of Jennifer Moore's tips for dealing with muscle soreness?,hydration&active recovery&stretching&foam rolling&rest,20260206_16:00,Single
+Why did Li Hua ask for extra exercises to do at home?,to complement his training schedule,20260205_13:00,Single
+What exercises did Jennifer Moore suggest Li Hua do at home?,bodyweight squats& plank holds&push-ups&lunges&glute bridges,20260205_13:00,Single
+What is Adam Smith's condition for Li Hua to add decorations to the basement?,must be reversible and not damage anything,20260219_20:00,Single
+When was the first time Wolfgang went to Li Hua's basement?,20260221,20260221_15:00,Single
+What ideas did Wolfgang Schulz suggest for Li Hua's basement practice spot?,good lighting&soundproofing&a comfy chair,20260219_20:10,Single
+Why did Adam remind Li Hua not to play guitar late at night?,a few neighbors have mentioned they're hearing guitar music late at night,20260216_10:00,Single
+What type of ambiance does YurikoYamamoto want for her studio's homepage?,more welcoming and engaging,20260223_15:00,Single
+When did Li Hua invite Adam Smith to check the basement renovation progress?,20260223_19:00,20260223_17:00,Single
+What is the name of the café where Li Hua and YurikoYamamoto first meeting to talk about Yuriko's website?,Central Perk,20260225_15:00,Single
+What is the essence of YurikoYamamoto Li Hua is helping with her homepage?,speech therapy,20260223_15:00,Single
+What type of instrument does Li Hua play in the basement?,guitar,20260223_17:00,Single
+When did Adam Smith inform Li Hua about potential issues with the pipes in the basement?,20260301_13:00,20260301_13:00,Single
+When did Li Hua inform Adam Smith that the rent was transferred?,20260301_10:00,20260301_10:00,Single
+When is the music concert that Wolfgang invites Li Hua to?,20260307_18:00,20260302_18:00,Single
+What dish does Li Hua agree to bring to the neighborhood potluck dinner?,Homemade pasta salad,20260302_18:45,Single
+Who is Li Hua meeting with to discuss homepage design updates?,Yuriko Yamamoto,20260303_09:30,Single
+What new feature does Yuriko Yamamoto consider adding to her studio's homepage?,A blog section,20260307_13:00,Single
+What time is the power outage in the neighborhood?,2 PM to 3 PM,20260307_14:45,Single
+What suggestions does Li Hua give for promoting the new scheduling feature?,Showcase it on social media platforms and include a short tutorial and send out a newsletter to clients,20260311_14:30,Single
+Who invites Li Hua to join the community bake sale?,Adam Smith,20260312_12:30,Single
+What day and time is the community bake sale taking place?,Sunday at 3 PM,20260312_12:30,Single
+When does Li Hua request a delivery from Hailey Johnson?,Tuesday,20260314_17:00,Single
+What is the address where Li Hua wants the bread delivery to be made?,123 Sunny Street,20260314_17:00,Single
+What service does Hailey Johnson offer to Li Hua?,Doorstep delivery service for fresh milk and bread,20260314_17:00,Single
+What time does Hailey Johnson start baking?,4 AM,20260317_08:00,Single
+Where does Li Hua plan to meet Yuriko Yamamoto to show the final website?,Central Perk café,20260317_15:30,Single
+What does Li Hua suggest to Hailey regarding the frequency of bread delivery?,Twice a week on Mondays and Fridays at 8am,20260318_15:00,Single
+What does Li Hua agree to bring to the bonfire singing party hosted by Chae Song-hwa?,Li Hua will bring his guitar,20260320_18:00,Single
+What is the focus of Li Hua's next month's fitness plan according to Jennifer?,Strengthening lower limbs,20260325_19:00,Single
+What is the building's policy that Adam reminds Li Hua about?,Recycling policy,20260328_15:00,Single
+What is the topic of the online tutorial Yuriko shares with the group?,Advanced drum techniques,20260329_10:00,Single
+What is Wolfgang looking for in his new drums?,Something versatile that sounds good for both rock and softer tunes like The Beatles,20260326_16:00,Single
+What song does Li Hua suggest for the jam session on 20260405?,Viva la Vida,20260405_10:00,Single
+What does Li Hua think about the rosemary focaccia?,Li Hua thinks the rosemary focaccia is amazing,20260331_14:00,Single
+When does Li Hua confirm the rent transfer to Adam?,20260331_17:00,20260331_17:00,Single
+What joke does Wolfgang make as an April Fool's joke?,That Wolfgang bought a set of expensive drums,20260401_15:00,Single
+Who is delivering the bread to Li Hua on 20260403?,Alice,20260403_08:00,Single
+What does Li Hua think about improvisation during the jam session?,Improvisation sounds great,20260402_19:00,Single
+When is ChaeSong-hwa hosting the community medical knowledge lecture?,7 PM on Saturday,20260407_19:00,Single
+What topics will be covered in the community medical knowledge lecture?,Basics of common health issues and how to prevent them,20260407_19:00,Single
+What new song does the Jolly band decide to work on for the jam session according to their discussion on 20260410?,Stand By Me,20260410_11:00,Single
+What is Li Hua's feedback on Chae Song-hwa's medical knowledge lecture?,It is insightful and makes complex topics easy to understand,20260411_21:00,Single
+When is the anniversary event of Hailey Johnson's bakery shop?,April 15 to 17,20260413_21:00,Single
+What does Li Hua want to have on Hailey's bakery shop anniversary event?,Sourdough and sweet pastries,20260413_21:00,Single
+Why does Li Hua ask ChaeSong-hwa about whether neurosurgeons actually use test tubes in their work?,Li Hua is trying to get some insights for a website design,20260414_16:00,Single
+Who proposes that the band takes a break from jamming this week?,Wolfgang Schulz,20260416_21:00,Single
+What suggestions does Li Hua propose to Adam Smith about the upcoming community garden renovation?,Add more seating areas for people to relax and enjoy the space and some flower beds with native plants,20260417_11:00,Single
+What kinds of flowers does Li Hua recommend to Adam Smith for the flower beds?,Lavender and coneflowers and fresh herbs,20260417_11:00,Single
+What will be a gift for Li Hua if he chooses to renew the fitness contract with Jennifer Moore?,A cool fitness bag as a gift for all the gym activities,20260420_21:00,Single
+When is the karaoke activity organized by ChaeSong-hwa?,Saturday at 7 PM,20260421_16:00,Single
+Who is Li Hua bringing to the band's jam session according to their discussion on 20260425?,ChaeSong-hwa,20260425_21:00,Single
+What garden-related activity is Thrall planning to organize?,A community planting day,20260426_1330,Single
+What is the proposed solution for making the garden more inviting on sunny days?,Adding shade with umbrellas or trees,20260427_10:30,Single
+What is the main topic of the conversation on 2026-04-28 at 5 PM?,Breathing techniques and tips for squats during workouts,20260428_17:00,Single
+When is Wolfgang Schulz's promotion celebration dinner?,6 PM on the day after tomorrow (implied to be 2026-04-30),20260428_18:30,Single
+What is the name of the Italian restaurant where Wolfgang and Li Hua are having dinner to celebrate Wolfgang's promotion?,Venedia Grancaffe,20260430_17:00,Single
+What is Li Hua's suggestion for scheduling the water pipe repairs in the garden?,During off-peak hours,20260501_16:00,Single
+When is the community meeting for the garden project scheduled according to the discussion on 20260507?,Saturday at 10 am,20260507_16:00,Single
+What percentage discount is Hailey Johnson offering for Mother's Day pastries?,15%,20260508_08:00,Single
+Which two specific pastries does Hailey Johnson recommend for Mother's Day?,Raspberry-filled croissants and chocolate eclairs,20260508_08:00,Single
+What type of stretches does JenniferMoore suggest before and after workouts?,Dynamic stretches before and static stretches after,20260510_11:30,Single
+When is the web design seminar at Wolfgang's company happening?,Thursday at 3 PM,20260511_11:00,Single
+What is Li Hua looking forward to trying from the summer promotion?,Fruity ice cream flavors and a mango-coconut pastry,20260514_14:00,Single
+What did Li Hua enjoy the most about the restaurant that he and Wolfgang visited for dinner on 20260514?,The pasta dish and the dessert,20260514_22:00,Single
+Why is Chae Song-hwa unable to join the rehearsal?,She has to attend a medical lecture,20260515_10:00,Single
+What type of lighting is preferred for the seating area in the community garden?,Soft white lights,20260516_10:00,Single
+When is the construction of the garden supposed to start?,This Wednesday (20260520),20260518_10:00,Single
+Which flowers does RexxarRemar suggest for a vibrant vibe?,Bluebell and Camellia and Tulip,20260518_10:00,Single
+How does RexxarRemar plan to spend time in the garden once it's done?,For family gatherings and relaxing afternoons,20260518_10:00,Single
+What does LiHua find as a perfect place to work in his conversation with Chae?,The Lighthouse Cafe,20260521_15:00,Single
+What songs does Chae propose to the Jolly band to try out on Sunday according to the conversation on 20260521?,"The Yellow Wind Rises and ""To the West""",20260521_20:00,Single
+What is IllidanStormrage's suggestion to the ongoing garden construction according to the discussion on 20260522?,plan some quiet hours when the kids are playing,20260522_20:00,Single
+What songs does LiHua suggest adding to the karaoke playlist in his conversation with Chae?,"I Will Survive and ""Sweet Caroline""",20260525_15:00,Single
+What time does Turalyon plan to limit noisy activities?,From 2-3 pm,20260527_10:00,Single
+Why does WolfgangSchulz inquire of the band members about health products?,Wolfgang is feeling very tired lately with the overtime and wants to boost up his energy,20260527_23:00,Single
+What is the name of the song LiHua suggests revisiting for the band's music session according to their conversation on 20260529?,The History of Everything,20260529_17:00,Single
+Which kind of pastry does Li Hua express his interest in trying in his conversation with Hailey on 20260531?,the new banana durian cheesecake,20260531_20:00,Single
+When did Li Hua inform Adam Smith that the rent for last month (implied to be May) had been transferred?,2024-06-02 at 10:00,20260602_10:00,Single
+What is the name of Hailey Johnson's new weekly flavor cheese?,Hazelnut Basque Roasted Cheese,20260603_09:45,Single
+Why can't people sit on the benches when Turalyon informs the community that the benches have been installed on 20260604?,the paint isn't dry yet,20260604_18:00,Single
+What is the phone number for the maintenance worker that Adam Smith provided to Li Hua for the broken streetlight?,314159,20260605_18:45,Single
+What breathing technique does Jennifer Moore suggest for running?,Inhaling for 3 steps and exhaling for 2 steps,20260606_09:00,Single
+When is the Freelancer Group Meeting scheduled for according to the conversation between LiHua and Yuriko?,This Wednesday at 3 pm,20260608_14:30,Single
+What suggestions does Jennifer give to LiHua to help boost his endurance?,try incorporating longer cardio sessions and interval training into your routine,20260610_16:00,Single
+What new pastry does Hailey think is perfect for a fitness lover like Li Hua?,high-protein breads,20260611_12:45,Single
+Who proposes adding some outdoor games or a small water feature to the children's play areain the community discussion?,Thrall,20260612_15:00,Single
+Who proposes adding picnic tables for families to enjoy some snacks after playing?,Li Hua,20260612_15:00,Single
+Who proposes creating a little garden area where kids can help plant flowers or vegetables?,GromHellscream,20260612_15:00,Single
+Why does LiHua bring up Adam Smith in the band's conversation on 20260616?,LiHua thinks it is really nice for Mr. Smith to rent this basement to us for practice,20260616_10:40,Single
+What event does LiHua think is an opportunity for the Jolly band to perform in front of the crowd?,the poster saying that the town is holding a local music festival,20260617_17:00,Single
+What dietary restrictions does LiHua have in his conversation with Hailey?,No dietary restrictions,20260618_11:30,Single
+What will special guest speaker be talking about as Jennifer mentions to LiHua?,about nutrition for athletes,20260619_0815,Single
+What is the survey that Jennifer wants LiHua to fill out about?,the experience with our training sessions so far,20260622_11:00,Single
+When does the special event at Hailey's bakery start according to Hailey and LiHua's conversation on 20260622?,The event starts at 10 AM this Saturday,20260622_13:30,Single
+What video does LiHua send to the band group chat on 20260624?,"a video of himself playing the intro to ""Stairway to Heaven""",20260624_14:00,Single
+Waht song does Wolfgang propose adding to the band's practice set on 20260625?,Viva la Vida by Coldplay,20260625_19:00,Single
+What is the main topic of the conversation between LiHua and ChaeSong-hwa on 20260626?,Chae and her team making a breakthrough in the research study and LiHua congratulating her,20260626_11:00,Single
+Waht songs does Chae recommend in the band's group discussion on 20260629?,"Uptown Funk or ""Happy""",20260629_18:30,Single
+What tip does Jennifer give to the gym members on 20260630?,Staying hydrated during workout is very important,20260630_15:30,Single
+Why deoes the construction have to be postponed according to Tirion Fordring?,the storm and the rain have been going on for days,20260701_10:00,Single
+What kinds of Thai food does Wolfgang want to try out at the Thai restaurant on 20260702?,pad thai and maybe some spring rolls,20260702_15:00,Single
+What song does Yuriko propose that the band can practice this weekend according to the band's discussion on 20260703?,Take Me Home Country Roads by John Denver,20260703_11:45,Single
+What is the name of the song that Yuriko recommend to the band on 20260706?,Rolling in the Deep by Adele,20260706_19:30,Single
+Why are LiHua and Adam concerned about the basement in their conversation on 20260707?,They want to check if there were any issues in the basement after those rainstorms,20260707_16:00,Single
+Why is Jennifer checking in in the gym group chat on 20260708?,She wants to hear how the members are all doing and offer some personalized advice.,20260708_14:00,Single
+What is the article that Chae shares with LiHua about?,how to fall asleep faster at night,20260711_11:00,Single
+What is LiHua's feeback on Hailey's new artisanal donuts?,The flavors are so unique and delicious,20260712_16:00,Single
+What new flavor is LiHua looking for as he mentions to Hailey in their conversation on 20260714?,a matcha flavor,20260714_12:00,Single
+Which performance does Chae want the band members to check out in their discussion on 20260715?,"the amazing live performance of ""Bohemian Rhapsody"" by Queen",20260715_19:00,Single
+Why is LiHua praising Chae in their conversation on 20260717?,LiHua was really amazed by Chae's performance in the band last Sunday because Chae's singing has really leveled up,20260717_12:00,Single
+What cool thing does Wolfgang find that he wants to share with LiHua?,this awesome guitar Wolfgang found online,20260718_18:00,Single
+What tips on balancing work and personal hobbies does LiHua give to Chae?,try setting specific hours for work and separate times for your hobbies and don't forget to schedule some fun time for yourself,20260722_13:00,Single
+What is the size of the basement?,The basement is approximately 15 feet by 20 feet,20260723_14:00,Single
+What is Wolfgang's favorite superhero?,Iron Man,20260726_16:00,Single
+What is LiHua's favorite superhero?,Spider-Man,20260726_16:00,Single
+What sports would LiHua like to see on the community sports day?,some team sports like soccer or basketball,20260727_16:30,Single
+What activities are being discussed for the community sports day?,Soccer and basketball and tug-of-war and sack race are mentioned as potential activities for the community sports day,20260727_16:30,Single
+What new hobbies does Wolfgang Schulz propose to Li Hua?,Wolfgang Schulz proposes pottery or painting as a hobby to Li Hua,20260728_18:00,Single
+What brands of air-conditioners does LiHua recommend to Wolfgang?,Mitsubishi and Daikin,20260729_14:00,Single
+Who is recommended by Jennifer to share some fitness tips in the gym group chat on 20260730?,LiHua,20260730_17:30,Single
+What does Hailey Johnson want Li Hua to help with at the bakery according to their discussion on 20260731?,Hailey Johnson wants Li Hua to help gather feedback from customers at the bread-tasting event,20260731_13:00,Single
+What day do Li Hua and Wolfgang Schulz plan to visit the photography exhibition?,Li Hua and Wolfgang Schulz plan to visit the photography exhibition on Friday evening,20260801_19:00,Single
+When is Wolfgang and LiHua going to the photography exhibition on 20260805?,19:00,20260805_16:00,Single
+What new components are being added to the garden project as Turalyon announces it on 20260806?,incorporating sustainable practices like recycling and composting into our renovation project,20260806_16:30,Single
+Why is Yuriko asking for LiHua's help on 20260807?,She has got ten logo designs for my speech therapy studio and she would love LiHua's opinion on which one stands out the most,20260807_12:00,Single
+When is the installation of the air-conditioner for the basement?,It's set for Wednesday next week,20260809_12:00,Single
+Why does Adam reach out to LiHua on 20260810?,To check if LiHua had a chance to look over the warranty and maintenance plans for the air conditioner,20260810_18:00,Single
+What tips does Jennifer give to LiHua about preventing muscle soreness after a tough workout?,make sure to warm up properly before workouts and cool down afterward & Stretching & Stay hydrated and consider foam rolling post-session to help with recovery,20260811_11:00,Single
+When is the contractor team going to install the air-conditioner?,20260812 6PM,20260812_11:00,Single
+When is the garage sale being discussed?,20260814_15:00,20260814_15:00,Single
+What is the reason Thrall cannot participate in the garage sale?,Thrall has a lot happening with the garden renovations,20260814_15:00,Single
+What does RexxarRemar suggest to make the garage sale more enjoyable?,RexxarRemar suggests having snacks or drinks at the garage sale,20260814_15:00,Single
+Who offers to help with setting up for the garage sale?,AdamSmith,20260814_15:00,Single
+What type of stretches does Sage prefer before hitting the gym?,Sage prefers dynamic stretches,20260816_17:00,Single
+What does JenniferMoore suggest to improve performance and prevent injuries?,JenniferMoore suggests incorporating stretching techniques,20260816_17:00,Single
+What does Viper plan to do to increase their stamina?,Viper plans to focus on cardio,20260817_12:15,Single
+What does Sova plan to do to keep their cardio interesting?,Sova plans to mix it up with different types of cardio exercises,20260817_12:15,Single
+How much discount is Hailey willing to give to LiHua according to their conversation on 20260816?,15% off,20260816_20:00,Single
+Who does Li Hua mention as their favorite character from The Witcher 3?,Geralt,20260818_10:00,Single
+Why does LiHua like Geralt from The Witcher 3?,He's such a complex character with that no-nonsense attitude but deep down he has a great sense of morality & his monster-slaying skills are just epic,20260818_10:00,Single
+What is Li Hua's opinion about Yennefer's character?,Intense & fiercely independent & her relationship with Geralt evolves,20260818_10:00,Single
+Which scene stood out for Li Hua in The Witcher 3?,"The ""Battle of Kaer Morhen""",20260818_10:00,Single
+Which scene in The Witcher 3 does ThaneChambers consider as one of his favorites involving Yennefer?,The scene when Geralt is looking for Yennefer in the early part of the game,20260818_10:00,Single
+What is Li Hua's impression of the Blood and Wine expansion?,Incredible & new area is stunning & story feels like a mini-epic,20260818_10:00,Single
+What is Li Hua's opinion on the characters in Succession?,Intense family dynamics and business war,20260818_14:00,Single
+What TV show does EmilyBurnett recommend to Li Hua after discussing Succession?,The Crown and Ted Lasso,20260818_14:00,Single
+When does Li Hua plan to meet JakeWatson for soccer practice?,Saturday afternoon at 3 PM,20260819_10:00,Single
+What are some of the upcoming PS5 exclusives discussed?,Final Fantasy XVI & Marvel's Spider-Man 2 & Ghostwire: Tokyo,20260819_18:00,Single
+Which upcoming PS5 exclusive game does Gavriel express curiosity about?,Ghostwire: Tokyo,20260819_18:00,Single
+Which UCL match does Jasper recommend as a classic must-watch?,The 2005 final between Liverpool and AC Milan,20260821_15:00,Single
+What skill does Li Hua plan to improve with WolfgangSchulz's help?,Data analysis and coding related to AI tools,20260822_17:00,Single
+Who shared a music theory tutorial with the group?,Chae Song-hwa,20260825_10:45,Single
+Which regions in Witcher 3 did Li Hua and Thane Chambers discuss?,Skellige and Toussaint,20260826_17:00,Single
+What is Kendall's relationship with Logan like according to the group's discussion?,Toxic,20260826_18:00,Single
+What is the topic of discussion between Li Hua and Wolfgang Schulz on burger preferences?,Medium rare vs. well-done meat patty in a classic American,20260828_10:00,Single
+What is Li Hua's preference for a burger patty's doneness?,Medium rare,20260828_10:00,Single
+What is Wolfgang's preference for a burger patty's doneness?,well-done,20260828_10:00,Single
+Why does Wolfgang prefer a well-done meat patty in a burger?,It is safer and more flavorful & he likes a little char on his burger,20260828_10:00,Single
+What toppings does Wolfgang Schulz prefer on his burger?,Cheese and bacon,20260828_10:00,Single
+What books does Jennifer recommend to LiHua for deepening understanding of fitness nutrition?,"The New Rules of Lifting and ""Precision Nutrition""",20260831_19:00,Single
+Why are Wolfgang and LiHua going to the local music store?,check out some new gear and get inspired for our next jam session,20260831_19:00,Single
+What is the main theme of game group's discussion on 20260901_13:00?,Dutch van der Linde's character and leadership,20260901_13:00,Single
+Who is the most hateable character in Game of Thrones in Orion's opinion?,Ramsay Bolton,20260902_16:00,Single
+Who is the most hateable character in Game of Thrones in Merrick's opinion?,Cersei Lannister,20260902_16:00,Single
+What is the common interest between Li Hua and Jake Watson?,Soccer,20260903_17:00,Single
+What kind of songs are Li Hua and Chae Song-hwa considering adding to their playlist for babies?,Lullabies,20260904_10:00,Single
+What makes Wolfgang feel under a lot of pressure according to his conversation with LiHua on 20260905?,this new software project,20260905_14:00,Single
+Why does AdamSmith ask LiHua about the basement on 20260907?,check in and see how the basement held up after the storm,20260907_14:00,Single
+How is the basement after the storm?,The basement is all good no water leakage at all,20260907_14:00,Single
+What is the latest good news from Wolfgang as of 20260908?,software project finally made some significant progress,20260908_15:00,Single
+What new songs are LiHua working on lately on his guitar as of 20260908?,"Blackbird and ""Hotel California""",20260908_15:00,Single
+What mission does Gavriel mention that shows Arthur's growth in the game?,the scene where Arthur tells John to take care of his family,20260909_10:00,Single
+What does Aisling find emotionally impactful about Red Dead Redemption 2's ending?,Arthur's realization of his fate and the music during the last ride,20260909_10:00,Single
+What side mission in Red Dead Redemption 2 resonated with Jareth?,the side mission with the widow in the honor system,20260909_10:00,Single
+What was ThaneChambers' favorite horse-related side quest in Red Dead Redemption 2?,the “troubled” horse,20260909_10:00,Single
+What funny glitch did Fionnuala experience in the game Red Dead Redemption 2?,Arthur ended up floating in mid-air after a cutscene,20260909_10:00,Single
+What was the final showdown that Elara found unforgettable in Red Dead Redemption 2?,with Dutch and Micah,20260909_10:00,Single
+How does Bronwyn describe the epilogue of the game Red Dead Redemption 2?,captivating after seeing John try to make a life for himself,20260909_10:00,Single
+Has LiHua ever met anyone like Shedlon Cooper in his real life?,No,20260910_12:00,Single
+Has EmilyBurnett ever met anyone like Shedlon Cooper in her real life?,No,20260910_12:00,Single
+What games are LiHua and ThaneChambers planning to check out during their shopping trip according to their discussion on 20260911?,"Spider-Man: Miles Morales and ""Demon's Souls""",20260911_14:00,Single
+Who is the best midfielder in the past decade in JakeWatson's opinion?,Luka Modrić,20260912_16:00,Single
+How many servings of vegetables should LiHua aim for each day to keep body in good shape according to Jennifer?,at least 5 servings of veggies a day,20260913_18:00,Single
+What does LiHua ask Jake about soccer?,the best strategies for soccer players to avoid injuries during games,20260914_10:00,Single
+What is Jake's first tip for avoiding soccer injuries?,proper warm-up and stretching,20260914_10:00,Single
+What does Jake suggest to improve flexibility and prevent muscle issues?,staying hydrated,20260914_10:00,Single
+Why is wearing the right footwear important in soccer?,to avoid slips or sprains,20260914_10:00,Single
+How does strength training help prevent soccer injuries?,It builds muscles around joints & reduces injury risk & improves stability and endurance,20260914_10:00,Single
+What type of exercises does Jake recommend for strength training?,squats & lunges & planks & balance exercises like single-leg stands,20260914_10:00,Single
+What is the purpose of starting with bodyweight exercises?,for those who are new to strength training,20260914_10:00,Single
+Who does EmilyBurnett think is a standout character in the Game of Thrones series?,Tyrion Lannister,20260915_12:00,Single
+What does Lachlan appreciate about Arya Stark's character development?,Her transition from an innocent girl to a fierce assassin,20260915_12:00,Single
+What moment does Rowan find mind-blowing in Arya's storyline?,When Arya confronts and takes down the Night King,20260915_12:00,Single
+Which moment involving Tyrion is Merrick's favorite?,When Tyrion blows up the Wildfire to save King's Landing,20260915_12:00,Single
+What memorable quote is mentioned by Phaedra?,"Jon Snow's ""You know nothing"" scene with Ygritte",20260915_12:00,Single
+"What does Rowan love about Tyrion's ""I drink and I know things"" line?",It captures his cleverness,20260915_12:00,Single
+What aspect of Jaime Lannister's storyline does Rowan find intriguing?,His transformation after meeting Brienne of Tarth,20260915_12:00,Single
+How does Phaedra view Jaime's character arc?,"Fascinating as he goes from ""Kingslayer"" to someone who values honor",20260915_12:00,Single
+What does Lachlan think about Jaime's final decision to protect Cersei?,It complicates his redemption and leaves mixed feelings,20260915_12:00,Single
+How does Saffron feel about Jaime's choice to protect Cersei?,Conflicted because it seems like he was slipping back into old ways,20260915_12:00,Single
+What does Kieran wish Jaime had done differently?,Chosen a different path after his character development,20260915_12:00,Single
+How does Niamh think Jaime could've ended up differently?,By staying true to the lessons he learned from Brienne,20260915_12:00,Single
+What could Jaime and Brienne have done to rebuild trust according to Merrick?,Worked on rebuilding trust through actions rather than just words,20260915_12:00,Single
+"What is LiHua curious about regarding the ""Man in Black"" in Westworld?",The motive behind his character,20260916_16:00,Single
+"What does EmilyBurnett think is the main motive for the ""Man in Black""?",To find deeper meaning and fulfillment in Westworld	20260916_16:00,What,Single
+"How does LiHua feel about the ""Man in Black's"" search for meaning?",Intrigued as it adds depth to the story,20260916_16:00,Single
+"What is EmilyBurnett's favorite moment of the ""Man in Black""?",When he confronts the truth about himself and his choices,20260916_16:00,Single
+"Which moment does LiHua find memorable for the ""Man in Black""?",When he shows vulnerability,20260916_16:00,Single
+What does LiHua think about the themes of Westworld?,They are thought-provoking by diving into consciousness and free will and what it means to be human,20260916_16:00,Single
+Which theme of Westworld resonates the most for LiHua?,The exploration of free will and choice,20260916_16:00,Single
+What does EmilyBurnett think about the future relevance of Westworld's themes?,They will become even more crucial as technology advances,20260916_16:00,Single
+What does WolfgangSchulz suggest incorporating into practice sessions according to the discussion on 20260916?,Improvisational solos,20260916_19:00,Single
+How does LiHua feel about the idea of improvisation based on the chat on 20260916?,It will make sessions more fun and creative,20260916_19:00,Single
+Who does JakeWatson consider the best defender in the history of FC Barcelona?,Carles Puyol,20260917_10:00,Single
+What does LiHua think about Gerard Piqué as a defender?,He deserves a mention for his skill and intelligence,20260917_10:00,Single
+Who are JakeWatson's and LiHua's favorite defenders?,Carles Puyol and Gerard Piqué,20260917_10:00,Single
+What is JakeWatson's favorite memory of Puyol and Piqué playing together?,The comeback against PSG in the Champions League,20260917_10:00,Single
+What match does JakeWatson cherish from the Champions League final against Manchester United?,The 2009 final,20260917_10:00,Single
+What is LiHua's favorite match involving Barcelona?,The 2013 Champions League match against AC Milan,20260917_10:00,Single
+Which goal does JakeWatson never forget from Messi?,Messi's solo goal against Getafe in 2007,20260917_10:00,Single
+What is LiHua's favorite player moment from Messi?,When Messi scored a header against Manchester United in 2011,20260917_10:00,Single
+What does LiHua think makes Messi the best?,His ability to create chances as well as his vision and work ethic and humility,20260917_10:00,Single
+How does JakeWatson view Messi's impact on future generations of players?,His dedication and consistency set the bar high for young players,20260917_10:00,Single
+Who suggests hitting Starbucks after work on 20260918?,WolfgangSchulz,20260918_18:00,Single
+What is JenniferMoore's reminder to LiHua about on 20260919?,Getting enough protein after workouts,20260919_10:00,Single
+Why is protein important according to JenniferMoore?,For keeping the body in shape,20260919_10:00,Single
+What is ThaneChambers' question to LiHua on 20260920?,LiHua's favorite first-person shooter game,20260920_16:00,Single
+Which game does LiHua mention he's been playing on 20260920?,Call of Duty: Warzone,20260920_16:00,Single
+"What does ThaneChambers say about ""Warzone""?",It's a blast,20260920_16:00,Single
+"What does LiHua like most about ""Warzone""?",The strategy involved and the adrenaline rush,20260920_16:00,Single
+"What is ThaneChambers' memorable ""Warzone"" experience?",Coming back from a tough spot to win at the last second,20260920_16:00,Single
+"What was LiHua's unforgettable match in ""Warzone""?",Being down to the last two and outsmarting the last team,20260920_16:00,Single
+"How does ThaneChambers feel about upcoming ""Warzone"" updates?",He's looking forward to them,20260920_16:00,Single
+"Does LiHua plan to play ""Warzone"" after the updates?",Yes because he loves exploring fresh content,20260920_16:00,Single
+"What does ThaneChambers propose to do with the ""Warzone"" updates?",Hop on together and tackle the new stuff,20260920_16:00,Single
+Who initiated the discussion about the community garden renovation and its benefits to local businesses?,Turalyon,20260921_10:00,Single
+What idea did IllidanStormrage propose to help involve the community in supporting local businesses?,Putting together a flyer with a list of local businesses,20260921_10:00,Single
+How does GromHellscream believe the local shops will benefit from the completed garden?,More foot traffic means more customers for them,20260921_10:00,Single
+What does ArthasMenethil describe the outcome of the garden project and supporting local vendors as?,A win-win,20260921_10:00,Single
+What does Thrall suggest they create to support local businesses?,A list of businesses they love to support,20260921_10:00,Single
+How does TirionFordring emphasize the importance of balancing the garden renovation with supporting local vendors?,By stating that both aspects are important for the best outcome,20260921_10:00,Single
+What does AdamSmith agree is crucial to align with the garden project schedule?,Support for local businesses,20260921_10:00,Single
+Who does LiHua ask for help with installing a curtain?,AdamSmith,20260921_16:00,Single
+When does AdamSmith say he is available to help with the curtain installation?,Wednesday or Thursday afternoon,20260921_16:00,Single
+On which day do LiHua and AdamSmith agree to install the curtain?,Thursday afternoon,20260921_16:00,Single
+Who will bring the tools for the curtain installation?,AdamSmith,20260921_16:00,Single
+Who suggests exploring the PS5 settings for accessibility features and Game Help?,ThaneChambers,20260922_12:00,Single
+What feature on the PS5 does Ileana praise for making switching between games smoother?,Control Center,20260922_12:00,Single
+How does Fionnuala describe the convenience of the Control Center's audio settings adjustment?,You can adjust audio settings on the fly without having to pause the game,20260922_12:00,Single
+What aspect does ThaneChambers enjoy about sharing clips on the PS5?,It's a great way to relive intense moments with friends,20260922_12:00,Single
+Which game is LiHua currently obsessed with for its graphics and storytelling?,God of War,20260922_12:00,Single
+What game is Helios currently playing and finds the world and gameplay mechanics stunning?,Horizon Forbidden West,20260922_12:00,Single
+"How does Dyllan describe the combat in ""Horizon Forbidden West""?",It can be tricky sometimes,20260922_12:00,Single
+"What strategy does Helios recommend for dealing with flying machines in ""Horizon Forbidden West""?",Using a bow with elemental arrows and focus to track their movements,20260922_12:00,Single
+Which game does Helios suggest for tactical fun with a great story and character development?,Fire Emblem,20260922_12:00,Single
+What type of combat does Gavriel prefer in games?,Turn-based combat,20260922_12:00,Single
+What game has Gavriel been hooked on recently for its blend of RPG elements and social simulation?,Persona 5 Royal,20260922_12:00,Single
+Which game does Dyllan recommend for turn-based combat and art style?,Octopath Traveler,20260922_12:00,Single
+"Who is ThaneChambers' favorite character in ""Octopath Traveler""?",Primrose,20260922_12:00,Single
+"What is Aisling's opinion on using traps in ""Octopath Traveler""?",They are a game changer,20260922_12:00,Single
+"How does Dyllan describe the importance of a balanced party in ""Octopath Traveler""?",It allows for more flexibility in battles and helps adapt to different challenges,20260922_12:00,Single
+"What is Bronwyn's experience with going full support in tough fights in ""Octopath Traveler""?",It can be risky but totally viable,20260922_12:00,Single
+"Which boss did Gavriel struggle with the most in ""Octopath Traveler""?",The final boss,20260922_12:00,Single
+What does Elara enjoy doing to unwind after intense battles in games?,Wander around and collect items or side quests,20260922_12:00,Single
+What type of gameplay does Jareth enjoy after a big challenge?,Relaxing gameplay,20260922_12:00,Single
+Who asks Emily if she has read The Lord of the Rings novels?,LiHua,20260922_17:00,Single
+What TV series does LiHua mention that they are curious about in relation to The Lord of the Rings?,The Rings of Power,20260922_17:00,Single
+"What aspect of ""The Rings of Power"" does Emily appreciate in terms of visuals?",How they are bringing Middle-earth to life,20260922_17:00,Single
+Which character does LiHua mention as their favorite and describe as inspiring?,Galadriel,20260922_17:00,Single
+What aspect of Elrond's character does Emily appreciate?,His mix of wisdom and vulnerability,20260922_17:00,Single
+"What does LiHua predict will happen in ""The Rings of Power""?",Epic battles and alliances forming,20260922_17:00,Single
+What does Emily think will happen next in the series in terms of story development?,The stakes will rise and alliances will evolve,20260922_17:00,Single
+What do LiHua and Emily agree on regarding the anticipation for the next episode?,They both can't wait for the next episode,20260922_17:00,Single
+What does LiHua suggest they should do as the series unfolds?,Keep sharing their thoughts,20260922_17:00,Single
+What does Emily say about the series making her feel like she is really getting to know the characters?,The emphasis on character development and their backstories,20260922_17:00,Single
+Who initiates the suggestion to go to the grocery store after work on 20260923?,WolfgangSchulz,20260923_16:00,Single
+What is the purpose of going to the grocery store on 20260923 according to WolfgangSchulz?,To grab some snacks for their next jam,20260923_16:00,Single
+What time does WolfgangSchulz propose for going to the grocery store on 20260923?,Around 5:30,20260923_16:00,Single
+How does LiHua feel about the bench press challenge proposed by JenniferMoore?,LiHua feels pretty strong and is up for the challenge,20260924_20:00,Single
+What exercises does JakeWatson recommend for leg strength?,Squats and lunges,20260925_21:00,Single
+What additional advice does JakeWatson give for maintaining muscle flexibility?,Stretch after workouts,20260925_21:00,Single
+Which stretches does JakeWatson recommend for soccer-specific areas?,Hamstring stretch and quad stretch,20260925_21:00,Single
+How long should each stretch be held according to JakeWatson?,About 30 seconds,20260925_21:00,Single
+When and where does JakeWatson propose to meet for the practice session according to the conversation on 20260925?,Saturday at 4 PM at the usual spot,20260925_21:00,Single
+What is LiHua's response to the suggestion of taking a warm shower before bed?,LiHua plans to try it tonight,20260926_10:30,Single
+Who asks LiHua to measure the window in the basement?,AdamSmith,20260928_10:00,Single
+Why does AdamSmith want the window measured?,To get some curtains made,20260928_10:00,Single
+What are the dimensions of the window that LiHua measures?,150 cm wide and 120 cm high,20260928_10:00,Single
+Who initiates the discussion about binaural tones?,ChaeSong-hwa,20260929_11:00,Single
+What does ChaeSong-hwa suggest binaural tones could enhance?,The sound and listening experience for their audience,20260929_11:00,Single
+How does YurikoYamamoto feel about adding a new dimension to their music with binaural tones?,Excited to give it a try,20260929_11:00,Single
+Who congratulates LiHua on pushing their limits with the bench press?,JenniferMoore,20260930_14:00,Single
+What does LiHua express feeling after the encouragement from Jennifer?,Strong and motivated,20260930_14:00,Single
+What is the focus for the next session suggested by JenniferMoore on 20260930?,Form and control,20260930_14:00,Single
+What technique does JenniferMoore suggest to gradually increase weight of the bench press exercise?,Progressive overload,20260930_14:00,Single
+Who suggests going out for hot pot?,LiHua,Time: 20260930_16:00,Single
+What type of hot pot does WolfgangSchulz prefer?,Sichuan,Time: 20260930_16:00,Single
+What time does LiHua propose for dinner on 20260930?,7 pm,Time: 20260930_16:00,Single
+Who comes up with the idea of picking up drinks on the way to hot pot?,LiHua,Time: 20260930_16:00,Single
+What type of drink does LiHua suggest to have with hot pot?,Cold beer,Time: 20260930_16:00,Single
+Who reminds everyone to drink water before and during the match?,JakeWatson,20261001_18:00,Single
+What does Ivor do the day before a game to help with hydration?,Hydrate well,20261001_18:00,Single
+What drink does JakeWatson recommend for hydration after a match?,Coconut water,20261001_18:00,Single
+What does Giselle emphasize about passing drills?,Accuracy is key in games,20261001_18:00,Single
+What practice idea does Dacey suggest to improve passing skills?,Target practice with cones,20261001_18:00,Single
+What does Briar remind everyone to bring to practice?,Water bottles,20261001_18:00,Single
+What does Giselle remind everyone to do before practice?,Stretch,20261001_18:00,Single
+Who is considering buying Sekiro: Shadows Die Twice and asks for ThaneChambers' opinion?,LiHua,20261001_20:00,Single
+What does ThaneChambers think about Sekiro: Shadows Die Twice?,It's amazing and worth grabbing,20261001_20:00,Single
+What aspects of Sekiro: Shadows Die Twice does ThaneChambers highlight?,Unique combat system & timing & strategy & world design,20261001_20:00,Single
+Does ThaneChambers recommend Sekiro: Shadows Die Twice to those who love a challenge?,Yes,20261001_20:00,Single
+What game is ThaneChambers currently playing?,Black Myth: Wukong,20261001_20:00,Single
+What does LiHua think about the graphics and gameplay of Black Myth: Wukong?,The graphics look insane and the gameplay seems really fluid,20261001_20:00,Single
+What part of Black Myth: Wukong does ThaneChambers like the most?,The fresh take on the classic tale and the unique combat mechanics,20261001_20:00,Single
+Which aspect of Black Myth: Wukong does LiHua find immersive?,The combat style and how it blends with the storyline,20261001_20:00,Single
+What does ThaneChambers love about the boss battles in Black Myth: Wukong?,The thrill of strategizing how to take them down and the breathtaking scenery,20261001_20:00,Single
+Who informs LiHua that the new software is almost done?,WolfgangSchulz,20261002_13:00,Single
+What is LiHua's reaction to the news about the new software?,LiHua is excited to check it out,20261002_13:00,Single
+Who asks Jake to play as the goalkeeper for soccer training?,LiHua,20261003_12:00,Single
+Does Jake agree to play as the goalkeeper?,Yes,20261003_12:00,Single
+Who discusses classic matches between Barca and Bayern with Jake?,LiHua,20261005_10:05,Single
+In which year did Barca lose 4-0 against Bayern as mentioned in the conversation?,2013,20261005_10:05,Single
+Which goal does Jake consider incredible from the 2015 match?,Neymar's goal,20261005_10:05,Single
+Who does Jake believe has the potential to be a game-changer for Barca?,Pedri,20261005_10:05,Single
+What is LiHua looking forward to in the upcoming matches?,Seeing how Pedri and the team perform and if they can go far in the UCL,20261005_10:05,Single
+What are Jake's go-to snacks for match days?,Chips and dip & pizza,20261005_10:05,Single
+What are LiHua's favorite snacks for the game?,Nachos with cheese and spicy salsa,20261005_10:05,Single
+Who does LiHua pick as the best football manager of all time?,Pep Guardiola,20261006_10:00,Single
+What aspect of Guardiola's career does JakeWatson admire?,The transformation of the teams he's managed,20261006_10:00,Single
+"Which team's style of play does Farrah remember as ""Total football""?",Barcelona,20261006_10:00,Single
+What playing style is associated with Guardiola's Barcelona team?,Tiki-taka,20261006_10:00,Single
+Which player's development under Guardiola does Jasper mention?,Phil Foden,20261006_10:00,Single
+What player's midfield importance does Aurora highlight?,Rodri,20261006_10:00,Single
+Who will bring cones for drills at the match?,Farrah,20261006_10:00,Single
+What type of passes does Briar suggest adding to the practice?,Quick one-twos,20261006_10:00,Single
+What will LiHua bring for everyone to enjoy after the match?,Snacks,20261006_10:00,Single
+Which match does Jasper mention that Manchester City played recently?,Against Chelsea in the Premier League,20261006_10:00,Single
+Who scored a stunner in the Manchester City vs. Chelsea match as mentioned in the conversation?,Phil Foden,20261006_10:00,Single
+What aspect of Rodri's performance does Henley praise?,His interceptions and passing,20261006_10:00,Single
+What does Farrah admire about Rodri's play?,His ability to break up opposition plays and transition the ball,20261006_10:00,Single
+Who checks in with LiHua about their arm muscles after the bench press session?,JenniferMoore,20261008_14:00,Single
+How does LiHua feel about the soreness in their arm muscles?,It feels good and they are ready for the next session,20261008_14:00,Single
+What snack does LiHua agree to bring for the movie?,Chips,20261009_17:00,Single
+What drinks does WolfgangSchulz want to have while watching the movie?,Soda and juice,20261009_17:00,Single
+Who will pick up the snacks and drinks on their way before the movie?,LiHua,20261009_17:00,Single
+Who initiated the discussion about Cersei's journey in Game of Thrones?,EmilyBurnett,20261010_10:10,Single
+How does Rowan describe Cersei's transformation throughout the series?,From vulnerable to ruthless,20261010_10:10,Single
+What significant event in Cersei's life does Saffron highlight as a turning point?,Losing her kids,20261010_10:10,Single
+Which character's dynamic with Cersei is mentioned as complicated?,Jaime,20261010_10:10,Single
+What does Lachlan find haunting about Cersei's end?,How much she fought to hold onto her power,20261010_10:10,Single
+What idea does Rowan say Cersei represents?,Power comes at a cost,20261010_10:10,Single
+What aspect of Cersei's character does Quillan say often shadows her vulnerability?,Her desperation for control,20261010_10:10,Single
+What relationship could have potentially changed Cersei's path according to Phaedra?,Her relationship with Tyrion,20261010_10:10,Single
+How does Kieran describe Tywin's influence on Cersei?,He taught her ruthless methods,20261010_10:10,Single
+What does Orion say about Cersei's mix of loyalty and fear towards Tywin?,It was a mix of both,20261010_10:10,Single
+What does Rowan say about Cersei's independence?,It made her stronger but also more isolated,20261010_10:10,Single
+What does LiHua say Cersei's power came at the price of?,Meaningful connections,20261010_10:10,Single
+What relationship could have shown Cersei a different perspective on power according to Lachlan?,Her relationship with Tyrion,20261010_10:10,Single
+What does Tamara say Cersei's love led to?,Her downfall,20261010_10:10,Single
+What could have changed everything for Cersei if she had embraced it according to Tamara?,Her family ties,20261010_10:10,Single
+What does EmilyBurnett say Cersei's choices led to?,Her isolation,20261010_10:10,Single
+What does Quillan suggest could have been a game-changer for Cersei?,Accepting Tyrion's advice,20261010_10:10,Single
+What does Saffron say Cersei was blinded by?,Her need for control,20261010_10:10,Single
+Who initiates the discussion about game storytelling?,ThaneChambers,20261011_11:00,Single
+What game does ThaneChambers consider their top choice for storytelling?,The Last of Us,20261011_11:00,Single
+Which game's narrative does Bronwyn appreciate for its father-son dynamic?,God of War,20261011_11:00,Single
+What game does Gavriel mention for its characters and plot twists?,Final Fantasy VII Remake,20261011_11:00,Single
+Which game does ThaneChambers recommend for its choice-based storytelling?,Life is Strange,20261011_11:00,Single
+What game does Elara mention for making her emotional?,Bioshock Infinite,20261011_11:00,Single
+Which game does Jareth praise for its storytelling in side quests?,The Witcher 3,20261011_11:00,Single
+What game does Dyllan describe as epic for its samurai culture?,Ghost of Tsushima,20261011_11:00,Single
+Which game does Helios call a masterpiece for making players feel connected to the characters?,Red Dead Redemption 2,20261011_11:00,Single
+"What scene from ""Red Dead Redemption 2"" does Bronwyn mention as emotional?",The scene with Arthur and John on the mountain,20261011_11:00,Single
+What game does Ileana say made her question the morality of her actions?,Red Dead Redemption 2,20261011_11:00,Single
+Which game does Caelum mention for encouraging emotional engagement with the narrative?,Red Dead Redemption 2,20261011_11:00,Single
+What game does Bronwyn say mixes daily life with deep story arcs?,Persona 5,20261011_11:00,Single
+Which game does Ileana mention for its unique world and storytelling?,Horizon Zero Dawn,20261011_11:00,Single
+What game does LiHua mention for exploring themes of choice & consciousness & humanity?,Detroit: Become Human,20261011_11:00,Single
+Which game does Dyllan say raised interesting questions and moral dilemmas?,Detroit: Become Human,20261011_11:00,Single
+What game does Dyllan recall for its intimate feel and dialogue?,Firewatch,20261011_11:00,Single
+Which game does LiHua mention for playing with various storytelling tropes?,The Stanley Parable,20261011_11:00,Single
+When did Li Hua receive the curtains?,Yesterday (implied to be 20261011),20261012_10:00,Single
+What does Li Hua like about the curtains?,the patterns,20261012_10:00,Single
+Who offers to help with the installation of the curtains?,AdamSmith,20261012_10:00,Single
+What does Turalyon want to hear everyone's thoughts on?,the new benches,20261013_13:30,Single
+What does MuradinBronzebeard think about the cushions idea?,It sounds like a nice idea,20261013_13:30,Single
+What does MalfurionStormrage suggest for sunny days?,umbrellas for shade,20261013_13:30,Single
+Who proposes the idea of adding tables near the benches?,RexxarRemar,20261013_13:30,Single
+What does GromHellscream suggest to brighten the area in the evening?,lanterns or string lights,20261013_13:30,Single
+What type of lanterns does MalfurionStormrage recommend?,solar-powered lanterns,20261013_13:30,Single
+Who is excited to see how the garden evolves?,Thrall,20261013_13:30,Single
+What does Turalyon think about the suggestions?,They are going to make the garden a fantastic community spot,20261013_13:30,Single
+What does ChaeSong-hwa suggest for the garden once everything is set up?,a little gathering,20261013_13:30,Single
+What kind of activities does MalfurionStormrage suggest for the gathering?,games for kids & flower planting workshops & good food,20261013_13:30,Single
+Who offers to check with friends for live music performance?,ArthasMenethil,20261013_13:30,Single
+What does MuradinBronzebeard think about live music?,It would really enhance the experience,20261013_13:30,Single
+Who suggests doing a group purchase of the new games?,ThaneChambers,20261014_14:00,Single
+What does ThaneChambers think about the new games?,excited and interested,20261014_14:00,Single
+What feature are they hoping for in the new game?,co-op missions and shared loot system,20261014_14:00,Single
+What does Helios want to know about the game?,whether it will feature crossplay,20261014_14:00,Single
+What does Dyllan think about crossplay?,it was awesome and makes playing together much easier,20261014_14:00,Single
+What does Ileana think is essential for the new game?,a robust matchmaking system,20261014_14:00,Single
+What does Fionnuala think about matchmaking systems?,they can make or break the experience,20261014_14:00,Single
+What does Helios think would be awesome for character development?,a deep skill tree or upgrade system,20261014_14:00,Single
+What does Gavriel think about crafting systems?,they offer a chance to create unique weapons and gear,20261014_14:00,Single
+What does Elara think about resource management?,it always makes it feel more immersive,20261014_14:00,Single
+What does Fionnuala think about potions and temporary boosts?,they are always helpful,20261014_14:00,Single
+What does ThaneChambers think about stealth boosts?,they add a whole new layer to gameplay,20261014_14:00,Single
+What does Bronwyn think about the idea of stealth missions?,it will be all about timing and communication,20261014_14:00,Single
+What does LiHua think about the potential for stealth in the game?,it could lead to some really engaging and strategic gameplay,20261014_14:00,Single
+What team does JakeWatson admire for their comebacks?,Real Madrid,20261014_20:00,Single
+What is one characteristic of Real Madrid that JakeWatson mentions?,they thrive under pressure,20261014_20:00,Single
+Which match does JakeWatson remember for Real Madrid's comeback?,Champions League against Manchester City in 2022,20261014_20:00,Single
+What was special about the match against Manchester City in 2022 according to the group discussion?,Rodrygo's stoppage-time goal,20261014_20:00,Single
+What does Aurora say about the atmosphere at the Bernabéu during tough games?,It was electric,20261014_20:00,Single
+What does Evangeline say about Real Madrid's history in the Champions League?,They have a legendary history and a winning culture,20261014_20:00,Single
+What does Briar think about Real Madrid's chances for the rest of the season?,They have a solid chance to challenge for every title,20261014_20:00,Single
+Who is Real Madrid's coach according to Ivor?,Ancelotti,20261014_20:00,Single
+What does Dacey propose to practice on Saturday?,soccer shooting skills,20261014_20:00,Single
+Where do they plan to meet for soccer practice?,the local park,20261014_20:00,Single
+What time do they plan to meet for soccer practice?,3 PM,20261014_20:00,Single
+Who offers to bring cones for the soccer practice?,Aurora,20261014_20:00,Single
+What else does Henley plan to work on during the practice?,dribbling skills,20261014_20:00,Single
+Who suggests getting pizza for dinner?,WolfgangSchulz,20261015_15:00,Single
+What does LiHua want to know about WolfgangSchulz's pizza preference?,his favorite place for pizza,20261015_15:00,Single
+What new pizza place does WolfgangSchulz suggest?,a new pizza place downtown,20261015_15:00,Single
+What time do they agree to meet for pizza?,around 7,20261015_15:00,Single
+Who invites LiHua to taste a new bread recipe?,HaileyJohnson,20261016_16:00,Single
+What does HaileyJohnson want LiHua to taste?,a new bread recipe,20261016_16:00,Single
+What type of pizza does WolfgangSchulz prefer?,margherita,20261017_17:00,Single
+What pizza does LiHua like?,pepperoni with extra cheese,20261017_17:00,Single
+What drink does WolfgangSchulz usually have with his pizza?,soda or craft beer,20261017_17:00,Single
+What is LiHua's suggestion regarding pizza places?,to check out the new downtown place,20261017_17:00,Single
+What do LiHua and Wolfgang plan to do together according to their conversation on 20261017?,plan a pizza night,20261017_17:00,Single
+What songs does ChaeSong-hwa suggest starting with according to the discussion on 20261019?,"Sweet Home Alabama or ""Let It Be""",20261019_19:00,Single
+What type of songs does YurikoYamamoto want to try according to the discussion on 2026101?,upbeat pop songs,20261019_19:00,Single
+What songs does ChaeSong-hwa suggest adding to the mix according to the discussion on 2026101?,"Shallow or ""Shake It Off""",20261019_19:00,Single
+What does ChaeSong-hwa plan to bring to karaoke night?,snacks,20261019_19:00,Single
+What does WolfgangSchulz suggest the group do before the upcoming karaoke night?,pick their songs,20261019_19:00,Single
+How does LiHua feel about the upcoming karaoke night?,excited and can't wait,20261019_19:00,Single
+What does ChaeSong-hwa say about the karaoke night?,it's going to be a blast,20261019_19:00,Single
+What is the current status of the curtains according to LiHua?,They are perfect and more than satisfies him,20261019_20:00,Single
+What offer does AdamSmith extend to LiHua regarding the basement?,It can be used for extra storage whenever LiHua is ready,20261019_20:00,Single
+What game does ThaneChambers remember as LiHua's favorite?,Black Myth: Wukong,20261020_10:00,Single
+"What aspect of ""Black Myth: Wukong"" does LiHua find the most impressive?",the visuals,20261020_10:00,Single
+"What does ThaneChambers like about the game ""Black Myth: Wukong""?",the storytelling,20261020_10:00,Single
+"Who is LiHua's favorite character in the game ""Black Myth: Wukong""?",Wukong,20261020_10:00,Single
+"What memorable moment does LiHua share from the game ""Black Myth: Wukong""?",an amazing battle scene where he had to outsmart a giant enemy,20261020_10:00,Single
+"What memorable moment does ThaneChambers share from the game ""Black Myth: Wukong""?",a moment where he faced a tricky puzzle,20261020_10:00,Single
+"How does LiHua describe the balance of combat and puzzles in the game ""Black Myth: Wukong""?",It's refreshing to switch between action and thinking,20261020_10:00,Single
+Does LiHua play the game solo or with friends?,Both,20261020_10:00,Single
+Would LiHua be interested in playing co-op with the group?,Yes,20261020_10:00,Single
+What TV series does EmilyBurnett remember LiHua likes?,Chernobyl,20261021_21:00,Single
+"What does LiHua appreciate about the series ""Chernobyl""?",the powerful storytelling and the cinematography,20261021_21:00,Single
+What is EmilyBurnett's favorite TV show?,Game of Thrones,20261021_21:00,Single
+"What memorable scene from ""Chernobyl"" does LiHua mention?",the scene where they're trying to contain the radiation,20261021_21:00,Single
+"Who is LiHua's favorite actor in ""Chernobyl""?",Stellan Skarsgård as Boris Shcherbina,20261021_21:00,Single
+What TV series is EmilyBurnett currently watching?,The Last of Us,20261021_21:00,Single
+What TV show does EmilyBurnett recommend LiHua to watch?,The Last of Us,20261021_21:00,Single
+"What does LiHua's plan about wathcing ""The Last of Us""?",binge it that weekend,20261021_21:00,Single
+What does JenniferMoore congratulate LiHua for pn 20261022?,for pushing his limits with the planks this week,20261022_22:00,Single
+What does JakeWatson suggest for refueling after a soccer game?,a snack with carbs and protein and staying hydrated,20261023_23:00,Single
+What are LiHua's usual recovery snacks after a game?,a protein shake and a fruit smoothie,20261023_23:00,Single
+What event is the community organizing?,a weekend picnic,20261024_11:00,Single
+What is the purpose of the community picnic?,to bring the community together and get to know each other better,20261024_11:00,Single
+What type of food event did Turalyon suggest?,a potluck,20261024_11:00,Single
+What does RexxarRemar suggest for the community picnic atmosphere?,organizing some music,20261024_11:00,Single
+What does TirionFordring suggest bringing for seating?,blankets or chairs,20261024_11:00,Single
+What day did the community members decide on for the picnic?,next Saturday,20261024_11:00,Single
+What does RexxarRemar suggest for keeping everyone energized during the picnic?,a mix of snacks and drinks,20261024_11:00,Single
+What does RexxarRemar suggest to have available to keep everyone hydrated during the picnic?,water and drinks,20261024_11:00,Single
+What does IllidanStormrage say he will bring to keep everyone entertained during the picnic?,some epic games,20261024_11:00,Single
+What places in Europe does WolfgangSchulz remember as stunning in Europe?,the fjords in Norway and the lavender fields in Provence,20261025_16:00,Single
+What does WolfgangSchulz like about Prague?,its architecture and the vibe of the old town,20261025_16:00,Single
+What local dish did WolfgangSchulz try in Prague?,goulash,20261025_16:00,Single
+What is WolfgangSchulz's favorite risotto memory?,having seafood risotto in Venice,20261025_16:00,Single
+What is on WolfgangSchulz's list for a future trip?,Italy,20261025_16:00,Single
+What does WolfgangSchulz want to check out in Italy?,Cinque Terre,20261025_16:00,Single
+What does LiHua say about the seafood in Cinque Terre?,it's supposed to be fresh and delicious,20261025_16:00,Single
+What time will Li Hua and Wolfgang meet for breakfast on the morning of the 9th?,Insufficient information,N/A,Null
+What type of equipment does Wolfgang Schulz use when he works out at the gym?,Insufficient information,N/A,Null
+"What type of exercise does Li Hua prefer to do at the gym, and what time does Wolfgang usually go to the gym?",Insufficient information,N/A,Null
+What type of traditional games did Wolfgang Schulz play with Li Hua during the Lunar New Year celebration?,Insufficient information,N/A,Null
+"What did Li Hua eat for dinner on January 20, 2026?",Insufficient information,N/A,Null
+What specific suggestions did Li Hua have regarding the construction schedule during the last community meeting?,Insufficient information,N/A,Null
+What movie did Li Hua and Wolfgang decide to watch together on New Year's Eve 2026?,Insufficient information,N/A,Null
+What dish did Li Hua order for dessert after having Sichuan hot pot with Wolfgang Schulz?,Insufficient information,N/A,Null
+What were the specific requirements that the customers modified and how did Li Hua respond to each change?,Insufficient information,N/A,Null
+"What payment method did Li Hua use to purchase groceries from the store on April 15, 2026?",Insufficient information,N/A,Null
+What specific diet did Li Hua follow to achieve his fitness results that Jennifer Moore recommended?,Insufficient information,N/A,Null
+What alternative training sports does Li Hua consider besides the current routine mentioned in his conversation with Jennifer?,Insufficient information,N/A,Null
+What type of diet is Li Hua following to support his training regimen and sleep schedule?,Insufficient information,N/A,Null
+What type of protein shake does Li Hua prefer to drink before his workout sessions?,Insufficient information,N/A,Null
+What specific brand of protein supplement does Jennifer recommend for Li Hua's weight loss journey?,Insufficient information,N/A,Null
+What specific colors of paint does Li Hua plan to use for the basement walls after decorating with potted plants?,Insufficient information,N/A,Null
+What color did Li Hua decide to paint the walls of the basement after completing the renovation?,Insufficient information,N/A,Null
+"What specific design features did Li Hua suggest for Yuriko's studio homepage during their meeting at ""Central Perk""?",Insufficient information,N/A,Null
+What is the name of the concert that Wolfgang and Li Hua will attend on March 7th?,Insufficient information,N/A,Null
+What type of cake does Li Hua plan to bring to the meeting with Yuriko to celebrate her studio's homepage redesign?,Insufficient information,N/A,Null
+What color did Li Hua paint his house before the community garden renovation began?,Insufficient information,N/A,Null
+What specific dietary changes did Li Hua implement in his training regimen as a result of Jennifer's advice on endurance and flexibility?,Insufficient information,N/A,Null
+"What specific feedback did Yuriko give to Li Hua about the demo website during their meeting at the cafe ""Central Perk"" on Thursday morning, and how did Li Hua respond to her comments?",Insufficient information,N/A,Null
+What is the exact reason for Li Hua's unexpected work meeting on Thursday?,Insufficient information,N/A,Null
+"What toppings did Hailey put on the bread for Li Hua's next delivery, and what is the name of the bakery she gets her bread from?",Insufficient information,N/A,Null
+What is the favorite type of music that Li Hua and Yuriko plan to play together?,Insufficient information,N/A,Null
+What are the specific reasons why Li Hua prefers classical music over pop music in their discussions?,Insufficient information,N/A,Null
+What song did Yuriko and Wolfgang decide to perform together after watching the drum tutorial?,Insufficient information,N/A,Null
+"What type of feedback did Li Hua provide to Chae regarding the community medical knowledge lecture, and what is Wolfgang's role in the band rehearsal?",Insufficient information,N/A,Null
+What is the name of the last song Wolfgang played using the new drum practice app?,Insufficient information,N/A,Null
+What flavor of new bread products did Li Hua really enjoy at the bakery's anniversary event?,Insufficient information,N/A,Null
+What is the name of the song that Li Hua will sing at the karaoke on 20260425?,Insufficient information,N/A,Null
+What flavor of cake did Hailey plan to bake for the bakery's anniversary celebration in February?,Insufficient information,N/A,Null
+What type of dessert did Wolfgang plan to order for Li Hua during their dinner celebration?,Insufficient information,N/A,Null
+What were Raze's personal reasons for becoming a fitness coach before discussing pull-up techniques?,Insufficient information,N/A,Null
+"What new species of flowers will be featured in the community garden renovation, according to the feedback provided by Li Hua during the progress reports?",Insufficient information,N/A,Null
+What specific construction projects were discussed at the meeting between Turalyon and the residents regarding noise control?,Insufficient information,N/A,Null
+What is the amount of rent Li Hua owes to Adam Smith for the months of April and May?,Insufficient information,N/A,Null
+What song did Li Hua perform at the local music festival in 2026?,Insufficient information,N/A,Null
+"What is the name of the restaurant where Li Hua and Wolfgang had dinner on the night of June 9, 2026?",Insufficient information,N/A,Null
+What is the nutritional value of the new line of high-protein breads compared to traditional white bread?,Insufficient information,N/A,Null
+What specific sleep techniques did Li Hua use to improve her study habits after reading the neuroscience article and discussing the warm shower with Chae?,Insufficient information,N/A,Null
+What type of special dietary restrictions does Li Hua follow when preparing meals for his family?,Insufficient information,N/A,Null
+"What is the total cost of the air conditioner installation, including labor and materials, if Li Hua had previously discussed a budget of $2,000 with a different contractor?",Insufficient information,N/A,Null
+What type of fusion music does Wolfgang Schulz plan to create with Li Hua during their weekend trip to the music store?,Insufficient information,N/A,Null
+What type of flowers were planted in the garden based on the residents' suggestions discussed by Turalyon?,Insufficient information,N/A,Null
+"What type of air-conditioner did Li Hua select for Adam's living room, and how does its temperature regulation compare to the one in the basement?",Insufficient information,N/A,Null
+What were Li Hua's specific fitness goals and how did Jennifer's advice on nutrition influence them during his training for a marathon?,Insufficient information,N/A,Null
+What are the sales figures for the PS5 exclusive games released in 2026 that Thane discussed with group members?,Insufficient information,N/A,Null
+"What specific techniques did Li Hua use to prepare for a marathon race that took place on September 1, 2026?",Insufficient information,N/A,Null
+What food did Emily order for the group's discussion about Game of Thrones characters?,Insufficient information,N/A,Null
+"What type of protein supplements did Li Hua use after the workout on September 19, 2026?",Insufficient information,N/A,Null
+What color was the curtain that Li Hua chose for his living room?,Insufficient information,N/A,Null
+What specific measurements did Li Hua take for the window size before the installation of the curtain?,Insufficient information,N/A,Null
+What is Thane's favorite type of food that he enjoys while playing video games?,Insufficient information,N/A,Null
+What were the specific details of the negotiation between Jake Watson and Li Hua regarding player transfers from FC Barcelona to FC Bayern Munich?,Insufficient information,N/A,Null
+What was the final match score of the 2025 UEFA Champions League final?,Insufficient information,N/A,Null
+What was the exact date and time when Cersei Lannister first met Jaime Lannister in the Game of Thrones series?,Insufficient information,N/A,Null
+What type of dessert did Wolfgang plan to have with Li Hua after their hot pot dinner on a different day?,Insufficient information,N/A,Null
+What did Jennifer say to Li Hua about their plan for a team swimming competition in December?,Insufficient information,N/A,Null
+What strategy did Li Hua use to score a goal in the championship match against their rival team?,Insufficient information,N/A,Null
+What is Li Hua's favorite type of exercise outside of pull-ups?,Insufficient information,N/A,Null
+What time did Chae first suggest they visit the music festival in the morning?,Insufficient information,N/A,Null
+What are the reasons behind Jennifer and Li Hua's decision to change their training routine for the following month?,Insufficient information,N/A,Null
+What specific gifts did Wolfgang buy for Li Hua during his trip to Hong Kong?,Insufficient information,N/A,Null
diff --git a/test/minirag/dataset/LiHua-World/qa/query_set.json b/test/minirag/dataset/LiHua-World/qa/query_set.json
new file mode 100755
index 0000000..e46aeeb
--- /dev/null
+++ b/test/minirag/dataset/LiHua-World/qa/query_set.json
@@ -0,0 +1,3824 @@
+{
+    "0": {
+        "question": "Did Adam Smith send a message to Li Hua about the upcoming building maintenance schedule before the administrators announced a temporary change in the construction schedule due to weather conditions?",
+        "answer": "Yes",
+        "evidence": "20260121_10:00<and>20260701_10:00",
+        "type": "Multi"
+    },
+    "1": {
+        "question": "Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li Hua about going to see \"Overwatch 3\"?",
+        "answer": "Yes",
+        "evidence": "20260121_13:00<and>20261009_17:00",
+        "type": "Multi"
+    },
+    "2": {
+        "question": "Did Li Hua agree to go out for dinner after Wolfgang first asked him if he wanted to go out for dinner?",
+        "answer": "Yes",
+        "evidence": "20260123_17:00<and>20260930_16:00",
+        "type": "Multi"
+    },
+    "3": {
+        "question": "Did Li Hua send a message to Jennifer thanking her for the new training schedule before he requested a change in his training schedule for Thursday?",
+        "answer": "Yes",
+        "evidence": "20260204_15:00<and>20260204_16:00<and>20260211_19:00",
+        "type": "Multi"
+    },
+    "4": {
+        "question": "Did Li Hua ask Jennifer for advice on how to prevent muscle soreness after an intense workout session before he told her that he feels soreness in his arm muscles after the workout this week?",
+        "answer": "Yes",
+        "evidence": "20260206_16:00<and>20260811_11:00<and>20261008_14:00<and>20261211_11:00",
+        "type": "Multi"
+    },
+    "5": {
+        "question": "Did Li Hua send a message to Jennifer asking for her opinion on protein supplements before he consulted her about his daily protein powder consumption?",
+        "answer": "Yes",
+        "evidence": "20260214_16:00<and>20261120_20:00",
+        "type": "Multi"
+    },
+    "6": {
+        "question": "Did Yuriko ask Li Hua for help with her studio's homepage before she booked a seat at the \"Central Perk\" cafe?",
+        "answer": "Yes",
+        "evidence": "20260223_15:00<and>20260225_15:00",
+        "type": "Multi"
+    },
+    "7": {
+        "question": "Did Li Hua discuss his progress with the fitness plan before he shared a blog post about his recent fitness achievements?",
+        "answer": "Yes",
+        "evidence": "20260305_17:00<and>20260325_19:00<and>20260610_16:00<and>20260630_18:00<and>20260708_14:00<and>20260817_12:15<and>20261022_22:00<and>20261202_14:00",
+        "type": "Multi"
+    },
+    "8": {
+        "question": "Did Li Hua send a message to Jennifer asking if he can turn the Thursday class to Friday after he requested a change in his training schedule for Thursday?",
+        "answer": "Yes",
+        "evidence": "20260211_19:00<and>20260309_12:00",
+        "type": "Multi"
+    },
+    "9": {
+        "question": "Did Li Hua ask Yuriko to play music together before Wolfgang proposed to pause playing musical instruments?",
+        "answer": "Yes",
+        "evidence": "20260318_15:10<and>20260416_21:00",
+        "type": "Multi"
+    },
+    "10": {
+        "question": "Did Wolfgang Schulz recommend the band learns \"Viva la Vida\" by Coldplay after he and Li Hua discussed what song to play this Sunday?",
+        "answer": "Yes",
+        "evidence": "20260318_15:30<and>20260625_19:00",
+        "type": "Multi"
+    },
+    "11": {
+        "question": "Did Wolfgang's promotion announcement occur before he invited Li Hua for dinner on 20260430?",
+        "answer": "Yes",
+        "evidence": "20260428_18:30<and>20260930_16:00",
+        "type": "Multi"
+    },
+    "12": {
+        "question": "Did Turalyon announce the construction updates and feedback from residents after Illidan Stormrage complained about the construction noise?",
+        "answer": "Yes",
+        "evidence": "20260526_15:40<and>20260527_10:00<and>20260528_15:00",
+        "type": "Multi"
+    },
+    "13": {
+        "question": "Did Hailey announce the new line of high-protein breads before inviting Li Hua to the special bakery event?",
+        "answer": "Yes",
+        "evidence": "20260611_12:45<and>20260622_13:30",
+        "type": "Multi"
+    },
+    "14": {
+        "question": "Did Chae tell Li Hua that taking a warm shower before sleeping can improve the sleep quality before sharing the neuroscience article with her?",
+        "answer": "No",
+        "evidence": "20260711_11:00<and>20260926_10:30",
+        "type": "Multi"
+    },
+    "15": {
+        "question": "Did Li Hua ask Jennifer Moore for book recommendations on fitness nutrition before she announced the special guest speaker at the gym?",
+        "answer": "No",
+        "evidence": "20260713_19:30<and>20260817_12:15<and>20260831_19:00",
+        "type": "Multi"
+    },
+    "16": {
+        "question": "Did Jennifer remind Li Hua about proper nutrition and hydration before Jake shared his tips for staying hydrated during the match?",
+        "answer": "Yes",
+        "evidence": "20260804_14:00<and>20261001_18:00",
+        "type": "Multi"
+    },
+    "17": {
+        "question": "Did the group members talk about their favorite characters in the TV series Game of Thrones after Emily started a vote on the most hateable character?",
+        "answer": "Yes",
+        "evidence": "20260902_16:00<and>20260915_12:00",
+        "type": "Multi"
+    },
+    "18": {
+        "question": "Question: Did Jennifer remind Li Hua to consume enough protein after the workout before she shared tips with the group members on common mistakes to avoid after an intense workout?",
+        "answer": "Answer: Yes",
+        "evidence": "20260919_10:00<and>20261101_11:00",
+        "type": "Multi"
+    },
+    "19": {
+        "question": "Did Li Hua ask Thane about his opinion on The Last of Us before he asked about Sekiro: Shadows Die Twice?",
+        "answer": "No",
+        "evidence": "20261001_20:00<and>20261228_14:00",
+        "type": "Multi"
+    },
+    "20": {
+        "question": "Did Jake Watson and Li Hua discuss the classic matches between FC Barcelona and FC Bayern Munich before the group members discussed the classic matches between FC Barcelona and Real Madrid?",
+        "answer": "Yes",
+        "evidence": "20261005_10:05<and>20261026_16:00",
+        "type": "Multi"
+    },
+    "21": {
+        "question": "Did the group members debate about the best football manager in the Premier League history after they debated if Pep Guardiola is the greatest soccer manager in football history?",
+        "answer": "Yes",
+        "evidence": "20261006_10:00<and>20261110_10:00",
+        "type": "Multi"
+    },
+    "22": {
+        "question": "Did the discussion about Jaime Lannister's character occur after the discussion about Cersei Lannister's character?",
+        "answer": "Yes",
+        "evidence": "20261010_10:10<and>20261026_20:00",
+        "type": "Multi"
+    },
+    "23": {
+        "question": "Did Wolfgang ask Li Hua if she wants to have pizza for dinner after work today before he wondered if she wanted to have Sichuan hot pot for dinner tonight?",
+        "answer": "No",
+        "evidence": "20260930_16:00<and>20261015_15:00",
+        "type": "Multi"
+    },
+    "24": {
+        "question": "Did Jennifer challenge Li Hua to do 60 pull-ups in a training session after she challenged him to do 100 pushups?",
+        "answer": "Yes",
+        "evidence": "20261104_18:00<and>20261129_19:00",
+        "type": "Multi"
+    },
+    "25": {
+        "question": "Did Jake share common knowledge about offside in soccer with Li Hua before he passed practical techniques to Li Hua on how to avoid offside for a forward?",
+        "answer": "Yes",
+        "evidence": "20261105_15:00<and>20261130_11:30",
+        "type": "Multi"
+    },
+    "26": {
+        "question": "Did Wolfgang arrive in Hong Kong after he informed Li Hua about his upcoming trip?",
+        "answer": "Yes",
+        "evidence": "20261219_19:00<and>20261223_23:00",
+        "type": "Multi"
+    },
+    "27": {
+        "question": "What time does Li Hua watch the movie \"Overwatch 3\"?",
+        "answer": "20260122",
+        "evidence": "20260122_17:00<and>20260121_13:00",
+        "type": "Multi"
+    },
+    "28": {
+        "question": "Who does Li Hua go to watch the movie \"Overwatch 3\" with?",
+        "answer": "Wolfgang",
+        "evidence": "20260122_17:00<and>20260121_13:00",
+        "type": "Multi"
+    },
+    "29": {
+        "question": "Has Wolfgang ever been to Hong Kong?",
+        "answer": "Yes",
+        "evidence": "20261219_19:00<and>20261220_20:00<and>20261221_12:00<and>20261228_10:00",
+        "type": "Multi"
+    },
+    "30": {
+        "question": "Who knows about Wolfgang going to Hong Kong?",
+        "answer": "LiHua & Chae & Yuriko",
+        "evidence": "20261219_19:00<and>20261220_20:00<and>20261221_12:00<and>20261228_10:00",
+        "type": "Multi"
+    },
+    "31": {
+        "question": "Who wished Li Hua a happy Lunar New Year?",
+        "answer": "Adam Smith & Jennifer Moore & Wolfgang Schulz",
+        "evidence": "20260119_11:30<and>20260119_14:30<and>20260119_09:30",
+        "type": "Multi"
+    },
+    "32": {
+        "question": "Who introduced the bread delivery service and recommend Alice for the delivery?",
+        "answer": "HaileyJohnson",
+        "evidence": "20260318_15:00<and>20260329_13:00",
+        "type": "Multi"
+    },
+    "33": {
+        "question": "What is the opportunity that makes Wolfgang and Yuriko acquaitances?",
+        "answer": "LiHua introduce them to each other by saying that they can play music together every Sunday",
+        "evidence": "20260318_15:10<and>20260319_16:00",
+        "type": "Multi"
+    },
+    "34": {
+        "question": "What was the content of the first-ever delivery from Hailey to LiHua and what was LiHua's opinion about it?",
+        "answer": "a fresh sourdough loaf and a bottle of milk and LiHua praises Hailey's bread and milk",
+        "evidence": "20260317_08:00<and>20260318_15:00",
+        "type": "Multi"
+    },
+    "35": {
+        "question": "What opportunity did LiHua create for Chae to meet Wolfgang and Yuriko?",
+        "answer": "LiHua introduced Chae to Wolfgang and Yuriko during the band's gathering on Sunday evening",
+        "evidence": "20260425_21:00<and>20260425_23:30",
+        "type": "Multi"
+    },
+    "36": {
+        "question": "What special offerings did Hailey have for her backery shop in the month of May?",
+        "answer": "a special Mother's Day bakery promotion & a special summer promotion on ice cream & a free baking class at the end of May & banana durian cheesecake",
+        "evidence": "20260508_08:00<and>20260514_14:00<and>20260527_16:00<and>20260531_20:00",
+        "type": "Multi"
+    },
+    "37": {
+        "question": "What feedbacks does Hailey ask from LiHua in July?",
+        "answer": "feedback on the bread delivery service & customer feedback on a new line of artisanal donuts",
+        "evidence": "20260710_08:30<and>20260712_16:00",
+        "type": "Multi"
+    },
+    "38": {
+        "question": "How long does it take in total from LiHua planning on getting the air-conditioner to the air-conditioner been installed?",
+        "answer": "about 27 days",
+        "evidence": "20260716_10:00<and>20260812_11:00",
+        "type": "Multi"
+    },
+    "39": {
+        "question": "Did it take more than 3 weeks from LiHua planning on getting the air-conditioner to the air-conditioner been actually installed?",
+        "answer": "Yes",
+        "evidence": "20260716_10:00<and>20260812_11:00",
+        "type": "Multi"
+    },
+    "40": {
+        "question": "Did it take more than a week from Adam asking LiHua about the ideal installation date to Adam reminding LiHua about the contractor team installing air-conditioner at 18:00?",
+        "answer": "Yes",
+        "evidence": "20260803_13:00<and>20260812_11:00",
+        "type": "Multi"
+    },
+    "41": {
+        "question": "Who does LiHua want to invite to the photo exhibition and who goes with him (during August)?",
+        "answer": "Wolfgang",
+        "evidence": "20260801_19:00<and>20260805_16:00",
+        "type": "Multi"
+    },
+    "42": {
+        "question": "Is the time interval between LiHua asking JakeWatson to help him with dribbling skills and Li Hua asking the group about classic must-watch UCL matches more than 2 days (restrict your search within August)?",
+        "answer": "Yes",
+        "evidence": "20260819_10:00<and>20260821_15:00",
+        "type": "Multi"
+    },
+    "43": {
+        "question": "Is the time interval more than 3 days between LiHua asking Adam to help him install a curtain on the basement window and Adam asking LiHua to measure the size of the window?",
+        "answer": "Yes",
+        "evidence": "20260921_16:00<and>20260928_10:00",
+        "type": "Multi"
+    },
+    "44": {
+        "question": "Is the time interval more than 7 days between Adam asking LiHua to measure the size of the window and Adam informing Li Hua that he has booked the curtain of the right size?",
+        "answer": "Yes",
+        "evidence": "20260928_10:00<and>20261007_12:00",
+        "type": "Multi"
+    },
+    "45": {
+        "question": "Is the time interval more than 3 days between LiHua confirming that he has received the curtain and Adam asking LiHua if the curtain is all good?",
+        "answer": "Yes",
+        "evidence": "20261012_10:00<and>20261019_20:00",
+        "type": "Multi"
+    },
+    "46": {
+        "question": "Is the time interval more than 3 days between LiHua first asking Adam if he can buy a small fridge for the basement and Adam asking LiHua about the size of the fridge?",
+        "answer": "Yes",
+        "evidence": "20261110_11:00<and>20261116_16:00",
+        "type": "Multi"
+    },
+    "47": {
+        "question": "Is the time interval more than 7 days between Adam asking LiHua about the size of the fridge and Adam informing LiHua that the fridge will be delivered at 4pm next day?",
+        "answer": "Yes",
+        "evidence": "20261116_16:00<and>20261123_23:00",
+        "type": "Multi"
+    },
+    "48": {
+        "question": "Wolfgang suddenly becomes very concerned about good body shape and healthy food choices in December. What are the two conversations he had with LiHua in December that reflect this?",
+        "answer": "20261202_14:00 & 20261209_19:00",
+        "evidence": "20261202_14:00<and>20261209_19:00",
+        "type": "Multi"
+    },
+    "49": {
+        "question": "Did Li Hua agree to have dinner with Wolfgang after he told Wolfgang about the lunch arrangement?",
+        "answer": "Yes",
+        "evidence": "20260105_11:00<and>20260930_16:00",
+        "type": "Multi"
+    },
+    "50": {
+        "question": "Did Li Hua ask Wolfgang Schulz for a recommendation on a gym or fitness center before asking Jennifer Moore for book recommendations on fitness nutrition?",
+        "answer": "Yes",
+        "evidence": "20260111_08:00<and>20260831_19:00",
+        "type": "Multi"
+    },
+    "51": {
+        "question": "Did Li Hua ask Wolfgang Schulz if he wants to go to the gym together before Jennifer reminded Li Hua to participate in the gym's membership feedback activity?",
+        "answer": "Yes",
+        "evidence": "20260112_10:00<and>20260605_11:00",
+        "type": "Multi"
+    },
+    "52": {
+        "question": "Did Li Hua send a message to Wolfgang Schulz saying that he has prepared all the delicious food for tonight's Chinese Lunar New Year before Wolfgang sent a message to Li Hua wishing him a happy Lunar New Year?",
+        "answer": "Yes",
+        "evidence": "20260113_11:00<and>20260118_12:00<and>20260119_11:30<and>20260119_14:30<and>20260119_09:30",
+        "type": "Multi"
+    },
+    "53": {
+        "question": "Did Li Hua provide feedback to Jennifer Moore on his new meal plan before he asked her for advice on a healthy meal plan?",
+        "answer": "No",
+        "evidence": "20260115_16:45<and>20260122_15:00",
+        "type": "Multi"
+    },
+    "54": {
+        "question": "Did Li Hua's complaint about the customer who modifies their requirements occur before Wolfgang comforted him?",
+        "answer": "No",
+        "evidence": "20260123_17:30<and>20260131_14:00",
+        "type": "Multi"
+    },
+    "55": {
+        "question": "Did Adam Smith send Li Hua a reminder about the upcoming rent due date before Li Hua sent a message about having already transferred the rent on 20260301?",
+        "answer": "Yes",
+        "evidence": "20260127_20:30<and>20260227_18:30<and>20260301_10:00<and>20260330_18:00<and>20260331_17:00<and>20260429_17:00<and>20260429_18:00",
+        "type": "Multi"
+    },
+    "56": {
+        "question": "Did Li Hua share a blog post about his recent fitness achievements after Jennifer sent him a motivational message?",
+        "answer": "Yes",
+        "evidence": "20260520_18:00<and>20260606_09:00<and>20260817_12:15<and>20261022_22:00<and>20261202_14:00",
+        "type": "Multi"
+    },
+    "57": {
+        "question": "Did Li Hua send a follow-up message to Jennifer before she asked him about his latest sleeping schedule?",
+        "answer": "Yes",
+        "evidence": "20260205_13:00<and>20260725_10:00",
+        "type": "Multi"
+    },
+    "58": {
+        "question": "Did Li Hua ask Adam Smith about placing potted plants in the basement before he asked about decorating the basement?",
+        "answer": "No",
+        "evidence": "20260219_20:00<and>20261214_14:00",
+        "type": "Multi"
+    },
+    "59": {
+        "question": "Did Li Hua ask Wolfgang for advice on renovating the basement before he invited Adam Smith to check the progress of the basement renovation?",
+        "answer": "Yes",
+        "evidence": "20260219_20:10<and>20260223_17:00<and>20260707_16:00",
+        "type": "Multi"
+    },
+    "60": {
+        "question": "Did Li Hua tell Wolfgang about making a new friend before Yuriko reminded the group members to meet for the music festival?",
+        "answer": "Yes",
+        "evidence": "20260302_18:00<and>20260318_14:27<and>20261212_12:00",
+        "type": "Multi"
+    },
+    "61": {
+        "question": "Did Yuriko tell Li Hua about booking a seat at the \"Central Perk\" cafe before Li Hua sent her a message to confirm the details of their next meeting?",
+        "answer": "Yes",
+        "evidence": "20260225_15:00<and>20260303_09:30",
+        "type": "Multi"
+    },
+    "62": {
+        "question": "What time does Li Hua check in with Adam about moving in?",
+        "answer": "5:30 PM",
+        "evidence": "20260105_14:00",
+        "type": "Single"
+    },
+    "63": {
+        "question": "When was the first time Li Hua had dinner with Wolfgang this year?",
+        "answer": "20260108",
+        "evidence": "20260108_11:00",
+        "type": "Single"
+    },
+    "64": {
+        "question": "Where was the first time Li Hua had dinner with Wolfgang this year?",
+        "answer": "the cozy café downtown",
+        "evidence": "20260108_11:00",
+        "type": "Single"
+    },
+    "65": {
+        "question": "What time is Li Hua's lunch with Wolfgang Schulz at the cozy café downtown?",
+        "answer": "20260108",
+        "evidence": "20260108_11:00",
+        "type": "Single"
+    },
+    "66": {
+        "question": "What is the Wi-Fi password at Li Hua's house?",
+        "answer": "Family123",
+        "evidence": "20260106_09:00",
+        "type": "Single"
+    },
+    "67": {
+        "question": "What does Adam say about having friends over?",
+        "answer": "having friends over occasionally is fine",
+        "evidence": "20260106_09:00",
+        "type": "Single"
+    },
+    "68": {
+        "question": "What house rule does Adam mention?",
+        "answer": "keep noise to a minimum during late hours",
+        "evidence": "20260106_09:00",
+        "type": "Single"
+    },
+    "69": {
+        "question": "What does Li Hua report to Adam on January 6th?",
+        "answer": "the water tab in the apartment is broken",
+        "evidence": "20260106_13:00",
+        "type": "Single"
+    },
+    "70": {
+        "question": "When does Adam confirm the plumber will arrive?",
+        "answer": "tomorrow at 10 AM",
+        "evidence": "20260106_15:00",
+        "type": "Single"
+    },
+    "71": {
+        "question": "What does Li Hua ask Adam about the door hinge?",
+        "answer": "a small repair",
+        "evidence": "20260107_15:00",
+        "type": "Single"
+    },
+    "72": {
+        "question": "What is the name of the gym that Wolfgang recommended LiHua to go to?",
+        "answer": "FitZone",
+        "evidence": "20260111_08:00",
+        "type": "Single"
+    },
+    "73": {
+        "question": "When does Li Hua ask Jennifer Moore about adjusting the protein in her meal plan?",
+        "answer": "20260122",
+        "evidence": "20260122_15:00<and>20260122_15:00",
+        "type": "Multi"
+    },
+    "74": {
+        "question": "What time does Li Hua watch the movie \"Overwatch 3\"?",
+        "answer": "20260122",
+        "evidence": "20260122_17:00<and>20260121_13:00",
+        "type": "Multi"
+    },
+    "75": {
+        "question": "Who does Li Hua go to watch the movie \"Overwatch 3\" with?",
+        "answer": "Wolfgang",
+        "evidence": "20260122_17:00<and>20260121_13:00",
+        "type": "Multi"
+    },
+    "76": {
+        "question": "When does Li Hua plan to celebrate Chinese Lunar New Year?",
+        "answer": "20260118",
+        "evidence": "20260113_11:00",
+        "type": "Single"
+    },
+    "77": {
+        "question": "What does Li Hua plan to celebrate Chinese Lunar New Year?",
+        "answer": "dumplings",
+        "evidence": "20260113_11:00",
+        "type": "Single"
+    },
+    "78": {
+        "question": "Who does Li Hua plan to celebrate Chinese Lunar New Year with?",
+        "answer": "Wolfgang",
+        "evidence": "20260113_11:00",
+        "type": "Single"
+    },
+    "79": {
+        "question": "Li Hua accidentally broke a light fixture once. What did Adam say about the light fixture?",
+        "answer": "I'll arrange for a professional to take a look",
+        "evidence": "20260108_19:00",
+        "type": "Single"
+    },
+    "80": {
+        "question": "Li Hua has a difficult client. When did he solve that client's project?",
+        "answer": "20260209",
+        "evidence": "20260123_17:30<and>20260209_21:00",
+        "type": "Multi"
+    },
+    "81": {
+        "question": "When did Li Hua express frustration about a client changing requirements?",
+        "answer": "20260131_14:00",
+        "evidence": "20260131_14:00",
+        "type": "Single"
+    },
+    "82": {
+        "question": "Who suggested Li Hua keep a clear log of all the changes requested by the client?",
+        "answer": "Wolfgang Schulz",
+        "evidence": "20260131_14:00",
+        "type": "Single"
+    },
+    "83": {
+        "question": "What did Wolfgang Schulz suggest Li Hua to do when Li Hua face a difficult client?",
+        "answer": "keep a clear log of all the changes requested by the client",
+        "evidence": "20260131_14:00",
+        "type": "Single"
+    },
+    "84": {
+        "question": "What does Li Hua plan to do to clear their head?",
+        "answer": "go for a walk and find a new café",
+        "evidence": "20260131_14:00",
+        "type": "Single"
+    },
+    "85": {
+        "question": "What is the name of the café Wolfgang Schulz recommends to Li Hua?",
+        "answer": "The Java Spot",
+        "evidence": "20260131_14:00",
+        "type": "Single"
+    },
+    "86": {
+        "question": "Can Li Hua hang his artworks on the wall?",
+        "answer": "Yes",
+        "evidence": "20260201_20:00",
+        "type": "Single"
+    },
+    "87": {
+        "question": "What does Adam Smith suggest Li Hua use to hang artwork without damaging the walls?",
+        "answer": "use removable hooks",
+        "evidence": "20260201_20:00",
+        "type": "Single"
+    },
+    "88": {
+        "question": "How many times does Li Hua train per week now?",
+        "answer": "2",
+        "evidence": "20260204_15:00 ",
+        "type": "Single"
+    },
+    "89": {
+        "question": "When did Li Hua change the training plan?",
+        "answer": "20260204",
+        "evidence": "20260204_15:00",
+        "type": "Single"
+    },
+    "90": {
+        "question": "What days does Li Hua takes for the training sessions?",
+        "answer": "Tuesdays and Thursdays",
+        "evidence": "20260204_15:00",
+        "type": "Single"
+    },
+    "91": {
+        "question": "When did Li Hua ask Jennifer Moore for tips on dealing with muscle soreness?",
+        "answer": "20260206_16:00",
+        "evidence": "20260206_16:00",
+        "type": "Single"
+    },
+    "92": {
+        "question": "What are some of Jennifer Moore's tips for dealing with muscle soreness?",
+        "answer": "hydration&active recovery&stretching&foam rolling&rest",
+        "evidence": "20260206_16:00",
+        "type": "Single"
+    },
+    "93": {
+        "question": "Why did Li Hua ask for extra exercises to do at home?",
+        "answer": "to complement his training schedule",
+        "evidence": "20260205_13:00",
+        "type": "Single"
+    },
+    "94": {
+        "question": "What exercises did Jennifer Moore suggest Li Hua do at home?",
+        "answer": "bodyweight squats& plank holds&push-ups&lunges&glute bridges",
+        "evidence": "20260205_13:00",
+        "type": "Single"
+    },
+    "95": {
+        "question": "What is Adam Smith's condition for Li Hua to add decorations to the basement?",
+        "answer": "must be reversible and not damage anything",
+        "evidence": "20260219_20:00",
+        "type": "Single"
+    },
+    "96": {
+        "question": "When was the first time Wolfgang went to Li Hua's basement?",
+        "answer": "20260221",
+        "evidence": "20260221_15:00",
+        "type": "Single"
+    },
+    "97": {
+        "question": "What ideas did Wolfgang Schulz suggest for Li Hua's basement practice spot?",
+        "answer": "good lighting&soundproofing&a comfy chair",
+        "evidence": "20260219_20:10",
+        "type": "Single"
+    },
+    "98": {
+        "question": "Why did Adam remind Li Hua not to play guitar late at night?",
+        "answer": "a few neighbors have mentioned they're hearing guitar music late at night",
+        "evidence": "20260216_10:00",
+        "type": "Single"
+    },
+    "99": {
+        "question": "What type of ambiance does YurikoYamamoto want for her studio's homepage?",
+        "answer": "more welcoming and engaging",
+        "evidence": "20260223_15:00",
+        "type": "Single"
+    },
+    "100": {
+        "question": "When did Li Hua invite Adam Smith to check the basement renovation progress?",
+        "answer": "20260223_19:00",
+        "evidence": "20260223_17:00",
+        "type": "Single"
+    },
+    "101": {
+        "question": "What is the name of the café where Li Hua and YurikoYamamoto first meeting to talk about Yuriko's website?",
+        "answer": "Central Perk",
+        "evidence": "20260225_15:00",
+        "type": "Single"
+    },
+    "102": {
+        "question": "What is the essence of YurikoYamamoto Li Hua is helping with her homepage?",
+        "answer": "speech therapy",
+        "evidence": "20260223_15:00",
+        "type": "Single"
+    },
+    "103": {
+        "question": "What type of instrument does Li Hua play in the basement?",
+        "answer": "guitar",
+        "evidence": "20260223_17:00",
+        "type": "Single"
+    },
+    "104": {
+        "question": "When did Adam Smith inform Li Hua about potential issues with the pipes in the basement?",
+        "answer": "20260301_13:00",
+        "evidence": "20260301_13:00",
+        "type": "Single"
+    },
+    "105": {
+        "question": "When did Li Hua inform Adam Smith that the rent was transferred?",
+        "answer": "20260301_10:00",
+        "evidence": "20260301_10:00",
+        "type": "Single"
+    },
+    "106": {
+        "question": "When is the music concert that Wolfgang invites Li Hua to?",
+        "answer": "20260307_18:00",
+        "evidence": "20260302_18:00",
+        "type": "Single"
+    },
+    "107": {
+        "question": "What dish does Li Hua agree to bring to the neighborhood potluck dinner?",
+        "answer": "Homemade pasta salad",
+        "evidence": "20260302_18:45",
+        "type": "Single"
+    },
+    "108": {
+        "question": "Who is Li Hua meeting with to discuss homepage design updates?",
+        "answer": "Yuriko Yamamoto",
+        "evidence": "20260303_09:30",
+        "type": "Single"
+    },
+    "109": {
+        "question": "What new feature does Yuriko Yamamoto consider adding to her studio's homepage?",
+        "answer": "A blog section",
+        "evidence": "20260307_13:00",
+        "type": "Single"
+    },
+    "110": {
+        "question": "What time is the power outage in the neighborhood?",
+        "answer": "2 PM to 3 PM",
+        "evidence": "20260307_14:45",
+        "type": "Single"
+    },
+    "111": {
+        "question": "What suggestions does Li Hua give for promoting the new scheduling feature?",
+        "answer": "Showcase it on social media platforms and include a short tutorial and send out a newsletter to clients",
+        "evidence": "20260311_14:30",
+        "type": "Single"
+    },
+    "112": {
+        "question": "Who invites Li Hua to join the community bake sale?",
+        "answer": "Adam Smith",
+        "evidence": "20260312_12:30",
+        "type": "Single"
+    },
+    "113": {
+        "question": "What day and time is the community bake sale taking place?",
+        "answer": "Sunday at 3 PM",
+        "evidence": "20260312_12:30",
+        "type": "Single"
+    },
+    "114": {
+        "question": "When does Li Hua request a delivery from Hailey Johnson?",
+        "answer": "Tuesday",
+        "evidence": "20260314_17:00",
+        "type": "Single"
+    },
+    "115": {
+        "question": "What is the address where Li Hua wants the bread delivery to be made?",
+        "answer": "123 Sunny Street",
+        "evidence": "20260314_17:00",
+        "type": "Single"
+    },
+    "116": {
+        "question": "What service does Hailey Johnson offer to Li Hua?",
+        "answer": "Doorstep delivery service for fresh milk and bread",
+        "evidence": "20260314_17:00",
+        "type": "Single"
+    },
+    "117": {
+        "question": "What time does Hailey Johnson start baking?",
+        "answer": "4 AM",
+        "evidence": "20260317_08:00",
+        "type": "Single"
+    },
+    "118": {
+        "question": "Where does Li Hua plan to meet Yuriko Yamamoto to show the final website?",
+        "answer": "Central Perk café",
+        "evidence": "20260317_15:30",
+        "type": "Single"
+    },
+    "119": {
+        "question": "What does Li Hua suggest to Hailey regarding the frequency of bread delivery?",
+        "answer": "Twice a week on Mondays and Fridays at 8am",
+        "evidence": "20260318_15:00",
+        "type": "Single"
+    },
+    "120": {
+        "question": "What does Li Hua agree to bring to the bonfire singing party hosted by Chae Song-hwa?",
+        "answer": "Li Hua will bring his guitar",
+        "evidence": "20260320_18:00",
+        "type": "Single"
+    },
+    "121": {
+        "question": "What is the focus of Li Hua's next month's fitness plan according to Jennifer?",
+        "answer": "Strengthening lower limbs",
+        "evidence": "20260325_19:00",
+        "type": "Single"
+    },
+    "122": {
+        "question": "What is the building's policy that Adam reminds Li Hua about?",
+        "answer": "Recycling policy",
+        "evidence": "20260328_15:00",
+        "type": "Single"
+    },
+    "123": {
+        "question": "What is the topic of the online tutorial Yuriko shares with the group?",
+        "answer": "Advanced drum techniques",
+        "evidence": "20260329_10:00",
+        "type": "Single"
+    },
+    "124": {
+        "question": "What is Wolfgang looking for in his new drums?",
+        "answer": "Something versatile that sounds good for both rock and softer tunes like The Beatles",
+        "evidence": "20260326_16:00",
+        "type": "Single"
+    },
+    "125": {
+        "question": "What song does Li Hua suggest for the jam session on 20260405?",
+        "answer": "Viva la Vida",
+        "evidence": "20260405_10:00",
+        "type": "Single"
+    },
+    "126": {
+        "question": "What does Li Hua think about the rosemary focaccia?",
+        "answer": "Li Hua thinks the rosemary focaccia is amazing",
+        "evidence": "20260331_14:00",
+        "type": "Single"
+    },
+    "127": {
+        "question": "When does Li Hua confirm the rent transfer to Adam?",
+        "answer": "20260331_17:00",
+        "evidence": "20260331_17:00",
+        "type": "Single"
+    },
+    "128": {
+        "question": "What joke does Wolfgang make as an April Fool's joke?",
+        "answer": "That Wolfgang bought a set of expensive drums",
+        "evidence": "20260401_15:00",
+        "type": "Single"
+    },
+    "129": {
+        "question": "Who is delivering the bread to Li Hua on 20260403?",
+        "answer": "Alice",
+        "evidence": "20260403_08:00",
+        "type": "Single"
+    },
+    "130": {
+        "question": "What does Li Hua think about improvisation during the jam session?",
+        "answer": "Improvisation sounds great",
+        "evidence": "20260402_19:00",
+        "type": "Single"
+    },
+    "131": {
+        "question": "When is ChaeSong-hwa hosting the community medical knowledge lecture?",
+        "answer": "7 PM on Saturday",
+        "evidence": "20260407_19:00",
+        "type": "Single"
+    },
+    "132": {
+        "question": "What topics will be covered in the community medical knowledge lecture?",
+        "answer": "Basics of common health issues and how to prevent them",
+        "evidence": "20260407_19:00",
+        "type": "Single"
+    },
+    "133": {
+        "question": "What new song does the Jolly band decide to work on for the jam session according to their discussion on 20260410?",
+        "answer": "Stand By Me",
+        "evidence": "20260410_11:00",
+        "type": "Single"
+    },
+    "134": {
+        "question": "What is Li Hua's feedback on Chae Song-hwa's medical knowledge lecture?",
+        "answer": "It is insightful and makes complex topics easy to understand",
+        "evidence": "20260411_21:00",
+        "type": "Single"
+    },
+    "135": {
+        "question": "When is the anniversary event of Hailey Johnson's bakery shop?",
+        "answer": "April 15 to 17",
+        "evidence": "20260413_21:00",
+        "type": "Single"
+    },
+    "136": {
+        "question": "What does Li Hua want to have on Hailey's bakery shop anniversary event?",
+        "answer": "Sourdough and sweet pastries",
+        "evidence": "20260413_21:00",
+        "type": "Single"
+    },
+    "137": {
+        "question": "Why does Li Hua ask ChaeSong-hwa about whether neurosurgeons actually use test tubes in their work?",
+        "answer": "Li Hua is trying to get some insights for a website design",
+        "evidence": "20260414_16:00",
+        "type": "Single"
+    },
+    "138": {
+        "question": "Who proposes that the band takes a break from jamming this week?",
+        "answer": "Wolfgang Schulz",
+        "evidence": "20260416_21:00",
+        "type": "Single"
+    },
+    "139": {
+        "question": "What suggestions does Li Hua propose to Adam Smith about the upcoming community garden renovation?",
+        "answer": "Add more seating areas for people to relax and enjoy the space and some flower beds with native plants",
+        "evidence": "20260417_11:00",
+        "type": "Single"
+    },
+    "140": {
+        "question": "What kinds of flowers does Li Hua recommend to Adam Smith for the flower beds?",
+        "answer": "Lavender and coneflowers and fresh herbs",
+        "evidence": "20260417_11:00",
+        "type": "Single"
+    },
+    "141": {
+        "question": "What will be a gift for Li Hua if he chooses to renew the fitness contract with Jennifer Moore?",
+        "answer": "A cool fitness bag as a gift for all the gym activities",
+        "evidence": "20260420_21:00",
+        "type": "Single"
+    },
+    "142": {
+        "question": "When is the karaoke activity organized by ChaeSong-hwa?",
+        "answer": "Saturday at 7 PM",
+        "evidence": "20260421_16:00",
+        "type": "Single"
+    },
+    "143": {
+        "question": "Who is Li Hua bringing to the band's jam session according to their discussion on 20260425?",
+        "answer": "ChaeSong-hwa",
+        "evidence": "20260425_21:00",
+        "type": "Single"
+    },
+    "144": {
+        "question": "What garden-related activity is Thrall planning to organize?",
+        "answer": "A community planting day",
+        "evidence": "20260426_1330",
+        "type": "Single"
+    },
+    "145": {
+        "question": "What is the proposed solution for making the garden more inviting on sunny days?",
+        "answer": "Adding shade with umbrellas or trees",
+        "evidence": "20260427_10:30",
+        "type": "Single"
+    },
+    "146": {
+        "question": "What is the main topic of the conversation on 2026-04-28 at 5 PM?",
+        "answer": "Breathing techniques and tips for squats during workouts",
+        "evidence": "20260428_17:00",
+        "type": "Single"
+    },
+    "147": {
+        "question": "When is Wolfgang Schulz's promotion celebration dinner?",
+        "answer": "6 PM on the day after tomorrow (implied to be 2026-04-30)",
+        "evidence": "20260428_18:30",
+        "type": "Single"
+    },
+    "148": {
+        "question": "What is the name of the Italian restaurant where Wolfgang and Li Hua are having dinner to celebrate Wolfgang's promotion?",
+        "answer": "Venedia Grancaffe",
+        "evidence": "20260430_17:00",
+        "type": "Single"
+    },
+    "149": {
+        "question": "What is Li Hua's suggestion for scheduling the water pipe repairs in the garden?",
+        "answer": "During off-peak hours",
+        "evidence": "20260501_16:00",
+        "type": "Single"
+    },
+    "150": {
+        "question": "When is the community meeting for the garden project scheduled according to the discussion on 20260507?",
+        "answer": "Saturday at 10 am",
+        "evidence": "20260507_16:00",
+        "type": "Single"
+    },
+    "151": {
+        "question": "What percentage discount is Hailey Johnson offering for Mother's Day pastries?",
+        "answer": "15%",
+        "evidence": "20260508_08:00",
+        "type": "Single"
+    },
+    "152": {
+        "question": "Which two specific pastries does Hailey Johnson recommend for Mother's Day?",
+        "answer": "Raspberry-filled croissants and chocolate eclairs",
+        "evidence": "20260508_08:00",
+        "type": "Single"
+    },
+    "153": {
+        "question": "What type of stretches does JenniferMoore suggest before and after workouts?",
+        "answer": "Dynamic stretches before and static stretches after",
+        "evidence": "20260510_11:30",
+        "type": "Single"
+    },
+    "154": {
+        "question": "When is the web design seminar at Wolfgang's company happening?",
+        "answer": "Thursday at 3 PM",
+        "evidence": "20260511_11:00",
+        "type": "Single"
+    },
+    "155": {
+        "question": "What is Li Hua looking forward to trying from the summer promotion?",
+        "answer": "Fruity ice cream flavors and a mango-coconut pastry",
+        "evidence": "20260514_14:00",
+        "type": "Single"
+    },
+    "156": {
+        "question": "What did Li Hua enjoy the most about the restaurant that he and Wolfgang visited for dinner on 20260514?",
+        "answer": "The pasta dish and the dessert",
+        "evidence": "20260514_22:00",
+        "type": "Single"
+    },
+    "157": {
+        "question": "Why is Chae Song-hwa unable to join the rehearsal?",
+        "answer": "She has to attend a medical lecture",
+        "evidence": "20260515_10:00",
+        "type": "Single"
+    },
+    "158": {
+        "question": "What type of lighting is preferred for the seating area in the community garden?",
+        "answer": "Soft white lights",
+        "evidence": "20260516_10:00",
+        "type": "Single"
+    },
+    "159": {
+        "question": "When is the construction of the garden supposed to start?",
+        "answer": "This Wednesday (20260520)",
+        "evidence": "20260518_10:00",
+        "type": "Single"
+    },
+    "160": {
+        "question": "Which flowers does RexxarRemar suggest for a vibrant vibe?",
+        "answer": "Bluebell and Camellia and Tulip",
+        "evidence": "20260518_10:00",
+        "type": "Single"
+    },
+    "161": {
+        "question": "How does RexxarRemar plan to spend time in the garden once it's done?",
+        "answer": "For family gatherings and relaxing afternoons",
+        "evidence": "20260518_10:00",
+        "type": "Single"
+    },
+    "162": {
+        "question": "What does LiHua find as a perfect place to work in his conversation with Chae?",
+        "answer": "The Lighthouse Cafe",
+        "evidence": "20260521_15:00",
+        "type": "Single"
+    },
+    "163": {
+        "question": "What songs does Chae propose to the Jolly band to try out on Sunday according to the conversation on 20260521?",
+        "answer": "The Yellow Wind Rises and \"To the West\"",
+        "evidence": "20260521_20:00",
+        "type": "Single"
+    },
+    "164": {
+        "question": "What is IllidanStormrage's suggestion to the ongoing garden construction according to the discussion on 20260522?",
+        "answer": "plan some quiet hours when the kids are playing",
+        "evidence": "20260522_20:00",
+        "type": "Single"
+    },
+    "165": {
+        "question": "What songs does LiHua suggest adding to the karaoke playlist in his conversation with Chae?",
+        "answer": "I Will Survive and \"Sweet Caroline\"",
+        "evidence": "20260525_15:00",
+        "type": "Single"
+    },
+    "166": {
+        "question": "What time does Turalyon plan to limit noisy activities?",
+        "answer": "From 2-3 pm",
+        "evidence": "20260527_10:00",
+        "type": "Single"
+    },
+    "167": {
+        "question": "Why does WolfgangSchulz inquire of the band members about health products?",
+        "answer": "Wolfgang is feeling very tired lately with the overtime and wants to boost up his energy",
+        "evidence": "20260527_23:00",
+        "type": "Single"
+    },
+    "168": {
+        "question": "What is the name of the song LiHua suggests revisiting for the band's music session according to their conversation on 20260529?",
+        "answer": "The History of Everything",
+        "evidence": "20260529_17:00",
+        "type": "Single"
+    },
+    "169": {
+        "question": "Which kind of pastry does Li Hua express his interest in trying in his conversation with Hailey on 20260531?",
+        "answer": "the new banana durian cheesecake",
+        "evidence": "20260531_20:00",
+        "type": "Single"
+    },
+    "170": {
+        "question": "When did Li Hua inform Adam Smith that the rent for last month (implied to be May) had been transferred?",
+        "answer": "2024-06-02 at 10:00",
+        "evidence": "20260602_10:00",
+        "type": "Single"
+    },
+    "171": {
+        "question": "What is the name of Hailey Johnson's new weekly flavor cheese?",
+        "answer": "Hazelnut Basque Roasted Cheese",
+        "evidence": "20260603_09:45",
+        "type": "Single"
+    },
+    "172": {
+        "question": "Why can't people sit on the benches when Turalyon informs the community that the benches have been installed on 20260604?",
+        "answer": "the paint isn't dry yet",
+        "evidence": "20260604_18:00",
+        "type": "Single"
+    },
+    "173": {
+        "question": "What is the phone number for the maintenance worker that Adam Smith provided to Li Hua for the broken streetlight?",
+        "answer": "314159",
+        "evidence": "20260605_18:45",
+        "type": "Single"
+    },
+    "174": {
+        "question": "What breathing technique does Jennifer Moore suggest for running?",
+        "answer": "Inhaling for 3 steps and exhaling for 2 steps",
+        "evidence": "20260606_09:00",
+        "type": "Single"
+    },
+    "175": {
+        "question": "When is the Freelancer Group Meeting scheduled for according to the conversation between LiHua and Yuriko?",
+        "answer": "This Wednesday at 3 pm",
+        "evidence": "20260608_14:30",
+        "type": "Single"
+    },
+    "176": {
+        "question": "What suggestions does Jennifer give to LiHua to help boost his endurance?",
+        "answer": "try incorporating longer cardio sessions and interval training into your routine",
+        "evidence": "20260610_16:00",
+        "type": "Single"
+    },
+    "177": {
+        "question": "What new pastry does Hailey think is perfect for a fitness lover like Li Hua?",
+        "answer": "high-protein breads",
+        "evidence": "20260611_12:45",
+        "type": "Single"
+    },
+    "178": {
+        "question": "Who proposes adding some outdoor games or a small water feature to the children's play areain the community discussion?",
+        "answer": "Thrall",
+        "evidence": "20260612_15:00",
+        "type": "Single"
+    },
+    "179": {
+        "question": "Who proposes adding picnic tables for families to enjoy some snacks after playing?",
+        "answer": "Li Hua",
+        "evidence": "20260612_15:00",
+        "type": "Single"
+    },
+    "180": {
+        "question": "Who proposes creating a little garden area where kids can help plant flowers or vegetables?",
+        "answer": "GromHellscream",
+        "evidence": "20260612_15:00",
+        "type": "Single"
+    },
+    "181": {
+        "question": "Why does LiHua bring up Adam Smith in the band's conversation on 20260616?",
+        "answer": "LiHua thinks it is really nice for Mr. Smith to rent this basement to us for practice",
+        "evidence": "20260616_10:40",
+        "type": "Single"
+    },
+    "182": {
+        "question": "What event does LiHua think is an opportunity for the Jolly band to perform in front of the crowd?",
+        "answer": "the poster saying that the town is holding a local music festival",
+        "evidence": "20260617_17:00",
+        "type": "Single"
+    },
+    "183": {
+        "question": "What dietary restrictions does LiHua have in his conversation with Hailey?",
+        "answer": "No dietary restrictions",
+        "evidence": "20260618_11:30",
+        "type": "Single"
+    },
+    "184": {
+        "question": "What will special guest speaker be talking about as Jennifer mentions to LiHua?",
+        "answer": "about nutrition for athletes",
+        "evidence": "20260619_0815",
+        "type": "Single"
+    },
+    "185": {
+        "question": "What is the survey that Jennifer wants LiHua to fill out about?",
+        "answer": "the experience with our training sessions so far",
+        "evidence": "20260622_11:00",
+        "type": "Single"
+    },
+    "186": {
+        "question": "When does the special event at Hailey's bakery start according to Hailey and LiHua's conversation on 20260622?",
+        "answer": "The event starts at 10 AM this Saturday",
+        "evidence": "20260622_13:30",
+        "type": "Single"
+    },
+    "187": {
+        "question": "What video does LiHua send to the band group chat on 20260624?",
+        "answer": "a video of himself playing the intro to \"Stairway to Heaven\"",
+        "evidence": "20260624_14:00",
+        "type": "Single"
+    },
+    "188": {
+        "question": "Waht song does Wolfgang propose adding to the band's practice set on 20260625?",
+        "answer": "Viva la Vida by Coldplay",
+        "evidence": "20260625_19:00",
+        "type": "Single"
+    },
+    "189": {
+        "question": "What is the main topic of the conversation between LiHua and ChaeSong-hwa on 20260626?",
+        "answer": "Chae and her team making a breakthrough in the research study and LiHua congratulating her",
+        "evidence": "20260626_11:00",
+        "type": "Single"
+    },
+    "190": {
+        "question": "Waht songs does Chae recommend in the band's group discussion on 20260629?",
+        "answer": "Uptown Funk or \"Happy\"",
+        "evidence": "20260629_18:30",
+        "type": "Single"
+    },
+    "191": {
+        "question": "What tip does Jennifer give to the gym members on 20260630?",
+        "answer": "Staying hydrated during workout is very important",
+        "evidence": "20260630_15:30",
+        "type": "Single"
+    },
+    "192": {
+        "question": "Why deoes the construction have to be postponed according to Tirion Fordring?",
+        "answer": "the storm and the rain have been going on for days",
+        "evidence": "20260701_10:00",
+        "type": "Single"
+    },
+    "193": {
+        "question": "What kinds of Thai food does Wolfgang want to try out at the Thai restaurant on 20260702?",
+        "answer": "pad thai and maybe some spring rolls",
+        "evidence": "20260702_15:00",
+        "type": "Single"
+    },
+    "194": {
+        "question": "What song does Yuriko propose that the band can practice this weekend according to the band's discussion on 20260703?",
+        "answer": "Take Me Home Country Roads by John Denver",
+        "evidence": "20260703_11:45",
+        "type": "Single"
+    },
+    "195": {
+        "question": "What is the name of the song that Yuriko recommend to the band on 20260706?",
+        "answer": "Rolling in the Deep by Adele",
+        "evidence": "20260706_19:30",
+        "type": "Single"
+    },
+    "196": {
+        "question": "Why are LiHua and Adam concerned about the basement in their conversation on 20260707?",
+        "answer": "They want to check if there were any issues in the basement after those rainstorms",
+        "evidence": "20260707_16:00",
+        "type": "Single"
+    },
+    "197": {
+        "question": "Why is Jennifer checking in in the gym group chat on 20260708?",
+        "answer": "She wants to hear how the members are all doing and offer some personalized advice.",
+        "evidence": "20260708_14:00",
+        "type": "Single"
+    },
+    "198": {
+        "question": "What is the article that Chae shares with LiHua about?",
+        "answer": "how to fall asleep faster at night",
+        "evidence": "20260711_11:00",
+        "type": "Single"
+    },
+    "199": {
+        "question": "What is LiHua's feeback on Hailey's new artisanal donuts?",
+        "answer": "The flavors are so unique and delicious",
+        "evidence": "20260712_16:00",
+        "type": "Single"
+    },
+    "200": {
+        "question": "What new flavor is LiHua looking for as he mentions to Hailey in their conversation on 20260714?",
+        "answer": "a matcha flavor",
+        "evidence": "20260714_12:00",
+        "type": "Single"
+    },
+    "201": {
+        "question": "Which performance does Chae want the band members to check out in their discussion on 20260715?",
+        "answer": "the amazing live performance of \"Bohemian Rhapsody\" by Queen",
+        "evidence": "20260715_19:00",
+        "type": "Single"
+    },
+    "202": {
+        "question": "Why is LiHua praising Chae in their conversation on 20260717?",
+        "answer": "LiHua was really amazed by Chae's performance in the band last Sunday because Chae's singing has really leveled up",
+        "evidence": "20260717_12:00",
+        "type": "Single"
+    },
+    "203": {
+        "question": "What cool thing does Wolfgang find that he wants to share with LiHua?",
+        "answer": "this awesome guitar Wolfgang found online",
+        "evidence": "20260718_18:00",
+        "type": "Single"
+    },
+    "204": {
+        "question": "What tips on balancing work and personal hobbies does LiHua give to Chae?",
+        "answer": "try setting specific hours for work and separate times for your hobbies and don't forget to schedule some fun time for yourself",
+        "evidence": "20260722_13:00",
+        "type": "Single"
+    },
+    "205": {
+        "question": "What is the size of the basement?",
+        "answer": "The basement is approximately 15 feet by 20 feet",
+        "evidence": "20260723_14:00",
+        "type": "Single"
+    },
+    "206": {
+        "question": "What is Wolfgang's favorite superhero?",
+        "answer": "Iron Man",
+        "evidence": "20260726_16:00",
+        "type": "Single"
+    },
+    "207": {
+        "question": "What is LiHua's favorite superhero?",
+        "answer": "Spider-Man",
+        "evidence": "20260726_16:00",
+        "type": "Single"
+    },
+    "208": {
+        "question": "What sports would LiHua like to see on the community sports day?",
+        "answer": "some team sports like soccer or basketball",
+        "evidence": "20260727_16:30",
+        "type": "Single"
+    },
+    "209": {
+        "question": "What activities are being discussed for the community sports day?",
+        "answer": "Soccer and basketball and tug-of-war and sack race are mentioned as potential activities for the community sports day",
+        "evidence": "20260727_16:30",
+        "type": "Single"
+    },
+    "210": {
+        "question": "What new hobbies does Wolfgang Schulz propose to Li Hua?",
+        "answer": "Wolfgang Schulz proposes pottery or painting as a hobby to Li Hua",
+        "evidence": "20260728_18:00",
+        "type": "Single"
+    },
+    "211": {
+        "question": "What brands of air-conditioners does LiHua recommend to Wolfgang?",
+        "answer": "Mitsubishi and Daikin",
+        "evidence": "20260729_14:00",
+        "type": "Single"
+    },
+    "212": {
+        "question": "Who is recommended by Jennifer to share some fitness tips in the gym group chat on 20260730?",
+        "answer": "LiHua",
+        "evidence": "20260730_17:30",
+        "type": "Single"
+    },
+    "213": {
+        "question": "What does Hailey Johnson want Li Hua to help with at the bakery according to their discussion on 20260731?",
+        "answer": "Hailey Johnson wants Li Hua to help gather feedback from customers at the bread-tasting event",
+        "evidence": "20260731_13:00",
+        "type": "Single"
+    },
+    "214": {
+        "question": "What day do Li Hua and Wolfgang Schulz plan to visit the photography exhibition?",
+        "answer": "Li Hua and Wolfgang Schulz plan to visit the photography exhibition on Friday evening",
+        "evidence": "20260801_19:00",
+        "type": "Single"
+    },
+    "215": {
+        "question": "When is Wolfgang and LiHua going to the photography exhibition on 20260805?",
+        "answer": "19:00",
+        "evidence": "20260805_16:00",
+        "type": "Single"
+    },
+    "216": {
+        "question": "What new components are being added to the garden project as Turalyon announces it on 20260806?",
+        "answer": "incorporating sustainable practices like recycling and composting into our renovation project",
+        "evidence": "20260806_16:30",
+        "type": "Single"
+    },
+    "217": {
+        "question": "Why is Yuriko asking for LiHua's help on 20260807?",
+        "answer": "She has got ten logo designs for my speech therapy studio and she would love LiHua's opinion on which one stands out the most",
+        "evidence": "20260807_12:00",
+        "type": "Single"
+    },
+    "218": {
+        "question": "When is the installation of the air-conditioner for the basement?",
+        "answer": "It's set for Wednesday next week",
+        "evidence": "20260809_12:00",
+        "type": "Single"
+    },
+    "219": {
+        "question": "Why does Adam reach out to LiHua on 20260810?",
+        "answer": "To check if LiHua had a chance to look over the warranty and maintenance plans for the air conditioner",
+        "evidence": "20260810_18:00",
+        "type": "Single"
+    },
+    "220": {
+        "question": "What tips does Jennifer give to LiHua about preventing muscle soreness after a tough workout?",
+        "answer": "make sure to warm up properly before workouts and cool down afterward & Stretching & Stay hydrated and consider foam rolling post-session to help with recovery",
+        "evidence": "20260811_11:00",
+        "type": "Single"
+    },
+    "221": {
+        "question": "When is the contractor team going to install the air-conditioner?",
+        "answer": "20260812 6PM",
+        "evidence": "20260812_11:00",
+        "type": "Single"
+    },
+    "222": {
+        "question": "When is the garage sale being discussed?",
+        "answer": "20260814_15:00",
+        "evidence": "20260814_15:00",
+        "type": "Single"
+    },
+    "223": {
+        "question": "What is the reason Thrall cannot participate in the garage sale?",
+        "answer": "Thrall has a lot happening with the garden renovations",
+        "evidence": "20260814_15:00",
+        "type": "Single"
+    },
+    "224": {
+        "question": "What does RexxarRemar suggest to make the garage sale more enjoyable?",
+        "answer": "RexxarRemar suggests having snacks or drinks at the garage sale",
+        "evidence": "20260814_15:00",
+        "type": "Single"
+    },
+    "225": {
+        "question": "Who offers to help with setting up for the garage sale?",
+        "answer": "AdamSmith",
+        "evidence": "20260814_15:00",
+        "type": "Single"
+    },
+    "226": {
+        "question": "What type of stretches does Sage prefer before hitting the gym?",
+        "answer": "Sage prefers dynamic stretches",
+        "evidence": "20260816_17:00",
+        "type": "Single"
+    },
+    "227": {
+        "question": "What does JenniferMoore suggest to improve performance and prevent injuries?",
+        "answer": "JenniferMoore suggests incorporating stretching techniques",
+        "evidence": "20260816_17:00",
+        "type": "Single"
+    },
+    "228": {
+        "question": "What does Viper plan to do to increase their stamina?",
+        "answer": "Viper plans to focus on cardio",
+        "evidence": "20260817_12:15",
+        "type": "Single"
+    },
+    "229": {
+        "question": "What does Sova plan to do to keep their cardio interesting?",
+        "answer": "Sova plans to mix it up with different types of cardio exercises",
+        "evidence": "20260817_12:15",
+        "type": "Single"
+    },
+    "230": {
+        "question": "How much discount is Hailey willing to give to LiHua according to their conversation on 20260816?",
+        "answer": "15% off",
+        "evidence": "20260816_20:00",
+        "type": "Single"
+    },
+    "231": {
+        "question": "Who does Li Hua mention as their favorite character from The Witcher 3?",
+        "answer": "Geralt",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "232": {
+        "question": "Why does LiHua like Geralt from The Witcher 3?",
+        "answer": "He's such a complex character with that no-nonsense attitude but deep down he has a great sense of morality & his monster-slaying skills are just epic",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "233": {
+        "question": "What is Li Hua's opinion about Yennefer's character?",
+        "answer": "Intense & fiercely independent & her relationship with Geralt evolves",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "234": {
+        "question": "Which scene stood out for Li Hua in The Witcher 3?",
+        "answer": "The \"Battle of Kaer Morhen\"",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "235": {
+        "question": "Which scene in The Witcher 3 does ThaneChambers consider as one of his favorites involving Yennefer?",
+        "answer": "The scene when Geralt is looking for Yennefer in the early part of the game",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "236": {
+        "question": "What is Li Hua's impression of the Blood and Wine expansion?",
+        "answer": "Incredible & new area is stunning & story feels like a mini-epic",
+        "evidence": "20260818_10:00",
+        "type": "Single"
+    },
+    "237": {
+        "question": "What is Li Hua's opinion on the characters in Succession?",
+        "answer": "Intense family dynamics and business war",
+        "evidence": "20260818_14:00",
+        "type": "Single"
+    },
+    "238": {
+        "question": "What TV show does EmilyBurnett recommend to Li Hua after discussing Succession?",
+        "answer": "The Crown and Ted Lasso",
+        "evidence": "20260818_14:00",
+        "type": "Single"
+    },
+    "239": {
+        "question": "When does Li Hua plan to meet JakeWatson for soccer practice?",
+        "answer": "Saturday afternoon at 3 PM",
+        "evidence": "20260819_10:00",
+        "type": "Single"
+    },
+    "240": {
+        "question": "What are some of the upcoming PS5 exclusives discussed?",
+        "answer": "Final Fantasy XVI & Marvel's Spider-Man 2 & Ghostwire: Tokyo",
+        "evidence": "20260819_18:00",
+        "type": "Single"
+    },
+    "241": {
+        "question": "Which upcoming PS5 exclusive game does Gavriel express curiosity about?",
+        "answer": "Ghostwire: Tokyo",
+        "evidence": "20260819_18:00",
+        "type": "Single"
+    },
+    "242": {
+        "question": "Which UCL match does Jasper recommend as a classic must-watch?",
+        "answer": "The 2005 final between Liverpool and AC Milan",
+        "evidence": "20260821_15:00",
+        "type": "Single"
+    },
+    "243": {
+        "question": "What skill does Li Hua plan to improve with WolfgangSchulz's help?",
+        "answer": "Data analysis and coding related to AI tools",
+        "evidence": "20260822_17:00",
+        "type": "Single"
+    },
+    "244": {
+        "question": "Who shared a music theory tutorial with the group?",
+        "answer": "Chae Song-hwa",
+        "evidence": "20260825_10:45",
+        "type": "Single"
+    },
+    "245": {
+        "question": "Which regions in Witcher 3 did Li Hua and Thane Chambers discuss?",
+        "answer": "Skellige and Toussaint",
+        "evidence": "20260826_17:00",
+        "type": "Single"
+    },
+    "246": {
+        "question": "What is Kendall's relationship with Logan like according to the group's discussion?",
+        "answer": "Toxic",
+        "evidence": "20260826_18:00",
+        "type": "Single"
+    },
+    "247": {
+        "question": "What is the topic of discussion between Li Hua and Wolfgang Schulz on burger preferences?",
+        "answer": "Medium rare vs. well-done meat patty in a classic American",
+        "evidence": "20260828_10:00",
+        "type": "Single"
+    },
+    "248": {
+        "question": "What is Li Hua's preference for a burger patty's doneness?",
+        "answer": "Medium rare",
+        "evidence": "20260828_10:00",
+        "type": "Single"
+    },
+    "249": {
+        "question": "What is Wolfgang's preference for a burger patty's doneness?",
+        "answer": "well-done",
+        "evidence": "20260828_10:00",
+        "type": "Single"
+    },
+    "250": {
+        "question": "Why does Wolfgang prefer a well-done meat patty in a burger?",
+        "answer": "It is safer and more flavorful & he likes a little char on his burger",
+        "evidence": "20260828_10:00",
+        "type": "Single"
+    },
+    "251": {
+        "question": "What toppings does Wolfgang Schulz prefer on his burger?",
+        "answer": "Cheese and bacon",
+        "evidence": "20260828_10:00",
+        "type": "Single"
+    },
+    "252": {
+        "question": "What books does Jennifer recommend to LiHua for deepening understanding of fitness nutrition?",
+        "answer": "The New Rules of Lifting and \"Precision Nutrition\"",
+        "evidence": "20260831_19:00",
+        "type": "Single"
+    },
+    "253": {
+        "question": "Why are Wolfgang and LiHua going to the local music store?",
+        "answer": "check out some new gear and get inspired for our next jam session",
+        "evidence": "20260831_19:00",
+        "type": "Single"
+    },
+    "254": {
+        "question": "What is the main theme of game group's discussion on 20260901_13:00?",
+        "answer": "Dutch van der Linde's character and leadership",
+        "evidence": "20260901_13:00",
+        "type": "Single"
+    },
+    "255": {
+        "question": "Who is the most hateable character in Game of Thrones in Orion's opinion?",
+        "answer": "Ramsay Bolton",
+        "evidence": "20260902_16:00",
+        "type": "Single"
+    },
+    "256": {
+        "question": "Who is the most hateable character in Game of Thrones in Merrick's opinion?",
+        "answer": "Cersei Lannister",
+        "evidence": "20260902_16:00",
+        "type": "Single"
+    },
+    "257": {
+        "question": "What is the common interest between Li Hua and Jake Watson?",
+        "answer": "Soccer",
+        "evidence": "20260903_17:00",
+        "type": "Single"
+    },
+    "258": {
+        "question": "What kind of songs are Li Hua and Chae Song-hwa considering adding to their playlist for babies?",
+        "answer": "Lullabies",
+        "evidence": "20260904_10:00",
+        "type": "Single"
+    },
+    "259": {
+        "question": "What makes Wolfgang feel under a lot of pressure according to his conversation with LiHua on 20260905?",
+        "answer": "this new software project",
+        "evidence": "20260905_14:00",
+        "type": "Single"
+    },
+    "260": {
+        "question": "Why does AdamSmith ask LiHua about the basement on 20260907?",
+        "answer": "check in and see how the basement held up after the storm",
+        "evidence": "20260907_14:00",
+        "type": "Single"
+    },
+    "261": {
+        "question": "How is the basement after the storm?",
+        "answer": "The basement is all good no water leakage at all",
+        "evidence": "20260907_14:00",
+        "type": "Single"
+    },
+    "262": {
+        "question": "What is the latest good news from Wolfgang as of 20260908?",
+        "answer": "software project finally made some significant progress",
+        "evidence": "20260908_15:00",
+        "type": "Single"
+    },
+    "263": {
+        "question": "What new songs are LiHua working on lately on his guitar as of 20260908?",
+        "answer": "Blackbird and \"Hotel California\"",
+        "evidence": "20260908_15:00",
+        "type": "Single"
+    },
+    "264": {
+        "question": "What mission does Gavriel mention that shows Arthur's growth in the game?",
+        "answer": "the scene where Arthur tells John to take care of his family",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "265": {
+        "question": "What does Aisling find emotionally impactful about Red Dead Redemption 2's ending?",
+        "answer": "Arthur's realization of his fate and the music during the last ride",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "266": {
+        "question": "What side mission in Red Dead Redemption 2 resonated with Jareth?",
+        "answer": "the side mission with the widow in the honor system",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "267": {
+        "question": "What was ThaneChambers' favorite horse-related side quest in Red Dead Redemption 2?",
+        "answer": "the “troubled” horse",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "268": {
+        "question": "What funny glitch did Fionnuala experience in the game Red Dead Redemption 2?",
+        "answer": "Arthur ended up floating in mid-air after a cutscene",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "269": {
+        "question": "What was the final showdown that Elara found unforgettable in Red Dead Redemption 2?",
+        "answer": "with Dutch and Micah",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "270": {
+        "question": "How does Bronwyn describe the epilogue of the game Red Dead Redemption 2?",
+        "answer": "captivating after seeing John try to make a life for himself",
+        "evidence": "20260909_10:00",
+        "type": "Single"
+    },
+    "271": {
+        "question": "Has LiHua ever met anyone like Shedlon Cooper in his real life?",
+        "answer": "No",
+        "evidence": "20260910_12:00",
+        "type": "Single"
+    },
+    "272": {
+        "question": "Has EmilyBurnett ever met anyone like Shedlon Cooper in her real life?",
+        "answer": "No",
+        "evidence": "20260910_12:00",
+        "type": "Single"
+    },
+    "273": {
+        "question": "What games are LiHua and ThaneChambers planning to check out during their shopping trip according to their discussion on 20260911?",
+        "answer": "Spider-Man: Miles Morales and \"Demon's Souls\"",
+        "evidence": "20260911_14:00",
+        "type": "Single"
+    },
+    "274": {
+        "question": "Who is the best midfielder in the past decade in JakeWatson's opinion?",
+        "answer": "Luka Modrić",
+        "evidence": "20260912_16:00",
+        "type": "Single"
+    },
+    "275": {
+        "question": "How many servings of vegetables should LiHua aim for each day to keep body in good shape according to Jennifer?",
+        "answer": "at least 5 servings of veggies a day",
+        "evidence": "20260913_18:00",
+        "type": "Single"
+    },
+    "276": {
+        "question": "What does LiHua ask Jake about soccer?",
+        "answer": "the best strategies for soccer players to avoid injuries during games",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "277": {
+        "question": "What is Jake's first tip for avoiding soccer injuries?",
+        "answer": "proper warm-up and stretching",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "278": {
+        "question": "What does Jake suggest to improve flexibility and prevent muscle issues?",
+        "answer": "staying hydrated",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "279": {
+        "question": "Why is wearing the right footwear important in soccer?",
+        "answer": "to avoid slips or sprains",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "280": {
+        "question": "How does strength training help prevent soccer injuries?",
+        "answer": "It builds muscles around joints & reduces injury risk & improves stability and endurance",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "281": {
+        "question": "What type of exercises does Jake recommend for strength training?",
+        "answer": "squats & lunges & planks & balance exercises like single-leg stands",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "282": {
+        "question": "What is the purpose of starting with bodyweight exercises?",
+        "answer": "for those who are new to strength training",
+        "evidence": "20260914_10:00",
+        "type": "Single"
+    },
+    "283": {
+        "question": "Who does EmilyBurnett think is a standout character in the Game of Thrones series?",
+        "answer": "Tyrion Lannister",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "284": {
+        "question": "What does Lachlan appreciate about Arya Stark's character development?",
+        "answer": "Her transition from an innocent girl to a fierce assassin",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "285": {
+        "question": "What moment does Rowan find mind-blowing in Arya's storyline?",
+        "answer": "When Arya confronts and takes down the Night King",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "286": {
+        "question": "Which moment involving Tyrion is Merrick's favorite?",
+        "answer": "When Tyrion blows up the Wildfire to save King's Landing",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "287": {
+        "question": "What memorable quote is mentioned by Phaedra?",
+        "answer": "Jon Snow's \"You know nothing\" scene with Ygritte",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "288": {
+        "question": "What does Rowan love about Tyrion's \"I drink and I know things\" line?",
+        "answer": "It captures his cleverness",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "289": {
+        "question": "What aspect of Jaime Lannister's storyline does Rowan find intriguing?",
+        "answer": "His transformation after meeting Brienne of Tarth",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "290": {
+        "question": "How does Phaedra view Jaime's character arc?",
+        "answer": "Fascinating as he goes from \"Kingslayer\" to someone who values honor",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "291": {
+        "question": "What does Lachlan think about Jaime's final decision to protect Cersei?",
+        "answer": "It complicates his redemption and leaves mixed feelings",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "292": {
+        "question": "How does Saffron feel about Jaime's choice to protect Cersei?",
+        "answer": "Conflicted because it seems like he was slipping back into old ways",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "293": {
+        "question": "What does Kieran wish Jaime had done differently?",
+        "answer": "Chosen a different path after his character development",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "294": {
+        "question": "How does Niamh think Jaime could've ended up differently?",
+        "answer": "By staying true to the lessons he learned from Brienne",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "295": {
+        "question": "What could Jaime and Brienne have done to rebuild trust according to Merrick?",
+        "answer": "Worked on rebuilding trust through actions rather than just words",
+        "evidence": "20260915_12:00",
+        "type": "Single"
+    },
+    "296": {
+        "question": "What is LiHua curious about regarding the \"Man in Black\" in Westworld?",
+        "answer": "The motive behind his character",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "297": {
+        "question": "What does EmilyBurnett think is the main motive for the \"Man in Black\"?",
+        "answer": "To find deeper meaning and fulfillment in Westworld\t20260916_16:00",
+        "evidence": "What",
+        "type": "Single"
+    },
+    "298": {
+        "question": "How does LiHua feel about the \"Man in Black's\" search for meaning?",
+        "answer": "Intrigued as it adds depth to the story",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "299": {
+        "question": "What is EmilyBurnett's favorite moment of the \"Man in Black\"?",
+        "answer": "When he confronts the truth about himself and his choices",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "300": {
+        "question": "Which moment does LiHua find memorable for the \"Man in Black\"?",
+        "answer": "When he shows vulnerability",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "301": {
+        "question": "What does LiHua think about the themes of Westworld?",
+        "answer": "They are thought-provoking by diving into consciousness and free will and what it means to be human",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "302": {
+        "question": "Which theme of Westworld resonates the most for LiHua?",
+        "answer": "The exploration of free will and choice",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "303": {
+        "question": "What does EmilyBurnett think about the future relevance of Westworld's themes?",
+        "answer": "They will become even more crucial as technology advances",
+        "evidence": "20260916_16:00",
+        "type": "Single"
+    },
+    "304": {
+        "question": "What does WolfgangSchulz suggest incorporating into practice sessions according to the discussion on 20260916?",
+        "answer": "Improvisational solos",
+        "evidence": "20260916_19:00",
+        "type": "Single"
+    },
+    "305": {
+        "question": "How does LiHua feel about the idea of improvisation based on the chat on 20260916?",
+        "answer": "It will make sessions more fun and creative",
+        "evidence": "20260916_19:00",
+        "type": "Single"
+    },
+    "306": {
+        "question": "Who does JakeWatson consider the best defender in the history of FC Barcelona?",
+        "answer": "Carles Puyol",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "307": {
+        "question": "What does LiHua think about Gerard Piqué as a defender?",
+        "answer": "He deserves a mention for his skill and intelligence",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "308": {
+        "question": "Who are JakeWatson's and LiHua's favorite defenders?",
+        "answer": "Carles Puyol and Gerard Piqué",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "309": {
+        "question": "What is JakeWatson's favorite memory of Puyol and Piqué playing together?",
+        "answer": "The comeback against PSG in the Champions League",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "310": {
+        "question": "What match does JakeWatson cherish from the Champions League final against Manchester United?",
+        "answer": "The 2009 final",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "311": {
+        "question": "What is LiHua's favorite match involving Barcelona?",
+        "answer": "The 2013 Champions League match against AC Milan",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "312": {
+        "question": "Which goal does JakeWatson never forget from Messi?",
+        "answer": "Messi's solo goal against Getafe in 2007",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "313": {
+        "question": "What is LiHua's favorite player moment from Messi?",
+        "answer": "When Messi scored a header against Manchester United in 2011",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "314": {
+        "question": "What does LiHua think makes Messi the best?",
+        "answer": "His ability to create chances as well as his vision and work ethic and humility",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "315": {
+        "question": "How does JakeWatson view Messi's impact on future generations of players?",
+        "answer": "His dedication and consistency set the bar high for young players",
+        "evidence": "20260917_10:00",
+        "type": "Single"
+    },
+    "316": {
+        "question": "Who suggests hitting Starbucks after work on 20260918?",
+        "answer": "WolfgangSchulz",
+        "evidence": "20260918_18:00",
+        "type": "Single"
+    },
+    "317": {
+        "question": "What is JenniferMoore's reminder to LiHua about on 20260919?",
+        "answer": "Getting enough protein after workouts",
+        "evidence": "20260919_10:00",
+        "type": "Single"
+    },
+    "318": {
+        "question": "Why is protein important according to JenniferMoore?",
+        "answer": "For keeping the body in shape",
+        "evidence": "20260919_10:00",
+        "type": "Single"
+    },
+    "319": {
+        "question": "What is ThaneChambers' question to LiHua on 20260920?",
+        "answer": "LiHua's favorite first-person shooter game",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "320": {
+        "question": "Which game does LiHua mention he's been playing on 20260920?",
+        "answer": "Call of Duty: Warzone",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "321": {
+        "question": "What does ThaneChambers say about \"Warzone\"?",
+        "answer": "It's a blast",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "322": {
+        "question": "What does LiHua like most about \"Warzone\"?",
+        "answer": "The strategy involved and the adrenaline rush",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "323": {
+        "question": "What is ThaneChambers' memorable \"Warzone\" experience?",
+        "answer": "Coming back from a tough spot to win at the last second",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "324": {
+        "question": "What was LiHua's unforgettable match in \"Warzone\"?",
+        "answer": "Being down to the last two and outsmarting the last team",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "325": {
+        "question": "How does ThaneChambers feel about upcoming \"Warzone\" updates?",
+        "answer": "He's looking forward to them",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "326": {
+        "question": "Does LiHua plan to play \"Warzone\" after the updates?",
+        "answer": "Yes because he loves exploring fresh content",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "327": {
+        "question": "What does ThaneChambers propose to do with the \"Warzone\" updates?",
+        "answer": "Hop on together and tackle the new stuff",
+        "evidence": "20260920_16:00",
+        "type": "Single"
+    },
+    "328": {
+        "question": "Who initiated the discussion about the community garden renovation and its benefits to local businesses?",
+        "answer": "Turalyon",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "329": {
+        "question": "What idea did IllidanStormrage propose to help involve the community in supporting local businesses?",
+        "answer": "Putting together a flyer with a list of local businesses",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "330": {
+        "question": "How does GromHellscream believe the local shops will benefit from the completed garden?",
+        "answer": "More foot traffic means more customers for them",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "331": {
+        "question": "What does ArthasMenethil describe the outcome of the garden project and supporting local vendors as?",
+        "answer": "A win-win",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "332": {
+        "question": "What does Thrall suggest they create to support local businesses?",
+        "answer": "A list of businesses they love to support",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "333": {
+        "question": "How does TirionFordring emphasize the importance of balancing the garden renovation with supporting local vendors?",
+        "answer": "By stating that both aspects are important for the best outcome",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "334": {
+        "question": "What does AdamSmith agree is crucial to align with the garden project schedule?",
+        "answer": "Support for local businesses",
+        "evidence": "20260921_10:00",
+        "type": "Single"
+    },
+    "335": {
+        "question": "Who does LiHua ask for help with installing a curtain?",
+        "answer": "AdamSmith",
+        "evidence": "20260921_16:00",
+        "type": "Single"
+    },
+    "336": {
+        "question": "When does AdamSmith say he is available to help with the curtain installation?",
+        "answer": "Wednesday or Thursday afternoon",
+        "evidence": "20260921_16:00",
+        "type": "Single"
+    },
+    "337": {
+        "question": "On which day do LiHua and AdamSmith agree to install the curtain?",
+        "answer": "Thursday afternoon",
+        "evidence": "20260921_16:00",
+        "type": "Single"
+    },
+    "338": {
+        "question": "Who will bring the tools for the curtain installation?",
+        "answer": "AdamSmith",
+        "evidence": "20260921_16:00",
+        "type": "Single"
+    },
+    "339": {
+        "question": "Who suggests exploring the PS5 settings for accessibility features and Game Help?",
+        "answer": "ThaneChambers",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "340": {
+        "question": "What feature on the PS5 does Ileana praise for making switching between games smoother?",
+        "answer": "Control Center",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "341": {
+        "question": "How does Fionnuala describe the convenience of the Control Center's audio settings adjustment?",
+        "answer": "You can adjust audio settings on the fly without having to pause the game",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "342": {
+        "question": "What aspect does ThaneChambers enjoy about sharing clips on the PS5?",
+        "answer": "It's a great way to relive intense moments with friends",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "343": {
+        "question": "Which game is LiHua currently obsessed with for its graphics and storytelling?",
+        "answer": "God of War",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "344": {
+        "question": "What game is Helios currently playing and finds the world and gameplay mechanics stunning?",
+        "answer": "Horizon Forbidden West",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "345": {
+        "question": "How does Dyllan describe the combat in \"Horizon Forbidden West\"?",
+        "answer": "It can be tricky sometimes",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "346": {
+        "question": "What strategy does Helios recommend for dealing with flying machines in \"Horizon Forbidden West\"?",
+        "answer": "Using a bow with elemental arrows and focus to track their movements",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "347": {
+        "question": "Which game does Helios suggest for tactical fun with a great story and character development?",
+        "answer": "Fire Emblem",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "348": {
+        "question": "What type of combat does Gavriel prefer in games?",
+        "answer": "Turn-based combat",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "349": {
+        "question": "What game has Gavriel been hooked on recently for its blend of RPG elements and social simulation?",
+        "answer": "Persona 5 Royal",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "350": {
+        "question": "Which game does Dyllan recommend for turn-based combat and art style?",
+        "answer": "Octopath Traveler",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "351": {
+        "question": "Who is ThaneChambers' favorite character in \"Octopath Traveler\"?",
+        "answer": "Primrose",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "352": {
+        "question": "What is Aisling's opinion on using traps in \"Octopath Traveler\"?",
+        "answer": "They are a game changer",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "353": {
+        "question": "How does Dyllan describe the importance of a balanced party in \"Octopath Traveler\"?",
+        "answer": "It allows for more flexibility in battles and helps adapt to different challenges",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "354": {
+        "question": "What is Bronwyn's experience with going full support in tough fights in \"Octopath Traveler\"?",
+        "answer": "It can be risky but totally viable",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "355": {
+        "question": "Which boss did Gavriel struggle with the most in \"Octopath Traveler\"?",
+        "answer": "The final boss",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "356": {
+        "question": "What does Elara enjoy doing to unwind after intense battles in games?",
+        "answer": "Wander around and collect items or side quests",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "357": {
+        "question": "What type of gameplay does Jareth enjoy after a big challenge?",
+        "answer": "Relaxing gameplay",
+        "evidence": "20260922_12:00",
+        "type": "Single"
+    },
+    "358": {
+        "question": "Who asks Emily if she has read The Lord of the Rings novels?",
+        "answer": "LiHua",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "359": {
+        "question": "What TV series does LiHua mention that they are curious about in relation to The Lord of the Rings?",
+        "answer": "The Rings of Power",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "360": {
+        "question": "What aspect of \"The Rings of Power\" does Emily appreciate in terms of visuals?",
+        "answer": "How they are bringing Middle-earth to life",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "361": {
+        "question": "Which character does LiHua mention as their favorite and describe as inspiring?",
+        "answer": "Galadriel",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "362": {
+        "question": "What aspect of Elrond's character does Emily appreciate?",
+        "answer": "His mix of wisdom and vulnerability",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "363": {
+        "question": "What does LiHua predict will happen in \"The Rings of Power\"?",
+        "answer": "Epic battles and alliances forming",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "364": {
+        "question": "What does Emily think will happen next in the series in terms of story development?",
+        "answer": "The stakes will rise and alliances will evolve",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "365": {
+        "question": "What do LiHua and Emily agree on regarding the anticipation for the next episode?",
+        "answer": "They both can't wait for the next episode",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "366": {
+        "question": "What does LiHua suggest they should do as the series unfolds?",
+        "answer": "Keep sharing their thoughts",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "367": {
+        "question": "What does Emily say about the series making her feel like she is really getting to know the characters?",
+        "answer": "The emphasis on character development and their backstories",
+        "evidence": "20260922_17:00",
+        "type": "Single"
+    },
+    "368": {
+        "question": "Who initiates the suggestion to go to the grocery store after work on 20260923?",
+        "answer": "WolfgangSchulz",
+        "evidence": "20260923_16:00",
+        "type": "Single"
+    },
+    "369": {
+        "question": "What is the purpose of going to the grocery store on 20260923 according to WolfgangSchulz?",
+        "answer": "To grab some snacks for their next jam",
+        "evidence": "20260923_16:00",
+        "type": "Single"
+    },
+    "370": {
+        "question": "What time does WolfgangSchulz propose for going to the grocery store on 20260923?",
+        "answer": "Around 5:30",
+        "evidence": "20260923_16:00",
+        "type": "Single"
+    },
+    "371": {
+        "question": "How does LiHua feel about the bench press challenge proposed by JenniferMoore?",
+        "answer": "LiHua feels pretty strong and is up for the challenge",
+        "evidence": "20260924_20:00",
+        "type": "Single"
+    },
+    "372": {
+        "question": "What exercises does JakeWatson recommend for leg strength?",
+        "answer": "Squats and lunges",
+        "evidence": "20260925_21:00",
+        "type": "Single"
+    },
+    "373": {
+        "question": "What additional advice does JakeWatson give for maintaining muscle flexibility?",
+        "answer": "Stretch after workouts",
+        "evidence": "20260925_21:00",
+        "type": "Single"
+    },
+    "374": {
+        "question": "Which stretches does JakeWatson recommend for soccer-specific areas?",
+        "answer": "Hamstring stretch and quad stretch",
+        "evidence": "20260925_21:00",
+        "type": "Single"
+    },
+    "375": {
+        "question": "How long should each stretch be held according to JakeWatson?",
+        "answer": "About 30 seconds",
+        "evidence": "20260925_21:00",
+        "type": "Single"
+    },
+    "376": {
+        "question": "When and where does JakeWatson propose to meet for the practice session according to the conversation on 20260925?",
+        "answer": "Saturday at 4 PM at the usual spot",
+        "evidence": "20260925_21:00",
+        "type": "Single"
+    },
+    "377": {
+        "question": "What is LiHua's response to the suggestion of taking a warm shower before bed?",
+        "answer": "LiHua plans to try it tonight",
+        "evidence": "20260926_10:30",
+        "type": "Single"
+    },
+    "378": {
+        "question": "Who asks LiHua to measure the window in the basement?",
+        "answer": "AdamSmith",
+        "evidence": "20260928_10:00",
+        "type": "Single"
+    },
+    "379": {
+        "question": "Why does AdamSmith want the window measured?",
+        "answer": "To get some curtains made",
+        "evidence": "20260928_10:00",
+        "type": "Single"
+    },
+    "380": {
+        "question": "What are the dimensions of the window that LiHua measures?",
+        "answer": "150 cm wide and 120 cm high",
+        "evidence": "20260928_10:00",
+        "type": "Single"
+    },
+    "381": {
+        "question": "Who initiates the discussion about binaural tones?",
+        "answer": "ChaeSong-hwa",
+        "evidence": "20260929_11:00",
+        "type": "Single"
+    },
+    "382": {
+        "question": "What does ChaeSong-hwa suggest binaural tones could enhance?",
+        "answer": "The sound and listening experience for their audience",
+        "evidence": "20260929_11:00",
+        "type": "Single"
+    },
+    "383": {
+        "question": "How does YurikoYamamoto feel about adding a new dimension to their music with binaural tones?",
+        "answer": "Excited to give it a try",
+        "evidence": "20260929_11:00",
+        "type": "Single"
+    },
+    "384": {
+        "question": "Who congratulates LiHua on pushing their limits with the bench press?",
+        "answer": "JenniferMoore",
+        "evidence": "20260930_14:00",
+        "type": "Single"
+    },
+    "385": {
+        "question": "What does LiHua express feeling after the encouragement from Jennifer?",
+        "answer": "Strong and motivated",
+        "evidence": "20260930_14:00",
+        "type": "Single"
+    },
+    "386": {
+        "question": "What is the focus for the next session suggested by JenniferMoore on 20260930?",
+        "answer": "Form and control",
+        "evidence": "20260930_14:00",
+        "type": "Single"
+    },
+    "387": {
+        "question": "What technique does JenniferMoore suggest to gradually increase weight of the bench press exercise?",
+        "answer": "Progressive overload",
+        "evidence": "20260930_14:00",
+        "type": "Single"
+    },
+    "388": {
+        "question": "Who suggests going out for hot pot?",
+        "answer": "LiHua",
+        "evidence": "Time: 20260930_16:00",
+        "type": "Single"
+    },
+    "389": {
+        "question": "What type of hot pot does WolfgangSchulz prefer?",
+        "answer": "Sichuan",
+        "evidence": "Time: 20260930_16:00",
+        "type": "Single"
+    },
+    "390": {
+        "question": "What time does LiHua propose for dinner on 20260930?",
+        "answer": "7 pm",
+        "evidence": "Time: 20260930_16:00",
+        "type": "Single"
+    },
+    "391": {
+        "question": "Who comes up with the idea of picking up drinks on the way to hot pot?",
+        "answer": "LiHua",
+        "evidence": "Time: 20260930_16:00",
+        "type": "Single"
+    },
+    "392": {
+        "question": "What type of drink does LiHua suggest to have with hot pot?",
+        "answer": "Cold beer",
+        "evidence": "Time: 20260930_16:00",
+        "type": "Single"
+    },
+    "393": {
+        "question": "Who reminds everyone to drink water before and during the match?",
+        "answer": "JakeWatson",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "394": {
+        "question": "What does Ivor do the day before a game to help with hydration?",
+        "answer": "Hydrate well",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "395": {
+        "question": "What drink does JakeWatson recommend for hydration after a match?",
+        "answer": "Coconut water",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "396": {
+        "question": "What does Giselle emphasize about passing drills?",
+        "answer": "Accuracy is key in games",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "397": {
+        "question": "What practice idea does Dacey suggest to improve passing skills?",
+        "answer": "Target practice with cones",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "398": {
+        "question": "What does Briar remind everyone to bring to practice?",
+        "answer": "Water bottles",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "399": {
+        "question": "What does Giselle remind everyone to do before practice?",
+        "answer": "Stretch",
+        "evidence": "20261001_18:00",
+        "type": "Single"
+    },
+    "400": {
+        "question": "Who is considering buying Sekiro: Shadows Die Twice and asks for ThaneChambers' opinion?",
+        "answer": "LiHua",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "401": {
+        "question": "What does ThaneChambers think about Sekiro: Shadows Die Twice?",
+        "answer": "It's amazing and worth grabbing",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "402": {
+        "question": "What aspects of Sekiro: Shadows Die Twice does ThaneChambers highlight?",
+        "answer": "Unique combat system & timing & strategy & world design",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "403": {
+        "question": "Does ThaneChambers recommend Sekiro: Shadows Die Twice to those who love a challenge?",
+        "answer": "Yes",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "404": {
+        "question": "What game is ThaneChambers currently playing?",
+        "answer": "Black Myth: Wukong",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "405": {
+        "question": "What does LiHua think about the graphics and gameplay of Black Myth: Wukong?",
+        "answer": "The graphics look insane and the gameplay seems really fluid",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "406": {
+        "question": "What part of Black Myth: Wukong does ThaneChambers like the most?",
+        "answer": "The fresh take on the classic tale and the unique combat mechanics",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "407": {
+        "question": "Which aspect of Black Myth: Wukong does LiHua find immersive?",
+        "answer": "The combat style and how it blends with the storyline",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "408": {
+        "question": "What does ThaneChambers love about the boss battles in Black Myth: Wukong?",
+        "answer": "The thrill of strategizing how to take them down and the breathtaking scenery",
+        "evidence": "20261001_20:00",
+        "type": "Single"
+    },
+    "409": {
+        "question": "Who informs LiHua that the new software is almost done?",
+        "answer": "WolfgangSchulz",
+        "evidence": "20261002_13:00",
+        "type": "Single"
+    },
+    "410": {
+        "question": "What is LiHua's reaction to the news about the new software?",
+        "answer": "LiHua is excited to check it out",
+        "evidence": "20261002_13:00",
+        "type": "Single"
+    },
+    "411": {
+        "question": "Who asks Jake to play as the goalkeeper for soccer training?",
+        "answer": "LiHua",
+        "evidence": "20261003_12:00",
+        "type": "Single"
+    },
+    "412": {
+        "question": "Does Jake agree to play as the goalkeeper?",
+        "answer": "Yes",
+        "evidence": "20261003_12:00",
+        "type": "Single"
+    },
+    "413": {
+        "question": "Who discusses classic matches between Barca and Bayern with Jake?",
+        "answer": "LiHua",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "414": {
+        "question": "In which year did Barca lose 4-0 against Bayern as mentioned in the conversation?",
+        "answer": "2013",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "415": {
+        "question": "Which goal does Jake consider incredible from the 2015 match?",
+        "answer": "Neymar's goal",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "416": {
+        "question": "Who does Jake believe has the potential to be a game-changer for Barca?",
+        "answer": "Pedri",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "417": {
+        "question": "What is LiHua looking forward to in the upcoming matches?",
+        "answer": "Seeing how Pedri and the team perform and if they can go far in the UCL",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "418": {
+        "question": "What are Jake's go-to snacks for match days?",
+        "answer": "Chips and dip & pizza",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "419": {
+        "question": "What are LiHua's favorite snacks for the game?",
+        "answer": "Nachos with cheese and spicy salsa",
+        "evidence": "20261005_10:05",
+        "type": "Single"
+    },
+    "420": {
+        "question": "Who does LiHua pick as the best football manager of all time?",
+        "answer": "Pep Guardiola",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "421": {
+        "question": "What aspect of Guardiola's career does JakeWatson admire?",
+        "answer": "The transformation of the teams he's managed",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "422": {
+        "question": "Which team's style of play does Farrah remember as \"Total football\"?",
+        "answer": "Barcelona",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "423": {
+        "question": "What playing style is associated with Guardiola's Barcelona team?",
+        "answer": "Tiki-taka",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "424": {
+        "question": "Which player's development under Guardiola does Jasper mention?",
+        "answer": "Phil Foden",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "425": {
+        "question": "What player's midfield importance does Aurora highlight?",
+        "answer": "Rodri",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "426": {
+        "question": "Who will bring cones for drills at the match?",
+        "answer": "Farrah",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "427": {
+        "question": "What type of passes does Briar suggest adding to the practice?",
+        "answer": "Quick one-twos",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "428": {
+        "question": "What will LiHua bring for everyone to enjoy after the match?",
+        "answer": "Snacks",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "429": {
+        "question": "Which match does Jasper mention that Manchester City played recently?",
+        "answer": "Against Chelsea in the Premier League",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "430": {
+        "question": "Who scored a stunner in the Manchester City vs. Chelsea match as mentioned in the conversation?",
+        "answer": "Phil Foden",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "431": {
+        "question": "What aspect of Rodri's performance does Henley praise?",
+        "answer": "His interceptions and passing",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "432": {
+        "question": "What does Farrah admire about Rodri's play?",
+        "answer": "His ability to break up opposition plays and transition the ball",
+        "evidence": "20261006_10:00",
+        "type": "Single"
+    },
+    "433": {
+        "question": "Who checks in with LiHua about their arm muscles after the bench press session?",
+        "answer": "JenniferMoore",
+        "evidence": "20261008_14:00",
+        "type": "Single"
+    },
+    "434": {
+        "question": "How does LiHua feel about the soreness in their arm muscles?",
+        "answer": "It feels good and they are ready for the next session",
+        "evidence": "20261008_14:00",
+        "type": "Single"
+    },
+    "435": {
+        "question": "What snack does LiHua agree to bring for the movie?",
+        "answer": "Chips",
+        "evidence": "20261009_17:00",
+        "type": "Single"
+    },
+    "436": {
+        "question": "What drinks does WolfgangSchulz want to have while watching the movie?",
+        "answer": "Soda and juice",
+        "evidence": "20261009_17:00",
+        "type": "Single"
+    },
+    "437": {
+        "question": "Who will pick up the snacks and drinks on their way before the movie?",
+        "answer": "LiHua",
+        "evidence": "20261009_17:00",
+        "type": "Single"
+    },
+    "438": {
+        "question": "Who initiated the discussion about Cersei's journey in Game of Thrones?",
+        "answer": "EmilyBurnett",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "439": {
+        "question": "How does Rowan describe Cersei's transformation throughout the series?",
+        "answer": "From vulnerable to ruthless",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "440": {
+        "question": "What significant event in Cersei's life does Saffron highlight as a turning point?",
+        "answer": "Losing her kids",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "441": {
+        "question": "Which character's dynamic with Cersei is mentioned as complicated?",
+        "answer": "Jaime",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "442": {
+        "question": "What does Lachlan find haunting about Cersei's end?",
+        "answer": "How much she fought to hold onto her power",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "443": {
+        "question": "What idea does Rowan say Cersei represents?",
+        "answer": "Power comes at a cost",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "444": {
+        "question": "What aspect of Cersei's character does Quillan say often shadows her vulnerability?",
+        "answer": "Her desperation for control",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "445": {
+        "question": "What relationship could have potentially changed Cersei's path according to Phaedra?",
+        "answer": "Her relationship with Tyrion",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "446": {
+        "question": "How does Kieran describe Tywin's influence on Cersei?",
+        "answer": "He taught her ruthless methods",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "447": {
+        "question": "What does Orion say about Cersei's mix of loyalty and fear towards Tywin?",
+        "answer": "It was a mix of both",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "448": {
+        "question": "What does Rowan say about Cersei's independence?",
+        "answer": "It made her stronger but also more isolated",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "449": {
+        "question": "What does LiHua say Cersei's power came at the price of?",
+        "answer": "Meaningful connections",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "450": {
+        "question": "What relationship could have shown Cersei a different perspective on power according to Lachlan?",
+        "answer": "Her relationship with Tyrion",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "451": {
+        "question": "What does Tamara say Cersei's love led to?",
+        "answer": "Her downfall",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "452": {
+        "question": "What could have changed everything for Cersei if she had embraced it according to Tamara?",
+        "answer": "Her family ties",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "453": {
+        "question": "What does EmilyBurnett say Cersei's choices led to?",
+        "answer": "Her isolation",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "454": {
+        "question": "What does Quillan suggest could have been a game-changer for Cersei?",
+        "answer": "Accepting Tyrion's advice",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "455": {
+        "question": "What does Saffron say Cersei was blinded by?",
+        "answer": "Her need for control",
+        "evidence": "20261010_10:10",
+        "type": "Single"
+    },
+    "456": {
+        "question": "Who initiates the discussion about game storytelling?",
+        "answer": "ThaneChambers",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "457": {
+        "question": "What game does ThaneChambers consider their top choice for storytelling?",
+        "answer": "The Last of Us",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "458": {
+        "question": "Which game's narrative does Bronwyn appreciate for its father-son dynamic?",
+        "answer": "God of War",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "459": {
+        "question": "What game does Gavriel mention for its characters and plot twists?",
+        "answer": "Final Fantasy VII Remake",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "460": {
+        "question": "Which game does ThaneChambers recommend for its choice-based storytelling?",
+        "answer": "Life is Strange",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "461": {
+        "question": "What game does Elara mention for making her emotional?",
+        "answer": "Bioshock Infinite",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "462": {
+        "question": "Which game does Jareth praise for its storytelling in side quests?",
+        "answer": "The Witcher 3",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "463": {
+        "question": "What game does Dyllan describe as epic for its samurai culture?",
+        "answer": "Ghost of Tsushima",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "464": {
+        "question": "Which game does Helios call a masterpiece for making players feel connected to the characters?",
+        "answer": "Red Dead Redemption 2",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "465": {
+        "question": "What scene from \"Red Dead Redemption 2\" does Bronwyn mention as emotional?",
+        "answer": "The scene with Arthur and John on the mountain",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "466": {
+        "question": "What game does Ileana say made her question the morality of her actions?",
+        "answer": "Red Dead Redemption 2",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "467": {
+        "question": "Which game does Caelum mention for encouraging emotional engagement with the narrative?",
+        "answer": "Red Dead Redemption 2",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "468": {
+        "question": "What game does Bronwyn say mixes daily life with deep story arcs?",
+        "answer": "Persona 5",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "469": {
+        "question": "Which game does Ileana mention for its unique world and storytelling?",
+        "answer": "Horizon Zero Dawn",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "470": {
+        "question": "What game does LiHua mention for exploring themes of choice & consciousness & humanity?",
+        "answer": "Detroit: Become Human",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "471": {
+        "question": "Which game does Dyllan say raised interesting questions and moral dilemmas?",
+        "answer": "Detroit: Become Human",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "472": {
+        "question": "What game does Dyllan recall for its intimate feel and dialogue?",
+        "answer": "Firewatch",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "473": {
+        "question": "Which game does LiHua mention for playing with various storytelling tropes?",
+        "answer": "The Stanley Parable",
+        "evidence": "20261011_11:00",
+        "type": "Single"
+    },
+    "474": {
+        "question": "When did Li Hua receive the curtains?",
+        "answer": "Yesterday (implied to be 20261011)",
+        "evidence": "20261012_10:00",
+        "type": "Single"
+    },
+    "475": {
+        "question": "What does Li Hua like about the curtains?",
+        "answer": "the patterns",
+        "evidence": "20261012_10:00",
+        "type": "Single"
+    },
+    "476": {
+        "question": "Who offers to help with the installation of the curtains?",
+        "answer": "AdamSmith",
+        "evidence": "20261012_10:00",
+        "type": "Single"
+    },
+    "477": {
+        "question": "What does Turalyon want to hear everyone's thoughts on?",
+        "answer": "the new benches",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "478": {
+        "question": "What does MuradinBronzebeard think about the cushions idea?",
+        "answer": "It sounds like a nice idea",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "479": {
+        "question": "What does MalfurionStormrage suggest for sunny days?",
+        "answer": "umbrellas for shade",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "480": {
+        "question": "Who proposes the idea of adding tables near the benches?",
+        "answer": "RexxarRemar",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "481": {
+        "question": "What does GromHellscream suggest to brighten the area in the evening?",
+        "answer": "lanterns or string lights",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "482": {
+        "question": "What type of lanterns does MalfurionStormrage recommend?",
+        "answer": "solar-powered lanterns",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "483": {
+        "question": "Who is excited to see how the garden evolves?",
+        "answer": "Thrall",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "484": {
+        "question": "What does Turalyon think about the suggestions?",
+        "answer": "They are going to make the garden a fantastic community spot",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "485": {
+        "question": "What does ChaeSong-hwa suggest for the garden once everything is set up?",
+        "answer": "a little gathering",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "486": {
+        "question": "What kind of activities does MalfurionStormrage suggest for the gathering?",
+        "answer": "games for kids & flower planting workshops & good food",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "487": {
+        "question": "Who offers to check with friends for live music performance?",
+        "answer": "ArthasMenethil",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "488": {
+        "question": "What does MuradinBronzebeard think about live music?",
+        "answer": "It would really enhance the experience",
+        "evidence": "20261013_13:30",
+        "type": "Single"
+    },
+    "489": {
+        "question": "Who suggests doing a group purchase of the new games?",
+        "answer": "ThaneChambers",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "490": {
+        "question": "What does ThaneChambers think about the new games?",
+        "answer": "excited and interested",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "491": {
+        "question": "What feature are they hoping for in the new game?",
+        "answer": "co-op missions and shared loot system",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "492": {
+        "question": "What does Helios want to know about the game?",
+        "answer": "whether it will feature crossplay",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "493": {
+        "question": "What does Dyllan think about crossplay?",
+        "answer": "it was awesome and makes playing together much easier",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "494": {
+        "question": "What does Ileana think is essential for the new game?",
+        "answer": "a robust matchmaking system",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "495": {
+        "question": "What does Fionnuala think about matchmaking systems?",
+        "answer": "they can make or break the experience",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "496": {
+        "question": "What does Helios think would be awesome for character development?",
+        "answer": "a deep skill tree or upgrade system",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "497": {
+        "question": "What does Gavriel think about crafting systems?",
+        "answer": "they offer a chance to create unique weapons and gear",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "498": {
+        "question": "What does Elara think about resource management?",
+        "answer": "it always makes it feel more immersive",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "499": {
+        "question": "What does Fionnuala think about potions and temporary boosts?",
+        "answer": "they are always helpful",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "500": {
+        "question": "What does ThaneChambers think about stealth boosts?",
+        "answer": "they add a whole new layer to gameplay",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "501": {
+        "question": "What does Bronwyn think about the idea of stealth missions?",
+        "answer": "it will be all about timing and communication",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "502": {
+        "question": "What does LiHua think about the potential for stealth in the game?",
+        "answer": "it could lead to some really engaging and strategic gameplay",
+        "evidence": "20261014_14:00",
+        "type": "Single"
+    },
+    "503": {
+        "question": "What team does JakeWatson admire for their comebacks?",
+        "answer": "Real Madrid",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "504": {
+        "question": "What is one characteristic of Real Madrid that JakeWatson mentions?",
+        "answer": "they thrive under pressure",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "505": {
+        "question": "Which match does JakeWatson remember for Real Madrid's comeback?",
+        "answer": "Champions League against Manchester City in 2022",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "506": {
+        "question": "What was special about the match against Manchester City in 2022 according to the group discussion?",
+        "answer": "Rodrygo's stoppage-time goal",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "507": {
+        "question": "What does Aurora say about the atmosphere at the Bernabéu during tough games?",
+        "answer": "It was electric",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "508": {
+        "question": "What does Evangeline say about Real Madrid's history in the Champions League?",
+        "answer": "They have a legendary history and a winning culture",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "509": {
+        "question": "What does Briar think about Real Madrid's chances for the rest of the season?",
+        "answer": "They have a solid chance to challenge for every title",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "510": {
+        "question": "Who is Real Madrid's coach according to Ivor?",
+        "answer": "Ancelotti",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "511": {
+        "question": "What does Dacey propose to practice on Saturday?",
+        "answer": "soccer shooting skills",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "512": {
+        "question": "Where do they plan to meet for soccer practice?",
+        "answer": "the local park",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "513": {
+        "question": "What time do they plan to meet for soccer practice?",
+        "answer": "3 PM",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "514": {
+        "question": "Who offers to bring cones for the soccer practice?",
+        "answer": "Aurora",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "515": {
+        "question": "What else does Henley plan to work on during the practice?",
+        "answer": "dribbling skills",
+        "evidence": "20261014_20:00",
+        "type": "Single"
+    },
+    "516": {
+        "question": "Who suggests getting pizza for dinner?",
+        "answer": "WolfgangSchulz",
+        "evidence": "20261015_15:00",
+        "type": "Single"
+    },
+    "517": {
+        "question": "What does LiHua want to know about WolfgangSchulz's pizza preference?",
+        "answer": "his favorite place for pizza",
+        "evidence": "20261015_15:00",
+        "type": "Single"
+    },
+    "518": {
+        "question": "What new pizza place does WolfgangSchulz suggest?",
+        "answer": "a new pizza place downtown",
+        "evidence": "20261015_15:00",
+        "type": "Single"
+    },
+    "519": {
+        "question": "What time do they agree to meet for pizza?",
+        "answer": "around 7",
+        "evidence": "20261015_15:00",
+        "type": "Single"
+    },
+    "520": {
+        "question": "Who invites LiHua to taste a new bread recipe?",
+        "answer": "HaileyJohnson",
+        "evidence": "20261016_16:00",
+        "type": "Single"
+    },
+    "521": {
+        "question": "What does HaileyJohnson want LiHua to taste?",
+        "answer": "a new bread recipe",
+        "evidence": "20261016_16:00",
+        "type": "Single"
+    },
+    "522": {
+        "question": "What type of pizza does WolfgangSchulz prefer?",
+        "answer": "margherita",
+        "evidence": "20261017_17:00",
+        "type": "Single"
+    },
+    "523": {
+        "question": "What pizza does LiHua like?",
+        "answer": "pepperoni with extra cheese",
+        "evidence": "20261017_17:00",
+        "type": "Single"
+    },
+    "524": {
+        "question": "What drink does WolfgangSchulz usually have with his pizza?",
+        "answer": "soda or craft beer",
+        "evidence": "20261017_17:00",
+        "type": "Single"
+    },
+    "525": {
+        "question": "What is LiHua's suggestion regarding pizza places?",
+        "answer": "to check out the new downtown place",
+        "evidence": "20261017_17:00",
+        "type": "Single"
+    },
+    "526": {
+        "question": "What do LiHua and Wolfgang plan to do together according to their conversation on 20261017?",
+        "answer": "plan a pizza night",
+        "evidence": "20261017_17:00",
+        "type": "Single"
+    },
+    "527": {
+        "question": "What songs does ChaeSong-hwa suggest starting with according to the discussion on 20261019?",
+        "answer": "Sweet Home Alabama or \"Let It Be\"",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "528": {
+        "question": "What type of songs does YurikoYamamoto want to try according to the discussion on 2026101?",
+        "answer": "upbeat pop songs",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "529": {
+        "question": "What songs does ChaeSong-hwa suggest adding to the mix according to the discussion on 2026101?",
+        "answer": "Shallow or \"Shake It Off\"",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "530": {
+        "question": "What does ChaeSong-hwa plan to bring to karaoke night?",
+        "answer": "snacks",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "531": {
+        "question": "What does WolfgangSchulz suggest the group do before the upcoming karaoke night?",
+        "answer": "pick their songs",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "532": {
+        "question": "How does LiHua feel about the upcoming karaoke night?",
+        "answer": "excited and can't wait",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "533": {
+        "question": "What does ChaeSong-hwa say about the karaoke night?",
+        "answer": "it's going to be a blast",
+        "evidence": "20261019_19:00",
+        "type": "Single"
+    },
+    "534": {
+        "question": "What is the current status of the curtains according to LiHua?",
+        "answer": "They are perfect and more than satisfies him",
+        "evidence": "20261019_20:00",
+        "type": "Single"
+    },
+    "535": {
+        "question": "What offer does AdamSmith extend to LiHua regarding the basement?",
+        "answer": "It can be used for extra storage whenever LiHua is ready",
+        "evidence": "20261019_20:00",
+        "type": "Single"
+    },
+    "536": {
+        "question": "What game does ThaneChambers remember as LiHua's favorite?",
+        "answer": "Black Myth: Wukong",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "537": {
+        "question": "What aspect of \"Black Myth: Wukong\" does LiHua find the most impressive?",
+        "answer": "the visuals",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "538": {
+        "question": "What does ThaneChambers like about the game \"Black Myth: Wukong\"?",
+        "answer": "the storytelling",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "539": {
+        "question": "Who is LiHua's favorite character in the game \"Black Myth: Wukong\"?",
+        "answer": "Wukong",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "540": {
+        "question": "What memorable moment does LiHua share from the game \"Black Myth: Wukong\"?",
+        "answer": "an amazing battle scene where he had to outsmart a giant enemy",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "541": {
+        "question": "What memorable moment does ThaneChambers share from the game \"Black Myth: Wukong\"?",
+        "answer": "a moment where he faced a tricky puzzle",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "542": {
+        "question": "How does LiHua describe the balance of combat and puzzles in the game \"Black Myth: Wukong\"?",
+        "answer": "It's refreshing to switch between action and thinking",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "543": {
+        "question": "Does LiHua play the game solo or with friends?",
+        "answer": "Both",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "544": {
+        "question": "Would LiHua be interested in playing co-op with the group?",
+        "answer": "Yes",
+        "evidence": "20261020_10:00",
+        "type": "Single"
+    },
+    "545": {
+        "question": "What TV series does EmilyBurnett remember LiHua likes?",
+        "answer": "Chernobyl",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "546": {
+        "question": "What does LiHua appreciate about the series \"Chernobyl\"?",
+        "answer": "the powerful storytelling and the cinematography",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "547": {
+        "question": "What is EmilyBurnett's favorite TV show?",
+        "answer": "Game of Thrones",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "548": {
+        "question": "What memorable scene from \"Chernobyl\" does LiHua mention?",
+        "answer": "the scene where they're trying to contain the radiation",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "549": {
+        "question": "Who is LiHua's favorite actor in \"Chernobyl\"?",
+        "answer": "Stellan Skarsgård as Boris Shcherbina",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "550": {
+        "question": "What TV series is EmilyBurnett currently watching?",
+        "answer": "The Last of Us",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "551": {
+        "question": "What TV show does EmilyBurnett recommend LiHua to watch?",
+        "answer": "The Last of Us",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "552": {
+        "question": "What does LiHua's plan about wathcing \"The Last of Us\"?",
+        "answer": "binge it that weekend",
+        "evidence": "20261021_21:00",
+        "type": "Single"
+    },
+    "553": {
+        "question": "What does JenniferMoore congratulate LiHua for pn 20261022?",
+        "answer": "for pushing his limits with the planks this week",
+        "evidence": "20261022_22:00",
+        "type": "Single"
+    },
+    "554": {
+        "question": "What does JakeWatson suggest for refueling after a soccer game?",
+        "answer": "a snack with carbs and protein and staying hydrated",
+        "evidence": "20261023_23:00",
+        "type": "Single"
+    },
+    "555": {
+        "question": "What are LiHua's usual recovery snacks after a game?",
+        "answer": "a protein shake and a fruit smoothie",
+        "evidence": "20261023_23:00",
+        "type": "Single"
+    },
+    "556": {
+        "question": "What event is the community organizing?",
+        "answer": "a weekend picnic",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "557": {
+        "question": "What is the purpose of the community picnic?",
+        "answer": "to bring the community together and get to know each other better",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "558": {
+        "question": "What type of food event did Turalyon suggest?",
+        "answer": "a potluck",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "559": {
+        "question": "What does RexxarRemar suggest for the community picnic atmosphere?",
+        "answer": "organizing some music",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "560": {
+        "question": "What does TirionFordring suggest bringing for seating?",
+        "answer": "blankets or chairs",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "561": {
+        "question": "What day did the community members decide on for the picnic?",
+        "answer": "next Saturday",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "562": {
+        "question": "What does RexxarRemar suggest for keeping everyone energized during the picnic?",
+        "answer": "a mix of snacks and drinks",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "563": {
+        "question": "What does RexxarRemar suggest to have available to keep everyone hydrated during the picnic?",
+        "answer": "water and drinks",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "564": {
+        "question": "What does IllidanStormrage say he will bring to keep everyone entertained during the picnic?",
+        "answer": "some epic games",
+        "evidence": "20261024_11:00",
+        "type": "Single"
+    },
+    "565": {
+        "question": "What places in Europe does WolfgangSchulz remember as stunning in Europe?",
+        "answer": "the fjords in Norway and the lavender fields in Provence",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "566": {
+        "question": "What does WolfgangSchulz like about Prague?",
+        "answer": "its architecture and the vibe of the old town",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "567": {
+        "question": "What local dish did WolfgangSchulz try in Prague?",
+        "answer": "goulash",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "568": {
+        "question": "What is WolfgangSchulz's favorite risotto memory?",
+        "answer": "having seafood risotto in Venice",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "569": {
+        "question": "What is on WolfgangSchulz's list for a future trip?",
+        "answer": "Italy",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "570": {
+        "question": "What does WolfgangSchulz want to check out in Italy?",
+        "answer": "Cinque Terre",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "571": {
+        "question": "What does LiHua say about the seafood in Cinque Terre?",
+        "answer": "it's supposed to be fresh and delicious",
+        "evidence": "20261025_16:00",
+        "type": "Single"
+    },
+    "572": {
+        "question": "What time will Li Hua and Wolfgang meet for breakfast on the morning of the 9th?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "573": {
+        "question": "What type of equipment does Wolfgang Schulz use when he works out at the gym?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "574": {
+        "question": "What type of exercise does Li Hua prefer to do at the gym, and what time does Wolfgang usually go to the gym?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "575": {
+        "question": "What type of traditional games did Wolfgang Schulz play with Li Hua during the Lunar New Year celebration?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "576": {
+        "question": "What did Li Hua eat for dinner on January 20, 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "577": {
+        "question": "What specific suggestions did Li Hua have regarding the construction schedule during the last community meeting?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "578": {
+        "question": "What movie did Li Hua and Wolfgang decide to watch together on New Year's Eve 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "579": {
+        "question": "What dish did Li Hua order for dessert after having Sichuan hot pot with Wolfgang Schulz?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "580": {
+        "question": "What were the specific requirements that the customers modified and how did Li Hua respond to each change?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "581": {
+        "question": "What payment method did Li Hua use to purchase groceries from the store on April 15, 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "582": {
+        "question": "What specific diet did Li Hua follow to achieve his fitness results that Jennifer Moore recommended?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "583": {
+        "question": "What alternative training sports does Li Hua consider besides the current routine mentioned in his conversation with Jennifer?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "584": {
+        "question": "What type of diet is Li Hua following to support his training regimen and sleep schedule?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "585": {
+        "question": "What type of protein shake does Li Hua prefer to drink before his workout sessions?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "586": {
+        "question": "What specific brand of protein supplement does Jennifer recommend for Li Hua's weight loss journey?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "587": {
+        "question": "What specific colors of paint does Li Hua plan to use for the basement walls after decorating with potted plants?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "588": {
+        "question": "What color did Li Hua decide to paint the walls of the basement after completing the renovation?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "589": {
+        "question": "What specific design features did Li Hua suggest for Yuriko's studio homepage during their meeting at \"Central Perk\"?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "590": {
+        "question": "What is the name of the concert that Wolfgang and Li Hua will attend on March 7th?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "591": {
+        "question": "What type of cake does Li Hua plan to bring to the meeting with Yuriko to celebrate her studio's homepage redesign?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "592": {
+        "question": "What color did Li Hua paint his house before the community garden renovation began?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "593": {
+        "question": "What specific dietary changes did Li Hua implement in his training regimen as a result of Jennifer's advice on endurance and flexibility?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "594": {
+        "question": "What specific feedback did Yuriko give to Li Hua about the demo website during their meeting at the cafe \"Central Perk\" on Thursday morning, and how did Li Hua respond to her comments?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "595": {
+        "question": "What is the exact reason for Li Hua's unexpected work meeting on Thursday?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "596": {
+        "question": "What toppings did Hailey put on the bread for Li Hua's next delivery, and what is the name of the bakery she gets her bread from?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "597": {
+        "question": "What is the favorite type of music that Li Hua and Yuriko plan to play together?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "598": {
+        "question": "What are the specific reasons why Li Hua prefers classical music over pop music in their discussions?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "599": {
+        "question": "What song did Yuriko and Wolfgang decide to perform together after watching the drum tutorial?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "600": {
+        "question": "What type of feedback did Li Hua provide to Chae regarding the community medical knowledge lecture, and what is Wolfgang's role in the band rehearsal?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "601": {
+        "question": "What is the name of the last song Wolfgang played using the new drum practice app?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "602": {
+        "question": "What flavor of new bread products did Li Hua really enjoy at the bakery's anniversary event?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "603": {
+        "question": "What is the name of the song that Li Hua will sing at the karaoke on 20260425?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "604": {
+        "question": "What flavor of cake did Hailey plan to bake for the bakery's anniversary celebration in February?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "605": {
+        "question": "What type of dessert did Wolfgang plan to order for Li Hua during their dinner celebration?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "606": {
+        "question": "What were Raze's personal reasons for becoming a fitness coach before discussing pull-up techniques?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "607": {
+        "question": "What new species of flowers will be featured in the community garden renovation, according to the feedback provided by Li Hua during the progress reports?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "608": {
+        "question": "What specific construction projects were discussed at the meeting between Turalyon and the residents regarding noise control?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "609": {
+        "question": "What is the amount of rent Li Hua owes to Adam Smith for the months of April and May?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "610": {
+        "question": "What song did Li Hua perform at the local music festival in 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "611": {
+        "question": "What is the name of the restaurant where Li Hua and Wolfgang had dinner on the night of June 9, 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "612": {
+        "question": "What is the nutritional value of the new line of high-protein breads compared to traditional white bread?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "613": {
+        "question": "What specific sleep techniques did Li Hua use to improve her study habits after reading the neuroscience article and discussing the warm shower with Chae?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "614": {
+        "question": "What type of special dietary restrictions does Li Hua follow when preparing meals for his family?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "615": {
+        "question": "What is the total cost of the air conditioner installation, including labor and materials, if Li Hua had previously discussed a budget of $2,000 with a different contractor?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "616": {
+        "question": "What type of fusion music does Wolfgang Schulz plan to create with Li Hua during their weekend trip to the music store?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "617": {
+        "question": "What type of flowers were planted in the garden based on the residents' suggestions discussed by Turalyon?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "618": {
+        "question": "What type of air-conditioner did Li Hua select for Adam's living room, and how does its temperature regulation compare to the one in the basement?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "619": {
+        "question": "What were Li Hua's specific fitness goals and how did Jennifer's advice on nutrition influence them during his training for a marathon?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "620": {
+        "question": "What are the sales figures for the PS5 exclusive games released in 2026 that Thane discussed with group members?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "621": {
+        "question": "What specific techniques did Li Hua use to prepare for a marathon race that took place on September 1, 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "622": {
+        "question": "What food did Emily order for the group's discussion about Game of Thrones characters?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "623": {
+        "question": "What type of protein supplements did Li Hua use after the workout on September 19, 2026?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "624": {
+        "question": "What color was the curtain that Li Hua chose for his living room?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "625": {
+        "question": "What specific measurements did Li Hua take for the window size before the installation of the curtain?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "626": {
+        "question": "What is Thane's favorite type of food that he enjoys while playing video games?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "627": {
+        "question": "What were the specific details of the negotiation between Jake Watson and Li Hua regarding player transfers from FC Barcelona to FC Bayern Munich?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "628": {
+        "question": "What was the final match score of the 2025 UEFA Champions League final?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "629": {
+        "question": "What was the exact date and time when Cersei Lannister first met Jaime Lannister in the Game of Thrones series?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "630": {
+        "question": "What type of dessert did Wolfgang plan to have with Li Hua after their hot pot dinner on a different day?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "631": {
+        "question": "What did Jennifer say to Li Hua about their plan for a team swimming competition in December?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "632": {
+        "question": "What strategy did Li Hua use to score a goal in the championship match against their rival team?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "633": {
+        "question": "What is Li Hua's favorite type of exercise outside of pull-ups?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "634": {
+        "question": "What time did Chae first suggest they visit the music festival in the morning?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "635": {
+        "question": "What are the reasons behind Jennifer and Li Hua's decision to change their training routine for the following month?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    },
+    "636": {
+        "question": "What specific gifts did Wolfgang buy for Li Hua during his trip to Hong Kong?",
+        "answer": "Insufficient information",
+        "evidence": "N/A",
+        "type": "Null"
+    }
+}
diff --git a/test/minirag/docker-compose.yml b/test/minirag/docker-compose.yml
new file mode 100755
index 0000000..7854374
--- /dev/null
+++ b/test/minirag/docker-compose.yml
@@ -0,0 +1,22 @@
+version: '3.8'
+
+services:
+  lightrag:
+    build: .
+    ports:
+      - "${PORT:-9721}:9721"
+    volumes:
+      - ./data/rag_storage:/app/data/rag_storage
+      - ./data/inputs:/app/data/inputs
+    env_file:
+      - .env
+    environment:
+      - TZ=UTC
+    restart: unless-stopped
+    networks:
+      - lightrag_net
+    extra_hosts:
+      - "host.docker.internal:host-gateway"
+networks:
+  lightrag_net:
+    driver: bridge
diff --git a/test/minirag/graph-visuals/graph_with_html.py b/test/minirag/graph-visuals/graph_with_html.py
new file mode 100755
index 0000000..dc9b87a
--- /dev/null
+++ b/test/minirag/graph-visuals/graph_with_html.py
@@ -0,0 +1,34 @@
+import pipmaster as pm
+
+if not pm.is_installed("pyvis"):
+    pm.install("pyvis")
+if not pm.is_installed("networkx"):
+    pm.install("networkx")
+
+import networkx as nx
+from pyvis.network import Network
+import random
+
+# Load the GraphML file
+G = nx.read_graphml("./LiHua-World/graph_chunk_entity_relation.graphml")
+
+# Create a Pyvis network
+net = Network(height="100vh", notebook=True)
+
+# Convert NetworkX graph to Pyvis network
+net.from_nx(G)
+
+
+# Add colors and title to nodes
+for node in net.nodes:
+    node["color"] = "#{:06x}".format(random.randint(0, 0xFFFFFF))
+    if "description" in node:
+        node["title"] = node["description"]
+
+# Add title to edges
+for edge in net.edges:
+    if "description" in edge:
+        edge["title"] = edge["description"]
+
+# Save and display the network
+net.show("knowledge_graph.html")
diff --git a/test/minirag/graph-visuals/graph_with_neo4j.py b/test/minirag/graph-visuals/graph_with_neo4j.py
new file mode 100755
index 0000000..e7441d1
--- /dev/null
+++ b/test/minirag/graph-visuals/graph_with_neo4j.py
@@ -0,0 +1,126 @@
+import os
+import json
+from minirag.utils import xml_to_json
+from neo4j import GraphDatabase
+
+# Constants
+WORKING_DIR = "./LiHua-World"
+BATCH_SIZE_NODES = 500
+BATCH_SIZE_EDGES = 100
+
+# Neo4j connection credentials
+NEO4J_URI = "bolt://localhost:7687"
+NEO4J_USERNAME = "neo4j"
+NEO4J_PASSWORD = "your_password"
+
+
+def convert_xml_to_json(xml_path, output_path):
+    """Converts XML file to JSON and saves the output."""
+    if not os.path.exists(xml_path):
+        print(f"Error: File not found - {xml_path}")
+        return None
+
+    json_data = xml_to_json(xml_path)
+    if json_data:
+        with open(output_path, "w", encoding="utf-8") as f:
+            json.dump(json_data, f, ensure_ascii=False, indent=2)
+        print(f"JSON file created: {output_path}")
+        return json_data
+    else:
+        print("Failed to create JSON data")
+        return None
+
+
+def process_in_batches(tx, query, data, batch_size):
+    """Process data in batches and execute the given query."""
+    for i in range(0, len(data), batch_size):
+        batch = data[i : i + batch_size]
+        tx.run(query, {"nodes": batch} if "nodes" in query else {"edges": batch})
+
+
+def main():
+    # Paths
+    xml_file = os.path.join(WORKING_DIR, "graph_chunk_entity_relation.graphml")
+    json_file = os.path.join(WORKING_DIR, "graph_data.json")
+
+    # Convert XML to JSON
+    json_data = convert_xml_to_json(xml_file, json_file)
+    if json_data is None:
+        return
+
+    # Load nodes and edges
+    nodes = json_data.get("nodes", [])
+    edges = json_data.get("edges", [])
+
+    # Neo4j queries
+    create_nodes_query = """
+    UNWIND $nodes AS node
+    MERGE (e:Entity {id: node.id})
+    SET e.entity_type = node.entity_type,
+        e.description = node.description,
+        e.source_id = node.source_id,
+        e.displayName = node.id
+    REMOVE e:Entity
+    WITH e, node
+    CALL apoc.create.addLabels(e, [node.id]) YIELD node AS labeledNode
+    RETURN count(*)
+    """
+
+    create_edges_query = """
+    UNWIND $edges AS edge
+    MATCH (source {id: edge.source})
+    MATCH (target {id: edge.target})
+    WITH source, target, edge,
+         CASE
+            WHEN edge.keywords CONTAINS 'lead' THEN 'lead'
+            WHEN edge.keywords CONTAINS 'participate' THEN 'participate'
+            WHEN edge.keywords CONTAINS 'uses' THEN 'uses'
+            WHEN edge.keywords CONTAINS 'located' THEN 'located'
+            WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'
+           ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\"', '')
+         END AS relType
+    CALL apoc.create.relationship(source, relType, {
+      weight: edge.weight,
+      description: edge.description,
+      keywords: edge.keywords,
+      source_id: edge.source_id
+    }, target) YIELD rel
+    RETURN count(*)
+    """
+
+    set_displayname_and_labels_query = """
+    MATCH (n)
+    SET n.displayName = n.id
+    WITH n
+    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node
+    RETURN count(*)
+    """
+
+    # Create a Neo4j driver
+    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))
+
+    try:
+        # Execute queries in batches
+        with driver.session() as session:
+            # Insert nodes in batches
+            session.execute_write(
+                process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES
+            )
+
+            # Insert edges in batches
+            session.execute_write(
+                process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES
+            )
+
+            # Set displayName and labels
+            session.run(set_displayname_and_labels_query)
+
+    except Exception as e:
+        print(f"Error occurred: {e}")
+
+    finally:
+        driver.close()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/test/minirag/main.py b/test/minirag/main.py
new file mode 100755
index 0000000..03006d1
--- /dev/null
+++ b/test/minirag/main.py
@@ -0,0 +1,97 @@
+# from huggingface_hub import login
+# your_token = "INPUT YOUR TOKEN HERE"
+# login(your_token)
+
+import os
+from minirag import MiniRAG, QueryParam
+from minirag.llm.hf import (
+    hf_model_complete,
+    hf_embed,
+)
+from minirag.utils import EmbeddingFunc
+from transformers import AutoModel, AutoTokenizer
+
+EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
+
+import argparse
+
+
+def get_args():
+    parser = argparse.ArgumentParser(description="MiniRAG")
+    parser.add_argument("--model", type=str, default="PHI")
+    parser.add_argument("--outputpath", type=str, default="./logs/Default_output.csv")
+    parser.add_argument("--workingdir", type=str, default="./LiHua-World")
+    parser.add_argument("--datapath", type=str, default="./dataset/LiHua-World/data/")
+    parser.add_argument(
+        "--querypath", type=str, default="./dataset/LiHua-World/qa/query_set.csv"
+    )
+    args = parser.parse_args()
+    return args
+
+
+args = get_args()
+
+
+if args.model == "PHI":
+    LLM_MODEL = "microsoft/Phi-3.5-mini-instruct"
+elif args.model == "GLM":
+    LLM_MODEL = "THUDM/glm-edge-1.5b-chat"
+elif args.model == "MiniCPM":
+    LLM_MODEL = "openbmb/MiniCPM3-4B"
+elif args.model == "qwen":
+    LLM_MODEL = "Qwen/Qwen2.5-3B-Instruct"
+else:
+    print("Invalid model name")
+    exit(1)
+
+WORKING_DIR = args.workingdir
+DATA_PATH = args.datapath
+QUERY_PATH = args.querypath
+OUTPUT_PATH = args.outputpath
+print("USING LLM:", LLM_MODEL)
+print("USING WORKING DIR:", WORKING_DIR)
+
+
+if not os.path.exists(WORKING_DIR):
+    os.mkdir(WORKING_DIR)
+
+rag = MiniRAG(
+    working_dir=WORKING_DIR,
+    llm_model_func=hf_model_complete,
+    llm_model_max_token_size=200,
+    llm_model_name=LLM_MODEL,
+    embedding_func=EmbeddingFunc(
+        embedding_dim=384,
+        max_token_size=1000,
+        func=lambda texts: hf_embed(
+            texts,
+            tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),
+            embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),
+        ),
+    ),
+)
+
+
+# Now indexing
+def find_txt_files(root_path):
+    txt_files = []
+    for root, dirs, files in os.walk(root_path):
+        for file in files:
+            if file.endswith(".txt"):
+                txt_files.append(os.path.join(root, file))
+    return txt_files
+
+
+WEEK_LIST = find_txt_files(DATA_PATH)
+for WEEK in WEEK_LIST:
+    id = WEEK_LIST.index(WEEK)
+    print(f"{id}/{len(WEEK_LIST)}")
+    with open(WEEK) as f:
+        rag.insert(f.read())
+
+# A toy query
+query = 'What does LiHua predict will happen in "The Rings of Power"?'
+answer = (
+    rag.query(query, param=QueryParam(mode="mini")).replace("\n", "").replace("\r", "")
+)
+print(answer)
diff --git a/test/minirag/minirag/__init__.py b/test/minirag/minirag/__init__.py
new file mode 100755
index 0000000..0144540
--- /dev/null
+++ b/test/minirag/minirag/__init__.py
@@ -0,0 +1,5 @@
+from .minirag import MiniRAG as MiniRAG, QueryParam as QueryParam
+
+__version__ = "0.0.2"
+__author__ = "Tianyu Fan"
+__url__ = "https://github.com/HKUDS/MiniRAG"
diff --git a/test/minirag/minirag/api/.env.aoi.example b/test/minirag/minirag/api/.env.aoi.example
new file mode 100755
index 0000000..cea86da
--- /dev/null
+++ b/test/minirag/minirag/api/.env.aoi.example
@@ -0,0 +1,7 @@
+AZURE_OPENAI_API_VERSION=2024-08-01-preview
+AZURE_OPENAI_DEPLOYMENT=gpt-4o
+AZURE_OPENAI_API_KEY=myapikey
+AZURE_OPENAI_ENDPOINT=https://myendpoint.openai.azure.com
+
+AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
+AZURE_EMBEDDING_API_VERSION=2023-05-15
diff --git a/test/minirag/minirag/api/.gitignore b/test/minirag/minirag/api/.gitignore
new file mode 100755
index 0000000..8cb6821
--- /dev/null
+++ b/test/minirag/minirag/api/.gitignore
@@ -0,0 +1,2 @@
+inputs
+rag_storage
diff --git a/test/minirag/minirag/api/README.md b/test/minirag/minirag/api/README.md
new file mode 100755
index 0000000..0d8466d
--- /dev/null
+++ b/test/minirag/minirag/api/README.md
@@ -0,0 +1,464 @@
+## Install with API Support
+
+MiniRAG now provides optional API support through FastAPI servers that add RAG capabilities to existing LLM services. You can install MiniRAG with API support in two ways: (using MiniRAG is the same as LightRAG)
+
+### 1. Installation from PyPI
+
+```bash
+pip install "lightrag-hku[api]"
+```
+Note: we use the same package for the MiniRAG.
+
+### 2. Installation from Source (Development)
+
+```bash
+# Clone the repository
+git clone https://github.com/HKUDS/minirag.git
+
+# Change to the repository directory
+cd minirag
+
+# create a Python virtual enviroment if neccesary
+# Install in editable mode with API support
+pip install -e ".[api]"
+```
+
+### Prerequisites
+
+Before running any of the servers, ensure you have the corresponding backend service running for both llm and embedding.
+The new api allows you to mix different bindings for llm/embeddings.
+For example, you have the possibility to use ollama for the embedding and openai for the llm.
+
+#### For LoLLMs Server
+- LoLLMs must be running and accessible
+- Default connection: http://localhost:9600
+- Configure using --llm-binding-host and/or --embedding-binding-host if running on a different host/port
+
+#### For Ollama Server
+- Ollama must be running and accessible
+- Requires environment variables setup or command line argument provided
+- Environment variables: LLM_BINDING=ollama, LLM_BINDING_HOST, LLM_MODEL
+- Command line arguments: --llm-binding=ollama, --llm-binding-host, --llm-model
+- Default connection is  http://localhost:11434 if not priveded
+
+> The default MAX_TOKENS(num_ctx) for Ollama is 32768. If your Ollama server is lacking or GPU memory, set it to a lower value.
+
+#### For OpenAI Alike Server
+- Requires environment variables setup or command line argument provided
+- Environment variables: LLM_BINDING=ollama, LLM_BINDING_HOST, LLM_MODEL, LLM_BINDING_API_KEY
+- Command line arguments: --llm-binding=ollama, --llm-binding-host, --llm-model, --llm-binding-api-key
+- Default connection is https://api.openai.com/v1 if not priveded
+
+#### For Azure OpenAI Server
+Azure OpenAI API can be created using the following commands in Azure CLI (you need to install Azure CLI first from [https://docs.microsoft.com/en-us/cli/azure/install-azure-cli](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)):
+```bash
+# Change the resource group name, location and OpenAI resource name as needed
+RESOURCE_GROUP_NAME=MiniRAG
+LOCATION=swedencentral
+RESOURCE_NAME=MiniRAG-OpenAI
+
+az login
+az group create --name $RESOURCE_GROUP_NAME --location $LOCATION
+az cognitiveservices account create --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME  --kind OpenAI --sku S0 --location swedencentral
+az cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name gpt-4o --model-name gpt-4o --model-version "2024-08-06"  --sku-capacity 100 --sku-name "Standard"
+az cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name text-embedding-3-large --model-name text-embedding-3-large --model-version "1"  --sku-capacity 80 --sku-name "Standard"
+az cognitiveservices account show --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME --query "properties.endpoint"
+az cognitiveservices account keys list --name $RESOURCE_NAME -g $RESOURCE_GROUP_NAME
+
+```
+The output of the last command will give you the endpoint and the key for the OpenAI API. You can use these values to set the environment variables in the `.env` file.
+
+```
+LLM_BINDING=azure_openai
+LLM_BINDING_HOST=endpoint_of_azure_ai
+LLM_MODEL=model_name_of_azure_ai
+LLM_BINDING_API_KEY=api_key_of_azure_ai
+```
+
+### About Ollama API
+
+We provide an Ollama-compatible interfaces for MiniRAG, aiming to emulate MiniRAG as an Ollama chat model. This allows AI chat frontends supporting Ollama, such as Open WebUI, to access MiniRAG easily.
+
+#### Connect Open WebUI to MiniRAG
+
+After starting the minirag-server, you can add an Ollama-type connection in the Open WebUI admin pannel. And then a model named minirag:latest will appear in Open WebUI's model management interface. Users can then send queries to MiniRAG through the chat interface.
+
+## Configuration
+
+MiniRAG can be configured using either command-line arguments or environment variables. When both are provided, command-line arguments take precedence over environment variables.
+
+For better performance, the API server's default values for TOP_K and COSINE_THRESHOLD are set to 50 and 0.4 respectively. If COSINE_THRESHOLD remains at its default value of 0.2 in MiniRAG, many irrelevant entities and relations would be retrieved and sent to the LLM.
+
+### Environment Variables
+
+You can configure MiniRAG using environment variables by creating a `.env` file in your project root directory. Here's a complete example of available environment variables:
+
+```env
+# Server Configuration
+HOST=0.0.0.0
+PORT=9721
+
+# Directory Configuration
+WORKING_DIR=/app/data/rag_storage
+INPUT_DIR=/app/data/inputs
+
+# RAG Configuration
+MAX_ASYNC=4
+MAX_TOKENS=32768
+EMBEDDING_DIM=1024
+MAX_EMBED_TOKENS=8192
+#HISTORY_TURNS=3
+#CHUNK_SIZE=1200
+#CHUNK_OVERLAP_SIZE=100
+#COSINE_THRESHOLD=0.4
+#TOP_K=50
+
+# LLM Configuration
+LLM_BINDING=ollama
+LLM_BINDING_HOST=http://localhost:11434
+LLM_MODEL=mistral-nemo:latest
+
+# must be set if using OpenAI LLM (LLM_MODEL must be set or set by command line parms)
+OPENAI_API_KEY=you_api_key
+
+# Embedding Configuration
+EMBEDDING_BINDING=ollama
+EMBEDDING_BINDING_HOST=http://localhost:11434
+EMBEDDING_MODEL=bge-m3:latest
+
+# Security
+#MINIRAG_API_KEY=you-api-key-for-accessing-MiniRAG
+
+# Logging
+LOG_LEVEL=INFO
+
+# Optional SSL Configuration
+#SSL=true
+#SSL_CERTFILE=/path/to/cert.pem
+#SSL_KEYFILE=/path/to/key.pem
+
+# Optional Timeout
+#TIMEOUT=30
+```
+
+### Configuration Priority
+
+The configuration values are loaded in the following order (highest priority first):
+1. Command-line arguments
+2. Environment variables
+3. Default values
+
+For example:
+```bash
+# This command-line argument will override both the environment variable and default value
+python minirag.py --port 8080
+
+# The environment variable will override the default value but not the command-line argument
+PORT=7000 python minirag.py
+```
+
+#### MiniRag Server Options
+
+| Parameter | Default | Description |
+|-----------|---------|-------------|
+| --host | 0.0.0.0 | Server host |
+| --port | 9721 | Server port |
+| --llm-binding | ollama | LLM binding to be used. Supported: lollms, ollama, openai |
+| --llm-binding-host | (dynamic) | LLM server host URL. Defaults based on binding: http://localhost:11434 (ollama), http://localhost:9600 (lollms), https://api.openai.com/v1 (openai) |
+| --llm-model | mistral-nemo:latest | LLM model name |
+| --llm-binding-api-key | None | API Key for OpenAI Alike LLM |
+| --embedding-binding | ollama | Embedding binding to be used. Supported: lollms, ollama, openai |
+| --embedding-binding-host | (dynamic) | Embedding server host URL. Defaults based on binding: http://localhost:11434 (ollama), http://localhost:9600 (lollms), https://api.openai.com/v1 (openai) |
+| --embedding-model | bge-m3:latest | Embedding model name |
+| --working-dir | ./rag_storage | Working directory for RAG storage |
+| --input-dir | ./inputs | Directory containing input documents |
+| --max-async | 4 | Maximum async operations |
+| --max-tokens | 32768 | Maximum token size |
+| --embedding-dim | 1024 | Embedding dimensions |
+| --max-embed-tokens | 8192 | Maximum embedding token size |
+| --timeout | None | Timeout in seconds (useful when using slow AI). Use None for infinite timeout |
+| --log-level | INFO | Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) |
+| --key | None | API key for authentication. Protects minirag server against unauthorized access |
+| --ssl | False | Enable HTTPS |
+| --ssl-certfile | None | Path to SSL certificate file (required if --ssl is enabled) |
+| --ssl-keyfile | None | Path to SSL private key file (required if --ssl is enabled) |
+| --top-k | 50 | Number of top-k items to retrieve; corresponds to entities in "local" mode and relationships in "global" mode. |
+| --cosine-threshold | 0.4 | The cossine threshold for nodes and relations retrieval, works with top-k to control the retrieval of nodes and relations. |
+
+### Example Usage
+
+#### Running a MiniRag server with ollama default local server as llm and embedding backends
+
+Ollama is the default backend for both llm and embedding, so by default you can run minirag-server with no parameters and the default ones will be used. Make sure ollama is installed and is running and default models are already installed on ollama.
+
+```bash
+# Run minirag with ollama, mistral-nemo:latest for llm, and bge-m3:latest for embedding
+minirag-server
+
+# Using specific models (ensure they are installed in your ollama instance)
+minirag-server --llm-model adrienbrault/nous-hermes2theta-llama3-8b:f16 --embedding-model nomic-embed-text --embedding-dim 1024
+
+# Using an authentication key
+minirag-server --key my-key
+
+# Using lollms for llm and ollama for embedding
+minirag-server --llm-binding lollms
+```
+
+#### Running a MiniRAG server with lollms default local server as llm and embedding backends
+
+```bash
+# Run minirag with lollms, mistral-nemo:latest for llm, and bge-m3:latest for embedding, use lollms for both llm and embedding
+minirag-server --llm-binding lollms --embedding-binding lollms
+
+# Using specific models (ensure they are installed in your ollama instance)
+minirag-server --llm-binding lollms --llm-model adrienbrault/nous-hermes2theta-llama3-8b:f16 --embedding-binding lollms --embedding-model nomic-embed-text --embedding-dim 1024
+
+# Using an authentication key
+minirag-server --key my-key
+
+# Using lollms for llm and openai for embedding
+minirag-server --llm-binding lollms --embedding-binding openai --embedding-model text-embedding-3-small
+```
+
+
+#### Running a MiniRAG server with openai server as llm and embedding backends
+
+```bash
+# Run minirag with lollms, GPT-4o-mini  for llm, and text-embedding-3-small for embedding, use openai for both llm and embedding
+minirag-server --llm-binding openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small
+
+# Using an authentication key
+minirag-server --llm-binding openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small --key my-key
+
+# Using lollms for llm and openai for embedding
+minirag-server --llm-binding lollms --embedding-binding openai --embedding-model text-embedding-3-small
+```
+
+#### Running a MiniRAG server with azure openai server as llm and embedding backends
+
+```bash
+# Run minirag with lollms, GPT-4o-mini  for llm, and text-embedding-3-small for embedding, use openai for both llm and embedding
+minirag-server --llm-binding azure_openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small
+
+# Using an authentication key
+minirag-server --llm-binding azure_openai --llm-model GPT-4o-mini --embedding-binding azure_openai --embedding-model text-embedding-3-small --key my-key
+
+# Using lollms for llm and azure_openai for embedding
+minirag-server --llm-binding lollms --embedding-binding azure_openai --embedding-model text-embedding-3-small
+```
+
+**Important Notes:**
+- For LoLLMs: Make sure the specified models are installed in your LoLLMs instance
+- For Ollama: Make sure the specified models are installed in your Ollama instance
+- For OpenAI: Ensure you have set up your OPENAI_API_KEY environment variable
+- For Azure OpenAI: Build and configure your server as stated in the Prequisites section
+
+For help on any server, use the --help flag:
+```bash
+minirag-server --help
+```
+
+Note: If you don't need the API functionality, you can install the base package without API support using:
+```bash
+pip install lightrag-hku
+```
+
+## API Endpoints
+
+All servers (LoLLMs, Ollama, OpenAI and Azure OpenAI) provide the same REST API endpoints for RAG functionality.
+
+### Query Endpoints
+
+#### POST /query
+Query the RAG system with options for different search modes.
+
+```bash
+curl -X POST "http://localhost:9721/query" \
+    -H "Content-Type: application/json" \
+    -d '{"query": "Your question here", "mode": "hybrid", ""}'
+```
+
+#### POST /query/stream
+Stream responses from the RAG system.
+
+```bash
+curl -X POST "http://localhost:9721/query/stream" \
+    -H "Content-Type: application/json" \
+    -d '{"query": "Your question here", "mode": "hybrid"}'
+```
+
+### Document Management Endpoints
+
+#### POST /documents/text
+Insert text directly into the RAG system.
+
+```bash
+curl -X POST "http://localhost:9721/documents/text" \
+    -H "Content-Type: application/json" \
+    -d '{"text": "Your text content here", "description": "Optional description"}'
+```
+
+#### POST /documents/file
+Upload a single file to the RAG system.
+
+```bash
+curl -X POST "http://localhost:9721/documents/file" \
+    -F "file=@/path/to/your/document.txt" \
+    -F "description=Optional description"
+```
+
+#### POST /documents/batch
+Upload multiple files at once.
+
+```bash
+curl -X POST "http://localhost:9721/documents/batch" \
+    -F "files=@/path/to/doc1.txt" \
+    -F "files=@/path/to/doc2.txt"
+```
+
+#### POST /documents/scan
+
+Trigger document scan for new files in the Input directory.
+
+```bash
+curl -X POST "http://localhost:9721/documents/scan" --max-time 1800
+```
+
+> Ajust max-time according to the estimated index time  for all new files.
+
+### Ollama Emulation Endpoints
+
+#### GET /api/version
+
+Get Ollama version information
+
+```bash
+curl http://localhost:9721/api/version
+```
+
+#### GET /api/tags
+
+Get Ollama available models
+
+```bash
+curl http://localhost:9721/api/tags
+```
+
+#### POST /api/chat
+
+Handle chat completion requests
+
+```shell
+curl -N -X POST http://localhost:9721/api/chat -H "Content-Type: application/json" -d \
+  '{"model":"minirag:latest","messages":[{"role":"user","content":"猪八戒是谁"}],"stream":true}'
+```
+
+> For more information about Ollama API pls. visit :  [Ollama API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
+
+#### DELETE /documents
+
+Clear all documents from the RAG system.
+
+```bash
+curl -X DELETE "http://localhost:9721/documents"
+```
+
+### Utility Endpoints
+
+#### GET /health
+Check server health and configuration.
+
+```bash
+curl "http://localhost:9721/health"
+```
+
+## Development
+Contribute to the project: [Guide](contributor-readme.MD)
+
+### Running in Development Mode
+
+For LoLLMs:
+```bash
+uvicorn lollms_minirag_server:app --reload --port 9721
+```
+
+For Ollama:
+```bash
+uvicorn ollama_minirag_server:app --reload --port 9721
+```
+
+For OpenAI:
+```bash
+uvicorn openai_minirag_server:app --reload --port 9721
+```
+For Azure OpenAI:
+```bash
+uvicorn azure_openai_minirag_server:app --reload --port 9721
+```
+### API Documentation
+
+When any server is running, visit:
+- Swagger UI: http://localhost:9721/docs
+- ReDoc: http://localhost:9721/redoc
+
+### Testing API Endpoints
+
+You can test the API endpoints using the provided curl commands or through the Swagger UI interface. Make sure to:
+1. Start the appropriate backend service (LoLLMs, Ollama, or OpenAI)
+2. Start the RAG server
+3. Upload some documents using the document management endpoints
+4. Query the system using the query endpoints
+5. Trigger document scan if new files is put into inputs directory
+
+### Important Features
+
+#### Automatic Document Vectorization
+When starting any of the servers with the `--input-dir` parameter, the system will automatically:
+1. Check for existing vectorized content in the database
+2. Only vectorize new documents that aren't already in the database
+3. Make all content immediately available for RAG queries
+
+This intelligent caching mechanism:
+- Prevents unnecessary re-vectorization of existing documents
+- Reduces startup time for subsequent runs
+- Preserves system resources
+- Maintains consistency across restarts
+
+**Important Notes:**
+- The `--input-dir` parameter enables automatic document processing at startup
+- Documents already in the database are not re-vectorized
+- Only new documents in the input directory will be processed
+- This optimization significantly reduces startup time for subsequent runs
+- The working directory (`--working-dir`) stores the vectorized documents database
+
+## Install Lightrag as a Linux Service
+
+Create your service file: `minirag-server.sevice`. Modified the following lines from `minirag-server.sevice.example`
+
+```text
+Description=MiniRAG Ollama Service
+WorkingDirectory=<minirag installed directory>
+ExecStart=<minirag installed directory>/minirag/api/start_minirag.sh
+```
+
+Create your service startup script: `start_minirag.sh`. Change you python virtual environment activation method as need:
+
+```shell
+#!/bin/bash
+
+# python virtual environment activation
+source /home/netman/minirag-xyj/venv/bin/activate
+# start lightrag api server
+lightrag-server
+```
+
+Install minirag.service in Linux.  Sample commands in Ubuntu server look like:
+#Note: lightrag-server.service is the service file name, you can change it to minirag-server.service as needed.
+```shell
+sudo cp lightrag-server.service /etc/systemd/system/
+sudo systemctl daemon-reload
+sudo systemctl start lightrag-server.service
+sudo systemctl status lightrag-server.service
+sudo systemctl enable lightrag-server.service
+```
diff --git a/test/minirag/minirag/api/__init__.py b/test/minirag/minirag/api/__init__.py
new file mode 100755
index 0000000..70239ed
--- /dev/null
+++ b/test/minirag/minirag/api/__init__.py
@@ -0,0 +1 @@
+__api_version__ = "1.0.3"
diff --git a/test/minirag/minirag/api/minirag_server.py b/test/minirag/minirag/api/minirag_server.py
new file mode 100755
index 0000000..f9d9a78
--- /dev/null
+++ b/test/minirag/minirag/api/minirag_server.py
@@ -0,0 +1,1946 @@
+from fastapi import FastAPI, HTTPException, File, UploadFile, Form, Request
+
+# Backend (Python)
+# Add this to store progress globally
+from typing import Dict
+import threading
+
+# Global progress tracker
+scan_progress: Dict = {
+    "is_scanning": False,
+    "current_file": "",
+    "indexed_count": 0,
+    "total_files": 0,
+    "progress": 0,
+}
+
+# Lock for thread-safe operations
+progress_lock = threading.Lock()
+
+import json
+import os
+
+from fastapi.staticfiles import StaticFiles
+from pydantic import BaseModel
+import logging
+import argparse
+import time
+import re
+from typing import List, Dict, Any, Optional, Union
+from minirag import MiniRAG, QueryParam
+from minirag.api import __api_version__
+
+from minirag.utils import EmbeddingFunc
+from enum import Enum
+from pathlib import Path
+import shutil
+import aiofiles
+from ascii_colors import trace_exception, ASCIIColors
+import sys
+import configparser
+
+from fastapi import Depends, Security
+from fastapi.security import APIKeyHeader
+from fastapi.middleware.cors import CORSMiddleware
+from contextlib import asynccontextmanager
+
+from starlette.status import HTTP_403_FORBIDDEN
+import pipmaster as pm
+
+from dotenv import load_dotenv
+
+load_dotenv()
+
+
+def estimate_tokens(text: str) -> int:
+    """Estimate the number of tokens in text
+    Chinese characters: approximately 1.5 tokens per character
+    English characters: approximately 0.25 tokens per character
+    """
+    # Use regex to match Chinese and non-Chinese characters separately
+    chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
+    non_chinese_chars = len(re.findall(r"[^\u4e00-\u9fff]", text))
+
+    # Calculate estimated token count
+    tokens = chinese_chars * 1.5 + non_chinese_chars * 0.25
+
+    return int(tokens)
+
+
+class OllamaServerInfos:
+    # Constants for emulated Ollama model information
+    LIGHTRAG_NAME = "minirag"
+    LIGHTRAG_TAG = os.getenv("OLLAMA_EMULATING_MODEL_TAG", "latest")
+    LIGHTRAG_MODEL = f"{LIGHTRAG_NAME}:{LIGHTRAG_TAG}"
+    LIGHTRAG_SIZE = 7365960935  # it's a dummy value
+    LIGHTRAG_CREATED_AT = "2024-01-15T00:00:00Z"
+    LIGHTRAG_DIGEST = "sha256:minirag"
+
+    KV_STORAGE = "JsonKVStorage"
+    DOC_STATUS_STORAGE = "JsonDocStatusStorage"
+    GRAPH_STORAGE = "NetworkXStorage"
+    VECTOR_STORAGE = "NanoVectorDBStorage"
+
+
+# Add infos
+ollama_server_infos = OllamaServerInfos()
+
+# read config.ini
+config = configparser.ConfigParser()
+config.read("config.ini", "utf-8")
+# Redis config
+redis_uri = config.get("redis", "uri", fallback=None)
+if redis_uri:
+    os.environ["REDIS_URI"] = redis_uri
+    ollama_server_infos.KV_STORAGE = "RedisKVStorage"
+    ollama_server_infos.DOC_STATUS_STORAGE = "RedisKVStorage"
+
+# Neo4j config
+neo4j_uri = config.get("neo4j", "uri", fallback=None)
+neo4j_username = config.get("neo4j", "username", fallback=None)
+neo4j_password = config.get("neo4j", "password", fallback=None)
+if neo4j_uri:
+    os.environ["NEO4J_URI"] = neo4j_uri
+    os.environ["NEO4J_USERNAME"] = neo4j_username
+    os.environ["NEO4J_PASSWORD"] = neo4j_password
+    ollama_server_infos.GRAPH_STORAGE = "Neo4JStorage"
+
+# Milvus config
+milvus_uri = config.get("milvus", "uri", fallback=None)
+milvus_user = config.get("milvus", "user", fallback=None)
+milvus_password = config.get("milvus", "password", fallback=None)
+milvus_db_name = config.get("milvus", "db_name", fallback=None)
+if milvus_uri:
+    os.environ["MILVUS_URI"] = milvus_uri
+    os.environ["MILVUS_USER"] = milvus_user
+    os.environ["MILVUS_PASSWORD"] = milvus_password
+    os.environ["MILVUS_DB_NAME"] = milvus_db_name
+    ollama_server_infos.VECTOR_STORAGE = "MilvusVectorDBStorge"
+
+# MongoDB config
+mongo_uri = config.get("mongodb", "uri", fallback=None)
+mongo_database = config.get("mongodb", "MiniRAG", fallback=None)
+if mongo_uri:
+    os.environ["MONGO_URI"] = mongo_uri
+    os.environ["MONGO_DATABASE"] = mongo_database
+    ollama_server_infos.KV_STORAGE = "MongoKVStorage"
+    ollama_server_infos.DOC_STATUS_STORAGE = "MongoKVStorage"
+
+
+def get_default_host(binding_type: str) -> str:
+    default_hosts = {
+        "ollama": os.getenv("LLM_BINDING_HOST", "http://localhost:11434"),
+        "lollms": os.getenv("LLM_BINDING_HOST", "http://localhost:9600"),
+        "azure_openai": os.getenv("AZURE_OPENAI_ENDPOINT", "https://api.openai.com/v1"),
+        "openai": os.getenv("LLM_BINDING_HOST", "https://api.openai.com/v1"),
+    }
+    return default_hosts.get(
+        binding_type, os.getenv("LLM_BINDING_HOST", "http://localhost:11434")
+    )  # fallback to ollama if unknown
+
+
+def get_env_value(env_key: str, default: Any, value_type: type = str) -> Any:
+    """
+    Get value from environment variable with type conversion
+
+    Args:
+        env_key (str): Environment variable key
+        default (Any): Default value if env variable is not set
+        value_type (type): Type to convert the value to
+
+    Returns:
+        Any: Converted value from environment or default
+    """
+    value = os.getenv(env_key)
+    if value is None:
+        return default
+
+    if isinstance(value_type, bool):
+        return value.lower() in ("true", "1", "yes")
+    try:
+        return value_type(value)
+    except ValueError:
+        return default
+
+
+def display_splash_screen(args: argparse.Namespace) -> None:
+    """
+    Display a colorful splash screen showing MiniRAG server configuration
+
+    Args:
+        args: Parsed command line arguments
+    """
+    # Banner
+    ASCIIColors.cyan(f"""
+    ╔══════════════════════════════════════════════════════════════╗
+    ║                   🚀 MiniRAG Server v{__api_version__}                  ║
+    ║          Fast, Lightweight RAG Server Implementation         ║
+    ╚══════════════════════════════════════════════════════════════╝
+    """)
+
+    # Server Configuration
+    ASCIIColors.magenta("\n📡 Server Configuration:")
+    ASCIIColors.white("    ├─ Host: ", end="")
+    ASCIIColors.yellow(f"{args.host}")
+    ASCIIColors.white("    ├─ Port: ", end="")
+    ASCIIColors.yellow(f"{args.port}")
+    ASCIIColors.white("    ├─ SSL Enabled: ", end="")
+    ASCIIColors.yellow(f"{args.ssl}")
+    if args.ssl:
+        ASCIIColors.white("    ├─ SSL Cert: ", end="")
+        ASCIIColors.yellow(f"{args.ssl_certfile}")
+        ASCIIColors.white("    └─ SSL Key: ", end="")
+        ASCIIColors.yellow(f"{args.ssl_keyfile}")
+
+    # Directory Configuration
+    ASCIIColors.magenta("\n📂 Directory Configuration:")
+    ASCIIColors.white("    ├─ Working Directory: ", end="")
+    ASCIIColors.yellow(f"{args.working_dir}")
+    ASCIIColors.white("    └─ Input Directory: ", end="")
+    ASCIIColors.yellow(f"{args.input_dir}")
+
+    # LLM Configuration
+    ASCIIColors.magenta("\n🤖 LLM Configuration:")
+    ASCIIColors.white("    ├─ Binding: ", end="")
+    ASCIIColors.yellow(f"{args.llm_binding}")
+    ASCIIColors.white("    ├─ Host: ", end="")
+    ASCIIColors.yellow(f"{args.llm_binding_host}")
+    ASCIIColors.white("    └─ Model: ", end="")
+    ASCIIColors.yellow(f"{args.llm_model}")
+
+    # Embedding Configuration
+    ASCIIColors.magenta("\n📊 Embedding Configuration:")
+    ASCIIColors.white("    ├─ Binding: ", end="")
+    ASCIIColors.yellow(f"{args.embedding_binding}")
+    ASCIIColors.white("    ├─ Host: ", end="")
+    ASCIIColors.yellow(f"{args.embedding_binding_host}")
+    ASCIIColors.white("    ├─ Model: ", end="")
+    ASCIIColors.yellow(f"{args.embedding_model}")
+    ASCIIColors.white("    └─ Dimensions: ", end="")
+    ASCIIColors.yellow(f"{args.embedding_dim}")
+
+    # RAG Configuration
+    ASCIIColors.magenta("\n⚙️ RAG Configuration:")
+    ASCIIColors.white("    ├─ Max Async Operations: ", end="")
+    ASCIIColors.yellow(f"{args.max_async}")
+    ASCIIColors.white("    ├─ Max Tokens: ", end="")
+    ASCIIColors.yellow(f"{args.max_tokens}")
+    ASCIIColors.white("    ├─ Max Embed Tokens: ", end="")
+    ASCIIColors.yellow(f"{args.max_embed_tokens}")
+    ASCIIColors.white("    ├─ Chunk Size: ", end="")
+    ASCIIColors.yellow(f"{args.chunk_size}")
+    ASCIIColors.white("    ├─ Chunk Overlap Size: ", end="")
+    ASCIIColors.yellow(f"{args.chunk_overlap_size}")
+    ASCIIColors.white("    ├─ History Turns: ", end="")
+    ASCIIColors.yellow(f"{args.history_turns}")
+    ASCIIColors.white("    ├─ Cosine Threshold: ", end="")
+    ASCIIColors.yellow(f"{args.cosine_threshold}")
+    ASCIIColors.white("    └─ Top-K: ", end="")
+    ASCIIColors.yellow(f"{args.top_k}")
+
+    # System Configuration
+    ASCIIColors.magenta("\n🛠️ System Configuration:")
+    ASCIIColors.white("    ├─ Ollama Emulating Model: ", end="")
+    ASCIIColors.yellow(f"{ollama_server_infos.LIGHTRAG_MODEL}")
+    ASCIIColors.white("    ├─ Log Level: ", end="")
+    ASCIIColors.yellow(f"{args.log_level}")
+    ASCIIColors.white("    ├─ Timeout: ", end="")
+    ASCIIColors.yellow(f"{args.timeout if args.timeout else 'None (infinite)'}")
+    ASCIIColors.white("    └─ API Key: ", end="")
+    ASCIIColors.yellow("Set" if args.key else "Not Set")
+
+    # Server Status
+    ASCIIColors.green("\n✨ Server starting up...\n")
+
+    # Server Access Information
+    protocol = "https" if args.ssl else "http"
+    if args.host == "0.0.0.0":
+        ASCIIColors.magenta("\n🌐 Server Access Information:")
+        ASCIIColors.white("    ├─ Local Access: ", end="")
+        ASCIIColors.yellow(f"{protocol}://localhost:{args.port}")
+        ASCIIColors.white("    ├─ Remote Access: ", end="")
+        ASCIIColors.yellow(f"{protocol}://<your-ip-address>:{args.port}")
+        ASCIIColors.white("    ├─ API Documentation (local): ", end="")
+        ASCIIColors.yellow(f"{protocol}://localhost:{args.port}/docs")
+        ASCIIColors.white("    └─ Alternative Documentation (local): ", end="")
+        ASCIIColors.yellow(f"{protocol}://localhost:{args.port}/redoc")
+
+        ASCIIColors.yellow("\n📝 Note:")
+        ASCIIColors.white("""    Since the server is running on 0.0.0.0:
+    - Use 'localhost' or '127.0.0.1' for local access
+    - Use your machine's IP address for remote access
+    - To find your IP address:
+      • Windows: Run 'ipconfig' in terminal
+      • Linux/Mac: Run 'ifconfig' or 'ip addr' in terminal
+    """)
+    else:
+        base_url = f"{protocol}://{args.host}:{args.port}"
+        ASCIIColors.magenta("\n🌐 Server Access Information:")
+        ASCIIColors.white("    ├─ Base URL: ", end="")
+        ASCIIColors.yellow(f"{base_url}")
+        ASCIIColors.white("    ├─ API Documentation: ", end="")
+        ASCIIColors.yellow(f"{base_url}/docs")
+        ASCIIColors.white("    └─ Alternative Documentation: ", end="")
+        ASCIIColors.yellow(f"{base_url}/redoc")
+
+    # Usage Examples
+    ASCIIColors.magenta("\n📚 Quick Start Guide:")
+    ASCIIColors.cyan("""
+    1. Access the Swagger UI:
+       Open your browser and navigate to the API documentation URL above
+
+    2. API Authentication:""")
+    if args.key:
+        ASCIIColors.cyan("""       Add the following header to your requests:
+       X-API-Key: <your-api-key>
+    """)
+    else:
+        ASCIIColors.cyan("       No authentication required\n")
+
+    ASCIIColors.cyan("""    3. Basic Operations:
+       - POST /upload_document: Upload new documents to RAG
+       - POST /query: Query your document collection
+       - GET /collections: List available collections
+
+    4. Monitor the server:
+       - Check server logs for detailed operation information
+       - Use healthcheck endpoint: GET /health
+    """)
+
+    # Security Notice
+    if args.key:
+        ASCIIColors.yellow("\n⚠️  Security Notice:")
+        ASCIIColors.white("""    API Key authentication is enabled.
+    Make sure to include the X-API-Key header in all your requests.
+    """)
+
+    ASCIIColors.green("Server is ready to accept connections! 🚀\n")
+
+    # Ensure splash output flush to system log
+    sys.stdout.flush()
+
+
+def parse_args() -> argparse.Namespace:
+    """
+    Parse command line arguments with environment variable fallback
+
+    Returns:
+        argparse.Namespace: Parsed arguments
+    """
+
+    parser = argparse.ArgumentParser(
+        description="MiniRAG FastAPI Server with separate working and input directories"
+    )
+
+    # Bindings configuration
+    parser.add_argument(
+        "--llm-binding",
+        default=get_env_value("LLM_BINDING", "ollama"),
+        help="LLM binding to be used. Supported: lollms, ollama, openai (default: from env or ollama)",
+    )
+    parser.add_argument(
+        "--embedding-binding",
+        default=get_env_value("EMBEDDING_BINDING", "ollama"),
+        help="Embedding binding to be used. Supported: lollms, ollama, openai (default: from env or ollama)",
+    )
+
+    # Server configuration
+    parser.add_argument(
+        "--host",
+        default=get_env_value("HOST", "0.0.0.0"),
+        help="Server host (default: from env or 0.0.0.0)",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        default=get_env_value("PORT", 9721, int),
+        help="Server port (default: from env or 9721)",
+    )
+
+    # Directory configuration
+    parser.add_argument(
+        "--working-dir",
+        default=get_env_value("WORKING_DIR", "./rag_storage"),
+        help="Working directory for RAG storage (default: from env or ./rag_storage)",
+    )
+    parser.add_argument(
+        "--input-dir",
+        default=get_env_value("INPUT_DIR", "./inputs"),
+        help="Directory containing input documents (default: from env or ./inputs)",
+    )
+
+    # LLM Model configuration
+    parser.add_argument(
+        "--llm-binding-host",
+        default=get_env_value("LLM_BINDING_HOST", None),
+        help="LLM server host URL. If not provided, defaults based on llm-binding:\n"
+        + "- ollama: http://localhost:11434\n"
+        + "- lollms: http://localhost:9600\n"
+        + "- openai: https://api.openai.com/v1",
+    )
+
+    default_llm_api_key = get_env_value("LLM_BINDING_API_KEY", None)
+
+    parser.add_argument(
+        "--llm-binding-api-key",
+        default=default_llm_api_key,
+        help="llm server API key (default: from env or empty string)",
+    )
+
+    parser.add_argument(
+        "--llm-model",
+        default=get_env_value("LLM_MODEL", "mistral-nemo:latest"),
+        help="LLM model name (default: from env or mistral-nemo:latest)",
+    )
+
+    # Embedding model configuration
+    parser.add_argument(
+        "--embedding-binding-host",
+        default=get_env_value("EMBEDDING_BINDING_HOST", None),
+        help="Embedding server host URL. If not provided, defaults based on embedding-binding:\n"
+        + "- ollama: http://localhost:11434\n"
+        + "- lollms: http://localhost:9600\n"
+        + "- openai: https://api.openai.com/v1",
+    )
+
+    default_embedding_api_key = get_env_value("EMBEDDING_BINDING_API_KEY", "")
+    parser.add_argument(
+        "--embedding-binding-api-key",
+        default=default_embedding_api_key,
+        help="embedding server API key (default: from env or empty string)",
+    )
+
+    parser.add_argument(
+        "--embedding-model",
+        default=get_env_value("EMBEDDING_MODEL", "bge-m3:latest"),
+        help="Embedding model name (default: from env or bge-m3:latest)",
+    )
+
+    parser.add_argument(
+        "--chunk_size",
+        default=get_env_value("CHUNK_SIZE", 1200),
+        help="chunk chunk size default 1200",
+    )
+
+    parser.add_argument(
+        "--chunk_overlap_size",
+        default=get_env_value("CHUNK_OVERLAP_SIZE", 100),
+        help="chunk overlap size default 100",
+    )
+
+    def timeout_type(value):
+        if value is None or value == "None":
+            return None
+        return int(value)
+
+    parser.add_argument(
+        "--timeout",
+        default=get_env_value("TIMEOUT", None, timeout_type),
+        type=timeout_type,
+        help="Timeout in seconds (useful when using slow AI). Use None for infinite timeout",
+    )
+
+    # RAG configuration
+    parser.add_argument(
+        "--max-async",
+        type=int,
+        default=get_env_value("MAX_ASYNC", 4, int),
+        help="Maximum async operations (default: from env or 4)",
+    )
+    parser.add_argument(
+        "--max-tokens",
+        type=int,
+        default=get_env_value("MAX_TOKENS", 32768, int),
+        help="Maximum token size (default: from env or 32768)",
+    )
+    parser.add_argument(
+        "--embedding-dim",
+        type=int,
+        default=get_env_value("EMBEDDING_DIM", 1024, int),
+        help="Embedding dimensions (default: from env or 1024)",
+    )
+    parser.add_argument(
+        "--max-embed-tokens",
+        type=int,
+        default=get_env_value("MAX_EMBED_TOKENS", 8192, int),
+        help="Maximum embedding token size (default: from env or 8192)",
+    )
+
+    # Logging configuration
+    parser.add_argument(
+        "--log-level",
+        default=get_env_value("LOG_LEVEL", "INFO"),
+        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+        help="Logging level (default: from env or INFO)",
+    )
+
+    parser.add_argument(
+        "--key",
+        type=str,
+        default=get_env_value("LIGHTRAG_API_KEY", None),
+        help="API key for authentication. This protects minirag server against unauthorized access",
+    )
+
+    # Optional https parameters
+    parser.add_argument(
+        "--ssl",
+        action="store_true",
+        default=get_env_value("SSL", False, bool),
+        help="Enable HTTPS (default: from env or False)",
+    )
+    parser.add_argument(
+        "--ssl-certfile",
+        default=get_env_value("SSL_CERTFILE", None),
+        help="Path to SSL certificate file (required if --ssl is enabled)",
+    )
+    parser.add_argument(
+        "--ssl-keyfile",
+        default=get_env_value("SSL_KEYFILE", None),
+        help="Path to SSL private key file (required if --ssl is enabled)",
+    )
+    parser.add_argument(
+        "--auto-scan-at-startup",
+        action="store_true",
+        default=False,
+        help="Enable automatic scanning when the program starts",
+    )
+
+    parser.add_argument(
+        "--history-turns",
+        type=int,
+        default=get_env_value("HISTORY_TURNS", 3, int),
+        help="Number of conversation history turns to include (default: from env or 3)",
+    )
+
+    # Search parameters
+    parser.add_argument(
+        "--top-k",
+        type=int,
+        default=get_env_value("TOP_K", 50, int),
+        help="Number of most similar results to return (default: from env or 50)",
+    )
+    parser.add_argument(
+        "--cosine-threshold",
+        type=float,
+        default=get_env_value("COSINE_THRESHOLD", 0.4, float),
+        help="Cosine similarity threshold (default: from env or 0.4)",
+    )
+
+    parser.add_argument(
+        "--simulated-model-name",
+        type=str,
+        default=get_env_value(
+            "SIMULATED_MODEL_NAME", ollama_server_infos.LIGHTRAG_MODEL
+        ),
+        help="Number of conversation history turns to include (default: from env or 3)",
+    )
+
+    args = parser.parse_args()
+
+    ollama_server_infos.LIGHTRAG_MODEL = args.simulated_model_name
+
+    return args
+
+
+class DocumentManager:
+    """Handles document operations and tracking"""
+
+    def __init__(
+        self,
+        input_dir: str,
+        supported_extensions: tuple = (".txt", ".md", ".pdf", ".docx", ".pptx"),
+    ):
+        self.input_dir = Path(input_dir)
+        self.supported_extensions = supported_extensions
+        self.indexed_files = set()
+
+        # Create input directory if it doesn't exist
+        self.input_dir.mkdir(parents=True, exist_ok=True)
+
+    def scan_directory_for_new_files(self) -> List[Path]:
+        """Scan input directory for new files"""
+        new_files = []
+        for ext in self.supported_extensions:
+            for file_path in self.input_dir.rglob(f"*{ext}"):
+                if file_path not in self.indexed_files:
+                    new_files.append(file_path)
+        return new_files
+
+    def scan_directory(self) -> List[Path]:
+        """Scan input directory for new files"""
+        new_files = []
+        for ext in self.supported_extensions:
+            for file_path in self.input_dir.rglob(f"*{ext}"):
+                new_files.append(file_path)
+        return new_files
+
+    def mark_as_indexed(self, file_path: Path):
+        """Mark a file as indexed"""
+        self.indexed_files.add(file_path)
+
+    def is_supported_file(self, filename: str) -> bool:
+        """Check if file type is supported"""
+        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)
+
+
+# Pydantic models
+class SearchMode(str, Enum):
+    light = "light"
+    naive = "naive"
+    mini = "mini"
+
+
+class OllamaMessage(BaseModel):
+    role: str
+    content: str
+    images: Optional[List[str]] = None
+
+
+class OllamaChatRequest(BaseModel):
+    model: str = ollama_server_infos.LIGHTRAG_MODEL
+    messages: List[OllamaMessage]
+    stream: bool = True  # Default to streaming mode
+    options: Optional[Dict[str, Any]] = None
+    system: Optional[str] = None
+
+
+class OllamaChatResponse(BaseModel):
+    model: str
+    created_at: str
+    message: OllamaMessage
+    done: bool
+
+
+class OllamaGenerateRequest(BaseModel):
+    model: str = ollama_server_infos.LIGHTRAG_MODEL
+    prompt: str
+    system: Optional[str] = None
+    stream: bool = False
+    options: Optional[Dict[str, Any]] = None
+
+
+class OllamaGenerateResponse(BaseModel):
+    model: str
+    created_at: str
+    response: str
+    done: bool
+    context: Optional[List[int]]
+    total_duration: Optional[int]
+    load_duration: Optional[int]
+    prompt_eval_count: Optional[int]
+    prompt_eval_duration: Optional[int]
+    eval_count: Optional[int]
+    eval_duration: Optional[int]
+
+
+class OllamaVersionResponse(BaseModel):
+    version: str
+
+
+class OllamaModelDetails(BaseModel):
+    parent_model: str
+    format: str
+    family: str
+    families: List[str]
+    parameter_size: str
+    quantization_level: str
+
+
+class OllamaModel(BaseModel):
+    name: str
+    model: str
+    size: int
+    digest: str
+    modified_at: str
+    details: OllamaModelDetails
+
+
+class OllamaTagResponse(BaseModel):
+    models: List[OllamaModel]
+
+
+class QueryRequest(BaseModel):
+    query: str
+    mode: SearchMode = SearchMode.light
+    stream: bool = False
+    only_need_context: bool = False
+
+
+class QueryResponse(BaseModel):
+    response: str
+
+
+class InsertTextRequest(BaseModel):
+    text: str
+    description: Optional[str] = None
+
+
+class InsertResponse(BaseModel):
+    status: str
+    message: str
+    document_count: int
+
+
+def get_api_key_dependency(api_key: Optional[str]):
+    if not api_key:
+        # If no API key is configured, return a dummy dependency that always succeeds
+        async def no_auth():
+            return None
+
+        return no_auth
+
+    # If API key is configured, use proper authentication
+    api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)
+
+    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):
+        if not api_key_header_value:
+            raise HTTPException(
+                status_code=HTTP_403_FORBIDDEN, detail="API Key required"
+            )
+        if api_key_header_value != api_key:
+            raise HTTPException(
+                status_code=HTTP_403_FORBIDDEN, detail="Invalid API Key"
+            )
+        return api_key_header_value
+
+    return api_key_auth
+
+
+def create_app(args):
+    # Verify that bindings are correctly setup
+    if args.llm_binding not in [
+        "lollms",
+        "ollama",
+        "openai",
+        "openai-ollama",
+        "azure_openai",
+    ]:
+        raise Exception("llm binding not supported")
+
+    if args.embedding_binding not in ["lollms", "ollama", "openai", "azure_openai"]:
+        raise Exception("embedding binding not supported")
+
+    # Set default hosts if not provided
+    if args.llm_binding_host is None:
+        args.llm_binding_host = get_default_host(args.llm_binding)
+
+    if args.embedding_binding_host is None:
+        args.embedding_binding_host = get_default_host(args.embedding_binding)
+
+    # Add SSL validation
+    if args.ssl:
+        if not args.ssl_certfile or not args.ssl_keyfile:
+            raise Exception(
+                "SSL certificate and key files must be provided when SSL is enabled"
+            )
+        if not os.path.exists(args.ssl_certfile):
+            raise Exception(f"SSL certificate file not found: {args.ssl_certfile}")
+        if not os.path.exists(args.ssl_keyfile):
+            raise Exception(f"SSL key file not found: {args.ssl_keyfile}")
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(levelname)s:%(message)s", level=getattr(logging, args.log_level)
+    )
+
+    # Check if API key is provided either through env var or args
+    api_key = os.getenv("LIGHTRAG_API_KEY") or args.key
+
+    # Initialize document manager
+    doc_manager = DocumentManager(args.input_dir)
+
+    @asynccontextmanager
+    async def lifespan(app: FastAPI):
+        """Lifespan context manager for startup and shutdown events"""
+        # Startup logic
+        if args.auto_scan_at_startup:
+            try:
+                new_files = doc_manager.scan_directory_for_new_files()
+                for file_path in new_files:
+                    try:
+                        await index_file(file_path)
+                    except Exception as e:
+                        trace_exception(e)
+                        logging.error(f"Error indexing file {file_path}: {str(e)}")
+
+                ASCIIColors.info(
+                    f"Indexed {len(new_files)} documents from {args.input_dir}"
+                )
+            except Exception as e:
+                logging.error(f"Error during startup indexing: {str(e)}")
+        yield
+        # Cleanup logic (if needed)
+        pass
+
+    # Initialize FastAPI
+    app = FastAPI(
+        title="MiniRAG API",
+        description="API for querying text using MiniRAG with separate storage and input directories"
+        + "(With authentication)"
+        if api_key
+        else "",
+        version=__api_version__,
+        openapi_tags=[{"name": "api"}],
+        lifespan=lifespan,
+    )
+
+    # Add CORS middleware
+    app.add_middleware(
+        CORSMiddleware,
+        allow_origins=["*"],
+        allow_credentials=True,
+        allow_methods=["*"],
+        allow_headers=["*"],
+    )
+
+    # Create the optional API key dependency
+    optional_api_key = get_api_key_dependency(api_key)
+
+    # Create working directory if it doesn't exist
+    Path(args.working_dir).mkdir(parents=True, exist_ok=True)
+    if args.llm_binding == "lollms" or args.embedding_binding == "lollms":
+        from minirag.llm.lollms import lollms_model_complete, lollms_embed
+    if args.llm_binding == "ollama" or args.embedding_binding == "ollama":
+        from minirag.llm.ollama import ollama_model_complete, ollama_embed
+    if args.llm_binding == "openai" or args.embedding_binding == "openai":
+        from minirag.llm.openai import openai_complete_if_cache, openai_embed
+    if args.llm_binding == "azure_openai" or args.embedding_binding == "azure_openai":
+        from minirag.llm.azure_openai import (
+            azure_openai_complete_if_cache,
+            azure_openai_embed,
+        )
+    if args.llm_binding_host == "openai-ollama" or args.embedding_binding == "ollama":
+        from minirag.llm.openai import openai_complete_if_cache
+        from minirag.llm.ollama import ollama_embed
+
+    async def openai_alike_model_complete(
+        prompt,
+        system_prompt=None,
+        history_messages=[],
+        keyword_extraction=False,
+        **kwargs,
+    ) -> str:
+        return await openai_complete_if_cache(
+            args.llm_model,
+            prompt,
+            system_prompt=system_prompt,
+            history_messages=history_messages,
+            base_url=args.llm_binding_host,
+            api_key=args.llm_binding_api_key,
+            **kwargs,
+        )
+
+    async def azure_openai_model_complete(
+        prompt,
+        system_prompt=None,
+        history_messages=[],
+        keyword_extraction=False,
+        **kwargs,
+    ) -> str:
+        return await azure_openai_complete_if_cache(
+            args.llm_model,
+            prompt,
+            system_prompt=system_prompt,
+            history_messages=history_messages,
+            base_url=args.llm_binding_host,
+            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
+            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview"),
+            **kwargs,
+        )
+
+    embedding_func = EmbeddingFunc(
+        embedding_dim=args.embedding_dim,
+        max_token_size=args.max_embed_tokens,
+        func=lambda texts: lollms_embed(
+            texts,
+            embed_model=args.embedding_model,
+            host=args.embedding_binding_host,
+            api_key=args.embedding_binding_api_key,
+        )
+        if args.embedding_binding == "lollms"
+        else ollama_embed(
+            texts,
+            embed_model=args.embedding_model,
+            host=args.embedding_binding_host,
+            api_key=args.embedding_binding_api_key,
+        )
+        if args.embedding_binding == "ollama"
+        else azure_openai_embed(
+            texts,
+            model=args.embedding_model,  # no host is used for openai,
+            api_key=args.embedding_binding_api_key,
+        )
+        if args.embedding_binding == "azure_openai"
+        else openai_embed(
+            texts,
+            model=args.embedding_model,  # no host is used for openai,
+            api_key=args.embedding_binding_api_key,
+        ),
+    )
+
+    # Initialize RAG
+    if args.llm_binding in ["lollms", "ollama", "openai-ollama"]:
+        rag = MiniRAG(
+            working_dir=args.working_dir,
+            llm_model_func=lollms_model_complete
+            if args.llm_binding == "lollms"
+            else ollama_model_complete
+            if args.llm_binding == "ollama"
+            else openai_alike_model_complete,
+            llm_model_name=args.llm_model,
+            llm_model_max_async=args.max_async,
+            llm_model_max_token_size=args.max_tokens,
+            chunk_token_size=int(args.chunk_size),
+            chunk_overlap_token_size=int(args.chunk_overlap_size),
+            llm_model_kwargs={
+                "host": args.llm_binding_host,
+                "timeout": args.timeout,
+                "options": {"num_ctx": args.max_tokens},
+                "api_key": args.llm_binding_api_key,
+            }
+            if args.llm_binding == "lollms" or args.llm_binding == "ollama"
+            else {},
+            embedding_func=embedding_func,
+            kv_storage=ollama_server_infos.KV_STORAGE,
+            graph_storage=ollama_server_infos.GRAPH_STORAGE,
+            vector_storage=ollama_server_infos.VECTOR_STORAGE,
+            doc_status_storage=ollama_server_infos.DOC_STATUS_STORAGE,
+            vector_db_storage_cls_kwargs={
+                "cosine_better_than_threshold": args.cosine_threshold
+            },
+        )
+    else:
+        rag = MiniRAG(
+            working_dir=args.working_dir,
+            llm_model_func=azure_openai_model_complete
+            if args.llm_binding == "azure_openai"
+            else openai_alike_model_complete,
+            chunk_token_size=int(args.chunk_size),
+            chunk_overlap_token_size=int(args.chunk_overlap_size),
+            llm_model_kwargs={
+                "timeout": args.timeout,
+            },
+            llm_model_name=args.llm_model,
+            llm_model_max_async=args.max_async,
+            llm_model_max_token_size=args.max_tokens,
+            embedding_func=embedding_func,
+            kv_storage=ollama_server_infos.KV_STORAGE,
+            graph_storage=ollama_server_infos.GRAPH_STORAGE,
+            vector_storage=ollama_server_infos.VECTOR_STORAGE,
+            doc_status_storage=ollama_server_infos.DOC_STATUS_STORAGE,
+            vector_db_storage_cls_kwargs={
+                "cosine_better_than_threshold": args.cosine_threshold
+            },
+        )
+
+    async def index_file(file_path: Union[str, Path]) -> None:
+        """Index all files inside the folder with support for multiple file formats
+
+        Args:
+            file_path: Path to the file to be indexed (str or Path object)
+
+        Raises:
+            ValueError: If file format is not supported
+            FileNotFoundError: If file doesn't exist
+        """
+        if not pm.is_installed("aiofiles"):
+            pm.install("aiofiles")
+
+        # Convert to Path object if string
+        file_path = Path(file_path)
+
+        # Check if file exists
+        if not file_path.exists():
+            raise FileNotFoundError(f"File not found: {file_path}")
+
+        content = ""
+        # Get file extension in lowercase
+        ext = file_path.suffix.lower()
+
+        match ext:
+            case ".txt" | ".md":
+                # Text files handling
+                async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
+                    content = await f.read()
+
+            case ".pdf":
+                if not pm.is_installed("pypdf2"):
+                    pm.install("pypdf2")
+                from PyPDF2 import PdfReader
+
+                # PDF handling
+                reader = PdfReader(str(file_path))
+                content = ""
+                for page in reader.pages:
+                    content += page.extract_text() + "\n"
+
+            case ".docx":
+                if not pm.is_installed("python-docx"):
+                    pm.install("python-docx")
+                from docx import Document
+
+                # Word document handling
+                doc = Document(file_path)
+                content = "\n".join([paragraph.text for paragraph in doc.paragraphs])
+
+            case ".pptx":
+                if not pm.is_installed("pptx"):
+                    pm.install("pptx")
+                from pptx import Presentation  # type: ignore
+
+                # PowerPoint handling
+                prs = Presentation(file_path)
+                content = ""
+                for slide in prs.slides:
+                    for shape in slide.shapes:
+                        if hasattr(shape, "text"):
+                            content += shape.text + "\n"
+
+            case _:
+                raise ValueError(f"Unsupported file format: {ext}")
+
+        # Insert content into RAG system
+        if content:
+            await rag.ainsert(content)
+            doc_manager.mark_as_indexed(file_path)
+            logging.info(f"Successfully indexed file: {file_path}")
+        else:
+            logging.warning(f"No content extracted from file: {file_path}")
+
+    @app.post("/documents/scan", dependencies=[Depends(optional_api_key)])
+    async def scan_for_new_documents():
+        """Trigger the scanning process"""
+        global scan_progress
+
+        try:
+            with progress_lock:
+                if scan_progress["is_scanning"]:
+                    return {"status": "already_scanning"}
+
+                scan_progress["is_scanning"] = True
+                scan_progress["indexed_count"] = 0
+                scan_progress["progress"] = 0
+
+            new_files = doc_manager.scan_directory_for_new_files()
+            scan_progress["total_files"] = len(new_files)
+
+            for file_path in new_files:
+                try:
+                    with progress_lock:
+                        scan_progress["current_file"] = os.path.basename(file_path)
+
+                    await index_file(file_path)
+
+                    with progress_lock:
+                        scan_progress["indexed_count"] += 1
+                        scan_progress["progress"] = (
+                            scan_progress["indexed_count"]
+                            / scan_progress["total_files"]
+                        ) * 100
+
+                except Exception as e:
+                    logging.error(f"Error indexing file {file_path}: {str(e)}")
+
+            return {
+                "status": "success",
+                "indexed_count": scan_progress["indexed_count"],
+                "total_documents": len(doc_manager.indexed_files),
+            }
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
+        finally:
+            with progress_lock:
+                scan_progress["is_scanning"] = False
+
+    @app.get("/documents/scan-progress")
+    async def get_scan_progress():
+        """Get the current scanning progress"""
+        with progress_lock:
+            return scan_progress
+
+    @app.post("/documents/upload", dependencies=[Depends(optional_api_key)])
+    async def upload_to_input_dir(file: UploadFile = File(...)):
+        """
+        Endpoint for uploading a file to the input directory and indexing it.
+
+        This API endpoint accepts a file through an HTTP POST request, checks if the
+        uploaded file is of a supported type, saves it in the specified input directory,
+        indexes it for retrieval, and returns a success status with relevant details.
+
+        Parameters:
+            file (UploadFile): The file to be uploaded. It must have an allowed extension as per
+                               `doc_manager.supported_extensions`.
+
+        Returns:
+            dict: A dictionary containing the upload status ("success"),
+                  a message detailing the operation result, and
+                  the total number of indexed documents.
+
+        Raises:
+            HTTPException: If the file type is not supported, it raises a 400 Bad Request error.
+                           If any other exception occurs during the file handling or indexing,
+                           it raises a 500 Internal Server Error with details about the exception.
+        """
+        try:
+            if not doc_manager.is_supported_file(file.filename):
+                raise HTTPException(
+                    status_code=400,
+                    detail=f"Unsupported file type. Supported types: {doc_manager.supported_extensions}",
+                )
+
+            file_path = doc_manager.input_dir / file.filename
+            with open(file_path, "wb") as buffer:
+                shutil.copyfileobj(file.file, buffer)
+
+            # Immediately index the uploaded file
+            await index_file(file_path)
+
+            return {
+                "status": "success",
+                "message": f"File uploaded and indexed: {file.filename}",
+                "total_documents": len(doc_manager.indexed_files),
+            }
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post(
+        "/query", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]
+    )
+    async def query_text(request: QueryRequest):
+        """
+        Handle a POST request at the /query endpoint to process user queries using RAG capabilities.
+
+        Parameters:
+            request (QueryRequest): A Pydantic model containing the following fields:
+                - query (str): The text of the user's query.
+                - mode (ModeEnum): Optional. Specifies the mode of retrieval augmentation.
+                - stream (bool): Optional. Determines if the response should be streamed.
+                - only_need_context (bool): Optional. If true, returns only the context without further processing.
+
+        Returns:
+            QueryResponse: A Pydantic model containing the result of the query processing.
+                           If a string is returned (e.g., cache hit), it's directly returned.
+                           Otherwise, an async generator may be used to build the response.
+
+        Raises:
+            HTTPException: Raised when an error occurs during the request handling process,
+                           with status code 500 and detail containing the exception message.
+        """
+        try:
+            response = await rag.aquery(
+                request.query,
+                param=QueryParam(
+                    mode=request.mode,
+                    stream=request.stream,
+                    only_need_context=request.only_need_context,
+                    top_k=args.top_k,
+                ),
+            )
+
+            # If response is a string (e.g. cache hit), return directly
+            if isinstance(response, str):
+                return QueryResponse(response=response)
+
+            # If it's an async generator, decide whether to stream based on stream parameter
+            if request.stream:
+                result = ""
+                async for chunk in response:
+                    result += chunk
+                return QueryResponse(response=result)
+            else:
+                result = ""
+                async for chunk in response:
+                    result += chunk
+                return QueryResponse(response=result)
+        except Exception as e:
+            trace_exception(e)
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post("/query/stream", dependencies=[Depends(optional_api_key)])
+    async def query_text_stream(request: QueryRequest):
+        """
+        This endpoint performs a retrieval-augmented generation (RAG) query and streams the response.
+
+        Args:
+            request (QueryRequest): The request object containing the query parameters.
+            optional_api_key (Optional[str], optional): An optional API key for authentication. Defaults to None.
+
+        Returns:
+            StreamingResponse: A streaming response containing the RAG query results.
+        """
+        try:
+            response = await rag.aquery(  # Use aquery instead of query, and add await
+                request.query,
+                param=QueryParam(
+                    mode=request.mode,
+                    stream=True,
+                    only_need_context=request.only_need_context,
+                    top_k=args.top_k,
+                ),
+            )
+
+            from fastapi.responses import StreamingResponse
+
+            async def stream_generator():
+                if isinstance(response, str):
+                    # If it's a string, send it all at once
+                    yield f"{json.dumps({'response': response})}\n"
+                else:
+                    # If it's an async generator, send chunks one by one
+                    try:
+                        async for chunk in response:
+                            if chunk:  # Only send non-empty content
+                                yield f"{json.dumps({'response': chunk})}\n"
+                    except Exception as e:
+                        logging.error(f"Streaming error: {str(e)}")
+                        yield f"{json.dumps({'error': str(e)})}\n"
+
+            return StreamingResponse(
+                stream_generator(),
+                media_type="application/x-ndjson",
+                headers={
+                    "Cache-Control": "no-cache",
+                    "Connection": "keep-alive",
+                    "Content-Type": "application/x-ndjson",
+                    "Access-Control-Allow-Origin": "*",
+                    "Access-Control-Allow-Methods": "POST, OPTIONS",
+                    "Access-Control-Allow-Headers": "Content-Type",
+                    "X-Accel-Buffering": "no",  # Disable Nginx buffering
+                },
+            )
+        except Exception as e:
+            trace_exception(e)
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post(
+        "/documents/text",
+        response_model=InsertResponse,
+        dependencies=[Depends(optional_api_key)],
+    )
+    async def insert_text(request: InsertTextRequest):
+        """
+        Insert text into the Retrieval-Augmented Generation (RAG) system.
+
+        This endpoint allows you to insert text data into the RAG system for later retrieval and use in generating responses.
+
+        Args:
+            request (InsertTextRequest): The request body containing the text to be inserted.
+
+        Returns:
+            InsertResponse: A response object containing the status of the operation, a message, and the number of documents inserted.
+        """
+        try:
+            await rag.ainsert(request.text)
+            return InsertResponse(
+                status="success",
+                message="Text successfully inserted",
+                document_count=1,
+            )
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post(
+        "/documents/file",
+        response_model=InsertResponse,
+        dependencies=[Depends(optional_api_key)],
+    )
+    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):
+        """Insert a file directly into the RAG system
+
+        Args:
+            file: Uploaded file
+            description: Optional description of the file
+
+        Returns:
+            InsertResponse: Status of the insertion operation
+
+        Raises:
+            HTTPException: For unsupported file types or processing errors
+        """
+        try:
+            content = ""
+            # Get file extension in lowercase
+            ext = Path(file.filename).suffix.lower()
+
+            match ext:
+                case ".txt" | ".md":
+                    # Text files handling
+                    text_content = await file.read()
+                    content = text_content.decode("utf-8")
+
+                case ".pdf":
+                    if not pm.is_installed("pypdf2"):
+                        pm.install("pypdf2")
+                    from PyPDF2 import PdfReader
+                    from io import BytesIO
+
+                    # Read PDF from memory
+                    pdf_content = await file.read()
+                    pdf_file = BytesIO(pdf_content)
+                    reader = PdfReader(pdf_file)
+                    content = ""
+                    for page in reader.pages:
+                        content += page.extract_text() + "\n"
+
+                case ".docx":
+                    if not pm.is_installed("python-docx"):
+                        pm.install("python-docx")
+                    from docx import Document
+                    from io import BytesIO
+
+                    # Read DOCX from memory
+                    docx_content = await file.read()
+                    docx_file = BytesIO(docx_content)
+                    doc = Document(docx_file)
+                    content = "\n".join(
+                        [paragraph.text for paragraph in doc.paragraphs]
+                    )
+
+                case ".pptx":
+                    if not pm.is_installed("pptx"):
+                        pm.install("pptx")
+                    from pptx import Presentation  # type: ignore
+                    from io import BytesIO
+
+                    # Read PPTX from memory
+                    pptx_content = await file.read()
+                    pptx_file = BytesIO(pptx_content)
+                    prs = Presentation(pptx_file)
+                    content = ""
+                    for slide in prs.slides:
+                        for shape in slide.shapes:
+                            if hasattr(shape, "text"):
+                                content += shape.text + "\n"
+
+                case _:
+                    raise HTTPException(
+                        status_code=400,
+                        detail=f"Unsupported file type. Supported types: {doc_manager.supported_extensions}",
+                    )
+
+            # Insert content into RAG system
+            if content:
+                # Add description if provided
+                if description:
+                    content = f"{description}\n\n{content}"
+
+                await rag.ainsert(content)
+                logging.info(f"Successfully indexed file: {file.filename}")
+
+                return InsertResponse(
+                    status="success",
+                    message=f"File '{file.filename}' successfully inserted",
+                    document_count=1,
+                )
+            else:
+                raise HTTPException(
+                    status_code=400,
+                    detail="No content could be extracted from the file",
+                )
+
+        except UnicodeDecodeError:
+            raise HTTPException(status_code=400, detail="File encoding not supported")
+        except Exception as e:
+            logging.error(f"Error processing file {file.filename}: {str(e)}")
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post(
+        "/documents/batch",
+        response_model=InsertResponse,
+        dependencies=[Depends(optional_api_key)],
+    )
+    async def insert_batch(files: List[UploadFile] = File(...)):
+        """Process multiple files in batch mode
+
+        Args:
+            files: List of files to process
+
+        Returns:
+            InsertResponse: Status of the batch insertion operation
+
+        Raises:
+            HTTPException: For processing errors
+        """
+        try:
+            inserted_count = 0
+            failed_files = []
+
+            for file in files:
+                try:
+                    content = ""
+                    ext = Path(file.filename).suffix.lower()
+
+                    match ext:
+                        case ".txt" | ".md":
+                            text_content = await file.read()
+                            content = text_content.decode("utf-8")
+
+                        case ".pdf":
+                            if not pm.is_installed("pypdf2"):
+                                pm.install("pypdf2")
+                            from PyPDF2 import PdfReader
+                            from io import BytesIO
+
+                            pdf_content = await file.read()
+                            pdf_file = BytesIO(pdf_content)
+                            reader = PdfReader(pdf_file)
+                            for page in reader.pages:
+                                content += page.extract_text() + "\n"
+
+                        case ".docx":
+                            if not pm.is_installed("docx"):
+                                pm.install("docx")
+                            from docx import Document
+                            from io import BytesIO
+
+                            docx_content = await file.read()
+                            docx_file = BytesIO(docx_content)
+                            doc = Document(docx_file)
+                            content = "\n".join(
+                                [paragraph.text for paragraph in doc.paragraphs]
+                            )
+
+                        case ".pptx":
+                            if not pm.is_installed("pptx"):
+                                pm.install("pptx")
+                            from pptx import Presentation  # type: ignore
+                            from io import BytesIO
+
+                            pptx_content = await file.read()
+                            pptx_file = BytesIO(pptx_content)
+                            prs = Presentation(pptx_file)
+                            for slide in prs.slides:
+                                for shape in slide.shapes:
+                                    if hasattr(shape, "text"):
+                                        content += shape.text + "\n"
+
+                        case _:
+                            failed_files.append(f"{file.filename} (unsupported type)")
+                            continue
+
+                    if content:
+                        await rag.ainsert(content)
+                        inserted_count += 1
+                        logging.info(f"Successfully indexed file: {file.filename}")
+                    else:
+                        failed_files.append(f"{file.filename} (no content extracted)")
+
+                except UnicodeDecodeError:
+                    failed_files.append(f"{file.filename} (encoding error)")
+                except Exception as e:
+                    failed_files.append(f"{file.filename} ({str(e)})")
+                    logging.error(f"Error processing file {file.filename}: {str(e)}")
+
+            # Prepare status message
+            if inserted_count == len(files):
+                status = "success"
+                status_message = f"Successfully inserted all {inserted_count} documents"
+            elif inserted_count > 0:
+                status = "partial_success"
+                status_message = f"Successfully inserted {inserted_count} out of {len(files)} documents"
+                if failed_files:
+                    status_message += f". Failed files: {', '.join(failed_files)}"
+            else:
+                status = "failure"
+                status_message = "No documents were successfully inserted"
+                if failed_files:
+                    status_message += f". Failed files: {', '.join(failed_files)}"
+
+            return InsertResponse(
+                status=status,
+                message=status_message,
+                document_count=inserted_count,
+            )
+
+        except Exception as e:
+            logging.error(f"Batch processing error: {str(e)}")
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.delete(
+        "/documents",
+        response_model=InsertResponse,
+        dependencies=[Depends(optional_api_key)],
+    )
+    async def clear_documents():
+        """
+        Clear all documents from the MiniRAG system.
+
+        This endpoint deletes all text chunks, entities vector database, and relationships vector database,
+        effectively clearing all documents from the MiniRAG system.
+
+        Returns:
+            InsertResponse: A response object containing the status, message, and the new document count (0 in this case).
+        """
+        try:
+            rag.text_chunks = []
+            rag.entities_vdb = None
+            rag.relationships_vdb = None
+            return InsertResponse(
+                status="success",
+                message="All documents cleared successfully",
+                document_count=0,
+            )
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
+
+    # query all graph labels
+    @app.get("/graph/label/list")
+    async def get_graph_labels():
+        return await rag.get_graph_labels()
+
+    # query all graph
+    @app.get("/graphs")
+    async def get_graphs(label: str):
+        return await rag.get_graps(nodel_label=label, max_depth=100)
+
+    # Ollama compatible API endpoints
+    # -------------------------------------------------
+    @app.get("/api/version")
+    async def get_version():
+        """Get Ollama version information"""
+        return OllamaVersionResponse(version="0.5.4")
+
+    @app.get("/api/tags")
+    async def get_tags():
+        """Get available models"""
+        return OllamaTagResponse(
+            models=[
+                {
+                    "name": ollama_server_infos.LIGHTRAG_MODEL,
+                    "model": ollama_server_infos.LIGHTRAG_MODEL,
+                    "size": ollama_server_infos.LIGHTRAG_SIZE,
+                    "digest": ollama_server_infos.LIGHTRAG_DIGEST,
+                    "modified_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                    "details": {
+                        "parent_model": "",
+                        "format": "gguf",
+                        "family": ollama_server_infos.LIGHTRAG_NAME,
+                        "families": [ollama_server_infos.LIGHTRAG_NAME],
+                        "parameter_size": "13B",
+                        "quantization_level": "Q4_0",
+                    },
+                }
+            ]
+        )
+
+    def parse_query_mode(query: str) -> tuple[str, SearchMode]:
+        """Parse query prefix to determine search mode
+        Returns tuple of (cleaned_query, search_mode)
+        """
+        mode_map = {
+            "/light ": SearchMode.light,
+            "/naive ": SearchMode.naive,
+            "/mini ": SearchMode.mini,
+        }
+
+        for prefix, mode in mode_map.items():
+            if query.startswith(prefix):
+                # After removing prefix an leading spaces
+                cleaned_query = query[len(prefix) :].lstrip()
+                return cleaned_query, mode
+
+        return query, SearchMode.hybrid
+
+    @app.post("/api/generate")
+    async def generate(raw_request: Request, request: OllamaGenerateRequest):
+        """Handle generate completion requests
+        For compatiblity purpuse, the request is not processed by MiniRAG,
+        and will be handled by underlying LLM model.
+        """
+        try:
+            query = request.prompt
+            start_time = time.time_ns()
+            prompt_tokens = estimate_tokens(query)
+
+            if request.system:
+                rag.llm_model_kwargs["system_prompt"] = request.system
+
+            if request.stream:
+                from fastapi.responses import StreamingResponse
+
+                response = await rag.llm_model_func(
+                    query, stream=True, **rag.llm_model_kwargs
+                )
+
+                async def stream_generator():
+                    try:
+                        first_chunk_time = None
+                        last_chunk_time = None
+                        total_response = ""
+
+                        # Ensure response is an async generator
+                        if isinstance(response, str):
+                            # If it's a string, send in two parts
+                            first_chunk_time = time.time_ns()
+                            last_chunk_time = first_chunk_time
+                            total_response = response
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "response": response,
+                                "done": False,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+
+                            completion_tokens = estimate_tokens(total_response)
+                            total_time = last_chunk_time - start_time
+                            prompt_eval_time = first_chunk_time - start_time
+                            eval_time = last_chunk_time - first_chunk_time
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "done": True,
+                                "total_duration": total_time,
+                                "load_duration": 0,
+                                "prompt_eval_count": prompt_tokens,
+                                "prompt_eval_duration": prompt_eval_time,
+                                "eval_count": completion_tokens,
+                                "eval_duration": eval_time,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+                        else:
+                            async for chunk in response:
+                                if chunk:
+                                    if first_chunk_time is None:
+                                        first_chunk_time = time.time_ns()
+
+                                    last_chunk_time = time.time_ns()
+
+                                    total_response += chunk
+                                    data = {
+                                        "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                        "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                        "response": chunk,
+                                        "done": False,
+                                    }
+                                    yield f"{json.dumps(data, ensure_ascii=False)}\n"
+
+                            completion_tokens = estimate_tokens(total_response)
+                            total_time = last_chunk_time - start_time
+                            prompt_eval_time = first_chunk_time - start_time
+                            eval_time = last_chunk_time - first_chunk_time
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "done": True,
+                                "total_duration": total_time,
+                                "load_duration": 0,
+                                "prompt_eval_count": prompt_tokens,
+                                "prompt_eval_duration": prompt_eval_time,
+                                "eval_count": completion_tokens,
+                                "eval_duration": eval_time,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+                            return
+
+                    except Exception as e:
+                        logging.error(f"Error in stream_generator: {str(e)}")
+                        raise
+
+                return StreamingResponse(
+                    stream_generator(),
+                    media_type="application/x-ndjson",
+                    headers={
+                        "Cache-Control": "no-cache",
+                        "Connection": "keep-alive",
+                        "Content-Type": "application/x-ndjson",
+                        "Access-Control-Allow-Origin": "*",
+                        "Access-Control-Allow-Methods": "POST, OPTIONS",
+                        "Access-Control-Allow-Headers": "Content-Type",
+                    },
+                )
+            else:
+                first_chunk_time = time.time_ns()
+                response_text = await rag.llm_model_func(
+                    query, stream=False, **rag.llm_model_kwargs
+                )
+                last_chunk_time = time.time_ns()
+
+                if not response_text:
+                    response_text = "No response generated"
+
+                completion_tokens = estimate_tokens(str(response_text))
+                total_time = last_chunk_time - start_time
+                prompt_eval_time = first_chunk_time - start_time
+                eval_time = last_chunk_time - first_chunk_time
+
+                return {
+                    "model": ollama_server_infos.LIGHTRAG_MODEL,
+                    "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                    "response": str(response_text),
+                    "done": True,
+                    "total_duration": total_time,
+                    "load_duration": 0,
+                    "prompt_eval_count": prompt_tokens,
+                    "prompt_eval_duration": prompt_eval_time,
+                    "eval_count": completion_tokens,
+                    "eval_duration": eval_time,
+                }
+        except Exception as e:
+            trace_exception(e)
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.post("/api/chat")
+    async def chat(raw_request: Request, request: OllamaChatRequest):
+        """Process chat completion requests.
+        Routes user queries through MiniRAG by selecting query mode based on prefix indicators.
+        Detects and forwards OpenWebUI session-related requests (for meta data generation task) directly to LLM.
+        """
+        try:
+            # Get all messages
+            messages = request.messages
+            if not messages:
+                raise HTTPException(status_code=400, detail="No messages provided")
+
+            # Get the last message as query and previous messages as history
+            query = messages[-1].content
+            # Convert OllamaMessage objects to dictionaries
+            conversation_history = [
+                {"role": msg.role, "content": msg.content} for msg in messages[:-1]
+            ]
+
+            # Check for query prefix
+            cleaned_query, mode = parse_query_mode(query)
+
+            start_time = time.time_ns()
+            prompt_tokens = estimate_tokens(cleaned_query)
+
+            param_dict = {
+                "mode": mode,
+                "stream": request.stream,
+                "only_need_context": False,
+                "conversation_history": conversation_history,
+                "top_k": args.top_k,
+            }
+
+            if args.history_turns is not None:
+                param_dict["history_turns"] = args.history_turns
+
+            query_param = QueryParam(**param_dict)
+
+            if request.stream:
+                from fastapi.responses import StreamingResponse
+
+                response = await rag.aquery(  # Need await to get async generator
+                    cleaned_query, param=query_param
+                )
+
+                async def stream_generator():
+                    try:
+                        first_chunk_time = None
+                        last_chunk_time = None
+                        total_response = ""
+
+                        # Ensure response is an async generator
+                        if isinstance(response, str):
+                            # If it's a string, send in two parts
+                            first_chunk_time = time.time_ns()
+                            last_chunk_time = first_chunk_time
+                            total_response = response
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "message": {
+                                    "role": "assistant",
+                                    "content": response,
+                                    "images": None,
+                                },
+                                "done": False,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+
+                            completion_tokens = estimate_tokens(total_response)
+                            total_time = last_chunk_time - start_time
+                            prompt_eval_time = first_chunk_time - start_time
+                            eval_time = last_chunk_time - first_chunk_time
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "done": True,
+                                "total_duration": total_time,
+                                "load_duration": 0,
+                                "prompt_eval_count": prompt_tokens,
+                                "prompt_eval_duration": prompt_eval_time,
+                                "eval_count": completion_tokens,
+                                "eval_duration": eval_time,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+                        else:
+                            async for chunk in response:
+                                if chunk:
+                                    if first_chunk_time is None:
+                                        first_chunk_time = time.time_ns()
+
+                                    last_chunk_time = time.time_ns()
+
+                                    total_response += chunk
+                                    data = {
+                                        "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                        "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                        "message": {
+                                            "role": "assistant",
+                                            "content": chunk,
+                                            "images": None,
+                                        },
+                                        "done": False,
+                                    }
+                                    yield f"{json.dumps(data, ensure_ascii=False)}\n"
+
+                            completion_tokens = estimate_tokens(total_response)
+                            total_time = last_chunk_time - start_time
+                            prompt_eval_time = first_chunk_time - start_time
+                            eval_time = last_chunk_time - first_chunk_time
+
+                            data = {
+                                "model": ollama_server_infos.LIGHTRAG_MODEL,
+                                "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                                "done": True,
+                                "total_duration": total_time,
+                                "load_duration": 0,
+                                "prompt_eval_count": prompt_tokens,
+                                "prompt_eval_duration": prompt_eval_time,
+                                "eval_count": completion_tokens,
+                                "eval_duration": eval_time,
+                            }
+                            yield f"{json.dumps(data, ensure_ascii=False)}\n"
+                            return  # Ensure the generator ends immediately after sending the completion marker
+                    except Exception as e:
+                        logging.error(f"Error in stream_generator: {str(e)}")
+                        raise
+
+                return StreamingResponse(
+                    stream_generator(),
+                    media_type="application/x-ndjson",
+                    headers={
+                        "Cache-Control": "no-cache",
+                        "Connection": "keep-alive",
+                        "Content-Type": "application/x-ndjson",
+                        "Access-Control-Allow-Origin": "*",
+                        "Access-Control-Allow-Methods": "POST, OPTIONS",
+                        "Access-Control-Allow-Headers": "Content-Type",
+                    },
+                )
+            else:
+                first_chunk_time = time.time_ns()
+
+                # Determine if the request is from Open WebUI's session title and session keyword generation task
+                match_result = re.search(
+                    r"\n<chat_history>\nUSER:", cleaned_query, re.MULTILINE
+                )
+                if match_result:
+                    if request.system:
+                        rag.llm_model_kwargs["system_prompt"] = request.system
+
+                    response_text = await rag.llm_model_func(
+                        cleaned_query, stream=False, **rag.llm_model_kwargs
+                    )
+                else:
+                    response_text = await rag.aquery(cleaned_query, param=query_param)
+
+                last_chunk_time = time.time_ns()
+
+                if not response_text:
+                    response_text = "No response generated"
+
+                completion_tokens = estimate_tokens(str(response_text))
+                total_time = last_chunk_time - start_time
+                prompt_eval_time = first_chunk_time - start_time
+                eval_time = last_chunk_time - first_chunk_time
+
+                return {
+                    "model": ollama_server_infos.LIGHTRAG_MODEL,
+                    "created_at": ollama_server_infos.LIGHTRAG_CREATED_AT,
+                    "message": {
+                        "role": "assistant",
+                        "content": str(response_text),
+                        "images": None,
+                    },
+                    "done": True,
+                    "total_duration": total_time,
+                    "load_duration": 0,
+                    "prompt_eval_count": prompt_tokens,
+                    "prompt_eval_duration": prompt_eval_time,
+                    "eval_count": completion_tokens,
+                    "eval_duration": eval_time,
+                }
+        except Exception as e:
+            trace_exception(e)
+            raise HTTPException(status_code=500, detail=str(e))
+
+    @app.get("/documents", dependencies=[Depends(optional_api_key)])
+    async def documents():
+        """Get current system status"""
+        return doc_manager.indexed_files
+
+    @app.get("/health", dependencies=[Depends(optional_api_key)])
+    async def get_status():
+        """Get current system status"""
+        files = doc_manager.scan_directory()
+        return {
+            "status": "healthy",
+            "working_directory": str(args.working_dir),
+            "input_directory": str(args.input_dir),
+            "indexed_files": [str(f) for f in files],
+            "indexed_files_count": len(files),
+            "configuration": {
+                # LLM configuration binding/host address (if applicable)/model (if applicable)
+                "llm_binding": args.llm_binding,
+                "llm_binding_host": args.llm_binding_host,
+                "llm_model": args.llm_model,
+                # embedding model configuration binding/host address (if applicable)/model (if applicable)
+                "embedding_binding": args.embedding_binding,
+                "embedding_binding_host": args.embedding_binding_host,
+                "embedding_model": args.embedding_model,
+                "max_tokens": args.max_tokens,
+                "kv_storage": ollama_server_infos.KV_STORAGE,
+                "doc_status_storage": ollama_server_infos.DOC_STATUS_STORAGE,
+                "graph_storage": ollama_server_infos.GRAPH_STORAGE,
+                "vector_storage": ollama_server_infos.VECTOR_STORAGE,
+            },
+        }
+
+    # webui mount /webui/index.html
+    # app.mount(
+    #     "/webui",
+    #     StaticFiles(
+    #         directory=Path(__file__).resolve().parent / "webui" / "static", html=True
+    #     ),
+    #     name="webui_static",
+    # )
+
+    # Serve the static files
+    static_dir = Path(__file__).parent / "static"
+    static_dir.mkdir(exist_ok=True)
+    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")
+
+    return app
+
+
+def main():
+    args = parse_args()
+    import uvicorn
+
+    app = create_app(args)
+    display_splash_screen(args)
+    uvicorn_config = {
+        "app": app,
+        "host": args.host,
+        "port": args.port,
+    }
+    if args.ssl:
+        uvicorn_config.update(
+            {
+                "ssl_certfile": args.ssl_certfile,
+                "ssl_keyfile": args.ssl_keyfile,
+            }
+        )
+    uvicorn.run(**uvicorn_config)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/test/minirag/minirag/api/requirements.txt b/test/minirag/minirag/api/requirements.txt
new file mode 100755
index 0000000..34bf432
--- /dev/null
+++ b/test/minirag/minirag/api/requirements.txt
@@ -0,0 +1,13 @@
+ascii_colors
+fastapi
+nest_asyncio
+numpy
+pipmaster
+python-dotenv
+python-multipart
+tenacity
+tiktoken
+torch
+tqdm
+uvicorn
+json_repair
diff --git a/test/minirag/minirag/api/static/README.md b/test/minirag/minirag/api/static/README.md
new file mode 100755
index 0000000..a8c6b1f
--- /dev/null
+++ b/test/minirag/minirag/api/static/README.md
@@ -0,0 +1,2 @@
+# LightRag Webui
+A simple webui to interact with the lightrag datalake
diff --git a/test/minirag/minirag/api/static/favicon.ico b/test/minirag/minirag/api/static/favicon.ico
new file mode 100755
index 0000000..3e7fc41
Binary files /dev/null and b/test/minirag/minirag/api/static/favicon.ico differ
diff --git a/test/minirag/minirag/api/static/index.html b/test/minirag/minirag/api/static/index.html
new file mode 100755
index 0000000..43edbf7
--- /dev/null
+++ b/test/minirag/minirag/api/static/index.html
@@ -0,0 +1,104 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>MiniRag Interface</title>
+    <script src="https://cdn.tailwindcss.com"></script>
+    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
+    <style>
+        .fade-in {
+            animation: fadeIn 0.3s ease-in;
+        }
+
+        @keyframes fadeIn {
+            from { opacity: 0; }
+            to { opacity: 1; }
+        }
+
+        .spin {
+            animation: spin 1s linear infinite;
+        }
+
+        @keyframes spin {
+            from { transform: rotate(0deg); }
+            to { transform: rotate(360deg); }
+        }
+
+        .slide-in {
+            animation: slideIn 0.3s ease-out;
+        }
+
+        @keyframes slideIn {
+            from { transform: translateX(-100%); }
+            to { transform: translateX(0); }
+        }
+    </style>
+</head>
+<body class="bg-gray-50">
+    <div class="flex h-screen">
+        <!-- Sidebar -->
+        <div class="w-64 bg-white shadow-lg">
+            <div class="p-4">
+                <h1 class="text-xl font-bold text-gray-800 mb-6">MiniRag</h1>
+                <nav class="space-y-2">
+                    <a href="#" class="nav-item" data-page="file-manager">
+                        <div class="flex items-center p-2 rounded-lg hover:bg-gray-100 transition-colors">
+                            <svg class="w-5 h-5 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5a2 2 0 00-2 2z"/>
+                            </svg>
+                            File Manager
+                        </div>
+                    </a>
+                    <a href="#" class="nav-item" data-page="query">
+                        <div class="flex items-center p-2 rounded-lg hover:bg-gray-100 transition-colors">
+                            <svg class="w-5 h-5 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/>
+                            </svg>
+                            Query Database
+                        </div>
+                    </a>
+                    <a href="#" class="nav-item" data-page="knowledge-graph">
+                        <div class="flex items-center p-2 rounded-lg hover:bg-gray-100 transition-colors">
+                            <svg class="w-5 h-5 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 10V3L4 14h7v7l9-11h-7z"/>
+                            </svg>
+                            Knowledge Graph
+                        </div>
+                    </a>
+                    <a href="#" class="nav-item" data-page="status">
+                        <div class="flex items-center p-2 rounded-lg hover:bg-gray-100 transition-colors">
+                            <svg class="w-5 h-5 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
+                            </svg>
+                            Status
+                        </div>
+                    </a>
+                    <a href="#" class="nav-item" data-page="settings">
+                        <div class="flex items-center p-2 rounded-lg hover:bg-gray-100 transition-colors">
+                            <svg class="w-5 h-5 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z"/>
+                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"/>
+                            </svg>
+                            Settings
+                        </div>
+                    </a>
+                </nav>
+            </div>
+        </div>
+
+        <!-- Main Content -->
+        <div class="flex-1 overflow-auto p-6">
+            <div id="content" class="fade-in"></div>
+        </div>
+
+        <!-- Toast Notification -->
+        <div id="toast" class="fixed bottom-4 right-4 hidden">
+            <div class="bg-gray-800 text-white px-6 py-3 rounded-lg shadow-lg"></div>
+        </div>
+    </div>
+
+    <script src="/js/api.js"></script>
+
+</body>
+</html>
diff --git a/test/minirag/minirag/api/static/js/api.js b/test/minirag/minirag/api/static/js/api.js
new file mode 100755
index 0000000..a6bb1e0
--- /dev/null
+++ b/test/minirag/minirag/api/static/js/api.js
@@ -0,0 +1,404 @@
+// State management
+const state = {
+    apiKey: localStorage.getItem('apiKey') || '',
+    files: [],
+    indexedFiles: [],
+    currentPage: 'file-manager'
+};
+
+// Utility functions
+const showToast = (message, duration = 3000) => {
+    const toast = document.getElementById('toast');
+    toast.querySelector('div').textContent = message;
+    toast.classList.remove('hidden');
+    setTimeout(() => toast.classList.add('hidden'), duration);
+};
+
+const fetchWithAuth = async (url, options = {}) => {
+    const headers = {
+        ...(options.headers || {}),
+        ...(state.apiKey ? { 'Authorization': `Bearer ${state.apiKey}` } : {})
+    };
+    return fetch(url, { ...options, headers });
+};
+
+// Page renderers
+const pages = {
+    'file-manager': () => `
+        <div class="space-y-6">
+            <h2 class="text-2xl font-bold text-gray-800">File Manager</h2>
+
+            <div class="border-2 border-dashed border-gray-300 rounded-lg p-8 text-center hover:border-gray-400 transition-colors">
+                <input type="file" id="fileInput" multiple accept=".txt,.md,.doc,.docx,.pdf,.pptx" class="hidden">
+                <label for="fileInput" class="cursor-pointer">
+                    <svg class="mx-auto h-12 w-12 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12"/>
+                    </svg>
+                    <p class="mt-2 text-gray-600">Drag files here or click to select</p>
+                    <p class="text-sm text-gray-500">Supported formats: TXT, MD, DOC, PDF, PPTX</p>
+                </label>
+            </div>
+
+            <div id="fileList" class="space-y-2">
+                <h3 class="text-lg font-semibold text-gray-700">Selected Files</h3>
+                <div class="space-y-2"></div>
+            </div>
+            <div id="uploadProgress" class="hidden mt-4">
+                <div class="w-full bg-gray-200 rounded-full h-2.5">
+                    <div class="bg-blue-600 h-2.5 rounded-full" style="width: 0%"></div>
+                </div>
+                <p class="text-sm text-gray-600 mt-2"><span id="uploadStatus">0</span> files processed</p>
+            </div>
+            <button id="rescanBtn" class="flex items-center bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">
+                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20" fill="currentColor" class="mr-2">
+                    <path d="M12 4a8 8 0 1 1-8 8H2.5a9.5 9.5 0 1 0 2.8-6.7L2 3v6h6L5.7 6.7A7.96 7.96 0 0 1 12 4z"/>
+                </svg>
+                Rescan Files
+            </button>
+            <button id="uploadBtn" class="bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">
+                Upload & Index Files
+            </button>
+
+            <div id="indexedFiles" class="space-y-2">
+                <h3 class="text-lg font-semibold text-gray-700">Indexed Files</h3>
+                <div class="space-y-2"></div>
+            </div>
+
+
+
+        </div>
+    `,
+
+    'query': () => `
+        <div class="space-y-6">
+            <h2 class="text-2xl font-bold text-gray-800">Query Database</h2>
+
+            <div class="space-y-4">
+                <div>
+                    <label class="block text-sm font-medium text-gray-700">Query Mode</label>
+                    <select id="queryMode" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-blue-500 focus:ring-blue-500">
+                        <option value="light">Light</option>
+                        <option value="naive">Naive</option>
+                        <option value="mini">Mini</option>
+                    </select>
+                </div>
+
+                <div>
+                    <label class="block text-sm font-medium text-gray-700">Query</label>
+                    <textarea id="queryInput" rows="4" class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-blue-500 focus:ring-blue-500"></textarea>
+                </div>
+
+                <button id="queryBtn" class="bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">
+                    Send Query
+                </button>
+
+                <div id="queryResult" class="mt-4 p-4 bg-white rounded-lg shadow"></div>
+            </div>
+        </div>
+    `,
+
+    'knowledge-graph': () => `
+        <div class="flex items-center justify-center h-full">
+            <div class="text-center">
+                <svg class="mx-auto h-12 w-12 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/>
+                </svg>
+                <h3 class="mt-2 text-sm font-medium text-gray-900">Under Construction</h3>
+                <p class="mt-1 text-sm text-gray-500">Knowledge graph visualization will be available in a future update.</p>
+            </div>
+        </div>
+    `,
+
+    'status': () => `
+        <div class="space-y-6">
+            <h2 class="text-2xl font-bold text-gray-800">System Status</h2>
+            <div id="statusContent" class="grid grid-cols-1 md:grid-cols-2 gap-6">
+                <div class="p-6 bg-white rounded-lg shadow-sm">
+                    <h3 class="text-lg font-semibold mb-4">System Health</h3>
+                    <div id="healthStatus"></div>
+                </div>
+                <div class="p-6 bg-white rounded-lg shadow-sm">
+                    <h3 class="text-lg font-semibold mb-4">Configuration</h3>
+                    <div id="configStatus"></div>
+                </div>
+            </div>
+        </div>
+    `,
+
+    'settings': () => `
+        <div class="space-y-6">
+            <h2 class="text-2xl font-bold text-gray-800">Settings</h2>
+
+            <div class="max-w-xl">
+                <div class="space-y-4">
+                    <div>
+                        <label class="block text-sm font-medium text-gray-700">API Key</label>
+                        <input type="password" id="apiKeyInput" value="${state.apiKey}"
+                            class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-blue-500 focus:ring-blue-500">
+                    </div>
+
+                    <button id="saveSettings" class="bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 transition-colors">
+                        Save Settings
+                    </button>
+                </div>
+            </div>
+        </div>
+    `
+};
+
+// Page handlers
+const handlers = {
+    'file-manager': () => {
+        const fileInput = document.getElementById('fileInput');
+        const dropZone = fileInput.parentElement.parentElement;
+        const fileList = document.querySelector('#fileList div');
+        const indexedFiles = document.querySelector('#indexedFiles div');
+        const uploadBtn = document.getElementById('uploadBtn');
+
+        const updateFileList = () => {
+            fileList.innerHTML = state.files.map(file => `
+                <div class="flex items-center justify-between bg-white p-3 rounded-lg shadow-sm">
+                    <span>${file.name}</span>
+                    <button class="text-red-600 hover:text-red-700" onclick="removeFile('${file.name}')">
+                        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/>
+                        </svg>
+                    </button>
+                </div>
+            `).join('');
+        };
+
+        const updateIndexedFiles = async () => {
+            const response = await fetchWithAuth('/health');
+            const data = await response.json();
+            indexedFiles.innerHTML = data.indexed_files.map(file => `
+                <div class="flex items-center justify-between bg-white p-3 rounded-lg shadow-sm">
+                    <span>${file}</span>
+                </div>
+            `).join('');
+        };
+
+        dropZone.addEventListener('dragover', (e) => {
+            e.preventDefault();
+            dropZone.classList.add('border-blue-500');
+        });
+
+        dropZone.addEventListener('dragleave', () => {
+            dropZone.classList.remove('border-blue-500');
+        });
+
+        dropZone.addEventListener('drop', (e) => {
+            e.preventDefault();
+            dropZone.classList.remove('border-blue-500');
+            const files = Array.from(e.dataTransfer.files);
+            state.files.push(...files);
+            updateFileList();
+        });
+
+        fileInput.addEventListener('change', () => {
+            state.files.push(...Array.from(fileInput.files));
+            updateFileList();
+        });
+
+        uploadBtn.addEventListener('click', async () => {
+            if (state.files.length === 0) {
+                showToast('Please select files to upload');
+                return;
+            }
+            let apiKey = localStorage.getItem('apiKey') || '';
+            const progress = document.getElementById('uploadProgress');
+            const progressBar = progress.querySelector('div');
+            const statusText = document.getElementById('uploadStatus');
+            progress.classList.remove('hidden');
+
+            for (let i = 0; i < state.files.length; i++) {
+                const formData = new FormData();
+                formData.append('file', state.files[i]);
+
+                try {
+                    await fetch('/documents/upload', {
+                        method: 'POST',
+                        headers: apiKey ? { 'Authorization': `Bearer ${apiKey}` } : {},
+                        body: formData
+                    });
+
+                    const percentage = ((i + 1) / state.files.length) * 100;
+                    progressBar.style.width = `${percentage}%`;
+                    statusText.textContent = `${i + 1}/${state.files.length}`;
+                } catch (error) {
+                    console.error('Upload error:', error);
+                }
+            }
+            progress.classList.add('hidden');
+        });
+
+        rescanBtn.addEventListener('click', async () => {
+            const progress = document.getElementById('uploadProgress');
+            const progressBar = progress.querySelector('div');
+            const statusText = document.getElementById('uploadStatus');
+            progress.classList.remove('hidden');
+
+            try {
+                // Start the scanning process
+                const scanResponse = await fetch('/documents/scan', {
+                    method: 'POST',
+                });
+
+                if (!scanResponse.ok) {
+                    throw new Error('Scan failed to start');
+                }
+
+                // Start polling for progress
+                const pollInterval = setInterval(async () => {
+                    const progressResponse = await fetch('/documents/scan-progress');
+                    const progressData = await progressResponse.json();
+
+                    // Update progress bar
+                    progressBar.style.width = `${progressData.progress}%`;
+
+                    // Update status text
+                    if (progressData.total_files > 0) {
+                        statusText.textContent = `Processing ${progressData.current_file} (${progressData.indexed_count}/${progressData.total_files})`;
+                    }
+
+                    // Check if scanning is complete
+                    if (!progressData.is_scanning) {
+                        clearInterval(pollInterval);
+                        progress.classList.add('hidden');
+                        statusText.textContent = 'Scan complete!';
+                    }
+                }, 1000); // Poll every second
+
+            } catch (error) {
+                console.error('Upload error:', error);
+                progress.classList.add('hidden');
+                statusText.textContent = 'Error during scanning process';
+            }
+        });
+
+
+        updateIndexedFiles();
+    },
+
+    'query': () => {
+        const queryBtn = document.getElementById('queryBtn');
+        const queryInput = document.getElementById('queryInput');
+        const queryMode = document.getElementById('queryMode');
+        const queryResult = document.getElementById('queryResult');
+
+        let apiKey = localStorage.getItem('apiKey') || '';
+
+        queryBtn.addEventListener('click', async () => {
+            const query = queryInput.value.trim();
+            if (!query) {
+                showToast('Please enter a query');
+                return;
+            }
+
+            queryBtn.disabled = true;
+            queryBtn.innerHTML = `
+                <svg class="animate-spin h-5 w-5 mr-3" viewBox="0 0 24 24">
+                    <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" fill="none"/>
+                    <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"/>
+                </svg>
+                Processing...
+            `;
+
+            try {
+                const response = await fetchWithAuth('/query', {
+                    method: 'POST',
+                    headers: { 'Content-Type': 'application/json' },
+                    body: JSON.stringify({
+                        query,
+                        mode: queryMode.value,
+                        stream: false,
+                        only_need_context: false
+                    })
+                });
+
+                const data = await response.json();
+                queryResult.innerHTML = marked.parse(data.response);
+            } catch (error) {
+                showToast('Error processing query');
+            } finally {
+                queryBtn.disabled = false;
+                queryBtn.textContent = 'Send Query';
+            }
+        });
+    },
+
+    'status': async () => {
+        const healthStatus = document.getElementById('healthStatus');
+        const configStatus = document.getElementById('configStatus');
+
+        try {
+            const response = await fetchWithAuth('/health');
+            const data = await response.json();
+
+            healthStatus.innerHTML = `
+                <div class="space-y-2">
+                    <div class="flex items-center">
+                        <div class="w-3 h-3 rounded-full ${data.status === 'healthy' ? 'bg-green-500' : 'bg-red-500'} mr-2"></div>
+                        <span class="font-medium">${data.status}</span>
+                    </div>
+                    <div>
+                        <p class="text-sm text-gray-600">Working Directory: ${data.working_directory}</p>
+                        <p class="text-sm text-gray-600">Input Directory: ${data.input_directory}</p>
+                        <p class="text-sm text-gray-600">Indexed Files: ${data.indexed_files_count}</p>
+                    </div>
+                </div>
+            `;
+
+            configStatus.innerHTML = Object.entries(data.configuration)
+                .map(([key, value]) => `
+                    <div class="mb-2">
+                        <span class="text-sm font-medium text-gray-700">${key}:</span>
+                        <span class="text-sm text-gray-600 ml-2">${value}</span>
+                    </div>
+                `).join('');
+        } catch (error) {
+            showToast('Error fetching status');
+        }
+    },
+
+    'settings': () => {
+        const saveBtn = document.getElementById('saveSettings');
+        const apiKeyInput = document.getElementById('apiKeyInput');
+
+        saveBtn.addEventListener('click', () => {
+            state.apiKey = apiKeyInput.value;
+            localStorage.setItem('apiKey', state.apiKey);
+            showToast('Settings saved successfully');
+        });
+    }
+};
+
+// Navigation handling
+document.querySelectorAll('.nav-item').forEach(item => {
+    item.addEventListener('click', (e) => {
+        e.preventDefault();
+        const page = item.dataset.page;
+        document.getElementById('content').innerHTML = pages[page]();
+        if (handlers[page]) handlers[page]();
+        state.currentPage = page;
+    });
+});
+
+// Initialize with file manager
+document.getElementById('content').innerHTML = pages['file-manager']();
+handlers['file-manager']();
+
+// Global functions
+window.removeFile = (fileName) => {
+    state.files = state.files.filter(file => file.name !== fileName);
+    document.querySelector('#fileList div').innerHTML = state.files.map(file => `
+        <div class="flex items-center justify-between bg-white p-3 rounded-lg shadow-sm">
+            <span>${file.name}</span>
+            <button class="text-red-600 hover:text-red-700" onclick="removeFile('${file.name}')">
+                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
+                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16"/>
+                </svg>
+            </button>
+        </div>
+    `).join('');
+};
diff --git a/test/minirag/minirag/api/static/js/graph.js b/test/minirag/minirag/api/static/js/graph.js
new file mode 100755
index 0000000..140a778
--- /dev/null
+++ b/test/minirag/minirag/api/static/js/graph.js
@@ -0,0 +1,211 @@
+// js/graph.js
+function openGraphModal(label) {
+    const modal = document.getElementById("graph-modal");
+    const graphTitle = document.getElementById("graph-title");
+
+    if (!modal || !graphTitle) {
+        console.error("Key element not found");
+        return;
+    }
+
+    graphTitle.textContent = `Knowledge Graph - ${label}`;
+    modal.style.display = "flex";
+
+    renderGraph(label);
+}
+
+function closeGraphModal() {
+    const modal = document.getElementById("graph-modal");
+    modal.style.display = "none";
+    clearGraph();
+}
+
+function clearGraph() {
+    const svg = document.getElementById("graph-svg");
+    svg.innerHTML = "";
+}
+
+
+async function getGraph(label) {
+    try {
+        const response = await fetch(`/graphs?label=${label}`);
+        const rawData = await response.json();
+        console.log({data: JSON.parse(JSON.stringify(rawData))});
+
+        const nodes = rawData.nodes
+
+        nodes.forEach(node => {
+            node.id = Date.now().toString(36) + Math.random().toString(36).substring(2); // 使用 crypto.randomUUID() 生成唯一 UUID
+        });
+
+        //  Strictly verify edge data
+        const edges = (rawData.edges || []).map(edge => {
+            const sourceNode = nodes.find(n => n.labels.includes(edge.source));
+            const targetNode = nodes.find(n => n.labels.includes(edge.target)
+                )
+            ;
+            if (!sourceNode || !targetNode) {
+                console.warn("NOT VALID EDGE:", edge);
+                return null;
+            }
+            return {
+                source: sourceNode,
+                target: targetNode,
+                type: edge.type || ""
+            };
+        }).filter(edge => edge !== null);
+
+        return {nodes, edges};
+    } catch (error) {
+        console.error("Loading graph failed:", error);
+        return {nodes: [], edges: []};
+    }
+}
+
+async function renderGraph(label) {
+    const data = await getGraph(label);
+
+
+    if (!data.nodes || data.nodes.length === 0) {
+        d3.select("#graph-svg")
+            .html(`<text x="50%" y="50%" text-anchor="middle">No valid nodes</text>`);
+        return;
+    }
+
+
+    const svg = d3.select("#graph-svg");
+    const width = svg.node().clientWidth;
+    const height = svg.node().clientHeight;
+
+    svg.selectAll("*").remove();
+
+    //  Create a force oriented diagram layout
+    const simulation = d3.forceSimulation(data.nodes)
+        .force("charge", d3.forceManyBody().strength(-300))
+        .force("center", d3.forceCenter(width / 2, height / 2));
+
+    //  Add a connection (if there are valid edges)
+    if (data.edges.length > 0) {
+        simulation.force("link",
+            d3.forceLink(data.edges)
+                .id(d => d.id)
+                .distance(100)
+        );
+    }
+
+    //  Draw nodes
+    const nodes = svg.selectAll(".node")
+        .data(data.nodes)
+        .enter()
+        .append("circle")
+        .attr("class", "node")
+        .attr("r", 10)
+        .call(d3.drag()
+            .on("start", dragStarted)
+            .on("drag", dragged)
+            .on("end", dragEnded)
+        );
+
+
+    svg.append("defs")
+        .append("marker")
+        .attr("id", "arrow-out")
+        .attr("viewBox", "0 0 10 10")
+        .attr("refX", 8)
+        .attr("refY", 5)
+        .attr("markerWidth", 6)
+        .attr("markerHeight", 6)
+        .attr("orient", "auto")
+        .append("path")
+        .attr("d", "M0,0 L10,5 L0,10 Z")
+        .attr("fill", "#999");
+
+    //  Draw edges (with arrows)
+    const links = svg.selectAll(".link")
+        .data(data.edges)
+        .enter()
+        .append("line")
+        .attr("class", "link")
+        .attr("marker-end", "url(#arrow-out)"); //  Always draw arrows on the target side
+
+    //  Edge style configuration
+    links
+        .attr("stroke", "#999")
+        .attr("stroke-width", 2)
+        .attr("stroke-opacity", 0.8);
+
+    //  Draw label (with background box)
+    const labels = svg.selectAll(".label")
+        .data(data.nodes)
+        .enter()
+        .append("text")
+        .attr("class", "label")
+        .text(d => d.labels[0] || "")
+        .attr("text-anchor", "start")
+        .attr("dy", "0.3em")
+        .attr("fill", "#333");
+
+    //  Update Location
+    simulation.on("tick", () => {
+        links
+            .attr("x1", d => {
+                //  Calculate the direction vector from the source node to the target node
+                const dx = d.target.x - d.source.x;
+                const dy = d.target.y - d.source.y;
+                const distance = Math.sqrt(dx * dx + dy * dy);
+                if (distance === 0) return d.source.x; // 避免除以零 Avoid dividing by zero
+                // Adjust the starting point coordinates (source node edge) based on radius 10
+                return d.source.x + (dx / distance) * 10;
+            })
+            .attr("y1", d => {
+                const dx = d.target.x - d.source.x;
+                const dy = d.target.y - d.source.y;
+                const distance = Math.sqrt(dx * dx + dy * dy);
+                if (distance === 0) return d.source.y;
+                return d.source.y + (dy / distance) * 10;
+            })
+            .attr("x2", d => {
+                // Adjust the endpoint coordinates (target node edge) based on a radius of 10
+                const dx = d.target.x - d.source.x;
+                const dy = d.target.y - d.source.y;
+                const distance = Math.sqrt(dx * dx + dy * dy);
+                if (distance === 0) return d.target.x;
+                return d.target.x - (dx / distance) * 10;
+            })
+            .attr("y2", d => {
+                const dx = d.target.x - d.source.x;
+                const dy = d.target.y - d.source.y;
+                const distance = Math.sqrt(dx * dx + dy * dy);
+                if (distance === 0) return d.target.y;
+                return d.target.y - (dy / distance) * 10;
+            });
+
+        // Update the position of nodes and labels (keep unchanged)
+        nodes
+            .attr("cx", d => d.x)
+            .attr("cy", d => d.y);
+
+        labels
+            .attr("x", d => d.x + 12)
+            .attr("y", d => d.y + 4);
+    });
+
+    // Drag and drop logic
+    function dragStarted(event, d) {
+        if (!event.active) simulation.alphaTarget(0.3).restart();
+        d.fx = d.x;
+        d.fy = d.y;
+    }
+
+    function dragged(event, d) {
+        d.fx = event.x;
+        d.fy = event.y;
+        simulation.alpha(0.3).restart();
+    }
+
+    function dragEnded(event, d) {
+        if (!event.active) simulation.alphaTarget(0);
+        d.fx = null;
+        d.fy = null;
+    }
+}
diff --git a/test/minirag/minirag/base.py b/test/minirag/minirag/base.py
new file mode 100755
index 0000000..06497a3
--- /dev/null
+++ b/test/minirag/minirag/base.py
@@ -0,0 +1,197 @@
+from abc import abstractmethod
+from dataclasses import dataclass, field
+from enum import Enum
+from typing import Any, TypedDict, Optional, Union, Literal, Generic, TypeVar
+import os
+import numpy as np
+from .utils import EmbeddingFunc
+
+TextChunkSchema = TypedDict(
+    "TextChunkSchema",
+    {"tokens": int, "content": str, "full_doc_id": str, "chunk_order_index": int},
+)
+
+T = TypeVar("T")
+
+
+@dataclass
+class QueryParam:
+    mode: Literal["light", "naive", "mini"] = "mini"
+    only_need_context: bool = False
+    only_need_prompt: bool = False
+    response_type: str = "Multiple Paragraphs"
+    stream: bool = False
+    # Number of top-k items to retrieve; corresponds to entities in "local" mode and relationships in "global" mode.
+    top_k: int = int(os.getenv("TOP_K", "60"))
+    # Number of document chunks to retrieve.
+    # top_n: int = 10
+    # Number of tokens for the original chunks.
+    max_token_for_text_unit: int = 4000
+    # Number of tokens for the relationship descriptions
+    max_token_for_global_context: int = 4000
+    # Number of tokens for the entity descriptions
+    max_token_for_local_context: int = 4000
+
+    max_token_for_node_context: int = 500#For Mini, if too long, SLM may be fail to generate any response
+
+    hl_keywords: list[str] = field(default_factory=list)
+    ll_keywords: list[str] = field(default_factory=list)
+    # Conversation history support
+    conversation_history: list[dict] = field(
+        default_factory=list
+    )  # Format: [{"role": "user/assistant", "content": "message"}]
+    history_turns: int = (
+        3  # Number of complete conversation turns (user-assistant pairs) to consider
+    )
+
+
+@dataclass
+class StorageNameSpace:
+    namespace: str
+    global_config: dict
+
+    async def index_done_callback(self):
+        """commit the storage operations after indexing"""
+        pass
+
+    async def query_done_callback(self):
+        """commit the storage operations after querying"""
+        pass
+
+
+@dataclass
+class BaseVectorStorage(StorageNameSpace):
+    embedding_func: EmbeddingFunc
+    meta_fields: set = field(default_factory=set)
+
+    async def query(self, query: str, top_k: int) -> list[dict]:
+        raise NotImplementedError
+
+    async def upsert(self, data: dict[str, dict]):
+        """Use 'content' field from value for embedding, use key as id.
+        If embedding_func is None, use 'embedding' field from value
+        """
+        raise NotImplementedError
+
+
+@dataclass
+class BaseKVStorage(Generic[T], StorageNameSpace):
+    embedding_func: EmbeddingFunc
+
+    async def all_keys(self) -> list[str]:
+        raise NotImplementedError
+
+    async def get_by_id(self, id: str) -> Union[T, None]:
+        raise NotImplementedError
+
+    async def get_by_ids(
+        self, ids: list[str], fields: Union[set[str], None] = None
+    ) -> list[Union[T, None]]:
+        raise NotImplementedError
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        """return un-exist keys"""
+        raise NotImplementedError
+
+    async def upsert(self, data: dict[str, T]):
+        raise NotImplementedError
+
+    async def drop(self):
+        raise NotImplementedError
+
+
+@dataclass
+class BaseGraphStorage(StorageNameSpace):
+    embedding_func: EmbeddingFunc = None
+
+    @abstractmethod
+    async def get_types(self) -> tuple[list[str], list[str]]:
+        raise NotImplementedError
+
+    async def has_node(self, node_id: str) -> bool:
+        raise NotImplementedError
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        raise NotImplementedError
+
+    async def node_degree(self, node_id: str) -> int:
+        raise NotImplementedError
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        raise NotImplementedError
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        raise NotImplementedError
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        raise NotImplementedError
+
+    async def get_node_edges(
+        self, source_node_id: str
+    ) -> Union[list[tuple[str, str]], None]:
+        raise NotImplementedError
+
+    async def upsert_node(self, node_id: str, node_data: dict[str, str]):
+        raise NotImplementedError
+
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]
+    ):
+        raise NotImplementedError
+
+    async def delete_node(self, node_id: str):
+        raise NotImplementedError
+
+    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:
+        raise NotImplementedError("Node embedding is not used in minirag.")
+
+
+class DocStatus(str, Enum):
+    """Document processing status enum"""
+
+    PENDING = "pending"
+    PROCESSING = "processing"
+    PROCESSED = "processed"
+    FAILED = "failed"
+
+
+@dataclass
+class DocProcessingStatus:
+    """Document processing status data structure"""
+
+    content: str
+    """Original content of the document"""
+    content_summary: str
+    """First 100 chars of document content, used for preview"""
+    content_length: int
+    """Total length of document"""
+    status: DocStatus
+    """Current processing status"""
+    created_at: str
+    """ISO format timestamp when document was created"""
+    updated_at: str
+    """ISO format timestamp when document was last updated"""
+    chunks_count: Optional[int] = None
+    """Number of chunks after splitting, used for processing"""
+    error: Optional[str] = None
+    """Error message if failed"""
+    metadata: dict[str, Any] = field(default_factory=dict)
+    """Additional metadata"""
+
+
+class DocStatusStorage(BaseKVStorage):
+    """Base class for document status storage"""
+
+    async def get_status_counts(self) -> dict[str, int]:
+        """Get counts of documents in each status"""
+        raise NotImplementedError
+
+    async def get_failed_docs(self) -> dict[str, DocProcessingStatus]:
+        """Get all failed documents"""
+        raise NotImplementedError
+
+    async def get_pending_docs(self) -> dict[str, DocProcessingStatus]:
+        """Get all pending documents"""
+        raise NotImplementedError
diff --git a/test/minirag/minirag/exceptions.py b/test/minirag/minirag/exceptions.py
new file mode 100755
index 0000000..5de6b33
--- /dev/null
+++ b/test/minirag/minirag/exceptions.py
@@ -0,0 +1,58 @@
+import httpx
+from typing import Literal
+
+
+class APIStatusError(Exception):
+    """Raised when an API response has a status code of 4xx or 5xx."""
+
+    response: httpx.Response
+    status_code: int
+    request_id: str | None
+
+    def __init__(
+        self, message: str, *, response: httpx.Response, body: object | None
+    ) -> None:
+        super().__init__(message, response.request, body=body)
+        self.response = response
+        self.status_code = response.status_code
+        self.request_id = response.headers.get("x-request-id")
+
+
+class APIConnectionError(Exception):
+    def __init__(
+        self, *, message: str = "Connection error.", request: httpx.Request
+    ) -> None:
+        super().__init__(message, request, body=None)
+
+
+class BadRequestError(APIStatusError):
+    status_code: Literal[400] = 400  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class AuthenticationError(APIStatusError):
+    status_code: Literal[401] = 401  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class PermissionDeniedError(APIStatusError):
+    status_code: Literal[403] = 403  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class NotFoundError(APIStatusError):
+    status_code: Literal[404] = 404  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class ConflictError(APIStatusError):
+    status_code: Literal[409] = 409  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class UnprocessableEntityError(APIStatusError):
+    status_code: Literal[422] = 422  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class RateLimitError(APIStatusError):
+    status_code: Literal[429] = 429  # pyright: ignore[reportIncompatibleVariableOverride]
+
+
+class APITimeoutError(APIConnectionError):
+    def __init__(self, request: httpx.Request) -> None:
+        super().__init__(message="Request timed out.", request=request)
diff --git a/test/minirag/minirag/kg/__init__.py b/test/minirag/minirag/kg/__init__.py
new file mode 100755
index 0000000..087eaac
--- /dev/null
+++ b/test/minirag/minirag/kg/__init__.py
@@ -0,0 +1 @@
+# print ("init package vars here. ......")
diff --git a/test/minirag/minirag/kg/age_impl.py b/test/minirag/minirag/kg/age_impl.py
new file mode 100755
index 0000000..7fc4c02
--- /dev/null
+++ b/test/minirag/minirag/kg/age_impl.py
@@ -0,0 +1,666 @@
+import asyncio
+import inspect
+import json
+import os
+import sys
+from contextlib import asynccontextmanager
+from dataclasses import dataclass
+from typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union
+import pipmaster as pm
+
+if not pm.is_installed("psycopg-pool"):
+    pm.install("psycopg-pool")
+    pm.install("psycopg[binary,pool]")
+if not pm.is_installed("asyncpg"):
+    pm.install("asyncpg")
+
+import copy
+import psycopg
+from psycopg.rows import namedtuple_row
+from psycopg_pool import AsyncConnectionPool, PoolTimeout
+from tenacity import (
+    retry,
+    retry_if_exception_type,
+    stop_after_attempt,
+    wait_exponential,
+)
+
+from minirag.utils import logger
+from minirag.utils import merge_tuples
+from ..base import BaseGraphStorage
+
+if sys.platform.startswith("win"):
+    import asyncio.windows_events
+
+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
+
+
+class AGEQueryException(Exception):
+    """Exception for the AGE queries."""
+
+    def __init__(self, exception: Union[str, Dict]) -> None:
+        if isinstance(exception, dict):
+            self.message = exception["message"] if "message" in exception else "unknown"
+            self.details = exception["details"] if "details" in exception else "unknown"
+        else:
+            self.message = exception
+            self.details = "unknown"
+
+    def get_message(self) -> str:
+        return self.message
+
+    def get_details(self) -> Any:
+        return self.details
+
+
+@dataclass
+class AGEStorage(BaseGraphStorage):
+    @staticmethod
+    def load_nx_graph(file_name):
+        print("no preloading of graph with AGE in production")
+
+    def __init__(self, namespace, global_config, embedding_func):
+        super().__init__(
+            namespace=namespace,
+            global_config=global_config,
+            embedding_func=embedding_func,
+        )
+        self._driver = None
+        self._driver_lock = asyncio.Lock()
+        DB = os.environ["AGE_POSTGRES_DB"].replace("\\", "\\\\").replace("'", "\\'")
+        USER = os.environ["AGE_POSTGRES_USER"].replace("\\", "\\\\").replace("'", "\\'")
+        PASSWORD = (
+            os.environ["AGE_POSTGRES_PASSWORD"]
+            .replace("\\", "\\\\")
+            .replace("'", "\\'")
+        )
+        HOST = os.environ["AGE_POSTGRES_HOST"].replace("\\", "\\\\").replace("'", "\\'")
+        PORT = int(os.environ["AGE_POSTGRES_PORT"])
+        self.graph_name = os.environ["AGE_GRAPH_NAME"]
+
+        connection_string = f"dbname='{DB}' user='{USER}' password='{PASSWORD}' host='{HOST}' port={PORT}"
+
+        self._driver = AsyncConnectionPool(connection_string, open=False)
+
+        return None
+
+    def __post_init__(self):
+        self._node_embed_algorithms = {
+            "node2vec": self._node2vec_embed,
+        }
+
+    async def close(self):
+        if self._driver:
+            await self._driver.close()
+            self._driver = None
+
+    async def __aexit__(self, exc_type, exc, tb):
+        if self._driver:
+            await self._driver.close()
+
+    async def index_done_callback(self):
+        print("KG successfully indexed.")
+
+    @staticmethod
+    def _record_to_dict(record: NamedTuple) -> Dict[str, Any]:
+        """
+        Convert a record returned from an age query to a dictionary
+
+        Args:
+            record (): a record from an age query result
+
+        Returns:
+            Dict[str, Any]: a dictionary representation of the record where
+                the dictionary key is the field name and the value is the
+                value converted to a python type
+        """
+        # result holder
+        d = {}
+
+        # prebuild a mapping of vertex_id to vertex mappings to be used
+        # later to build edges
+        vertices = {}
+        for k in record._fields:
+            v = getattr(record, k)
+            # agtype comes back '{key: value}::type' which must be parsed
+            if isinstance(v, str) and "::" in v:
+                dtype = v.split("::")[-1]
+                v = v.split("::")[0]
+                if dtype == "vertex":
+                    vertex = json.loads(v)
+                    vertices[vertex["id"]] = vertex.get("properties")
+
+        # iterate returned fields and parse appropriately
+        for k in record._fields:
+            v = getattr(record, k)
+            if isinstance(v, str) and "::" in v:
+                dtype = v.split("::")[-1]
+                v = v.split("::")[0]
+            else:
+                dtype = ""
+
+            if dtype == "vertex":
+                vertex = json.loads(v)
+                field = json.loads(v).get("properties")
+                if not field:
+                    field = {}
+                field["label"] = AGEStorage._decode_graph_label(vertex["label"])
+                d[k] = field
+            # convert edge from id-label->id by replacing id with node information
+            # we only do this if the vertex was also returned in the query
+            # this is an attempt to be consistent with neo4j implementation
+            elif dtype == "edge":
+                edge = json.loads(v)
+                d[k] = (
+                    vertices.get(edge["start_id"], {}),
+                    edge[
+                        "label"
+                    ],  # we don't use decode_graph_label(), since edge label is always "DIRECTED"
+                    vertices.get(edge["end_id"], {}),
+                )
+            else:
+                d[k] = json.loads(v) if isinstance(v, str) else v
+
+        return d
+
+    @staticmethod
+    def _format_properties(
+        properties: Dict[str, Any], _id: Union[str, None] = None
+    ) -> str:
+        """
+        Convert a dictionary of properties to a string representation that
+        can be used in a cypher query insert/merge statement.
+
+        Args:
+            properties (Dict[str,str]): a dictionary containing node/edge properties
+            id (Union[str, None]): the id of the node or None if none exists
+
+        Returns:
+            str: the properties dictionary as a properly formatted string
+        """
+        props = []
+        # wrap property key in backticks to escape
+        for k, v in properties.items():
+            prop = f"`{k}`: {json.dumps(v)}"
+            props.append(prop)
+        if _id is not None and "id" not in properties:
+            props.append(
+                f"id: {json.dumps(_id)}" if isinstance(_id, str) else f"id: {_id}"
+            )
+        return "{" + ", ".join(props) + "}"
+
+    @staticmethod
+    def _encode_graph_label(label: str) -> str:
+        """
+        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string
+
+        Args:
+            label (str): the original label
+
+        Returns:
+            str: the encoded label
+        """
+        return "x" + label.encode().hex()
+
+    @staticmethod
+    def _decode_graph_label(encoded_label: str) -> str:
+        """
+        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string
+
+        Args:
+            encoded_label (str): the encoded label
+
+        Returns:
+            str: the decoded label
+        """
+        return bytes.fromhex(encoded_label.removeprefix("x")).decode()
+
+    @staticmethod
+    def _get_col_name(field: str, idx: int) -> str:
+        """
+        Convert a cypher return field to a pgsql select field
+        If possible keep the cypher column name, but create a generic name if necessary
+
+        Args:
+            field (str): a return field from a cypher query to be formatted for pgsql
+            idx (int): the position of the field in the return statement
+
+        Returns:
+            str: the field to be used in the pgsql select statement
+        """
+        # remove white space
+        field = field.strip()
+        # if an alias is provided for the field, use it
+        if " as " in field:
+            return field.split(" as ")[-1].strip()
+        # if the return value is an unnamed primitive, give it a generic name
+        if field.isnumeric() or field in ("true", "false", "null"):
+            return f"column_{idx}"
+        # otherwise return the value stripping out some common special chars
+        return field.replace("(", "_").replace(")", "")
+
+    @staticmethod
+    def _wrap_query(query: str, graph_name: str, **params: str) -> str:
+        """
+        Convert a cypher query to an Apache Age compatible
+        sql query by wrapping the cypher query in ag_catalog.cypher,
+        casting results to agtype and building a select statement
+
+        Args:
+            query (str): a valid cypher query
+            graph_name (str): the name of the graph to query
+            params (dict): parameters for the query
+
+        Returns:
+            str: an equivalent pgsql query
+        """
+
+        # pgsql template
+        template = """SELECT {projection} FROM ag_catalog.cypher('{graph_name}', $$
+            {query}
+        $$) AS ({fields});"""
+
+        # if there are any returned fields they must be added to the pgsql query
+        if "return" in query.lower():
+            # parse return statement to identify returned fields
+            fields = (
+                query.lower()
+                .split("return")[-1]
+                .split("distinct")[-1]
+                .split("order by")[0]
+                .split("skip")[0]
+                .split("limit")[0]
+                .split(",")
+            )
+
+            # raise exception if RETURN * is found as we can't resolve the fields
+            if "*" in [x.strip() for x in fields]:
+                raise ValueError(
+                    "AGE graph does not support 'RETURN *'"
+                    + " statements in Cypher queries"
+                )
+
+            # get pgsql formatted field names
+            fields = [
+                AGEStorage._get_col_name(field, idx) for idx, field in enumerate(fields)
+            ]
+
+            # build resulting pgsql relation
+            fields_str = ", ".join(
+                [field.split(".")[-1] + " agtype" for field in fields]
+            )
+
+        # if no return statement we still need to return a single field of type agtype
+        else:
+            fields_str = "a agtype"
+
+        select_str = "*"
+
+        return template.format(
+            graph_name=graph_name,
+            query=query.format(**params),
+            fields=fields_str,
+            projection=select_str,
+        )
+
+    async def _query(self, query: str, **params: str) -> List[Dict[str, Any]]:
+        """
+        Query the graph by taking a cypher query, converting it to an
+        age compatible query, executing it and converting the result
+
+        Args:
+            query (str): a cypher query to be executed
+            params (dict): parameters for the query
+
+        Returns:
+            List[Dict[str, Any]]: a list of dictionaries containing the result set
+        """
+        # convert cypher query to pgsql/age query
+        wrapped_query = self._wrap_query(query, self.graph_name, **params)
+
+        await self._driver.open()
+
+        # create graph if it doesn't exist
+        async with self._get_pool_connection() as conn:
+            async with conn.cursor() as curs:
+                try:
+                    await curs.execute('SET search_path = ag_catalog, "$user", public')
+                    await curs.execute(f"SELECT create_graph('{self.graph_name}')")
+                    await conn.commit()
+                except (
+                    psycopg.errors.InvalidSchemaName,
+                    psycopg.errors.UniqueViolation,
+                ):
+                    await conn.rollback()
+
+        # execute the query, rolling back on an error
+        async with self._get_pool_connection() as conn:
+            async with conn.cursor(row_factory=namedtuple_row) as curs:
+                try:
+                    await curs.execute('SET search_path = ag_catalog, "$user", public')
+                    await curs.execute(wrapped_query)
+                    await conn.commit()
+                except psycopg.Error as e:
+                    await conn.rollback()
+                    raise AGEQueryException(
+                        {
+                            "message": f"Error executing graph query: {query.format(**params)}",
+                            "detail": str(e),
+                        }
+                    ) from e
+
+                data = await curs.fetchall()
+                if data is None:
+                    result = []
+                # decode records
+                else:
+                    result = [AGEStorage._record_to_dict(d) for d in data]
+
+                return result
+
+    async def has_node(self, node_id: str) -> bool:
+        entity_name_label = node_id.strip('"')
+
+        query = """
+                MATCH (n:`{label}`) RETURN count(n) > 0 AS node_exists
+                """
+        params = {"label": AGEStorage._encode_graph_label(entity_name_label)}
+        single_result = (await self._query(query, **params))[0]
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query.format(**params),
+            single_result["node_exists"],
+        )
+
+        return single_result["node_exists"]
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        entity_name_label_source = source_node_id.strip('"')
+        entity_name_label_target = target_node_id.strip('"')
+
+        query = """
+                MATCH (a:`{src_label}`)-[r]-(b:`{tgt_label}`)
+                RETURN COUNT(r) > 0 AS edge_exists
+                """
+        params = {
+            "src_label": AGEStorage._encode_graph_label(entity_name_label_source),
+            "tgt_label": AGEStorage._encode_graph_label(entity_name_label_target),
+        }
+        single_result = (await self._query(query, **params))[0]
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query.format(**params),
+            single_result["edge_exists"],
+        )
+        return single_result["edge_exists"]
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        entity_name_label = node_id.strip('"')
+        query = """
+                MATCH (n:`{label}`) RETURN n
+                """
+        params = {"label": AGEStorage._encode_graph_label(entity_name_label)}
+        record = await self._query(query, **params)
+        if record:
+            node = record[0]
+            node_dict = node["n"]
+            logger.debug(
+                "{%s}: query: {%s}, result: {%s}",
+                inspect.currentframe().f_code.co_name,
+                query.format(**params),
+                node_dict,
+            )
+            return node_dict
+        return None
+
+    async def node_degree(self, node_id: str) -> int:
+        entity_name_label = node_id.strip('"')
+
+        query = """
+                MATCH (n:`{label}`)-[]->(x)
+                RETURN count(x) AS total_edge_count
+                """
+        params = {"label": AGEStorage._encode_graph_label(entity_name_label)}
+        record = (await self._query(query, **params))[0]
+        if record:
+            edge_count = int(record["total_edge_count"])
+            logger.debug(
+                "{%s}:query:{%s}:result:{%s}",
+                inspect.currentframe().f_code.co_name,
+                query.format(**params),
+                edge_count,
+            )
+            return edge_count
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        entity_name_label_source = src_id.strip('"')
+        entity_name_label_target = tgt_id.strip('"')
+        src_degree = await self.node_degree(entity_name_label_source)
+        trg_degree = await self.node_degree(entity_name_label_target)
+
+        # Convert None to 0 for addition
+        src_degree = 0 if src_degree is None else src_degree
+        trg_degree = 0 if trg_degree is None else trg_degree
+
+        degrees = int(src_degree) + int(trg_degree)
+        logger.debug(
+            "{%s}:query:src_Degree+trg_degree:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            degrees,
+        )
+        return degrees
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        """
+        Find all edges between nodes of two given labels
+
+        Args:
+            source_node_label (str): Label of the source nodes
+            target_node_label (str): Label of the target nodes
+
+        Returns:
+            list: List of all relationships/edges found
+        """
+        entity_name_label_source = source_node_id.strip('"')
+        entity_name_label_target = target_node_id.strip('"')
+
+        query = """
+                MATCH (a:`{src_label}`)-[r]->(b:`{tgt_label}`)
+                RETURN properties(r) as edge_properties
+                LIMIT 1
+                """
+        params = {
+            "src_label": AGEStorage._encode_graph_label(entity_name_label_source),
+            "tgt_label": AGEStorage._encode_graph_label(entity_name_label_target),
+        }
+        record = await self._query(query, **params)
+        if record and record[0] and record[0]["edge_properties"]:
+            result = record[0]["edge_properties"]
+            logger.debug(
+                "{%s}:query:{%s}:result:{%s}",
+                inspect.currentframe().f_code.co_name,
+                query.format(**params),
+                result,
+            )
+            return result
+
+    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:
+        """
+        Retrieves all edges (relationships) for a particular node identified by its label.
+        :return: List of dictionaries containing edge information
+        """
+        node_label = source_node_id.strip('"')
+
+        query = """
+                MATCH (n:`{label}`)
+                OPTIONAL MATCH (n)-[r]-(connected)
+                RETURN n, r, connected
+                """
+        params = {"label": AGEStorage._encode_graph_label(node_label)}
+        results = await self._query(query, **params)
+        edges = []
+        for record in results:
+            source_node = record["n"] if record["n"] else None
+            connected_node = record["connected"] if record["connected"] else None
+
+            source_label = (
+                source_node["label"] if source_node and source_node["label"] else None
+            )
+            target_label = (
+                connected_node["label"]
+                if connected_node and connected_node["label"]
+                else None
+            )
+
+            if source_label and target_label:
+                edges.append((source_label, target_label))
+
+        return edges
+
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((AGEQueryException,)),
+    )
+    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):
+        """
+        Upsert a node in the AGE database.
+
+        Args:
+            node_id: The unique identifier for the node (used as label)
+            node_data: Dictionary of node properties
+        """
+        label = node_id.strip('"')
+        properties = node_data
+
+        query = """
+                MERGE (n:`{label}`)
+                SET n += {properties}
+                """
+        params = {
+            "label": AGEStorage._encode_graph_label(label),
+            "properties": AGEStorage._format_properties(properties),
+        }
+        try:
+            await self._query(query, **params)
+            logger.debug(
+                "Upserted node with label '{%s}' and properties: {%s}",
+                label,
+                properties,
+            )
+        except Exception as e:
+            logger.error("Error during upsert: {%s}", e)
+            raise
+    async def get_types(self):
+        types = set()
+        types_with_case = set()
+
+        for _, data in self._graph.nodes(data=True):
+            if "type" in data:
+                types.add(data["type"].lower()) 
+                types_with_case.add(data["type"])  
+        return list(types), list(types_with_case)
+
+
+    async def get_node_from_types(self,type_list)  -> Union[dict, None]:
+        node_list = []
+        for name, arrt in self._graph.nodes(data = True):
+            node_type = arrt.get('entity_type').strip('\"')
+            if node_type in type_list:
+                node_list.append(name)
+        node_datas = await asyncio.gather(
+            *[self.get_node(name) for name in node_list]
+        )
+        node_datas = [
+            {**n, "entity_name": k}
+            for k, n in zip(node_list, node_datas)
+            if n is not None
+        ]
+        return node_datas#,node_dict
+    
+
+    async def get_neighbors_within_k_hops(self,source_node_id: str, k):
+        count = 0
+        if await self.has_node(source_node_id):
+            source_edge = list(self._graph.edges(source_node_id))
+        else:
+            print("NO THIS ID:",source_node_id)
+            return []
+        count = count+1
+        while count<k:
+            count = count+1
+            sc_edge = copy.deepcopy(source_edge)
+            source_edge =[]
+            for pair in sc_edge:
+                append_edge = list(self._graph.edges(pair[-1]))
+                for tuples in merge_tuples([pair],append_edge):
+                    source_edge.append(tuples)
+        return source_edge
+    
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((AGEQueryException,)),
+    )
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]
+    ):
+        """
+        Upsert an edge and its properties between two nodes identified by their labels.
+
+        Args:
+            source_node_id (str): Label of the source node (used as identifier)
+            target_node_id (str): Label of the target node (used as identifier)
+            edge_data (dict): Dictionary of properties to set on the edge
+        """
+        source_node_label = source_node_id.strip('"')
+        target_node_label = target_node_id.strip('"')
+        edge_properties = edge_data
+
+        query = """
+                MATCH (source:`{src_label}`)
+                WITH source
+                MATCH (target:`{tgt_label}`)
+                MERGE (source)-[r:DIRECTED]->(target)
+                SET r += {properties}
+                RETURN r
+                """
+        params = {
+            "src_label": AGEStorage._encode_graph_label(source_node_label),
+            "tgt_label": AGEStorage._encode_graph_label(target_node_label),
+            "properties": AGEStorage._format_properties(edge_properties),
+        }
+        try:
+            await self._query(query, **params)
+            logger.debug(
+                "Upserted edge from '{%s}' to '{%s}' with properties: {%s}",
+                source_node_label,
+                target_node_label,
+                edge_properties,
+            )
+        except Exception as e:
+            logger.error("Error during edge upsert: {%s}", e)
+            raise
+
+    async def _node2vec_embed(self):
+        print("Implemented but never called.")
+
+    @asynccontextmanager
+    async def _get_pool_connection(self, timeout: Optional[float] = None):
+        """Workaround for a psycopg_pool bug"""
+
+        try:
+            connection = await self._driver.getconn(timeout=timeout)
+        except PoolTimeout:
+            await self._driver._add_connection(None)  # workaround...
+            connection = await self._driver.getconn(timeout=timeout)
+
+        try:
+            async with connection:
+                yield connection
+        finally:
+            await self._driver.putconn(connection)
diff --git a/test/minirag/minirag/kg/chroma_impl.py b/test/minirag/minirag/kg/chroma_impl.py
new file mode 100755
index 0000000..e3e6de9
--- /dev/null
+++ b/test/minirag/minirag/kg/chroma_impl.py
@@ -0,0 +1,175 @@
+import os
+import asyncio
+from dataclasses import dataclass
+from typing import Union
+import numpy as np
+from chromadb import HttpClient
+from chromadb.config import Settings
+from minirag.base import BaseVectorStorage
+from minirag.utils import logger
+from minirag.utils import merge_tuples
+import copy
+
+
+@dataclass
+class ChromaVectorDBStorage(BaseVectorStorage):
+    """ChromaDB vector storage implementation."""
+
+    cosine_better_than_threshold: float = float(os.getenv("COSINE_THRESHOLD", "0.2"))
+
+    def __post_init__(self):
+        try:
+            # Use global config value if specified, otherwise use default
+            config = self.global_config.get("vector_db_storage_cls_kwargs", {})
+            self.cosine_better_than_threshold = config.get(
+                "cosine_better_than_threshold", self.cosine_better_than_threshold
+            )
+
+            user_collection_settings = config.get("collection_settings", {})
+            # Default HNSW index settings for ChromaDB
+            default_collection_settings = {
+                # Distance metric used for similarity search (cosine similarity)
+                "hnsw:space": "cosine",
+                # Number of nearest neighbors to explore during index construction
+                # Higher values = better recall but slower indexing
+                "hnsw:construction_ef": 128,
+                # Number of nearest neighbors to explore during search
+                # Higher values = better recall but slower search
+                "hnsw:search_ef": 128,
+                # Number of connections per node in the HNSW graph
+                # Higher values = better recall but more memory usage
+                "hnsw:M": 16,
+                # Number of vectors to process in one batch during indexing
+                "hnsw:batch_size": 100,
+                # Number of updates before forcing index synchronization
+                # Lower values = more frequent syncs but slower indexing
+                "hnsw:sync_threshold": 1000,
+            }
+            collection_settings = {
+                **default_collection_settings,
+                **user_collection_settings,
+            }
+
+            auth_provider = config.get(
+                "auth_provider", "chromadb.auth.token_authn.TokenAuthClientProvider"
+            )
+            auth_credentials = config.get("auth_token", "secret-token")
+            headers = {}
+
+            if "token_authn" in auth_provider:
+                headers = {
+                    config.get("auth_header_name", "X-Chroma-Token"): auth_credentials
+                }
+            elif "basic_authn" in auth_provider:
+                auth_credentials = config.get("auth_credentials", "admin:admin")
+
+            self._client = HttpClient(
+                host=config.get("host", "localhost"),
+                port=config.get("port", 8000),
+                headers=headers,
+                settings=Settings(
+                    chroma_api_impl="rest",
+                    chroma_client_auth_provider=auth_provider,
+                    chroma_client_auth_credentials=auth_credentials,
+                    allow_reset=True,
+                    anonymized_telemetry=False,
+                ),
+            )
+
+            self._collection = self._client.get_or_create_collection(
+                name=self.namespace,
+                metadata={
+                    **collection_settings,
+                    "dimension": self.embedding_func.embedding_dim,
+                },
+            )
+            # Use batch size from collection settings if specified
+            self._max_batch_size = self.global_config.get(
+                "embedding_batch_num", collection_settings.get("hnsw:batch_size", 32)
+            )
+        except Exception as e:
+            logger.error(f"ChromaDB initialization failed: {str(e)}")
+            raise
+
+    async def upsert(self, data: dict[str, dict]):
+        if not data:
+            logger.warning("Empty data provided to vector DB")
+            return []
+
+        try:
+            ids = list(data.keys())
+            documents = [v["content"] for v in data.values()]
+            metadatas = [
+                {k: v for k, v in item.items() if k in self.meta_fields}
+                or {"_default": "true"}
+                for item in data.values()
+            ]
+
+            # Process in batches
+            batches = [
+                documents[i : i + self._max_batch_size]
+                for i in range(0, len(documents), self._max_batch_size)
+            ]
+
+            embedding_tasks = [self.embedding_func(batch) for batch in batches]
+            embeddings_list = []
+
+            # Pre-allocate embeddings_list with known size
+            embeddings_list = [None] * len(embedding_tasks)
+
+            # Use asyncio.gather instead of as_completed if order doesn't matter
+            embeddings_results = await asyncio.gather(*embedding_tasks)
+            embeddings_list = list(embeddings_results)
+
+            embeddings = np.concatenate(embeddings_list)
+
+            # Upsert in batches
+            for i in range(0, len(ids), self._max_batch_size):
+                batch_slice = slice(i, i + self._max_batch_size)
+
+                self._collection.upsert(
+                    ids=ids[batch_slice],
+                    embeddings=embeddings[batch_slice].tolist(),
+                    documents=documents[batch_slice],
+                    metadatas=metadatas[batch_slice],
+                )
+
+            return ids
+
+        except Exception as e:
+            logger.error(f"Error during ChromaDB upsert: {str(e)}")
+            raise
+
+    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:
+        try:
+            embedding = await self.embedding_func([query])
+
+            results = self._collection.query(
+                query_embeddings=embedding.tolist(),
+                n_results=top_k * 2,  # Request more results to allow for filtering
+                include=["metadatas", "distances", "documents"],
+            )
+
+            # Filter results by cosine similarity threshold and take top k
+            # We request 2x results initially to have enough after filtering
+            # ChromaDB returns cosine similarity (1 = identical, 0 = orthogonal)
+            # We convert to distance (0 = identical, 1 = orthogonal) via (1 - similarity)
+            # Only keep results with distance below threshold, then take top k
+            return [
+                {
+                    "id": results["ids"][0][i],
+                    "distance": 1 - results["distances"][0][i],
+                    "content": results["documents"][0][i],
+                    **results["metadatas"][0][i],
+                }
+                for i in range(len(results["ids"][0]))
+                if (1 - results["distances"][0][i]) >= self.cosine_better_than_threshold
+            ][:top_k]
+
+        except Exception as e:
+            logger.error(f"Error during ChromaDB query: {str(e)}")
+            raise
+
+    async def index_done_callback(self):
+        # ChromaDB handles persistence automatically
+        pass
diff --git a/test/minirag/minirag/kg/gremlin_impl.py b/test/minirag/minirag/kg/gremlin_impl.py
new file mode 100755
index 0000000..4cbda2a
--- /dev/null
+++ b/test/minirag/minirag/kg/gremlin_impl.py
@@ -0,0 +1,434 @@
+import asyncio
+import inspect
+import json
+import os
+from dataclasses import dataclass
+from typing import Any, Dict, List, Tuple, Union
+
+from gremlin_python.driver import client, serializer
+from gremlin_python.driver.aiohttp.transport import AiohttpTransport
+from gremlin_python.driver.protocol import GremlinServerError
+from tenacity import (
+    retry,
+    retry_if_exception_type,
+    stop_after_attempt,
+    wait_exponential,
+)
+
+from minirag.utils import logger
+import copy
+from minirag.utils import merge_tuples
+from ..base import BaseGraphStorage
+
+
+@dataclass
+class GremlinStorage(BaseGraphStorage):
+    @staticmethod
+    def load_nx_graph(file_name):
+        print("no preloading of graph with Gremlin in production")
+
+    def __init__(self, namespace, global_config, embedding_func):
+        super().__init__(
+            namespace=namespace,
+            global_config=global_config,
+            embedding_func=embedding_func,
+        )
+
+        self._driver = None
+        self._driver_lock = asyncio.Lock()
+
+        USER = os.environ.get("GREMLIN_USER", "")
+        PASSWORD = os.environ.get("GREMLIN_PASSWORD", "")
+        HOST = os.environ["GREMLIN_HOST"]
+        PORT = int(os.environ["GREMLIN_PORT"])
+
+        # TraversalSource, a custom one has to be created manually,
+        # default it "g"
+        SOURCE = os.environ.get("GREMLIN_TRAVERSE_SOURCE", "g")
+
+        # All vertices will have graph={GRAPH} property, so that we can
+        # have several logical graphs for one source
+        GRAPH = GremlinStorage._to_value_map(os.environ["GREMLIN_GRAPH"])
+
+        self.graph_name = GRAPH
+
+        self._driver = client.Client(
+            f"ws://{HOST}:{PORT}/gremlin",
+            SOURCE,
+            username=USER,
+            password=PASSWORD,
+            message_serializer=serializer.GraphSONSerializersV3d0(),
+            transport_factory=lambda: AiohttpTransport(call_from_event_loop=True),
+        )
+
+    def __post_init__(self):
+        self._node_embed_algorithms = {
+            "node2vec": self._node2vec_embed,
+        }
+
+    async def close(self):
+        if self._driver:
+            self._driver.close()
+            self._driver = None
+
+    async def __aexit__(self, exc_type, exc, tb):
+        if self._driver:
+            self._driver.close()
+
+    async def index_done_callback(self):
+        print("KG successfully indexed.")
+
+    @staticmethod
+    def _to_value_map(value: Any) -> str:
+        """Dump supported Python object as Gremlin valueMap"""
+        json_str = json.dumps(value, ensure_ascii=False, sort_keys=False)
+        parsed_str = json_str.replace("'", r"\'")
+
+        # walk over the string and replace curly brackets with square brackets
+        # outside of strings, as well as replace double quotes with single quotes
+        # and "deescape" double quotes inside of strings
+        outside_str = True
+        escaped = False
+        remove_indices = []
+        for i, c in enumerate(parsed_str):
+            if escaped:
+                # previous character was an "odd" backslash
+                escaped = False
+                if c == '"':
+                    # we want to "deescape" double quotes: store indices to delete
+                    remove_indices.insert(0, i - 1)
+            elif c == "\\":
+                escaped = True
+            elif c == '"':
+                outside_str = not outside_str
+                parsed_str = parsed_str[:i] + "'" + parsed_str[i + 1 :]
+            elif c == "{" and outside_str:
+                parsed_str = parsed_str[:i] + "[" + parsed_str[i + 1 :]
+            elif c == "}" and outside_str:
+                parsed_str = parsed_str[:i] + "]" + parsed_str[i + 1 :]
+        for idx in remove_indices:
+            parsed_str = parsed_str[:idx] + parsed_str[idx + 1 :]
+        return parsed_str
+
+    @staticmethod
+    def _convert_properties(properties: Dict[str, Any]) -> str:
+        """Create chained .property() commands from properties dict"""
+        props = []
+        for k, v in properties.items():
+            prop_name = GremlinStorage._to_value_map(k)
+            props.append(f".property({prop_name}, {GremlinStorage._to_value_map(v)})")
+        return "".join(props)
+
+    @staticmethod
+    def _fix_name(name: str) -> str:
+        """Strip double quotes and format as a proper field name"""
+        name = GremlinStorage._to_value_map(name.strip('"').replace(r"\'", "'"))
+
+        return name
+
+    async def _query(self, query: str) -> List[Dict[str, Any]]:
+        """
+        Query the Gremlin graph
+
+        Args:
+            query (str): a query to be executed
+
+        Returns:
+            List[Dict[str, Any]]: a list of dictionaries containing the result set
+        """
+
+        result = list(await asyncio.wrap_future(self._driver.submit_async(query)))
+        if result:
+            result = result[0]
+
+        return result
+
+    async def has_node(self, node_id: str) -> bool:
+        entity_name = GremlinStorage._fix_name(node_id)
+
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name})
+                 .limit(1)
+                 .count()
+                 .project('has_node')
+                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))
+                 """
+        result = await self._query(query)
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query,
+            result[0]["has_node"],
+        )
+
+        return result[0]["has_node"]
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        entity_name_source = GremlinStorage._fix_name(source_node_id)
+        entity_name_target = GremlinStorage._fix_name(target_node_id)
+
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name_source})
+                 .outE()
+                 .inV().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name_target})
+                 .limit(1)
+                 .count()
+                 .project('has_edge')
+                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))
+                 """
+        result = await self._query(query)
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query,
+            result[0]["has_edge"],
+        )
+
+        return result[0]["has_edge"]
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        entity_name = GremlinStorage._fix_name(node_id)
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name})
+                 .limit(1)
+                 .project('properties')
+                    .by(elementMap())
+                 """
+        result = await self._query(query)
+        if result:
+            node = result[0]
+            node_dict = node["properties"]
+            logger.debug(
+                "{%s}: query: {%s}, result: {%s}",
+                inspect.currentframe().f_code.co_name,
+                query.format,
+                node_dict,
+            )
+            return node_dict
+
+    async def node_degree(self, node_id: str) -> int:
+        entity_name = GremlinStorage._fix_name(node_id)
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name})
+                 .outE()
+                 .inV().has('graph', {self.graph_name})
+                 .count()
+                 .project('total_edge_count')
+                    .by()
+                 """
+        result = await self._query(query)
+        edge_count = result[0]["total_edge_count"]
+
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query,
+            edge_count,
+        )
+
+        return edge_count
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        src_degree = await self.node_degree(src_id)
+        trg_degree = await self.node_degree(tgt_id)
+
+        # Convert None to 0 for addition
+        src_degree = 0 if src_degree is None else src_degree
+        trg_degree = 0 if trg_degree is None else trg_degree
+
+        degrees = int(src_degree) + int(trg_degree)
+        logger.debug(
+            "{%s}:query:src_Degree+trg_degree:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            degrees,
+        )
+        return degrees
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        """
+        Find all edges between nodes of two given names
+
+        Args:
+            source_node_id (str): Name of the source nodes
+            target_node_id (str): Name of the target nodes
+
+        Returns:
+            dict|None: Dict of found edge properties, or None if not found
+        """
+        entity_name_source = GremlinStorage._fix_name(source_node_id)
+        entity_name_target = GremlinStorage._fix_name(target_node_id)
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name_source})
+                 .outE()
+                 .inV().has('graph', {self.graph_name})
+                 .has('entity_name', {entity_name_target})
+                 .limit(1)
+                 .project('edge_properties')
+                 .by(__.bothE().elementMap())
+                 """
+        result = await self._query(query)
+        if result:
+            edge_properties = result[0]["edge_properties"]
+            logger.debug(
+                "{%s}:query:{%s}:result:{%s}",
+                inspect.currentframe().f_code.co_name,
+                query,
+                edge_properties,
+            )
+            return edge_properties
+
+    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:
+        """
+        Retrieves all edges (relationships) for a particular node identified by its name.
+        :return: List of tuples containing edge sources and targets
+        """
+        node_name = GremlinStorage._fix_name(source_node_id)
+        query = f"""g
+                 .E()
+                 .filter(
+                     __.or(
+                         __.outV().has('graph', {self.graph_name})
+                           .has('entity_name', {node_name}),
+                         __.inV().has('graph', {self.graph_name})
+                           .has('entity_name', {node_name})
+                     )
+                 )
+                 .project('source_name', 'target_name')
+                 .by(__.outV().values('entity_name'))
+                 .by(__.inV().values('entity_name'))
+                 """
+        result = await self._query(query)
+        edges = [(res["source_name"], res["target_name"]) for res in result]
+
+        return edges
+
+
+    async def get_node_from_types(self,type_list)  -> Union[dict, None]:
+        node_list = []
+        for name, arrt in self._graph.nodes(data = True):
+            node_type = arrt.get('entity_type').strip('\"')
+            if node_type in type_list:
+                node_list.append(name)
+        node_datas = await asyncio.gather(
+            *[self.get_node(name) for name in node_list]
+        )
+        node_datas = [
+            {**n, "entity_name": k}
+            for k, n in zip(node_list, node_datas)
+            if n is not None
+        ]
+        return node_datas#,node_dict
+    
+
+    async def get_neighbors_within_k_hops(self,source_node_id: str, k):
+        count = 0
+        if await self.has_node(source_node_id):
+            source_edge = list(self._graph.edges(source_node_id))
+        else:
+            print("NO THIS ID:",source_node_id)
+            return []
+        count = count+1
+        while count<k:
+            count = count+1
+            sc_edge = copy.deepcopy(source_edge)
+            source_edge =[]
+            for pair in sc_edge:
+                append_edge = list(self._graph.edges(pair[-1]))
+                for tuples in merge_tuples([pair],append_edge):
+                    source_edge.append(tuples)
+        return source_edge
+    
+    @retry(
+        stop=stop_after_attempt(10),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((GremlinServerError,)),
+    )
+    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):
+        """
+        Upsert a node in the Gremlin graph.
+
+        Args:
+            node_id: The unique identifier for the node (used as name)
+            node_data: Dictionary of node properties
+        """
+        name = GremlinStorage._fix_name(node_id)
+        properties = GremlinStorage._convert_properties(node_data)
+
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {name})
+                 .fold()
+                 .coalesce(
+                     __.unfold(),
+                     __.addV('ENTITY')
+                         .property('graph', {self.graph_name})
+                         .property('entity_name', {name})
+                 )
+                 {properties}
+                 """
+
+        try:
+            await self._query(query)
+            logger.debug(
+                "Upserted node with name {%s} and properties: {%s}",
+                name,
+                properties,
+            )
+        except Exception as e:
+            logger.error("Error during upsert: {%s}", e)
+            raise
+
+    @retry(
+        stop=stop_after_attempt(10),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((GremlinServerError,)),
+    )
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]
+    ):
+        """
+        Upsert an edge and its properties between two nodes identified by their names.
+
+        Args:
+            source_node_id (str): Name of the source node (used as identifier)
+            target_node_id (str): Name of the target node (used as identifier)
+            edge_data (dict): Dictionary of properties to set on the edge
+        """
+        source_node_name = GremlinStorage._fix_name(source_node_id)
+        target_node_name = GremlinStorage._fix_name(target_node_id)
+        edge_properties = GremlinStorage._convert_properties(edge_data)
+
+        query = f"""g
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {source_node_name}).as('source')
+                 .V().has('graph', {self.graph_name})
+                 .has('entity_name', {target_node_name}).as('target')
+                 .coalesce(
+                      __.select('source').outE('DIRECTED').where(__.inV().as('target')),
+                      __.select('source').addE('DIRECTED').to(__.select('target'))
+                  )
+                  .property('graph', {self.graph_name})
+                 {edge_properties}
+                 """
+        try:
+            await self._query(query)
+            logger.debug(
+                "Upserted edge from {%s} to {%s} with properties: {%s}",
+                source_node_name,
+                target_node_name,
+                edge_properties,
+            )
+        except Exception as e:
+            logger.error("Error during edge upsert: {%s}", e)
+            raise
+
+    async def _node2vec_embed(self):
+        print("Implemented but never called.")
diff --git a/test/minirag/minirag/kg/json_kv_impl.py b/test/minirag/minirag/kg/json_kv_impl.py
new file mode 100755
index 0000000..2936565
--- /dev/null
+++ b/test/minirag/minirag/kg/json_kv_impl.py
@@ -0,0 +1,134 @@
+"""
+JsonDocStatus Storage Module
+=======================
+
+This module provides a storage interface for graphs using NetworkX, a popular Python library for creating, manipulating, and studying the structure, dynamics, and functions of complex networks.
+
+The `NetworkXStorage` class extends the `BaseGraphStorage` class from the LightRAG library, providing methods to load, save, manipulate, and query graphs using NetworkX.
+
+Author: lightrag team
+Created: 2024-01-25
+License: MIT
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+Version: 1.0.0
+
+Dependencies:
+    - NetworkX
+    - NumPy
+    - LightRAG
+    - graspologic
+
+Features:
+    - Load and save graphs in various formats (e.g., GEXF, GraphML, JSON)
+    - Query graph nodes and edges
+    - Calculate node and edge degrees
+    - Embed nodes using various algorithms (e.g., Node2Vec)
+    - Remove nodes and edges from the graph
+
+Usage:
+    from minirag.storage.networkx_storage import NetworkXStorage
+
+"""
+
+import asyncio
+import os
+from dataclasses import dataclass
+
+from minirag.utils import (
+    logger,
+    load_json,
+    write_json,
+)
+
+from minirag.base import (
+    BaseKVStorage,
+)
+
+
+@dataclass
+class JsonKVStorage(BaseKVStorage):
+    def __post_init__(self):
+        working_dir = self.global_config["working_dir"]
+        self._file_name = os.path.join(working_dir, f"kv_store_{self.namespace}.json")
+        self._data = load_json(self._file_name) or {}
+        self._lock = asyncio.Lock()
+        logger.info(f"Load KV {self.namespace} with {len(self._data)} data")
+
+    async def all_keys(self) -> list[str]:
+        return list(self._data.keys())
+
+    async def index_done_callback(self):
+        write_json(self._data, self._file_name)
+
+    async def get_by_id(self, id):
+        return self._data.get(id, None)
+
+    async def get_by_ids(self, ids, fields=None):
+        if fields is None:
+            return [self._data.get(id, None) for id in ids]
+        return [
+            (
+                {k: v for k, v in self._data[id].items() if k in fields}
+                if self._data.get(id, None)
+                else None
+            )
+            for id in ids
+        ]
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        return set([s for s in data if s not in self._data])
+
+    async def upsert(self, data: dict[str, dict]):
+        left_data = {k: v for k, v in data.items() if k not in self._data}
+        self._data.update(left_data)
+        return left_data
+
+    async def drop(self):
+        self._data = {}
+
+    async def filter(self, filter_func):
+        """Filter key-value pairs based on a filter function
+
+        Args:
+            filter_func: The filter function, which takes a value as an argument and returns a boolean value
+
+        Returns:
+            Dict: Key-value pairs that meet the condition
+        """
+        result = {}
+        async with self._lock:
+            for key, value in self._data.items():
+                if filter_func(value):
+                    result[key] = value
+        return result
+
+    async def delete(self, ids: list[str]):
+        """Delete data with specified IDs
+
+        Args:
+            ids: List of IDs to delete
+        """
+        async with self._lock:
+            for id in ids:
+                if id in self._data:
+                    del self._data[id]
+            await self.index_done_callback()
+            logger.info(f"Successfully deleted {len(ids)} items from {self.namespace}")
diff --git a/test/minirag/minirag/kg/jsondocstatus_impl.py b/test/minirag/minirag/kg/jsondocstatus_impl.py
new file mode 100755
index 0000000..434bbff
--- /dev/null
+++ b/test/minirag/minirag/kg/jsondocstatus_impl.py
@@ -0,0 +1,147 @@
+"""
+JsonDocStatus Storage Module
+=======================
+
+This module provides a storage interface for graphs using NetworkX, a popular Python library for creating, manipulating, and studying the structure, dynamics, and functions of complex networks.
+
+The `NetworkXStorage` class extends the `BaseGraphStorage` class from the LightRAG library, providing methods to load, save, manipulate, and query graphs using NetworkX.
+
+Author: lightrag team
+Created: 2024-01-25
+License: MIT
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+Version: 1.0.0
+
+Dependencies:
+    - NetworkX
+    - NumPy
+    - LightRAG
+    - graspologic
+
+Features:
+    - Load and save graphs in various formats (e.g., GEXF, GraphML, JSON)
+    - Query graph nodes and edges
+    - Calculate node and edge degrees
+    - Embed nodes using various algorithms (e.g., Node2Vec)
+    - Remove nodes and edges from the graph
+
+Usage:
+    from minirag.storage.networkx_storage import NetworkXStorage
+
+"""
+
+import os
+from dataclasses import dataclass
+from typing import Union, Dict
+
+from minirag.utils import (
+    logger,
+    load_json,
+    write_json,
+)
+
+from minirag.base import (
+    DocStatus,
+    DocProcessingStatus,
+    DocStatusStorage,
+)
+
+
+@dataclass
+class JsonDocStatusStorage(DocStatusStorage):
+    """JSON implementation of document status storage"""
+
+    def __post_init__(self):
+        working_dir = self.global_config["working_dir"]
+        self._file_name = os.path.join(working_dir, f"kv_store_{self.namespace}.json")
+        self._data = load_json(self._file_name) or {}
+        logger.info(f"Loaded document status storage with {len(self._data)} records")
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        """Return keys that should be processed (not in storage or not successfully processed)"""
+        return set(
+            [
+                k
+                for k in data
+                if k not in self._data or self._data[k]["status"] != DocStatus.PROCESSED
+            ]
+        )
+
+    async def get_status_counts(self) -> Dict[str, int]:
+        """Get counts of documents in each status"""
+        counts = {status: 0 for status in DocStatus}
+        for doc in self._data.values():
+            counts[doc["status"]] += 1
+        return counts
+
+    async def get_docs_by_status(
+        self, status: DocStatus
+    ) -> dict[str, DocProcessingStatus]:
+        """Get all documents with a specific status"""
+        result = {}
+        for k, v in self._data.items():
+            if v["status"] == status.value:
+                try:
+                    # Make a copy of the data to avoid modifying the original
+                    data = v.copy()
+                    # If content is missing, use content_summary as content
+                    if "content" not in data and "content_summary" in data:
+                        data["content"] = data["content_summary"]
+                    result[k] = DocProcessingStatus(**data)
+                except KeyError as e:
+                    logger.error(f"Missing required field for document {k}: {e}")
+                    continue
+        return result
+
+    async def get_failed_docs(self) -> Dict[str, DocProcessingStatus]:
+        """Get all failed documents"""
+        return {k: v for k, v in self._data.items() if v["status"] == DocStatus.FAILED}
+
+    async def get_pending_docs(self) -> Dict[str, DocProcessingStatus]:
+        """Get all pending documents"""
+        return {k: v for k, v in self._data.items() if v["status"] == DocStatus.PENDING}
+
+    async def index_done_callback(self):
+        """Save data to file after indexing"""
+        write_json(self._data, self._file_name)
+
+    async def upsert(self, data: dict[str, dict]):
+        """Update or insert document status
+
+        Args:
+            data: Dictionary of document IDs and their status data
+        """
+        self._data.update(data)
+        await self.index_done_callback()
+        return data
+
+    async def get_by_id(self, id: str):
+        return self._data.get(id)
+
+    async def get(self, doc_id: str) -> Union[DocProcessingStatus, None]:
+        """Get document status by ID"""
+        return self._data.get(doc_id)
+
+    async def delete(self, doc_ids: list[str]):
+        """Delete document status by IDs"""
+        for doc_id in doc_ids:
+            self._data.pop(doc_id, None)
+        await self.index_done_callback()
diff --git a/test/minirag/minirag/kg/milvus_impl.py b/test/minirag/minirag/kg/milvus_impl.py
new file mode 100755
index 0000000..e8ffb1b
--- /dev/null
+++ b/test/minirag/minirag/kg/milvus_impl.py
@@ -0,0 +1,94 @@
+import asyncio
+import os
+from tqdm.asyncio import tqdm as tqdm_async
+from dataclasses import dataclass
+import numpy as np
+from minirag.utils import logger
+from ..base import BaseVectorStorage
+
+import pipmaster as pm
+
+if not pm.is_installed("pymilvus"):
+    pm.install("pymilvus")
+from pymilvus import MilvusClient
+
+
+@dataclass
+class MilvusVectorDBStorge(BaseVectorStorage):
+    @staticmethod
+    def create_collection_if_not_exist(
+        client: MilvusClient, collection_name: str, **kwargs
+    ):
+        if client.has_collection(collection_name):
+            return
+        client.create_collection(
+            collection_name, max_length=64, id_type="string", **kwargs
+        )
+
+    def __post_init__(self):
+        self._client = MilvusClient(
+            uri=os.environ.get(
+                "MILVUS_URI",
+                os.path.join(self.global_config["working_dir"], "milvus_lite.db"),
+            ),
+            user=os.environ.get("MILVUS_USER", ""),
+            password=os.environ.get("MILVUS_PASSWORD", ""),
+            token=os.environ.get("MILVUS_TOKEN", ""),
+            db_name=os.environ.get("MILVUS_DB_NAME", ""),
+        )
+        self._max_batch_size = self.global_config["embedding_batch_num"]
+        MilvusVectorDBStorge.create_collection_if_not_exist(
+            self._client,
+            self.namespace,
+            dimension=self.embedding_func.embedding_dim,
+        )
+
+    async def upsert(self, data: dict[str, dict]):
+        logger.info(f"Inserting {len(data)} vectors to {self.namespace}")
+        if not len(data):
+            logger.warning("You insert an empty data to vector DB")
+            return []
+        list_data = [
+            {
+                "id": k,
+                **{k1: v1 for k1, v1 in v.items() if k1 in self.meta_fields},
+            }
+            for k, v in data.items()
+        ]
+        contents = [v["content"] for v in data.values()]
+        batches = [
+            contents[i : i + self._max_batch_size]
+            for i in range(0, len(contents), self._max_batch_size)
+        ]
+
+        async def wrapped_task(batch):
+            result = await self.embedding_func(batch)
+            pbar.update(1)
+            return result
+
+        embedding_tasks = [wrapped_task(batch) for batch in batches]
+        pbar = tqdm_async(
+            total=len(embedding_tasks), desc="Generating embeddings", unit="batch"
+        )
+        embeddings_list = await asyncio.gather(*embedding_tasks)
+
+        embeddings = np.concatenate(embeddings_list)
+        for i, d in enumerate(list_data):
+            d["vector"] = embeddings[i]
+        results = self._client.upsert(collection_name=self.namespace, data=list_data)
+        return results
+
+    async def query(self, query, top_k=5):
+        embedding = await self.embedding_func([query])
+        results = self._client.search(
+            collection_name=self.namespace,
+            data=embedding,
+            limit=top_k,
+            output_fields=list(self.meta_fields),
+            search_params={"metric_type": "COSINE", "params": {"radius": 0.2}},
+        )
+        print(results)
+        return [
+            {**dp["entity"], "id": dp["id"], "distance": dp["distance"]}
+            for dp in results[0]
+        ]
diff --git a/test/minirag/minirag/kg/mongo_impl.py b/test/minirag/minirag/kg/mongo_impl.py
new file mode 100755
index 0000000..815edd0
--- /dev/null
+++ b/test/minirag/minirag/kg/mongo_impl.py
@@ -0,0 +1,440 @@
+import os
+from tqdm.asyncio import tqdm as tqdm_async
+from dataclasses import dataclass
+import pipmaster as pm
+import np
+
+if not pm.is_installed("pymongo"):
+    pm.install("pymongo")
+
+from pymongo import MongoClient
+from motor.motor_asyncio import AsyncIOMotorClient
+from typing import Union, List, Tuple
+from minirag.utils import logger
+
+from minirag.base import BaseKVStorage
+from minirag.base import BaseGraphStorage
+
+
+@dataclass
+class MongoKVStorage(BaseKVStorage):
+    def __post_init__(self):
+        client = MongoClient(
+            os.environ.get("MONGO_URI", "mongodb://root:root@localhost:27017/")
+        )
+        database = client.get_database(os.environ.get("MONGO_DATABASE", "LightRAG"))
+        self._data = database.get_collection(self.namespace)
+        logger.info(f"Use MongoDB as KV {self.namespace}")
+
+    async def all_keys(self) -> list[str]:
+        return [x["_id"] for x in self._data.find({}, {"_id": 1})]
+
+    async def get_by_id(self, id):
+        return self._data.find_one({"_id": id})
+
+    async def get_by_ids(self, ids, fields=None):
+        if fields is None:
+            return list(self._data.find({"_id": {"$in": ids}}))
+        return list(
+            self._data.find(
+                {"_id": {"$in": ids}},
+                {field: 1 for field in fields},
+            )
+        )
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        existing_ids = [
+            str(x["_id"]) for x in self._data.find({"_id": {"$in": data}}, {"_id": 1})
+        ]
+        return set([s for s in data if s not in existing_ids])
+
+    async def upsert(self, data: dict[str, dict]):
+        if self.namespace == "llm_response_cache":
+            for mode, items in data.items():
+                for k, v in tqdm_async(items.items(), desc="Upserting"):
+                    key = f"{mode}_{k}"
+                    result = self._data.update_one(
+                        {"_id": key}, {"$setOnInsert": v}, upsert=True
+                    )
+                    if result.upserted_id:
+                        logger.debug(f"\nInserted new document with key: {key}")
+                    data[mode][k]["_id"] = key
+        else:
+            for k, v in tqdm_async(data.items(), desc="Upserting"):
+                self._data.update_one({"_id": k}, {"$set": v}, upsert=True)
+                data[k]["_id"] = k
+        return data
+
+    async def get_by_mode_and_id(self, mode: str, id: str) -> Union[dict, None]:
+        if "llm_response_cache" == self.namespace:
+            res = {}
+            v = self._data.find_one({"_id": mode + "_" + id})
+            if v:
+                res[id] = v
+                logger.debug(f"llm_response_cache find one by:{id}")
+                return res
+            else:
+                return None
+        else:
+            return None
+
+    async def drop(self):
+        """ """
+        pass
+
+
+@dataclass
+class MongoGraphStorage(BaseGraphStorage):
+    """
+    A concrete implementation using MongoDB’s $graphLookup to demonstrate multi-hop queries.
+    """
+
+    def __init__(self, namespace, global_config, embedding_func):
+        super().__init__(
+            namespace=namespace,
+            global_config=global_config,
+            embedding_func=embedding_func,
+        )
+        self.client = AsyncIOMotorClient(
+            os.environ.get("MONGO_URI", "mongodb://root:root@localhost:27017/")
+        )
+        self.db = self.client[os.environ.get("MONGO_DATABASE", "LightRAG")]
+        self.collection = self.db[os.environ.get("MONGO_KG_COLLECTION", "MDB_KG")]
+
+    #
+    # -------------------------------------------------------------------------
+    # HELPER: $graphLookup pipeline
+    # -------------------------------------------------------------------------
+    #
+
+    async def _graph_lookup(
+        self, start_node_id: str, max_depth: int = None
+    ) -> List[dict]:
+        """
+        Performs a $graphLookup starting from 'start_node_id' and returns
+        all reachable documents (including the start node itself).
+
+        Pipeline Explanation:
+        - 1) $match: We match the start node document by _id = start_node_id.
+        - 2) $graphLookup:
+            "from": same collection,
+            "startWith": "$edges.target" (the immediate neighbors in 'edges'),
+            "connectFromField": "edges.target",
+            "connectToField": "_id",
+            "as": "reachableNodes",
+            "maxDepth": max_depth (if provided),
+            "depthField": "depth" (used for debugging or filtering).
+        - 3) We add an $project or $unwind as needed to extract data.
+        """
+        pipeline = [
+            {"$match": {"_id": start_node_id}},
+            {
+                "$graphLookup": {
+                    "from": self.collection.name,
+                    "startWith": "$edges.target",
+                    "connectFromField": "edges.target",
+                    "connectToField": "_id",
+                    "as": "reachableNodes",
+                    "depthField": "depth",
+                }
+            },
+        ]
+
+        # If you want a limited depth (e.g., only 1 or 2 hops), set maxDepth
+        if max_depth is not None:
+            pipeline[1]["$graphLookup"]["maxDepth"] = max_depth
+
+        # Return the matching doc plus a field "reachableNodes"
+        cursor = self.collection.aggregate(pipeline)
+        results = await cursor.to_list(None)
+
+        # If there's no matching node, results = [].
+        # Otherwise, results[0] is the start node doc,
+        # plus results[0]["reachableNodes"] is the array of connected docs.
+        return results
+
+    #
+    # -------------------------------------------------------------------------
+    # BASIC QUERIES
+    # -------------------------------------------------------------------------
+    #
+
+    async def has_node(self, node_id: str) -> bool:
+        """
+        Check if node_id is present in the collection by looking up its doc.
+        No real need for $graphLookup here, but let's keep it direct.
+        """
+        doc = await self.collection.find_one({"_id": node_id}, {"_id": 1})
+        return doc is not None
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        """
+        Check if there's a direct single-hop edge from source_node_id to target_node_id.
+
+        We'll do a $graphLookup with maxDepth=0 from the source node—meaning
+        “Look up zero expansions.” Actually, for a direct edge check, we can do maxDepth=1
+        and then see if the target node is in the "reachableNodes" at depth=0.
+
+        But typically for a direct edge, we might just do a find_one.
+        Below is a demonstration approach.
+        """
+
+        # We can do a single-hop graphLookup (maxDepth=0 or 1).
+        # Then check if the target_node appears among the edges array.
+        pipeline = [
+            {"$match": {"_id": source_node_id}},
+            {
+                "$graphLookup": {
+                    "from": self.collection.name,
+                    "startWith": "$edges.target",
+                    "connectFromField": "edges.target",
+                    "connectToField": "_id",
+                    "as": "reachableNodes",
+                    "depthField": "depth",
+                    "maxDepth": 0,  # means: do not follow beyond immediate edges
+                }
+            },
+            {
+                "$project": {
+                    "_id": 0,
+                    "reachableNodes._id": 1,  # only keep the _id from the subdocs
+                }
+            },
+        ]
+        cursor = self.collection.aggregate(pipeline)
+        results = await cursor.to_list(None)
+        if not results:
+            return False
+
+        # results[0]["reachableNodes"] are the immediate neighbors
+        reachable_ids = [d["_id"] for d in results[0].get("reachableNodes", [])]
+        return target_node_id in reachable_ids
+
+    #
+    # -------------------------------------------------------------------------
+    # DEGREES
+    # -------------------------------------------------------------------------
+    #
+
+    async def node_degree(self, node_id: str) -> int:
+        """
+        Returns the total number of edges connected to node_id (both inbound and outbound).
+        The easiest approach is typically two queries:
+         - count of edges array in node_id's doc
+         - count of how many other docs have node_id in their edges.target.
+
+        But we'll do a $graphLookup demonstration for inbound edges:
+        1) Outbound edges: direct from node's edges array
+        2) Inbound edges: we can do a special $graphLookup from all docs
+           or do an explicit match.
+
+        For demonstration, let's do this in two steps (with second step $graphLookup).
+        """
+        # --- 1) Outbound edges (direct from doc) ---
+        doc = await self.collection.find_one({"_id": node_id}, {"edges": 1})
+        if not doc:
+            return 0
+        outbound_count = len(doc.get("edges", []))
+
+        # --- 2) Inbound edges:
+        # A simple way is: find all docs where "edges.target" == node_id.
+        # But let's do a $graphLookup from `node_id` in REVERSE.
+        # There's a trick to do "reverse" graphLookups: you'd store
+        # reversed edges or do a more advanced pipeline. Typically you'd do
+        # a direct match. We'll just do a direct match for inbound.
+        inbound_count_pipeline = [
+            {"$match": {"edges.target": node_id}},
+            {
+                "$project": {
+                    "matchingEdgesCount": {
+                        "$size": {
+                            "$filter": {
+                                "input": "$edges",
+                                "as": "edge",
+                                "cond": {"$eq": ["$$edge.target", node_id]},
+                            }
+                        }
+                    }
+                }
+            },
+            {"$group": {"_id": None, "totalInbound": {"$sum": "$matchingEdgesCount"}}},
+        ]
+        inbound_cursor = self.collection.aggregate(inbound_count_pipeline)
+        inbound_result = await inbound_cursor.to_list(None)
+        inbound_count = inbound_result[0]["totalInbound"] if inbound_result else 0
+
+        return outbound_count + inbound_count
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        """
+        If your graph can hold multiple edges from the same src to the same tgt
+        (e.g. different 'relation' values), you can sum them. If it's always
+        one edge, this is typically 1 or 0.
+
+        We'll do a single-hop $graphLookup from src_id,
+        then count how many edges reference tgt_id at depth=0.
+        """
+        pipeline = [
+            {"$match": {"_id": src_id}},
+            {
+                "$graphLookup": {
+                    "from": self.collection.name,
+                    "startWith": "$edges.target",
+                    "connectFromField": "edges.target",
+                    "connectToField": "_id",
+                    "as": "neighbors",
+                    "depthField": "depth",
+                    "maxDepth": 0,
+                }
+            },
+            {"$project": {"edges": 1, "neighbors._id": 1, "neighbors.type": 1}},
+        ]
+        cursor = self.collection.aggregate(pipeline)
+        results = await cursor.to_list(None)
+        if not results:
+            return 0
+
+        # We can simply count how many edges in `results[0].edges` have target == tgt_id.
+        edges = results[0].get("edges", [])
+        count = sum(1 for e in edges if e.get("target") == tgt_id)
+        return count
+
+    #
+    # -------------------------------------------------------------------------
+    # GETTERS
+    # -------------------------------------------------------------------------
+    #
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        """
+        Return the full node document (including "edges"), or None if missing.
+        """
+        return await self.collection.find_one({"_id": node_id})
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        """
+        Return the first edge dict from source_node_id to target_node_id if it exists.
+        Uses a single-hop $graphLookup as demonstration, though a direct find is simpler.
+        """
+        pipeline = [
+            {"$match": {"_id": source_node_id}},
+            {
+                "$graphLookup": {
+                    "from": self.collection.name,
+                    "startWith": "$edges.target",
+                    "connectFromField": "edges.target",
+                    "connectToField": "_id",
+                    "as": "neighbors",
+                    "depthField": "depth",
+                    "maxDepth": 0,
+                }
+            },
+            {"$project": {"edges": 1}},
+        ]
+        cursor = self.collection.aggregate(pipeline)
+        docs = await cursor.to_list(None)
+        if not docs:
+            return None
+
+        for e in docs[0].get("edges", []):
+            if e.get("target") == target_node_id:
+                return e
+        return None
+
+    async def get_node_edges(
+        self, source_node_id: str
+    ) -> Union[List[Tuple[str, str]], None]:
+        """
+        Return a list of (target_id, relation) for direct edges from source_node_id.
+        Demonstrates $graphLookup at maxDepth=0, though direct doc retrieval is simpler.
+        """
+        pipeline = [
+            {"$match": {"_id": source_node_id}},
+            {
+                "$graphLookup": {
+                    "from": self.collection.name,
+                    "startWith": "$edges.target",
+                    "connectFromField": "edges.target",
+                    "connectToField": "_id",
+                    "as": "neighbors",
+                    "depthField": "depth",
+                    "maxDepth": 0,
+                }
+            },
+            {"$project": {"_id": 0, "edges": 1}},
+        ]
+        cursor = self.collection.aggregate(pipeline)
+        result = await cursor.to_list(None)
+        if not result:
+            return None
+
+        edges = result[0].get("edges", [])
+        return [(e["target"], e["relation"]) for e in edges]
+
+    #
+    # -------------------------------------------------------------------------
+    # UPSERTS
+    # -------------------------------------------------------------------------
+    #
+
+    async def upsert_node(self, node_id: str, node_data: dict):
+        """
+        Insert or update a node document. If new, create an empty edges array.
+        """
+        # By default, preserve existing 'edges'.
+        # We'll only set 'edges' to [] on insert (no overwrite).
+        update_doc = {"$set": {**node_data}, "$setOnInsert": {"edges": []}}
+        await self.collection.update_one({"_id": node_id}, update_doc, upsert=True)
+
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: dict
+    ):
+        """
+        Upsert an edge from source_node_id -> target_node_id with optional 'relation'.
+        If an edge with the same target exists, we remove it and re-insert with updated data.
+        """
+        # Ensure source node exists
+        await self.upsert_node(source_node_id, {})
+
+        # Remove existing edge (if any)
+        await self.collection.update_one(
+            {"_id": source_node_id}, {"$pull": {"edges": {"target": target_node_id}}}
+        )
+
+        # Insert new edge
+        new_edge = {"target": target_node_id}
+        new_edge.update(edge_data)
+        await self.collection.update_one(
+            {"_id": source_node_id}, {"$push": {"edges": new_edge}}
+        )
+
+    #
+    # -------------------------------------------------------------------------
+    # DELETION
+    # -------------------------------------------------------------------------
+    #
+
+    async def delete_node(self, node_id: str):
+        """
+        1) Remove node’s doc entirely.
+        2) Remove inbound edges from any doc that references node_id.
+        """
+        # Remove inbound edges from all other docs
+        await self.collection.update_many({}, {"$pull": {"edges": {"target": node_id}}})
+
+        # Remove the node doc
+        await self.collection.delete_one({"_id": node_id})
+
+    #
+    # -------------------------------------------------------------------------
+    # EMBEDDINGS (NOT IMPLEMENTED)
+    # -------------------------------------------------------------------------
+    #
+
+    async def embed_nodes(self, algorithm: str) -> Tuple[np.ndarray, List[str]]:
+        """
+        Placeholder for demonstration, raises NotImplementedError.
+        """
+        raise NotImplementedError("Node embedding is not used in minirag.")
diff --git a/test/minirag/minirag/kg/nano_vector_db_impl.py b/test/minirag/minirag/kg/nano_vector_db_impl.py
new file mode 100755
index 0000000..50f65f0
--- /dev/null
+++ b/test/minirag/minirag/kg/nano_vector_db_impl.py
@@ -0,0 +1,213 @@
+"""
+NanoVectorDB Storage Module
+=======================
+
+This module provides a storage interface for graphs using NetworkX, a popular Python library for creating, manipulating, and studying the structure, dynamics, and functions of complex networks.
+
+The `NetworkXStorage` class extends the `BaseGraphStorage` class from the LightRAG library, providing methods to load, save, manipulate, and query graphs using NetworkX.
+
+Author: lightrag team
+Created: 2024-01-25
+License: MIT
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+Version: 1.0.0
+
+Dependencies:
+    - NetworkX
+    - NumPy
+    - LightRAG
+    - graspologic
+
+Features:
+    - Load and save graphs in various formats (e.g., GEXF, GraphML, JSON)
+    - Query graph nodes and edges
+    - Calculate node and edge degrees
+    - Embed nodes using various algorithms (e.g., Node2Vec)
+    - Remove nodes and edges from the graph
+
+Usage:
+    from minirag.storage.networkx_storage import NetworkXStorage
+
+"""
+
+import asyncio
+import os
+from tqdm.asyncio import tqdm as tqdm_async
+from dataclasses import dataclass
+import numpy as np
+import pipmaster as pm
+
+if not pm.is_installed("nano-vectordb"):
+    pm.install("nano-vectordb")
+
+from nano_vectordb import NanoVectorDB
+import time
+
+from minirag.utils import (
+    logger,
+    compute_mdhash_id,
+)
+
+from minirag.base import (
+    BaseVectorStorage,
+)
+
+
+@dataclass
+class NanoVectorDBStorage(BaseVectorStorage):
+    cosine_better_than_threshold: float = float(os.getenv("COSINE_THRESHOLD", "0.2"))
+
+    def __post_init__(self):
+        # Use global config value if specified, otherwise use default
+        config = self.global_config.get("vector_db_storage_cls_kwargs", {})
+        self.cosine_better_than_threshold = config.get(
+            "cosine_better_than_threshold", self.cosine_better_than_threshold
+        )
+
+        self._client_file_name = os.path.join(
+            self.global_config["working_dir"], f"vdb_{self.namespace}.json"
+        )
+        self._max_batch_size = self.global_config["embedding_batch_num"]
+        self._client = NanoVectorDB(
+            self.embedding_func.embedding_dim, storage_file=self._client_file_name
+        )
+
+    async def upsert(self, data: dict[str, dict]):
+        logger.info(f"Inserting {len(data)} vectors to {self.namespace}")
+        if not len(data):
+            logger.warning("You insert an empty data to vector DB")
+            return []
+
+        current_time = time.time()
+        list_data = [
+            {
+                "__id__": k,
+                "__created_at__": current_time,
+                **{k1: v1 for k1, v1 in v.items() if k1 in self.meta_fields},
+            }
+            for k, v in data.items()
+        ]
+        contents = [v["content"] for v in data.values()]
+        batches = [
+            contents[i : i + self._max_batch_size]
+            for i in range(0, len(contents), self._max_batch_size)
+        ]
+
+        async def wrapped_task(batch):
+            result = await self.embedding_func(batch)
+            pbar.update(1)
+            return result
+
+        embedding_tasks = [wrapped_task(batch) for batch in batches]
+        pbar = tqdm_async(
+            total=len(embedding_tasks), desc="Generating embeddings", unit="batch"
+        )
+        embeddings_list = await asyncio.gather(*embedding_tasks)
+
+        embeddings = np.concatenate(embeddings_list)
+        if len(embeddings) == len(list_data):
+            for i, d in enumerate(list_data):
+                d["__vector__"] = embeddings[i]
+            results = self._client.upsert(datas=list_data)
+            return results
+        else:
+            # sometimes the embedding is not returned correctly. just log it.
+            logger.error(
+                f"embedding is not 1-1 with data, {len(embeddings)} != {len(list_data)}"
+            )
+
+    async def query(self, query: str, top_k=5):
+        embedding = await self.embedding_func([query])
+        embedding = embedding[0]
+        logger.info(
+            f"Query: {query}, top_k: {top_k}, cosine_better_than_threshold: {self.cosine_better_than_threshold}"
+        )
+        results = self._client.query(
+            query=embedding,
+            top_k=top_k,
+            better_than_threshold=self.cosine_better_than_threshold,
+        )
+        results = [
+            {
+                **dp,
+                "id": dp["__id__"],
+                "distance": dp["__metrics__"],
+                "created_at": dp.get("__created_at__"),
+            }
+            for dp in results
+        ]
+        return results
+
+    @property
+    def client_storage(self):
+        return getattr(self._client, "_NanoVectorDB__storage")
+
+    async def delete(self, ids: list[str]):
+        """Delete vectors with specified IDs
+
+        Args:
+            ids: List of vector IDs to be deleted
+        """
+        try:
+            self._client.delete(ids)
+            logger.info(
+                f"Successfully deleted {len(ids)} vectors from {self.namespace}"
+            )
+        except Exception as e:
+            logger.error(f"Error while deleting vectors from {self.namespace}: {e}")
+
+    async def delete_entity(self, entity_name: str):
+        try:
+            entity_id = compute_mdhash_id(entity_name, prefix="ent-")
+            logger.debug(
+                f"Attempting to delete entity {entity_name} with ID {entity_id}"
+            )
+            # Check if the entity exists
+            if self._client.get([entity_id]):
+                await self.delete([entity_id])
+                logger.debug(f"Successfully deleted entity {entity_name}")
+            else:
+                logger.debug(f"Entity {entity_name} not found in storage")
+        except Exception as e:
+            logger.error(f"Error deleting entity {entity_name}: {e}")
+
+    async def delete_entity_relation(self, entity_name: str):
+        try:
+            relations = [
+                dp
+                for dp in self.client_storage["data"]
+                if dp["src_id"] == entity_name or dp["tgt_id"] == entity_name
+            ]
+            logger.debug(f"Found {len(relations)} relations for entity {entity_name}")
+            ids_to_delete = [relation["__id__"] for relation in relations]
+
+            if ids_to_delete:
+                await self.delete(ids_to_delete)
+                logger.debug(
+                    f"Deleted {len(ids_to_delete)} relations for {entity_name}"
+                )
+            else:
+                logger.debug(f"No relations found for entity {entity_name}")
+        except Exception as e:
+            logger.error(f"Error deleting relations for {entity_name}: {e}")
+
+    async def index_done_callback(self):
+        self._client.save()
diff --git a/test/minirag/minirag/kg/neo4j_impl.py b/test/minirag/minirag/kg/neo4j_impl.py
new file mode 100755
index 0000000..27d773d
--- /dev/null
+++ b/test/minirag/minirag/kg/neo4j_impl.py
@@ -0,0 +1,556 @@
+import asyncio
+import inspect
+import os
+from dataclasses import dataclass
+from typing import Any, Union, Tuple, List, Dict
+import pipmaster as pm
+
+if not pm.is_installed("neo4j"):
+    pm.install("neo4j")
+
+from neo4j import (
+    AsyncGraphDatabase,
+    exceptions as neo4jExceptions,
+    AsyncDriver,
+    AsyncManagedTransaction,
+    GraphDatabase,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.utils import logger
+from ..base import BaseGraphStorage
+import copy
+from minirag.utils import merge_tuples
+
+@dataclass
+class Neo4JStorage(BaseGraphStorage):
+    @staticmethod
+    def load_nx_graph(file_name):
+        print("no preloading of graph with neo4j in production")
+
+    def __init__(self, namespace, global_config, embedding_func):
+        super().__init__(
+            namespace=namespace,
+            global_config=global_config,
+            embedding_func=embedding_func,
+        )
+        self._driver = None
+        self._driver_lock = asyncio.Lock()
+        URI = os.environ["NEO4J_URI"]
+        USERNAME = os.environ["NEO4J_USERNAME"]
+        PASSWORD = os.environ["NEO4J_PASSWORD"]
+        MAX_CONNECTION_POOL_SIZE = os.environ.get("NEO4J_MAX_CONNECTION_POOL_SIZE", 800)
+        DATABASE = os.environ.get(
+            "NEO4J_DATABASE"
+        )  # If this param is None, the home database will be used. If it is not None, the specified database will be used.
+        self._DATABASE = DATABASE
+        self._driver: AsyncDriver = AsyncGraphDatabase.driver(
+            URI, auth=(USERNAME, PASSWORD)
+        )
+        _database_name = "home database" if DATABASE is None else f"database {DATABASE}"
+        with GraphDatabase.driver(
+            URI,
+            auth=(USERNAME, PASSWORD),
+            max_connection_pool_size=MAX_CONNECTION_POOL_SIZE,
+        ) as _sync_driver:
+            try:
+                with _sync_driver.session(database=DATABASE) as session:
+                    try:
+                        session.run("MATCH (n) RETURN n LIMIT 0")
+                        logger.info(f"Connected to {DATABASE} at {URI}")
+                    except neo4jExceptions.ServiceUnavailable as e:
+                        logger.error(
+                            f"{DATABASE} at {URI} is not available".capitalize()
+                        )
+                        raise e
+            except neo4jExceptions.AuthError as e:
+                logger.error(f"Authentication failed for {DATABASE} at {URI}")
+                raise e
+            except neo4jExceptions.ClientError as e:
+                if e.code == "Neo.ClientError.Database.DatabaseNotFound":
+                    logger.info(
+                        f"{DATABASE} at {URI} not found. Try to create specified database.".capitalize()
+                    )
+                try:
+                    with _sync_driver.session() as session:
+                        session.run(f"CREATE DATABASE `{DATABASE}` IF NOT EXISTS")
+                        logger.info(f"{DATABASE} at {URI} created".capitalize())
+                except neo4jExceptions.ClientError as e:
+                    if (
+                        e.code
+                        == "Neo.ClientError.Statement.UnsupportedAdministrationCommand"
+                    ):
+                        logger.warning(
+                            "This Neo4j instance does not support creating databases. Try to use Neo4j Desktop/Enterprise version or DozerDB instead."
+                        )
+                    logger.error(f"Failed to create {DATABASE} at {URI}")
+                    raise e
+
+    def __post_init__(self):
+        self._node_embed_algorithms = {
+            "node2vec": self._node2vec_embed,
+        }
+
+    async def close(self):
+        if self._driver:
+            await self._driver.close()
+            self._driver = None
+
+    async def __aexit__(self, exc_type, exc, tb):
+        if self._driver:
+            await self._driver.close()
+
+    async def index_done_callback(self):
+        print("KG successfully indexed.")
+
+    async def has_node(self, node_id: str) -> bool:
+        entity_name_label = node_id.strip('"')
+
+        async with self._driver.session(database=self._DATABASE) as session:
+            query = (
+                f"MATCH (n:`{entity_name_label}`) RETURN count(n) > 0 AS node_exists"
+            )
+            result = await session.run(query)
+            single_result = await result.single()
+            logger.debug(
+                f'{inspect.currentframe().f_code.co_name}:query:{query}:result:{single_result["node_exists"]}'
+            )
+            return single_result["node_exists"]
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        entity_name_label_source = source_node_id.strip('"')
+        entity_name_label_target = target_node_id.strip('"')
+
+        async with self._driver.session(database=self._DATABASE) as session:
+            query = (
+                f"MATCH (a:`{entity_name_label_source}`)-[r]-(b:`{entity_name_label_target}`) "
+                "RETURN COUNT(r) > 0 AS edgeExists"
+            )
+            result = await session.run(query)
+            single_result = await result.single()
+            logger.debug(
+                f'{inspect.currentframe().f_code.co_name}:query:{query}:result:{single_result["edgeExists"]}'
+            )
+            return single_result["edgeExists"]
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        async with self._driver.session(database=self._DATABASE) as session:
+            entity_name_label = node_id.strip('"')
+            query = f"MATCH (n:`{entity_name_label}`) RETURN n"
+            result = await session.run(query)
+            record = await result.single()
+            if record:
+                node = record["n"]
+                node_dict = dict(node)
+                logger.debug(
+                    f"{inspect.currentframe().f_code.co_name}: query: {query}, result: {node_dict}"
+                )
+                return node_dict
+            return None
+
+
+    async def get_node_from_types(self,type_list)  -> Union[dict, None]:
+        node_list = []
+        for name, arrt in self._graph.nodes(data = True):
+            node_type = arrt.get('entity_type').strip('\"')
+            if node_type in type_list:
+                node_list.append(name)
+        node_datas = await asyncio.gather(
+            *[self.get_node(name) for name in node_list]
+        )
+        node_datas = [
+            {**n, "entity_name": k}
+            for k, n in zip(node_list, node_datas)
+            if n is not None
+        ]
+        return node_datas#,node_dict
+    
+
+    async def get_neighbors_within_k_hops(self,source_node_id: str, k):
+        count = 0
+        if await self.has_node(source_node_id):
+            source_edge = list(self._graph.edges(source_node_id))
+        else:
+            print("NO THIS ID:",source_node_id)
+            return []
+        count = count+1
+        while count<k:
+            count = count+1
+            sc_edge = copy.deepcopy(source_edge)
+            source_edge =[]
+            for pair in sc_edge:
+                append_edge = list(self._graph.edges(pair[-1]))
+                for tuples in merge_tuples([pair],append_edge):
+                    source_edge.append(tuples)
+        return source_edge
+    
+    async def node_degree(self, node_id: str) -> int:
+        entity_name_label = node_id.strip('"')
+
+        async with self._driver.session(database=self._DATABASE) as session:
+            query = f"""
+                MATCH (n:`{entity_name_label}`)
+                RETURN COUNT{{ (n)--() }} AS totalEdgeCount
+            """
+            result = await session.run(query)
+            record = await result.single()
+            if record:
+                edge_count = record["totalEdgeCount"]
+                logger.debug(
+                    f"{inspect.currentframe().f_code.co_name}:query:{query}:result:{edge_count}"
+                )
+                return edge_count
+            else:
+                return None
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        entity_name_label_source = src_id.strip('"')
+        entity_name_label_target = tgt_id.strip('"')
+        src_degree = await self.node_degree(entity_name_label_source)
+        trg_degree = await self.node_degree(entity_name_label_target)
+
+        # Convert None to 0 for addition
+        src_degree = 0 if src_degree is None else src_degree
+        trg_degree = 0 if trg_degree is None else trg_degree
+
+        degrees = int(src_degree) + int(trg_degree)
+        logger.debug(
+            f"{inspect.currentframe().f_code.co_name}:query:src_Degree+trg_degree:result:{degrees}"
+        )
+        return degrees
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        entity_name_label_source = source_node_id.strip('"')
+        entity_name_label_target = target_node_id.strip('"')
+        """
+        Find all edges between nodes of two given labels
+
+        Args:
+            source_node_label (str): Label of the source nodes
+            target_node_label (str): Label of the target nodes
+
+        Returns:
+            list: List of all relationships/edges found
+        """
+        async with self._driver.session(database=self._DATABASE) as session:
+            query = f"""
+            MATCH (start:`{entity_name_label_source}`)-[r]->(end:`{entity_name_label_target}`)
+            RETURN properties(r) as edge_properties
+            LIMIT 1
+            """.format(
+                entity_name_label_source=entity_name_label_source,
+                entity_name_label_target=entity_name_label_target,
+            )
+
+            result = await session.run(query)
+            record = await result.single()
+            if record:
+                result = dict(record["edge_properties"])
+                logger.debug(
+                    f"{inspect.currentframe().f_code.co_name}:query:{query}:result:{result}"
+                )
+                return result
+            else:
+                return None
+
+    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:
+        node_label = source_node_id.strip('"')
+
+        """
+        Retrieves all edges (relationships) for a particular node identified by its label.
+        :return: List of dictionaries containing edge information
+        """
+        query = f"""MATCH (n:`{node_label}`)
+                OPTIONAL MATCH (n)-[r]-(connected)
+                RETURN n, r, connected"""
+        async with self._driver.session(database=self._DATABASE) as session:
+            results = await session.run(query)
+            edges = []
+            async for record in results:
+                source_node = record["n"]
+                connected_node = record["connected"]
+
+                source_label = (
+                    list(source_node.labels)[0] if source_node.labels else None
+                )
+                target_label = (
+                    list(connected_node.labels)[0]
+                    if connected_node and connected_node.labels
+                    else None
+                )
+
+                if source_label and target_label:
+                    edges.append((source_label, target_label))
+
+            return edges
+
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type(
+            (
+                neo4jExceptions.ServiceUnavailable,
+                neo4jExceptions.TransientError,
+                neo4jExceptions.WriteServiceUnavailable,
+                neo4jExceptions.ClientError,
+            )
+        ),
+    )
+    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):
+        """
+        Upsert a node in the Neo4j database.
+
+        Args:
+            node_id: The unique identifier for the node (used as label)
+            node_data: Dictionary of node properties
+        """
+        label = node_id.strip('"')
+        properties = node_data
+
+        async def _do_upsert(tx: AsyncManagedTransaction):
+            query = f"""
+            MERGE (n:`{label}`)
+            SET n += $properties
+            """
+            await tx.run(query, properties=properties)
+            logger.debug(
+                f"Upserted node with label '{label}' and properties: {properties}"
+            )
+
+        try:
+            async with self._driver.session(database=self._DATABASE) as session:
+                await session.execute_write(_do_upsert)
+        except Exception as e:
+            logger.error(f"Error during upsert: {str(e)}")
+            raise
+
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type(
+            (
+                neo4jExceptions.ServiceUnavailable,
+                neo4jExceptions.TransientError,
+                neo4jExceptions.WriteServiceUnavailable,
+            )
+        ),
+    )
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]
+    ):
+        """
+        Upsert an edge and its properties between two nodes identified by their labels.
+
+        Args:
+            source_node_id (str): Label of the source node (used as identifier)
+            target_node_id (str): Label of the target node (used as identifier)
+            edge_data (dict): Dictionary of properties to set on the edge
+        """
+        source_node_label = source_node_id.strip('"')
+        target_node_label = target_node_id.strip('"')
+        edge_properties = edge_data
+
+        async def _do_upsert_edge(tx: AsyncManagedTransaction):
+            query = f"""
+            MATCH (source:`{source_node_label}`)
+            WITH source
+            MATCH (target:`{target_node_label}`)
+            MERGE (source)-[r:DIRECTED]->(target)
+            SET r += $properties
+            RETURN r
+            """
+            await tx.run(query, properties=edge_properties)
+            logger.debug(
+                f"Upserted edge from '{source_node_label}' to '{target_node_label}' with properties: {edge_properties}"
+            )
+
+        try:
+            async with self._driver.session(database=self._DATABASE) as session:
+                await session.execute_write(_do_upsert_edge)
+        except Exception as e:
+            logger.error(f"Error during edge upsert: {str(e)}")
+            raise
+
+    async def _node2vec_embed(self):
+        print("Implemented but never called.")
+
+    async def get_knowledge_graph(
+        self, node_label: str, max_depth: int = 5
+    ) -> Dict[str, List[Dict]]:
+        """
+        Get complete connected subgraph for specified node (including the starting node itself)
+
+        Key fixes:
+        1. Include the starting node itself
+        2. Handle multi-label nodes
+        3. Clarify relationship directions
+        4. Add depth control
+        """
+        label = node_label.strip('"')
+        result = {"nodes": [], "edges": []}
+        seen_nodes = set()
+        seen_edges = set()
+
+        async with self._driver.session(database=self._DATABASE) as session:
+            try:
+                # Critical debug step: first verify if starting node exists
+                validate_query = f"MATCH (n:`{label}`) RETURN n LIMIT 1"
+                validate_result = await session.run(validate_query)
+                if not await validate_result.single():
+                    logger.warning(f"Starting node {label} does not exist!")
+                    return result
+
+                # Optimized query (including direction handling and self-loops)
+                main_query = f"""
+                MATCH (start:`{label}`)
+                WITH start
+                CALL apoc.path.subgraphAll(start, {{
+                    relationshipFilter: '>',
+                    minLevel: 0,
+                    maxLevel: {max_depth},
+                    bfs: true
+                }})
+                YIELD nodes, relationships
+                RETURN nodes, relationships
+                """
+                result_set = await session.run(main_query)
+                record = await result_set.single()
+
+                if record:
+                    # Handle nodes (compatible with multi-label cases)
+                    for node in record["nodes"]:
+                        # Use node ID + label combination as unique identifier
+                        node_id = f"{node.id}_{'_'.join(node.labels)}"
+                        if node_id not in seen_nodes:
+                            node_data = dict(node)
+                            node_data["labels"] = list(node.labels)  # Keep all labels
+                            result["nodes"].append(node_data)
+                            seen_nodes.add(node_id)
+
+                    # Handle relationships (including direction information)
+                    for rel in record["relationships"]:
+                        edge_id = f"{rel.id}_{rel.type}"
+                        if edge_id not in seen_edges:
+                            start = rel.start_node
+                            end = rel.end_node
+                            edge_data = dict(rel)
+                            edge_data.update(
+                                {
+                                    "source": f"{start.id}_{'_'.join(start.labels)}",
+                                    "target": f"{end.id}_{'_'.join(end.labels)}",
+                                    "type": rel.type,
+                                    "direction": rel.element_id.split(
+                                        "->" if rel.end_node == end else "<-"
+                                    )[1],
+                                }
+                            )
+                            result["edges"].append(edge_data)
+                            seen_edges.add(edge_id)
+
+                    logger.info(
+                        f"Subgraph query successful | Node count: {len(result['nodes'])} | Edge count: {len(result['edges'])}"
+                    )
+
+            except neo4jExceptions.ClientError as e:
+                logger.error(f"APOC query failed: {str(e)}")
+                return await self._robust_fallback(label, max_depth)
+
+        return result
+
+    async def _robust_fallback(
+        self, label: str, max_depth: int
+    ) -> Dict[str, List[Dict]]:
+        """Enhanced fallback query solution"""
+        result = {"nodes": [], "edges": []}
+        visited_nodes = set()
+        visited_edges = set()
+
+        async def traverse(current_label: str, current_depth: int):
+            if current_depth > max_depth:
+                return
+
+            # Get current node details
+            node = await self.get_node(current_label)
+            if not node:
+                return
+
+            node_id = f"{current_label}"
+            if node_id in visited_nodes:
+                return
+            visited_nodes.add(node_id)
+
+            # Add node data (with complete labels)
+            node_data = {k: v for k, v in node.items()}
+            node_data["labels"] = [
+                current_label
+            ]  # Assume get_node method returns label information
+            result["nodes"].append(node_data)
+
+            # Get all outgoing and incoming edges
+            query = f"""
+            MATCH (a)-[r]-(b)
+            WHERE a:`{current_label}` OR b:`{current_label}`
+            RETURN a, r, b,
+                   CASE WHEN startNode(r) = a THEN 'OUTGOING' ELSE 'INCOMING' END AS direction
+            """
+            async with self._driver.session(database=self._DATABASE) as session:
+                results = await session.run(query)
+                async for record in results:
+                    # Handle edges
+                    rel = record["r"]
+                    edge_id = f"{rel.id}_{rel.type}"
+                    if edge_id not in visited_edges:
+                        edge_data = dict(rel)
+                        edge_data.update(
+                            {
+                                "source": list(record["a"].labels)[0],
+                                "target": list(record["b"].labels)[0],
+                                "type": rel.type,
+                                "direction": record["direction"],
+                            }
+                        )
+                        result["edges"].append(edge_data)
+                        visited_edges.add(edge_id)
+
+                        # Recursively traverse adjacent nodes
+                        next_label = (
+                            list(record["b"].labels)[0]
+                            if record["direction"] == "OUTGOING"
+                            else list(record["a"].labels)[0]
+                        )
+                        await traverse(next_label, current_depth + 1)
+
+        await traverse(label, 0)
+        return result
+
+    async def get_all_labels(self) -> List[str]:
+        """
+        Get all existing node labels in the database
+        Returns:
+            ["Person", "Company", ...]  # Alphabetically sorted label list
+        """
+        async with self._driver.session(database=self._DATABASE) as session:
+            # Method 1: Direct metadata query (Available for Neo4j 4.3+)
+            # query = "CALL db.labels() YIELD label RETURN label"
+
+            # Method 2: Query compatible with older versions
+            query = """
+                MATCH (n)
+                WITH DISTINCT labels(n) AS node_labels
+                UNWIND node_labels AS label
+                RETURN DISTINCT label
+                ORDER BY label
+            """
+
+            result = await session.run(query)
+            labels = []
+            async for record in result:
+                labels.append(record["label"])
+            return labels
diff --git a/test/minirag/minirag/kg/networkx_impl.py b/test/minirag/minirag/kg/networkx_impl.py
new file mode 100755
index 0000000..7f7087f
--- /dev/null
+++ b/test/minirag/minirag/kg/networkx_impl.py
@@ -0,0 +1,276 @@
+"""
+NetworkX Storage Module
+=======================
+
+This module provides a storage interface for graphs using NetworkX, a popular Python library for creating, manipulating, and studying the structure, dynamics, and functions of complex networks.
+
+The `NetworkXStorage` class extends the `BaseGraphStorage` class from the LightRAG library, providing methods to load, save, manipulate, and query graphs using NetworkX.
+
+Author: lightrag team
+Created: 2024-01-25
+License: MIT
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+Version: 1.0.0
+
+Dependencies:
+    - NetworkX
+    - NumPy
+    - LightRAG
+    - graspologic
+
+Features:
+    - Load and save graphs in various formats (e.g., GEXF, GraphML, JSON)
+    - Query graph nodes and edges
+    - Calculate node and edge degrees
+    - Embed nodes using various algorithms (e.g., Node2Vec)
+    - Remove nodes and edges from the graph
+
+Usage:
+    from minirags.kg.networkx_impl import NetworkXStorage
+
+"""
+import asyncio
+import html
+import os
+from dataclasses import dataclass
+from typing import Any, Union, cast
+import networkx as nx
+import numpy as np
+import copy
+
+from minirag.utils import (
+    logger,
+)
+
+from minirag.base import (
+    BaseGraphStorage,
+)
+
+from minirag.utils import merge_tuples
+
+@dataclass
+class NetworkXStorage(BaseGraphStorage):
+    @staticmethod
+    def load_nx_graph(file_name) -> nx.Graph:
+        if os.path.exists(file_name):
+            return nx.read_graphml(file_name)
+        return None
+
+    @staticmethod
+    def write_nx_graph(graph: nx.Graph, file_name):
+        logger.info(
+            f"Writing graph with {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges"
+        )
+        nx.write_graphml(graph, file_name)
+
+    @staticmethod
+    def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph:
+        """Refer to https://github.com/microsoft/graphrag/index/graph/utils/stable_lcc.py
+        Return the largest connected component of the graph, with nodes and edges sorted in a stable way.
+        """
+        from graspologic.utils import largest_connected_component
+
+        graph = graph.copy()
+        graph = cast(nx.Graph, largest_connected_component(graph))
+        node_mapping = {
+            node: html.unescape(node.upper().strip()) for node in graph.nodes()
+        }  # type: ignore
+        graph = nx.relabel_nodes(graph, node_mapping)
+        return NetworkXStorage._stabilize_graph(graph)
+
+    @staticmethod
+    def _stabilize_graph(graph: nx.Graph) -> nx.Graph:
+        """Refer to https://github.com/microsoft/graphrag/index/graph/utils/stable_lcc.py
+        Ensure an undirected graph with the same relationships will always be read the same way.
+        """
+        fixed_graph = nx.DiGraph() if graph.is_directed() else nx.Graph()
+
+        sorted_nodes = graph.nodes(data=True)
+        sorted_nodes = sorted(sorted_nodes, key=lambda x: x[0])
+
+        fixed_graph.add_nodes_from(sorted_nodes)
+        edges = list(graph.edges(data=True))
+
+        if not graph.is_directed():
+
+            def _sort_source_target(edge):
+                source, target, edge_data = edge
+                if source > target:
+                    temp = source
+                    source = target
+                    target = temp
+                return source, target, edge_data
+
+            edges = [_sort_source_target(edge) for edge in edges]
+
+        def _get_edge_key(source: Any, target: Any) -> str:
+            return f"{source} -> {target}"
+
+        edges = sorted(edges, key=lambda x: _get_edge_key(x[0], x[1]))
+
+        fixed_graph.add_edges_from(edges)
+        return fixed_graph
+
+    def __post_init__(self):
+        self._graphml_xml_file = os.path.join(
+            self.global_config["working_dir"], f"graph_{self.namespace}.graphml"
+        )
+        preloaded_graph = NetworkXStorage.load_nx_graph(self._graphml_xml_file)
+        if preloaded_graph is not None:
+            logger.info(
+                f"Loaded graph from {self._graphml_xml_file} with {preloaded_graph.number_of_nodes()} nodes, {preloaded_graph.number_of_edges()} edges"
+            )
+        self._graph = preloaded_graph or nx.Graph()
+        self._node_embed_algorithms = {
+            "node2vec": self._node2vec_embed,
+        }
+
+    async def index_done_callback(self):
+        NetworkXStorage.write_nx_graph(self._graph, self._graphml_xml_file)
+        
+    async def get_types(self):
+        types = set()
+        types_with_case = set()
+
+        for _, data in self._graph.nodes(data=True):
+            if "type" in data:
+                types.add(data["entity_type"].lower()) 
+                types_with_case.add(data["entity_type"])  
+        return list(types), list(types_with_case)
+
+
+    async def get_node_from_types(self,type_list)  -> Union[dict, None]:
+        node_list = []
+        for name, arrt in self._graph.nodes(data = True):
+            node_type = arrt.get('entity_type').strip('\"')
+            if node_type in type_list:
+                node_list.append(name)
+        node_datas = await asyncio.gather(
+            *[self.get_node(name) for name in node_list]
+        )
+        node_datas = [
+            {**n, "entity_name": k}
+            for k, n in zip(node_list, node_datas)
+            if n is not None
+        ]
+        return node_datas#,node_dict
+    
+
+    async def get_neighbors_within_k_hops(self,source_node_id: str, k):
+        count = 0
+        if await self.has_node(source_node_id):
+            source_edge = list(self._graph.edges(source_node_id))
+        else:
+            print("NO THIS ID:",source_node_id)
+            return []
+        count = count+1
+        while count<k:
+            count = count+1
+            sc_edge = copy.deepcopy(source_edge)
+            source_edge =[]
+            for pair in sc_edge:
+                append_edge = list(self._graph.edges(pair[-1]))
+                for tuples in merge_tuples([pair],append_edge):
+                    source_edge.append(tuples)
+        return source_edge
+    
+
+    async def has_node(self, node_id: str) -> bool:
+        return self._graph.has_node(node_id)
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        return self._graph.has_edge(source_node_id, target_node_id)
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        return self._graph.nodes.get(node_id)
+
+    async def node_degree(self, node_id: str) -> int:
+        return self._graph.degree(node_id)
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        return self._graph.degree(src_id) + self._graph.degree(tgt_id)
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        return self._graph.edges.get((source_node_id, target_node_id))
+
+    async def get_node_edges(self, source_node_id: str):
+        if self._graph.has_node(source_node_id):
+            return list(self._graph.edges(source_node_id))
+        return None
+
+    async def upsert_node(self, node_id: str, node_data: dict[str, str]):
+        self._graph.add_node(node_id, **node_data)
+
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]
+    ):
+        self._graph.add_edge(source_node_id, target_node_id, **edge_data)
+
+    async def delete_node(self, node_id: str):
+        """
+        Delete a node from the graph based on the specified node_id.
+
+        :param node_id: The node_id to delete
+        """
+        if self._graph.has_node(node_id):
+            self._graph.remove_node(node_id)
+            logger.info(f"Node {node_id} deleted from the graph.")
+        else:
+            logger.warning(f"Node {node_id} not found in the graph for deletion.")
+
+    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:
+        if algorithm not in self._node_embed_algorithms:
+            raise ValueError(f"Node embedding algorithm {algorithm} not supported")
+        return await self._node_embed_algorithms[algorithm]()
+
+    # @TODO: NOT USED
+    async def _node2vec_embed(self):
+        from graspologic import embed
+
+        embeddings, nodes = embed.node2vec_embed(
+            self._graph,
+            **self.global_config["node2vec_params"],
+        )
+
+        nodes_ids = [self._graph.nodes[node_id]["id"] for node_id in nodes]
+        return embeddings, nodes_ids
+
+    def remove_nodes(self, nodes: list[str]):
+        """Delete multiple nodes
+
+        Args:
+            nodes: List of node IDs to be deleted
+        """
+        for node in nodes:
+            if self._graph.has_node(node):
+                self._graph.remove_node(node)
+
+    def remove_edges(self, edges: list[tuple[str, str]]):
+        """Delete multiple edges
+
+        Args:
+            edges: List of edges to be deleted, each edge is a (source, target) tuple
+        """
+        for source, target in edges:
+            if self._graph.has_edge(source, target):
+                self._graph.remove_edge(source, target)
diff --git a/test/minirag/minirag/kg/oracle_impl.py b/test/minirag/minirag/kg/oracle_impl.py
new file mode 100755
index 0000000..368e861
--- /dev/null
+++ b/test/minirag/minirag/kg/oracle_impl.py
@@ -0,0 +1,856 @@
+import os
+import asyncio
+
+# import html
+# import os
+from dataclasses import dataclass
+from typing import Union
+import numpy as np
+import array
+import pipmaster as pm
+
+if not pm.is_installed("oracledb"):
+    pm.install("oracledb")
+
+
+from ..utils import logger
+from ..base import (
+    BaseGraphStorage,
+    BaseKVStorage,
+    BaseVectorStorage,
+)
+
+import oracledb
+
+
+class OracleDB:
+    def __init__(self, config, **kwargs):
+        self.host = config.get("host", None)
+        self.port = config.get("port", None)
+        self.user = config.get("user", None)
+        self.password = config.get("password", None)
+        self.dsn = config.get("dsn", None)
+        self.config_dir = config.get("config_dir", None)
+        self.wallet_location = config.get("wallet_location", None)
+        self.wallet_password = config.get("wallet_password", None)
+        self.workspace = config.get("workspace", None)
+        self.max = 12
+        self.increment = 1
+        logger.info(f"Using the label {self.workspace} for Oracle Graph as identifier")
+        if self.user is None or self.password is None:
+            raise ValueError("Missing database user or password in addon_params")
+
+        try:
+            oracledb.defaults.fetch_lobs = False
+
+            self.pool = oracledb.create_pool_async(
+                user=self.user,
+                password=self.password,
+                dsn=self.dsn,
+                config_dir=self.config_dir,
+                wallet_location=self.wallet_location,
+                wallet_password=self.wallet_password,
+                min=1,
+                max=self.max,
+                increment=self.increment,
+            )
+            logger.info(f"Connected to Oracle database at {self.dsn}")
+        except Exception as e:
+            logger.error(f"Failed to connect to Oracle database at {self.dsn}")
+            logger.error(f"Oracle database error: {e}")
+            raise
+
+    def numpy_converter_in(self, value):
+        """Convert numpy array to array.array"""
+        if value.dtype == np.float64:
+            dtype = "d"
+        elif value.dtype == np.float32:
+            dtype = "f"
+        else:
+            dtype = "b"
+        return array.array(dtype, value)
+
+    def input_type_handler(self, cursor, value, arraysize):
+        """Set the type handler for the input data"""
+        if isinstance(value, np.ndarray):
+            return cursor.var(
+                oracledb.DB_TYPE_VECTOR,
+                arraysize=arraysize,
+                inconverter=self.numpy_converter_in,
+            )
+
+    def numpy_converter_out(self, value):
+        """Convert array.array to numpy array"""
+        if value.typecode == "b":
+            dtype = np.int8
+        elif value.typecode == "f":
+            dtype = np.float32
+        else:
+            dtype = np.float64
+        return np.array(value, copy=False, dtype=dtype)
+
+    def output_type_handler(self, cursor, metadata):
+        """Set the type handler for the output data"""
+        if metadata.type_code is oracledb.DB_TYPE_VECTOR:
+            return cursor.var(
+                metadata.type_code,
+                arraysize=cursor.arraysize,
+                outconverter=self.numpy_converter_out,
+            )
+
+    async def check_tables(self):
+        for k, v in TABLES.items():
+            try:
+                if k.lower() == "lightrag_graph":
+                    await self.query(
+                        "SELECT id FROM GRAPH_TABLE (lightrag_graph MATCH (a) COLUMNS (a.id)) fetch first row only"
+                    )
+                else:
+                    await self.query("SELECT 1 FROM {k}".format(k=k))
+            except Exception as e:
+                logger.error(f"Failed to check table {k} in Oracle database")
+                logger.error(f"Oracle database error: {e}")
+                try:
+                    # print(v["ddl"])
+                    await self.execute(v["ddl"])
+                    logger.info(f"Created table {k} in Oracle database")
+                except Exception as e:
+                    logger.error(f"Failed to create table {k} in Oracle database")
+                    logger.error(f"Oracle database error: {e}")
+
+        logger.info("Finished check all tables in Oracle database")
+
+    async def query(
+        self, sql: str, params: dict = None, multirows: bool = False
+    ) -> Union[dict, None]:
+        async with self.pool.acquire() as connection:
+            connection.inputtypehandler = self.input_type_handler
+            connection.outputtypehandler = self.output_type_handler
+            with connection.cursor() as cursor:
+                try:
+                    await cursor.execute(sql, params)
+                except Exception as e:
+                    logger.error(f"Oracle database error: {e}")
+                    print(sql)
+                    print(params)
+                    raise
+                columns = [column[0].lower() for column in cursor.description]
+                if multirows:
+                    rows = await cursor.fetchall()
+                    if rows:
+                        data = [dict(zip(columns, row)) for row in rows]
+                    else:
+                        data = []
+                else:
+                    row = await cursor.fetchone()
+                    if row:
+                        data = dict(zip(columns, row))
+                    else:
+                        data = None
+                return data
+
+    async def execute(self, sql: str, data: Union[list, dict] = None):
+        # logger.info("go into OracleDB execute method")
+        try:
+            async with self.pool.acquire() as connection:
+                connection.inputtypehandler = self.input_type_handler
+                connection.outputtypehandler = self.output_type_handler
+                with connection.cursor() as cursor:
+                    if data is None:
+                        await cursor.execute(sql)
+                    else:
+                        await cursor.execute(sql, data)
+                    await connection.commit()
+        except Exception as e:
+            logger.error(f"Oracle database error: {e}")
+            print(sql)
+            print(data)
+            raise
+
+
+@dataclass
+class OracleKVStorage(BaseKVStorage):
+    # should pass db object to self.db
+    db: OracleDB = None
+    meta_fields = None
+
+    def __post_init__(self):
+        self._data = {}
+        self._max_batch_size = self.global_config.get("embedding_batch_num", 10)
+
+    ################ QUERY METHODS ################
+
+    async def get_by_id(self, id: str) -> Union[dict, None]:
+        """get doc_full data based on id."""
+        SQL = SQL_TEMPLATES["get_by_id_" + self.namespace]
+        params = {"workspace": self.db.workspace, "id": id}
+        # print("get_by_id:"+SQL)
+        if "llm_response_cache" == self.namespace:
+            array_res = await self.db.query(SQL, params, multirows=True)
+            res = {}
+            for row in array_res:
+                res[row["id"]] = row
+        else:
+            res = await self.db.query(SQL, params)
+        if res:
+            return res
+        else:
+            return None
+
+    async def get_by_mode_and_id(self, mode: str, id: str) -> Union[dict, None]:
+        """Specifically for llm_response_cache."""
+        SQL = SQL_TEMPLATES["get_by_mode_id_" + self.namespace]
+        params = {"workspace": self.db.workspace, "cache_mode": mode, "id": id}
+        if "llm_response_cache" == self.namespace:
+            array_res = await self.db.query(SQL, params, multirows=True)
+            res = {}
+            for row in array_res:
+                res[row["id"]] = row
+            return res
+        else:
+            return None
+
+    async def get_by_ids(self, ids: list[str], fields=None) -> Union[list[dict], None]:
+        """get doc_chunks data based on id"""
+        SQL = SQL_TEMPLATES["get_by_ids_" + self.namespace].format(
+            ids=",".join([f"'{id}'" for id in ids])
+        )
+        params = {"workspace": self.db.workspace}
+        # print("get_by_ids:"+SQL)
+        res = await self.db.query(SQL, params, multirows=True)
+        if "llm_response_cache" == self.namespace:
+            modes = set()
+            dict_res: dict[str, dict] = {}
+            for row in res:
+                modes.add(row["mode"])
+            for mode in modes:
+                if mode not in dict_res:
+                    dict_res[mode] = {}
+            for row in res:
+                dict_res[row["mode"]][row["id"]] = row
+            res = [{k: v} for k, v in dict_res.items()]
+        if res:
+            data = res  # [{"data":i} for i in res]
+            # print(data)
+            return data
+        else:
+            return None
+
+    async def get_by_status_and_ids(
+        self, status: str, ids: list[str]
+    ) -> Union[list[dict], None]:
+        """Specifically for llm_response_cache."""
+        if ids is not None:
+            SQL = SQL_TEMPLATES["get_by_status_ids_" + self.namespace].format(
+                ids=",".join([f"'{id}'" for id in ids])
+            )
+        else:
+            SQL = SQL_TEMPLATES["get_by_status_" + self.namespace]
+        params = {"workspace": self.db.workspace, "status": status}
+        res = await self.db.query(SQL, params, multirows=True)
+        if res:
+            return res
+        else:
+            return None
+
+    async def filter_keys(self, keys: list[str]) -> set[str]:
+        """Return keys that don't exist in storage"""
+        SQL = SQL_TEMPLATES["filter_keys"].format(
+            table_name=N_T[self.namespace], ids=",".join([f"'{id}'" for id in keys])
+        )
+        params = {"workspace": self.db.workspace}
+        res = await self.db.query(SQL, params, multirows=True)
+        if res:
+            exist_keys = [key["id"] for key in res]
+            data = set([s for s in keys if s not in exist_keys])
+            return data
+        else:
+            return set(keys)
+
+    ################ INSERT METHODS ################
+    async def upsert(self, data: dict[str, dict]):
+        if self.namespace == "text_chunks":
+            list_data = [
+                {
+                    "id": k,
+                    **{k1: v1 for k1, v1 in v.items()},
+                }
+                for k, v in data.items()
+            ]
+            contents = [v["content"] for v in data.values()]
+            batches = [
+                contents[i : i + self._max_batch_size]
+                for i in range(0, len(contents), self._max_batch_size)
+            ]
+            embeddings_list = await asyncio.gather(
+                *[self.embedding_func(batch) for batch in batches]
+            )
+            embeddings = np.concatenate(embeddings_list)
+            for i, d in enumerate(list_data):
+                d["__vector__"] = embeddings[i]
+
+            merge_sql = SQL_TEMPLATES["merge_chunk"]
+            for item in list_data:
+                _data = {
+                    "id": item["id"],
+                    "content": item["content"],
+                    "workspace": self.db.workspace,
+                    "tokens": item["tokens"],
+                    "chunk_order_index": item["chunk_order_index"],
+                    "full_doc_id": item["full_doc_id"],
+                    "content_vector": item["__vector__"],
+                    "status": item["status"],
+                }
+                await self.db.execute(merge_sql, _data)
+        if self.namespace == "full_docs":
+            for k, v in data.items():
+                # values.clear()
+                merge_sql = SQL_TEMPLATES["merge_doc_full"]
+                _data = {
+                    "id": k,
+                    "content": v["content"],
+                    "workspace": self.db.workspace,
+                }
+                await self.db.execute(merge_sql, _data)
+
+        if self.namespace == "llm_response_cache":
+            for mode, items in data.items():
+                for k, v in items.items():
+                    upsert_sql = SQL_TEMPLATES["upsert_llm_response_cache"]
+                    _data = {
+                        "workspace": self.db.workspace,
+                        "id": k,
+                        "original_prompt": v["original_prompt"],
+                        "return_value": v["return"],
+                        "cache_mode": mode,
+                    }
+
+                    await self.db.execute(upsert_sql, _data)
+        return None
+
+    async def change_status(self, id: str, status: str):
+        SQL = SQL_TEMPLATES["change_status"].format(table_name=N_T[self.namespace])
+        params = {"workspace": self.db.workspace, "id": id, "status": status}
+        await self.db.execute(SQL, params)
+
+    async def index_done_callback(self):
+        if self.namespace in ["full_docs", "text_chunks"]:
+            logger.info("full doc and chunk data had been saved into oracle db!")
+
+
+@dataclass
+class OracleVectorDBStorage(BaseVectorStorage):
+    # should pass db object to self.db
+    db: OracleDB = None
+    cosine_better_than_threshold: float = float(os.getenv("COSINE_THRESHOLD", "0.2"))
+
+    def __post_init__(self):
+        # Use global config value if specified, otherwise use default
+        config = self.global_config.get("vector_db_storage_cls_kwargs", {})
+        self.cosine_better_than_threshold = config.get(
+            "cosine_better_than_threshold", self.cosine_better_than_threshold
+        )
+
+    async def upsert(self, data: dict[str, dict]):
+        """向向量数据库中插入数据"""
+        pass
+
+    async def index_done_callback(self):
+        pass
+
+    #################### query method ###############
+    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:
+        """从向量数据库中查询数据"""
+        embeddings = await self.embedding_func([query])
+        embedding = embeddings[0]
+        # 转换精度
+        dtype = str(embedding.dtype).upper()
+        dimension = embedding.shape[0]
+        embedding_string = "[" + ", ".join(map(str, embedding.tolist())) + "]"
+
+        SQL = SQL_TEMPLATES[self.namespace].format(dimension=dimension, dtype=dtype)
+        params = {
+            "embedding_string": embedding_string,
+            "workspace": self.db.workspace,
+            "top_k": top_k,
+            "better_than_threshold": self.cosine_better_than_threshold,
+        }
+        # print(SQL)
+        results = await self.db.query(SQL, params=params, multirows=True)
+        # print("vector search result:",results)
+        return results
+
+
+@dataclass
+class OracleGraphStorage(BaseGraphStorage):
+    """基于Oracle的图存储模块"""
+
+    def __post_init__(self):
+        """从graphml文件加载图"""
+        self._max_batch_size = self.global_config.get("embedding_batch_num", 10)
+
+    #################### insert method ################
+
+    async def upsert_node(self, node_id: str, node_data: dict[str, str]):
+        """插入或更新节点"""
+        # print("go into upsert node method")
+        entity_name = node_id
+        entity_type = node_data["entity_type"]
+        description = node_data["description"]
+        source_id = node_data["source_id"]
+        logger.debug(f"entity_name:{entity_name}, entity_type:{entity_type}")
+
+        content = entity_name + description
+        contents = [content]
+        batches = [
+            contents[i : i + self._max_batch_size]
+            for i in range(0, len(contents), self._max_batch_size)
+        ]
+        embeddings_list = await asyncio.gather(
+            *[self.embedding_func(batch) for batch in batches]
+        )
+        embeddings = np.concatenate(embeddings_list)
+        content_vector = embeddings[0]
+        merge_sql = SQL_TEMPLATES["merge_node"]
+        data = {
+            "workspace": self.db.workspace,
+            "name": entity_name,
+            "entity_type": entity_type,
+            "description": description,
+            "source_chunk_id": source_id,
+            "content": content,
+            "content_vector": content_vector,
+        }
+        await self.db.execute(merge_sql, data)
+        # self._graph.add_node(node_id, **node_data)
+
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]
+    ):
+        """插入或更新边"""
+        # print("go into upsert edge method")
+        source_name = source_node_id
+        target_name = target_node_id
+        weight = edge_data["weight"]
+        keywords = edge_data["keywords"]
+        description = edge_data["description"]
+        source_chunk_id = edge_data["source_id"]
+        logger.debug(
+            f"source_name:{source_name}, target_name:{target_name}, keywords: {keywords}"
+        )
+
+        content = keywords + source_name + target_name + description
+        contents = [content]
+        batches = [
+            contents[i : i + self._max_batch_size]
+            for i in range(0, len(contents), self._max_batch_size)
+        ]
+        embeddings_list = await asyncio.gather(
+            *[self.embedding_func(batch) for batch in batches]
+        )
+        embeddings = np.concatenate(embeddings_list)
+        content_vector = embeddings[0]
+        merge_sql = SQL_TEMPLATES["merge_edge"]
+        data = {
+            "workspace": self.db.workspace,
+            "source_name": source_name,
+            "target_name": target_name,
+            "weight": weight,
+            "keywords": keywords,
+            "description": description,
+            "source_chunk_id": source_chunk_id,
+            "content": content,
+            "content_vector": content_vector,
+        }
+        # print(merge_sql)
+        await self.db.execute(merge_sql, data)
+        # self._graph.add_edge(source_node_id, target_node_id, **edge_data)
+
+    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:
+        """为节点生成向量"""
+        if algorithm not in self._node_embed_algorithms:
+            raise ValueError(f"Node embedding algorithm {algorithm} not supported")
+        return await self._node_embed_algorithms[algorithm]()
+
+    async def _node2vec_embed(self):
+        """为节点生成向量"""
+        from graspologic import embed
+
+        embeddings, nodes = embed.node2vec_embed(
+            self._graph,
+            **self.config["node2vec_params"],
+        )
+
+        nodes_ids = [self._graph.nodes[node_id]["id"] for node_id in nodes]
+        return embeddings, nodes_ids
+
+    async def index_done_callback(self):
+        """写入graphhml图文件"""
+        logger.info(
+            "Node and edge data had been saved into oracle db already, so nothing to do here!"
+        )
+
+    #################### query method #################
+    async def has_node(self, node_id: str) -> bool:
+        """根据节点id检查节点是否存在"""
+        SQL = SQL_TEMPLATES["has_node"]
+        params = {"workspace": self.db.workspace, "node_id": node_id}
+        # print(SQL)
+        # print(self.db.workspace, node_id)
+        res = await self.db.query(SQL, params)
+        if res:
+            # print("Node exist!",res)
+            return True
+        else:
+            # print("Node not exist!")
+            return False
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        """根据源和目标节点id检查边是否存在"""
+        SQL = SQL_TEMPLATES["has_edge"]
+        params = {
+            "workspace": self.db.workspace,
+            "source_node_id": source_node_id,
+            "target_node_id": target_node_id,
+        }
+        # print(SQL)
+        res = await self.db.query(SQL, params)
+        if res:
+            # print("Edge exist!",res)
+            return True
+        else:
+            # print("Edge not exist!")
+            return False
+
+    async def node_degree(self, node_id: str) -> int:
+        """根据节点id获取节点的度"""
+        SQL = SQL_TEMPLATES["node_degree"]
+        params = {"workspace": self.db.workspace, "node_id": node_id}
+        # print(SQL)
+        res = await self.db.query(SQL, params)
+        if res:
+            # print("Node degree",res["degree"])
+            return res["degree"]
+        else:
+            # print("Edge not exist!")
+            return 0
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        """根据源和目标节点id获取边的度"""
+        degree = await self.node_degree(src_id) + await self.node_degree(tgt_id)
+        # print("Edge degree",degree)
+        return degree
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        """根据节点id获取节点数据"""
+        SQL = SQL_TEMPLATES["get_node"]
+        params = {"workspace": self.db.workspace, "node_id": node_id}
+        # print(self.db.workspace, node_id)
+        # print(SQL)
+        res = await self.db.query(SQL, params)
+        if res:
+            # print("Get node!",self.db.workspace, node_id,res)
+            return res
+        else:
+            # print("Can't get node!",self.db.workspace, node_id)
+            return None
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        """根据源和目标节点id获取边"""
+        SQL = SQL_TEMPLATES["get_edge"]
+        params = {
+            "workspace": self.db.workspace,
+            "source_node_id": source_node_id,
+            "target_node_id": target_node_id,
+        }
+        res = await self.db.query(SQL, params)
+        if res:
+            # print("Get edge!",self.db.workspace, source_node_id, target_node_id,res[0])
+            return res
+        else:
+            # print("Edge not exist!",self.db.workspace, source_node_id, target_node_id)
+            return None
+
+    async def get_node_edges(self, source_node_id: str):
+        """根据节点id获取节点的所有边"""
+        if await self.has_node(source_node_id):
+            SQL = SQL_TEMPLATES["get_node_edges"]
+            params = {"workspace": self.db.workspace, "source_node_id": source_node_id}
+            res = await self.db.query(sql=SQL, params=params, multirows=True)
+            if res:
+                data = [(i["source_name"], i["target_name"]) for i in res]
+                # print("Get node edge!",self.db.workspace, source_node_id,data)
+                return data
+            else:
+                # print("Node Edge not exist!",self.db.workspace, source_node_id)
+                return []
+
+    async def get_all_nodes(self, limit: int):
+        """查询所有节点"""
+        SQL = SQL_TEMPLATES["get_all_nodes"]
+        params = {"workspace": self.db.workspace, "limit": str(limit)}
+        res = await self.db.query(sql=SQL, params=params, multirows=True)
+        if res:
+            return res
+
+    async def get_all_edges(self, limit: int):
+        """查询所有边"""
+        SQL = SQL_TEMPLATES["get_all_edges"]
+        params = {"workspace": self.db.workspace, "limit": str(limit)}
+        res = await self.db.query(sql=SQL, params=params, multirows=True)
+        if res:
+            return res
+
+    async def get_statistics(self):
+        SQL = SQL_TEMPLATES["get_statistics"]
+        params = {"workspace": self.db.workspace}
+        res = await self.db.query(sql=SQL, params=params, multirows=True)
+        if res:
+            return res
+
+
+N_T = {
+    "full_docs": "LIGHTRAG_DOC_FULL",
+    "text_chunks": "LIGHTRAG_DOC_CHUNKS",
+    "chunks": "LIGHTRAG_DOC_CHUNKS",
+    "entities": "LIGHTRAG_GRAPH_NODES",
+    "relationships": "LIGHTRAG_GRAPH_EDGES",
+}
+
+TABLES = {
+    "LIGHTRAG_DOC_FULL": {
+        "ddl": """CREATE TABLE LIGHTRAG_DOC_FULL (
+                    id varchar(256),
+                    workspace varchar(1024),
+                    doc_name varchar(1024),
+                    content CLOB,
+                    meta JSON,
+                    content_summary varchar(1024),
+                    content_length NUMBER,
+                    status varchar(256),
+                    chunks_count NUMBER,
+                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    updatetime TIMESTAMP DEFAULT NULL,
+                    error varchar(4096)
+                    )"""
+    },
+    "LIGHTRAG_DOC_CHUNKS": {
+        "ddl": """CREATE TABLE LIGHTRAG_DOC_CHUNKS (
+                    id varchar(256),
+                    workspace varchar(1024),
+                    full_doc_id varchar(256),
+                    status varchar(256),
+                    chunk_order_index NUMBER,
+                    tokens NUMBER,
+                    content CLOB,
+                    content_vector VECTOR,
+                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    updatetime TIMESTAMP DEFAULT NULL
+                    )"""
+    },
+    "LIGHTRAG_GRAPH_NODES": {
+        "ddl": """CREATE TABLE LIGHTRAG_GRAPH_NODES (
+                    id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
+                    workspace varchar(1024),
+                    name varchar(2048),
+                    entity_type varchar(1024),
+                    description CLOB,
+                    source_chunk_id varchar(256),
+                    content CLOB,
+                    content_vector VECTOR,
+                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    updatetime TIMESTAMP DEFAULT NULL
+                    )"""
+    },
+    "LIGHTRAG_GRAPH_EDGES": {
+        "ddl": """CREATE TABLE LIGHTRAG_GRAPH_EDGES (
+                    id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
+                    workspace varchar(1024),
+                    source_name varchar(2048),
+                    target_name varchar(2048),
+                    weight NUMBER,
+                    keywords CLOB,
+                    description CLOB,
+                    source_chunk_id varchar(256),
+                    content CLOB,
+                    content_vector VECTOR,
+                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    updatetime TIMESTAMP DEFAULT NULL
+                    )"""
+    },
+    "LIGHTRAG_LLM_CACHE": {
+        "ddl": """CREATE TABLE LIGHTRAG_LLM_CACHE (
+                    id varchar(256) PRIMARY KEY,
+                    workspace varchar(1024),
+                    cache_mode varchar(256),
+                    model_name varchar(256),
+                    original_prompt clob,
+                    return_value clob,
+                    embedding CLOB,
+                    embedding_shape NUMBER,
+                    embedding_min NUMBER,
+                    embedding_max NUMBER,
+                    createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    updatetime TIMESTAMP DEFAULT NULL
+                    )"""
+    },
+    "LIGHTRAG_GRAPH": {
+        "ddl": """CREATE OR REPLACE PROPERTY GRAPH lightrag_graph
+                VERTEX TABLES (
+                    lightrag_graph_nodes KEY (id)
+                        LABEL entity
+                        PROPERTIES (id,workspace,name) -- ,entity_type,description,source_chunk_id)
+                )
+                EDGE TABLES (
+                    lightrag_graph_edges KEY (id)
+                        SOURCE KEY (source_name) REFERENCES lightrag_graph_nodes(name)
+                        DESTINATION KEY (target_name) REFERENCES lightrag_graph_nodes(name)
+                        LABEL  has_relation
+                        PROPERTIES (id,workspace,source_name,target_name) -- ,weight, keywords,description,source_chunk_id)
+                ) OPTIONS(ALLOW MIXED PROPERTY TYPES)"""
+    },
+}
+
+
+SQL_TEMPLATES = {
+    # SQL for KVStorage
+    "get_by_id_full_docs": "select ID,content,status from LIGHTRAG_DOC_FULL where workspace=:workspace and ID=:id",
+    "get_by_id_text_chunks": "select ID,TOKENS,content,CHUNK_ORDER_INDEX,FULL_DOC_ID,status from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and ID=:id",
+    "get_by_id_llm_response_cache": """SELECT id, original_prompt, NVL(return_value, '') as "return", cache_mode as "mode"
+        FROM LIGHTRAG_LLM_CACHE WHERE workspace=:workspace AND id=:id""",
+    "get_by_mode_id_llm_response_cache": """SELECT id, original_prompt, NVL(return_value, '') as "return", cache_mode as "mode"
+        FROM LIGHTRAG_LLM_CACHE WHERE workspace=:workspace AND cache_mode=:cache_mode AND id=:id""",
+    "get_by_ids_llm_response_cache": """SELECT id, original_prompt, NVL(return_value, '') as "return", cache_mode as "mode"
+        FROM LIGHTRAG_LLM_CACHE WHERE workspace=:workspace  AND id IN ({ids})""",
+    "get_by_ids_full_docs": "select t.*,createtime as created_at from LIGHTRAG_DOC_FULL t where workspace=:workspace and ID in ({ids})",
+    "get_by_ids_text_chunks": "select ID,TOKENS,content,CHUNK_ORDER_INDEX,FULL_DOC_ID  from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and ID in ({ids})",
+    "get_by_status_ids_full_docs": "select id,status from LIGHTRAG_DOC_FULL t where workspace=:workspace AND status=:status and ID in ({ids})",
+    "get_by_status_ids_text_chunks": "select id,status from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and status=:status ID in ({ids})",
+    "get_by_status_full_docs": "select id,status from LIGHTRAG_DOC_FULL t where workspace=:workspace AND status=:status",
+    "get_by_status_text_chunks": "select id,status from LIGHTRAG_DOC_CHUNKS where workspace=:workspace and status=:status",
+    "filter_keys": "select id from {table_name} where workspace=:workspace and id in ({ids})",
+    "change_status": "update {table_name} set status=:status,updatetime=SYSDATE where workspace=:workspace and id=:id",
+    "merge_doc_full": """MERGE INTO LIGHTRAG_DOC_FULL a
+        USING DUAL
+        ON (a.id = :id and a.workspace = :workspace)
+        WHEN NOT MATCHED THEN
+        INSERT(id,content,workspace) values(:id,:content,:workspace)""",
+    "merge_chunk": """MERGE INTO LIGHTRAG_DOC_CHUNKS
+        USING DUAL
+        ON (id = :id and workspace = :workspace)
+        WHEN NOT MATCHED THEN INSERT
+            (id,content,workspace,tokens,chunk_order_index,full_doc_id,content_vector,status)
+            values (:id,:content,:workspace,:tokens,:chunk_order_index,:full_doc_id,:content_vector,:status) """,
+    "upsert_llm_response_cache": """MERGE INTO LIGHTRAG_LLM_CACHE a
+        USING DUAL
+        ON (a.id = :id)
+        WHEN NOT MATCHED THEN
+        INSERT (workspace,id,original_prompt,return_value,cache_mode)
+            VALUES (:workspace,:id,:original_prompt,:return_value,:cache_mode)
+        WHEN MATCHED THEN UPDATE
+            SET original_prompt = :original_prompt,
+            return_value = :return_value,
+            cache_mode = :cache_mode,
+            updatetime = SYSDATE""",
+    # SQL for VectorStorage
+    "entities": """SELECT name as entity_name FROM
+        (SELECT id,name,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance
+        FROM LIGHTRAG_GRAPH_NODES WHERE workspace=:workspace)
+        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY""",
+    "relationships": """SELECT source_name as src_id, target_name as tgt_id FROM
+        (SELECT id,source_name,target_name,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance
+        FROM LIGHTRAG_GRAPH_EDGES WHERE workspace=:workspace)
+        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY""",
+    "chunks": """SELECT id FROM
+        (SELECT id,VECTOR_DISTANCE(content_vector,vector(:embedding_string,{dimension},{dtype}),COSINE) as distance
+        FROM LIGHTRAG_DOC_CHUNKS WHERE workspace=:workspace)
+        WHERE distance>:better_than_threshold ORDER BY distance ASC FETCH FIRST :top_k ROWS ONLY""",
+    # SQL for GraphStorage
+    "has_node": """SELECT * FROM GRAPH_TABLE (lightrag_graph
+        MATCH (a)
+        WHERE a.workspace=:workspace AND a.name=:node_id
+        COLUMNS (a.name))""",
+    "has_edge": """SELECT * FROM GRAPH_TABLE (lightrag_graph
+        MATCH (a) -[e]-> (b)
+        WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace
+        AND a.name=:source_node_id AND b.name=:target_node_id
+        COLUMNS (e.source_name,e.target_name)  )""",
+    "node_degree": """SELECT count(1) as degree FROM GRAPH_TABLE (lightrag_graph
+        MATCH (a)-[e]->(b)
+        WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace
+        AND a.name=:node_id or b.name = :node_id
+        COLUMNS (a.name))""",
+    "get_node": """SELECT t1.name,t2.entity_type,t2.source_chunk_id as source_id,NVL(t2.description,'') AS description
+        FROM GRAPH_TABLE (lightrag_graph
+        MATCH (a)
+        WHERE a.workspace=:workspace AND a.name=:node_id
+        COLUMNS (a.name)
+        ) t1 JOIN LIGHTRAG_GRAPH_NODES t2 on t1.name=t2.name
+        WHERE t2.workspace=:workspace""",
+    "get_edge": """SELECT t1.source_id,t2.weight,t2.source_chunk_id as source_id,t2.keywords,
+        NVL(t2.description,'') AS description,NVL(t2.KEYWORDS,'') AS keywords
+        FROM GRAPH_TABLE (lightrag_graph
+        MATCH (a)-[e]->(b)
+        WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace
+        AND a.name=:source_node_id and b.name = :target_node_id
+        COLUMNS (e.id,a.name as source_id)
+        ) t1 JOIN LIGHTRAG_GRAPH_EDGES t2 on t1.id=t2.id""",
+    "get_node_edges": """SELECT source_name,target_name
+            FROM GRAPH_TABLE (lightrag_graph
+            MATCH (a)-[e]->(b)
+            WHERE e.workspace=:workspace and a.workspace=:workspace and b.workspace=:workspace
+            AND a.name=:source_node_id
+            COLUMNS (a.name as source_name,b.name as target_name))""",
+    "merge_node": """MERGE INTO LIGHTRAG_GRAPH_NODES a
+                    USING DUAL
+                    ON (a.workspace=:workspace and a.name=:name)
+                WHEN NOT MATCHED THEN
+                    INSERT(workspace,name,entity_type,description,source_chunk_id,content,content_vector)
+                    values (:workspace,:name,:entity_type,:description,:source_chunk_id,:content,:content_vector)
+                WHEN MATCHED THEN
+                    UPDATE SET
+                    entity_type=:entity_type,description=:description,source_chunk_id=:source_chunk_id,content=:content,content_vector=:content_vector,updatetime=SYSDATE""",
+    "merge_edge": """MERGE INTO LIGHTRAG_GRAPH_EDGES a
+                    USING DUAL
+                    ON (a.workspace=:workspace and a.source_name=:source_name and a.target_name=:target_name)
+                WHEN NOT MATCHED THEN
+                    INSERT(workspace,source_name,target_name,weight,keywords,description,source_chunk_id,content,content_vector)
+                    values (:workspace,:source_name,:target_name,:weight,:keywords,:description,:source_chunk_id,:content,:content_vector)
+                WHEN MATCHED THEN
+                    UPDATE SET
+                    weight=:weight,keywords=:keywords,description=:description,source_chunk_id=:source_chunk_id,content=:content,content_vector=:content_vector,updatetime=SYSDATE""",
+    "get_all_nodes": """WITH t0 AS (
+                        SELECT name AS id, entity_type AS label, entity_type, description,
+                            '["' || replace(source_chunk_id, '<SEP>', '","') || '"]'     source_chunk_ids
+                        FROM lightrag_graph_nodes
+                        WHERE workspace = :workspace
+                        ORDER BY createtime DESC fetch first :limit rows only
+                    ), t1 AS (
+                        SELECT t0.id, source_chunk_id
+                        FROM t0, JSON_TABLE ( source_chunk_ids, '$[*]' COLUMNS ( source_chunk_id PATH '$' ) )
+                    ), t2 AS (
+                        SELECT t1.id, LISTAGG(t2.content, '\n') content
+                        FROM t1 LEFT JOIN lightrag_doc_chunks t2 ON t1.source_chunk_id = t2.id
+                        GROUP BY t1.id
+                    )
+                    SELECT t0.id, label, entity_type, description, t2.content
+                    FROM t0 LEFT JOIN t2 ON t0.id = t2.id""",
+    "get_all_edges": """SELECT t1.id,t1.keywords as label,t1.keywords, t1.source_name as source, t1.target_name as target,
+                t1.weight,t1.DESCRIPTION,t2.content
+                FROM LIGHTRAG_GRAPH_EDGES t1
+                LEFT JOIN LIGHTRAG_DOC_CHUNKS t2 on t1.source_chunk_id=t2.id
+                WHERE t1.workspace=:workspace
+                order by t1.CREATETIME DESC
+                fetch first :limit rows only""",
+    "get_statistics": """select  count(distinct CASE WHEN type='node' THEN id END) as nodes_count,
+                count(distinct CASE WHEN type='edge' THEN id END) as edges_count
+                FROM (
+                select 'node' as type, id FROM GRAPH_TABLE (lightrag_graph
+                    MATCH (a) WHERE a.workspace=:workspace columns(a.name as id))
+                UNION
+                select 'edge' as type, TO_CHAR(id) id FROM GRAPH_TABLE (lightrag_graph
+                    MATCH (a)-[e]->(b) WHERE e.workspace=:workspace columns(e.id))
+                )""",
+}
diff --git a/test/minirag/minirag/kg/postgres_impl.py b/test/minirag/minirag/kg/postgres_impl.py
new file mode 100755
index 0000000..b315abc
--- /dev/null
+++ b/test/minirag/minirag/kg/postgres_impl.py
@@ -0,0 +1,1182 @@
+import asyncio
+import inspect
+import json
+import os
+import time
+from dataclasses import dataclass
+from typing import Union, List, Dict, Set, Any, Tuple
+import numpy as np
+
+import pipmaster as pm
+
+if not pm.is_installed("asyncpg"):
+    pm.install("asyncpg")
+
+import asyncpg
+import sys
+from tqdm.asyncio import tqdm as tqdm_async
+from tenacity import (
+    retry,
+    retry_if_exception_type,
+    stop_after_attempt,
+    wait_exponential,
+)
+
+from ..utils import logger
+from ..base import (
+    BaseKVStorage,
+    BaseVectorStorage,
+    DocStatusStorage,
+    DocStatus,
+    DocProcessingStatus,
+    BaseGraphStorage,
+)
+
+if sys.platform.startswith("win"):
+    import asyncio.windows_events
+
+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
+
+
+class PostgreSQLDB:
+    def __init__(self, config, **kwargs):
+        self.pool = None
+        self.host = config.get("host", "localhost")
+        self.port = config.get("port", 5432)
+        self.user = config.get("user", "postgres")
+        self.password = config.get("password", None)
+        self.database = config.get("database", "postgres")
+        self.workspace = config.get("workspace", "default")
+        self.max = 12
+        self.increment = 1
+        logger.info(f"Using the label {self.workspace} for PostgreSQL as identifier")
+
+        if self.user is None or self.password is None or self.database is None:
+            raise ValueError(
+                "Missing database user, password, or database in addon_params"
+            )
+
+    async def initdb(self):
+        try:
+            self.pool = await asyncpg.create_pool(
+                user=self.user,
+                password=self.password,
+                database=self.database,
+                host=self.host,
+                port=self.port,
+                min_size=1,
+                max_size=self.max,
+            )
+
+            logger.info(
+                f"Connected to PostgreSQL database at {self.host}:{self.port}/{self.database}"
+            )
+        except Exception as e:
+            logger.error(
+                f"Failed to connect to PostgreSQL database at {self.host}:{self.port}/{self.database}"
+            )
+            logger.error(f"PostgreSQL database error: {e}")
+            raise
+
+    async def check_tables(self):
+        for k, v in TABLES.items():
+            try:
+                await self.query("SELECT 1 FROM {k} LIMIT 1".format(k=k))
+            except Exception as e:
+                logger.error(f"Failed to check table {k} in PostgreSQL database")
+                logger.error(f"PostgreSQL database error: {e}")
+                try:
+                    await self.execute(v["ddl"])
+                    logger.info(f"Created table {k} in PostgreSQL database")
+                except Exception as e:
+                    logger.error(f"Failed to create table {k} in PostgreSQL database")
+                    logger.error(f"PostgreSQL database error: {e}")
+
+        logger.info("Finished checking all tables in PostgreSQL database")
+
+    async def query(
+        self,
+        sql: str,
+        params: dict = None,
+        multirows: bool = False,
+        for_age: bool = False,
+        graph_name: str = None,
+    ) -> Union[dict, None, list[dict]]:
+        async with self.pool.acquire() as connection:
+            try:
+                if for_age:
+                    await PostgreSQLDB._prerequisite(connection, graph_name)
+                if params:
+                    rows = await connection.fetch(sql, *params.values())
+                else:
+                    rows = await connection.fetch(sql)
+
+                if multirows:
+                    if rows:
+                        columns = [col for col in rows[0].keys()]
+                        data = [dict(zip(columns, row)) for row in rows]
+                    else:
+                        data = []
+                else:
+                    if rows:
+                        columns = rows[0].keys()
+                        data = dict(zip(columns, rows[0]))
+                    else:
+                        data = None
+                return data
+            except Exception as e:
+                logger.error(f"PostgreSQL database error: {e}")
+                print(sql)
+                print(params)
+                raise
+
+    async def execute(
+        self,
+        sql: str,
+        data: Union[list, dict] = None,
+        for_age: bool = False,
+        graph_name: str = None,
+        upsert: bool = False,
+    ):
+        try:
+            async with self.pool.acquire() as connection:
+                if for_age:
+                    await PostgreSQLDB._prerequisite(connection, graph_name)
+
+                if data is None:
+                    await connection.execute(sql)
+                else:
+                    await connection.execute(sql, *data.values())
+        except (
+            asyncpg.exceptions.UniqueViolationError,
+            asyncpg.exceptions.DuplicateTableError,
+        ) as e:
+            if upsert:
+                print("Key value duplicate, but upsert succeeded.")
+            else:
+                logger.error(f"Upsert error: {e}")
+        except Exception as e:
+            logger.error(f"PostgreSQL database error: {e.__class__} - {e}")
+            print(sql)
+            print(data)
+            raise
+
+    @staticmethod
+    async def _prerequisite(conn: asyncpg.Connection, graph_name: str):
+        try:
+            await conn.execute('SET search_path = ag_catalog, "$user", public')
+            await conn.execute(f"""select create_graph('{graph_name}')""")
+        except (
+            asyncpg.exceptions.InvalidSchemaNameError,
+            asyncpg.exceptions.UniqueViolationError,
+        ):
+            pass
+
+
+@dataclass
+class PGKVStorage(BaseKVStorage):
+    db: PostgreSQLDB = None
+
+    def __post_init__(self):
+        self._max_batch_size = self.global_config["embedding_batch_num"]
+
+    ################ QUERY METHODS ################
+
+    async def get_by_id(self, id: str) -> Union[dict, None]:
+        """Get doc_full data by id."""
+        sql = SQL_TEMPLATES["get_by_id_" + self.namespace]
+        params = {"workspace": self.db.workspace, "id": id}
+        if "llm_response_cache" == self.namespace:
+            array_res = await self.db.query(sql, params, multirows=True)
+            res = {}
+            for row in array_res:
+                res[row["id"]] = row
+        else:
+            res = await self.db.query(sql, params)
+        if res:
+            return res
+        else:
+            return None
+
+    async def get_by_mode_and_id(self, mode: str, id: str) -> Union[dict, None]:
+        """Specifically for llm_response_cache."""
+        sql = SQL_TEMPLATES["get_by_mode_id_" + self.namespace]
+        params = {"workspace": self.db.workspace, mode: mode, "id": id}
+        if "llm_response_cache" == self.namespace:
+            array_res = await self.db.query(sql, params, multirows=True)
+            res = {}
+            for row in array_res:
+                res[row["id"]] = row
+            return res
+        else:
+            return None
+
+    # Query by id
+    async def get_by_ids(self, ids: List[str], fields=None) -> Union[List[dict], None]:
+        """Get doc_chunks data by id"""
+        sql = SQL_TEMPLATES["get_by_ids_" + self.namespace].format(
+            ids=",".join([f"'{id}'" for id in ids])
+        )
+        params = {"workspace": self.db.workspace}
+        if "llm_response_cache" == self.namespace:
+            array_res = await self.db.query(sql, params, multirows=True)
+            modes = set()
+            dict_res: dict[str, dict] = {}
+            for row in array_res:
+                modes.add(row["mode"])
+            for mode in modes:
+                if mode not in dict_res:
+                    dict_res[mode] = {}
+            for row in array_res:
+                dict_res[row["mode"]][row["id"]] = row
+            res = [{k: v} for k, v in dict_res.items()]
+        else:
+            res = await self.db.query(sql, params, multirows=True)
+        if res:
+            return res
+        else:
+            return None
+
+    async def all_keys(self) -> list[dict]:
+        if "llm_response_cache" == self.namespace:
+            sql = "select workspace,mode,id from lightrag_llm_cache"
+            res = await self.db.query(sql, multirows=True)
+            return res
+        else:
+            logger.error(
+                f"all_keys is only implemented for llm_response_cache, not for {self.namespace}"
+            )
+
+    async def filter_keys(self, keys: List[str]) -> Set[str]:
+        """Filter out duplicated content"""
+        sql = SQL_TEMPLATES["filter_keys"].format(
+            table_name=NAMESPACE_TABLE_MAP[self.namespace],
+            ids=",".join([f"'{id}'" for id in keys]),
+        )
+        params = {"workspace": self.db.workspace}
+        try:
+            res = await self.db.query(sql, params, multirows=True)
+            if res:
+                exist_keys = [key["id"] for key in res]
+            else:
+                exist_keys = []
+            data = set([s for s in keys if s not in exist_keys])
+            return data
+        except Exception as e:
+            logger.error(f"PostgreSQL database error: {e}")
+            print(sql)
+            print(params)
+
+    ################ INSERT METHODS ################
+    async def upsert(self, data: Dict[str, dict]):
+        if self.namespace == "text_chunks":
+            pass
+        elif self.namespace == "full_docs":
+            for k, v in data.items():
+                upsert_sql = SQL_TEMPLATES["upsert_doc_full"]
+                _data = {
+                    "id": k,
+                    "content": v["content"],
+                    "workspace": self.db.workspace,
+                }
+                await self.db.execute(upsert_sql, _data)
+        elif self.namespace == "llm_response_cache":
+            for mode, items in data.items():
+                for k, v in items.items():
+                    upsert_sql = SQL_TEMPLATES["upsert_llm_response_cache"]
+                    _data = {
+                        "workspace": self.db.workspace,
+                        "id": k,
+                        "original_prompt": v["original_prompt"],
+                        "return_value": v["return"],
+                        "mode": mode,
+                    }
+
+                    await self.db.execute(upsert_sql, _data)
+
+    async def index_done_callback(self):
+        if self.namespace in ["full_docs", "text_chunks"]:
+            logger.info("full doc and chunk data had been saved into postgresql db!")
+
+
+@dataclass
+class PGVectorStorage(BaseVectorStorage):
+    cosine_better_than_threshold: float = float(os.getenv("COSINE_THRESHOLD", "0.2"))
+    db: PostgreSQLDB = None
+
+    def __post_init__(self):
+        self._max_batch_size = self.global_config["embedding_batch_num"]
+        # Use global config value if specified, otherwise use default
+        config = self.global_config.get("vector_db_storage_cls_kwargs", {})
+        self.cosine_better_than_threshold = config.get(
+            "cosine_better_than_threshold", self.cosine_better_than_threshold
+        )
+
+    def _upsert_chunks(self, item: dict):
+        try:
+            upsert_sql = SQL_TEMPLATES["upsert_chunk"]
+            data = {
+                "workspace": self.db.workspace,
+                "id": item["__id__"],
+                "tokens": item["tokens"],
+                "chunk_order_index": item["chunk_order_index"],
+                "full_doc_id": item["full_doc_id"],
+                "content": item["content"],
+                "content_vector": json.dumps(item["__vector__"].tolist()),
+            }
+        except Exception as e:
+            logger.error(f"Error to prepare upsert sql: {e}")
+            print(item)
+            raise e
+        return upsert_sql, data
+
+    def _upsert_entities(self, item: dict):
+        upsert_sql = SQL_TEMPLATES["upsert_entity"]
+        data = {
+            "workspace": self.db.workspace,
+            "id": item["__id__"],
+            "entity_name": item["entity_name"],
+            "content": item["content"],
+            "content_vector": json.dumps(item["__vector__"].tolist()),
+        }
+        return upsert_sql, data
+
+    def _upsert_relationships(self, item: dict):
+        upsert_sql = SQL_TEMPLATES["upsert_relationship"]
+        data = {
+            "workspace": self.db.workspace,
+            "id": item["__id__"],
+            "source_id": item["src_id"],
+            "target_id": item["tgt_id"],
+            "content": item["content"],
+            "content_vector": json.dumps(item["__vector__"].tolist()),
+        }
+        return upsert_sql, data
+
+    async def upsert(self, data: Dict[str, dict]):
+        logger.info(f"Inserting {len(data)} vectors to {self.namespace}")
+        if not len(data):
+            logger.warning("You insert an empty data to vector DB")
+            return []
+        current_time = time.time()
+        list_data = [
+            {
+                "__id__": k,
+                "__created_at__": current_time,
+                **{k1: v1 for k1, v1 in v.items()},
+            }
+            for k, v in data.items()
+        ]
+        contents = [v["content"] for v in data.values()]
+        batches = [
+            contents[i : i + self._max_batch_size]
+            for i in range(0, len(contents), self._max_batch_size)
+        ]
+
+        async def wrapped_task(batch):
+            result = await self.embedding_func(batch)
+            pbar.update(1)
+            return result
+
+        embedding_tasks = [wrapped_task(batch) for batch in batches]
+        pbar = tqdm_async(
+            total=len(embedding_tasks), desc="Generating embeddings", unit="batch"
+        )
+        embeddings_list = await asyncio.gather(*embedding_tasks)
+
+        embeddings = np.concatenate(embeddings_list)
+        for i, d in enumerate(list_data):
+            d["__vector__"] = embeddings[i]
+        for item in list_data:
+            if self.namespace == "chunks":
+                upsert_sql, data = self._upsert_chunks(item)
+            elif self.namespace == "entities":
+                upsert_sql, data = self._upsert_entities(item)
+            elif self.namespace == "relationships":
+                upsert_sql, data = self._upsert_relationships(item)
+            else:
+                raise ValueError(f"{self.namespace} is not supported")
+
+            await self.db.execute(upsert_sql, data)
+
+    async def index_done_callback(self):
+        logger.info("vector data had been saved into postgresql db!")
+
+    #################### query method ###############
+    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:
+        """从向量数据库中查询数据"""
+        embeddings = await self.embedding_func([query])
+        embedding = embeddings[0]
+        embedding_string = ",".join(map(str, embedding))
+
+        sql = SQL_TEMPLATES[self.namespace].format(embedding_string=embedding_string)
+        params = {
+            "workspace": self.db.workspace,
+            "better_than_threshold": self.cosine_better_than_threshold,
+            "top_k": top_k,
+        }
+        results = await self.db.query(sql, params=params, multirows=True)
+        return results
+
+
+@dataclass
+class PGDocStatusStorage(DocStatusStorage):
+    """PostgreSQL implementation of document status storage"""
+
+    db: PostgreSQLDB = None
+
+    def __post_init__(self):
+        pass
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        """Return keys that don't exist in storage"""
+        keys = ",".join([f"'{_id}'" for _id in data])
+        sql = (
+            f"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({keys})"
+        )
+        result = await self.db.query(sql, {"workspace": self.db.workspace}, True)
+        # The result is like [{'id': 'id1'}, {'id': 'id2'}, ...].
+        if result is None:
+            return set(data)
+        else:
+            existed = set([element["id"] for element in result])
+            return set(data) - existed
+
+    async def get_status_counts(self) -> Dict[str, int]:
+        """Get counts of documents in each status"""
+        sql = """SELECT status as "status", COUNT(1) as "count"
+                   FROM LIGHTRAG_DOC_STATUS
+                  where workspace=$1 GROUP BY STATUS
+                 """
+        result = await self.db.query(sql, {"workspace": self.db.workspace}, True)
+        # Result is like [{'status': 'PENDING', 'count': 1}, {'status': 'PROCESSING', 'count': 2}, ...]
+        counts = {}
+        for doc in result:
+            counts[doc["status"]] = doc["count"]
+        return counts
+
+    async def get_docs_by_status(
+        self, status: DocStatus
+    ) -> Dict[str, DocProcessingStatus]:
+        """Get all documents by status"""
+        sql = "select * from LIGHTRAG_DOC_STATUS where workspace=$1 and status=$1"
+        params = {"workspace": self.db.workspace, "status": status}
+        result = await self.db.query(sql, params, True)
+        # Result is like [{'id': 'id1', 'status': 'PENDING', 'updated_at': '2023-07-01 00:00:00'}, {'id': 'id2', 'status': 'PENDING', 'updated_at': '2023-07-01 00:00:00'}, ...]
+        # Converting to be a dict
+        return {
+            element["id"]: DocProcessingStatus(
+                content_summary=element["content_summary"],
+                content_length=element["content_length"],
+                status=element["status"],
+                created_at=element["created_at"],
+                updated_at=element["updated_at"],
+                chunks_count=element["chunks_count"],
+            )
+            for element in result
+        }
+
+    async def get_failed_docs(self) -> Dict[str, DocProcessingStatus]:
+        """Get all failed documents"""
+        return await self.get_docs_by_status(DocStatus.FAILED)
+
+    async def get_pending_docs(self) -> Dict[str, DocProcessingStatus]:
+        """Get all pending documents"""
+        return await self.get_docs_by_status(DocStatus.PENDING)
+
+    async def index_done_callback(self):
+        """Save data after indexing, but for PostgreSQL, we already saved them during the upsert stage, so no action to take here"""
+        logger.info("Doc status had been saved into postgresql db!")
+
+    async def upsert(self, data: dict[str, dict]):
+        """Update or insert document status
+
+        Args:
+            data: Dictionary of document IDs and their status data
+        """
+        sql = """insert into LIGHTRAG_DOC_STATUS(workspace,id,content_summary,content_length,chunks_count,status)
+                 values($1,$2,$3,$4,$5,$6)
+                  on conflict(id,workspace) do update set
+                  content_summary = EXCLUDED.content_summary,
+                  content_length = EXCLUDED.content_length,
+                  chunks_count = EXCLUDED.chunks_count,
+                  status = EXCLUDED.status,
+                  updated_at = CURRENT_TIMESTAMP"""
+        for k, v in data.items():
+            # chunks_count is optional
+            await self.db.execute(
+                sql,
+                {
+                    "workspace": self.db.workspace,
+                    "id": k,
+                    "content_summary": v["content_summary"],
+                    "content_length": v["content_length"],
+                    "chunks_count": v["chunks_count"] if "chunks_count" in v else -1,
+                    "status": v["status"],
+                },
+            )
+        return data
+
+
+class PGGraphQueryException(Exception):
+    """Exception for the AGE queries."""
+
+    def __init__(self, exception: Union[str, Dict]) -> None:
+        if isinstance(exception, dict):
+            self.message = exception["message"] if "message" in exception else "unknown"
+            self.details = exception["details"] if "details" in exception else "unknown"
+        else:
+            self.message = exception
+            self.details = "unknown"
+
+    def get_message(self) -> str:
+        return self.message
+
+    def get_details(self) -> Any:
+        return self.details
+
+
+@dataclass
+class PGGraphStorage(BaseGraphStorage):
+    db: PostgreSQLDB = None
+
+    @staticmethod
+    def load_nx_graph(file_name):
+        print("no preloading of graph with AGE in production")
+
+    def __init__(self, namespace, global_config, embedding_func):
+        super().__init__(
+            namespace=namespace,
+            global_config=global_config,
+            embedding_func=embedding_func,
+        )
+        self.graph_name = os.environ["AGE_GRAPH_NAME"]
+        self._node_embed_algorithms = {
+            "node2vec": self._node2vec_embed,
+        }
+
+    async def index_done_callback(self):
+        print("KG successfully indexed.")
+
+    @staticmethod
+    def _record_to_dict(record: asyncpg.Record) -> Dict[str, Any]:
+        """
+        Convert a record returned from an age query to a dictionary
+
+        Args:
+            record (): a record from an age query result
+
+        Returns:
+            Dict[str, Any]: a dictionary representation of the record where
+                the dictionary key is the field name and the value is the
+                value converted to a python type
+        """
+        # result holder
+        d = {}
+
+        # prebuild a mapping of vertex_id to vertex mappings to be used
+        # later to build edges
+        vertices = {}
+        for k in record.keys():
+            v = record[k]
+            # agtype comes back '{key: value}::type' which must be parsed
+            if isinstance(v, str) and "::" in v:
+                dtype = v.split("::")[-1]
+                v = v.split("::")[0]
+                if dtype == "vertex":
+                    vertex = json.loads(v)
+                    vertices[vertex["id"]] = vertex.get("properties")
+
+        # iterate returned fields and parse appropriately
+        for k in record.keys():
+            v = record[k]
+            if isinstance(v, str) and "::" in v:
+                dtype = v.split("::")[-1]
+                v = v.split("::")[0]
+            else:
+                dtype = ""
+
+            if dtype == "vertex":
+                vertex = json.loads(v)
+                field = vertex.get("properties")
+                if not field:
+                    field = {}
+                field["label"] = PGGraphStorage._decode_graph_label(field["node_id"])
+                d[k] = field
+            # convert edge from id-label->id by replacing id with node information
+            # we only do this if the vertex was also returned in the query
+            # this is an attempt to be consistent with neo4j implementation
+            elif dtype == "edge":
+                edge = json.loads(v)
+                d[k] = (
+                    vertices.get(edge["start_id"], {}),
+                    edge[
+                        "label"
+                    ],  # we don't use decode_graph_label(), since edge label is always "DIRECTED"
+                    vertices.get(edge["end_id"], {}),
+                )
+            else:
+                d[k] = json.loads(v) if isinstance(v, str) else v
+
+        return d
+
+    @staticmethod
+    def _format_properties(
+        properties: Dict[str, Any], _id: Union[str, None] = None
+    ) -> str:
+        """
+        Convert a dictionary of properties to a string representation that
+        can be used in a cypher query insert/merge statement.
+
+        Args:
+            properties (Dict[str,str]): a dictionary containing node/edge properties
+            _id (Union[str, None]): the id of the node or None if none exists
+
+        Returns:
+            str: the properties dictionary as a properly formatted string
+        """
+        props = []
+        # wrap property key in backticks to escape
+        for k, v in properties.items():
+            prop = f"`{k}`: {json.dumps(v)}"
+            props.append(prop)
+        if _id is not None and "id" not in properties:
+            props.append(
+                f"id: {json.dumps(_id)}" if isinstance(_id, str) else f"id: {_id}"
+            )
+        return "{" + ", ".join(props) + "}"
+
+    @staticmethod
+    def _encode_graph_label(label: str) -> str:
+        """
+        Since AGE supports only alphanumerical labels, we will encode generic label as HEX string
+
+        Args:
+            label (str): the original label
+
+        Returns:
+            str: the encoded label
+        """
+        return "x" + label.encode().hex()
+
+    @staticmethod
+    def _decode_graph_label(encoded_label: str) -> str:
+        """
+        Since AGE supports only alphanumerical labels, we will encode generic label as HEX string
+
+        Args:
+            encoded_label (str): the encoded label
+
+        Returns:
+            str: the decoded label
+        """
+        return bytes.fromhex(encoded_label.removeprefix("x")).decode()
+
+    @staticmethod
+    def _get_col_name(field: str, idx: int) -> str:
+        """
+        Convert a cypher return field to a pgsql select field
+        If possible keep the cypher column name, but create a generic name if necessary
+
+        Args:
+            field (str): a return field from a cypher query to be formatted for pgsql
+            idx (int): the position of the field in the return statement
+
+        Returns:
+            str: the field to be used in the pgsql select statement
+        """
+        # remove white space
+        field = field.strip()
+        # if an alias is provided for the field, use it
+        if " as " in field:
+            return field.split(" as ")[-1].strip()
+        # if the return value is an unnamed primitive, give it a generic name
+        if field.isnumeric() or field in ("true", "false", "null"):
+            return f"column_{idx}"
+        # otherwise return the value stripping out some common special chars
+        return field.replace("(", "_").replace(")", "")
+
+    async def _query(
+        self, query: str, readonly: bool = True, upsert: bool = False
+    ) -> List[Dict[str, Any]]:
+        """
+        Query the graph by taking a cypher query, converting it to an
+        age compatible query, executing it and converting the result
+
+        Args:
+            query (str): a cypher query to be executed
+            params (dict): parameters for the query
+
+        Returns:
+            List[Dict[str, Any]]: a list of dictionaries containing the result set
+        """
+        # convert cypher query to pgsql/age query
+        wrapped_query = query
+
+        # execute the query, rolling back on an error
+        try:
+            if readonly:
+                data = await self.db.query(
+                    wrapped_query,
+                    multirows=True,
+                    for_age=True,
+                    graph_name=self.graph_name,
+                )
+            else:
+                data = await self.db.execute(
+                    wrapped_query,
+                    for_age=True,
+                    graph_name=self.graph_name,
+                    upsert=upsert,
+                )
+        except Exception as e:
+            raise PGGraphQueryException(
+                {
+                    "message": f"Error executing graph query: {query}",
+                    "wrapped": wrapped_query,
+                    "detail": str(e),
+                }
+            ) from e
+
+        if data is None:
+            result = []
+        # decode records
+        else:
+            result = [PGGraphStorage._record_to_dict(d) for d in data]
+
+        return result
+
+    async def has_node(self, node_id: str) -> bool:
+        entity_name_label = PGGraphStorage._encode_graph_label(node_id.strip('"'))
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (n:Entity {node_id: "%s"})
+                     RETURN count(n) > 0 AS node_exists
+                   $$) AS (node_exists bool)""" % (self.graph_name, entity_name_label)
+
+        single_result = (await self._query(query))[0]
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query,
+            single_result["node_exists"],
+        )
+
+        return single_result["node_exists"]
+
+    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:
+        src_label = PGGraphStorage._encode_graph_label(source_node_id.strip('"'))
+        tgt_label = PGGraphStorage._encode_graph_label(target_node_id.strip('"'))
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (a:Entity {node_id: "%s"})-[r]-(b:Entity {node_id: "%s"})
+                     RETURN COUNT(r) > 0 AS edge_exists
+                   $$) AS (edge_exists bool)""" % (
+            self.graph_name,
+            src_label,
+            tgt_label,
+        )
+
+        single_result = (await self._query(query))[0]
+        logger.debug(
+            "{%s}:query:{%s}:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            query,
+            single_result["edge_exists"],
+        )
+        return single_result["edge_exists"]
+
+    async def get_node(self, node_id: str) -> Union[dict, None]:
+        label = PGGraphStorage._encode_graph_label(node_id.strip('"'))
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (n:Entity {node_id: "%s"})
+                     RETURN n
+                   $$) AS (n agtype)""" % (self.graph_name, label)
+        record = await self._query(query)
+        if record:
+            node = record[0]
+            node_dict = node["n"]
+            logger.debug(
+                "{%s}: query: {%s}, result: {%s}",
+                inspect.currentframe().f_code.co_name,
+                query,
+                node_dict,
+            )
+            return node_dict
+        return None
+
+    async def node_degree(self, node_id: str) -> int:
+        label = PGGraphStorage._encode_graph_label(node_id.strip('"'))
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (n:Entity {node_id: "%s"})-[]->(x)
+                     RETURN count(x) AS total_edge_count
+                   $$) AS (total_edge_count integer)""" % (self.graph_name, label)
+        record = (await self._query(query))[0]
+        if record:
+            edge_count = int(record["total_edge_count"])
+            logger.debug(
+                "{%s}:query:{%s}:result:{%s}",
+                inspect.currentframe().f_code.co_name,
+                query,
+                edge_count,
+            )
+            return edge_count
+
+    async def edge_degree(self, src_id: str, tgt_id: str) -> int:
+        src_degree = await self.node_degree(src_id)
+        trg_degree = await self.node_degree(tgt_id)
+
+        # Convert None to 0 for addition
+        src_degree = 0 if src_degree is None else src_degree
+        trg_degree = 0 if trg_degree is None else trg_degree
+
+        degrees = int(src_degree) + int(trg_degree)
+        logger.debug(
+            "{%s}:query:src_Degree+trg_degree:result:{%s}",
+            inspect.currentframe().f_code.co_name,
+            degrees,
+        )
+        return degrees
+
+    async def get_edge(
+        self, source_node_id: str, target_node_id: str
+    ) -> Union[dict, None]:
+        """
+        Find all edges between nodes of two given labels
+
+        Args:
+            source_node_id (str): Label of the source nodes
+            target_node_id (str): Label of the target nodes
+
+        Returns:
+            list: List of all relationships/edges found
+        """
+        src_label = PGGraphStorage._encode_graph_label(source_node_id.strip('"'))
+        tgt_label = PGGraphStorage._encode_graph_label(target_node_id.strip('"'))
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (a:Entity {node_id: "%s"})-[r]->(b:Entity {node_id: "%s"})
+                     RETURN properties(r) as edge_properties
+                     LIMIT 1
+                   $$) AS (edge_properties agtype)""" % (
+            self.graph_name,
+            src_label,
+            tgt_label,
+        )
+        record = await self._query(query)
+        if record and record[0] and record[0]["edge_properties"]:
+            result = record[0]["edge_properties"]
+            logger.debug(
+                "{%s}:query:{%s}:result:{%s}",
+                inspect.currentframe().f_code.co_name,
+                query,
+                result,
+            )
+            return result
+
+    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:
+        """
+        Retrieves all edges (relationships) for a particular node identified by its label.
+        :return: List of dictionaries containing edge information
+        """
+        label = PGGraphStorage._encode_graph_label(source_node_id.strip('"'))
+
+        query = """SELECT * FROM cypher('%s', $$
+                      MATCH (n:Entity {node_id: "%s"})
+                      OPTIONAL MATCH (n)-[r]-(connected)
+                      RETURN n, r, connected
+                    $$) AS (n agtype, r agtype, connected agtype)""" % (
+            self.graph_name,
+            label,
+        )
+
+        results = await self._query(query)
+        edges = []
+        for record in results:
+            source_node = record["n"] if record["n"] else None
+            connected_node = record["connected"] if record["connected"] else None
+
+            source_label = (
+                source_node["node_id"]
+                if source_node and source_node["node_id"]
+                else None
+            )
+            target_label = (
+                connected_node["node_id"]
+                if connected_node and connected_node["node_id"]
+                else None
+            )
+
+            if source_label and target_label:
+                edges.append(
+                    (
+                        PGGraphStorage._decode_graph_label(source_label),
+                        PGGraphStorage._decode_graph_label(target_label),
+                    )
+                )
+
+        return edges
+
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((PGGraphQueryException,)),
+    )
+    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):
+        """
+        Upsert a node in the AGE database.
+
+        Args:
+            node_id: The unique identifier for the node (used as label)
+            node_data: Dictionary of node properties
+        """
+        label = PGGraphStorage._encode_graph_label(node_id.strip('"'))
+        properties = node_data
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MERGE (n:Entity {node_id: "%s"})
+                     SET n += %s
+                     RETURN n
+                   $$) AS (n agtype)""" % (
+            self.graph_name,
+            label,
+            PGGraphStorage._format_properties(properties),
+        )
+
+        try:
+            await self._query(query, readonly=False, upsert=True)
+            logger.debug(
+                "Upserted node with label '{%s}' and properties: {%s}",
+                label,
+                properties,
+            )
+        except Exception as e:
+            logger.error("Error during upsert: {%s}", e)
+            raise
+
+    @retry(
+        stop=stop_after_attempt(3),
+        wait=wait_exponential(multiplier=1, min=4, max=10),
+        retry=retry_if_exception_type((PGGraphQueryException,)),
+    )
+    async def upsert_edge(
+        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]
+    ):
+        """
+        Upsert an edge and its properties between two nodes identified by their labels.
+
+        Args:
+            source_node_id (str): Label of the source node (used as identifier)
+            target_node_id (str): Label of the target node (used as identifier)
+            edge_data (dict): Dictionary of properties to set on the edge
+        """
+        src_label = PGGraphStorage._encode_graph_label(source_node_id.strip('"'))
+        tgt_label = PGGraphStorage._encode_graph_label(target_node_id.strip('"'))
+        edge_properties = edge_data
+
+        query = """SELECT * FROM cypher('%s', $$
+                     MATCH (source:Entity {node_id: "%s"})
+                     WITH source
+                     MATCH (target:Entity {node_id: "%s"})
+                     MERGE (source)-[r:DIRECTED]->(target)
+                     SET r += %s
+                     RETURN r
+                   $$) AS (r agtype)""" % (
+            self.graph_name,
+            src_label,
+            tgt_label,
+            PGGraphStorage._format_properties(edge_properties),
+        )
+        # logger.info(f"-- inserting edge after formatted: {params}")
+        try:
+            await self._query(query, readonly=False, upsert=True)
+            logger.debug(
+                "Upserted edge from '{%s}' to '{%s}' with properties: {%s}",
+                src_label,
+                tgt_label,
+                edge_properties,
+            )
+        except Exception as e:
+            logger.error("Error during edge upsert: {%s}", e)
+            raise
+
+    async def _node2vec_embed(self):
+        print("Implemented but never called.")
+
+
+NAMESPACE_TABLE_MAP = {
+    "full_docs": "LIGHTRAG_DOC_FULL",
+    "text_chunks": "LIGHTRAG_DOC_CHUNKS",
+    "chunks": "LIGHTRAG_DOC_CHUNKS",
+    "entities": "LIGHTRAG_VDB_ENTITY",
+    "relationships": "LIGHTRAG_VDB_RELATION",
+    "doc_status": "LIGHTRAG_DOC_STATUS",
+    "llm_response_cache": "LIGHTRAG_LLM_CACHE",
+}
+
+
+TABLES = {
+    "LIGHTRAG_DOC_FULL": {
+        "ddl": """CREATE TABLE LIGHTRAG_DOC_FULL (
+                    id VARCHAR(255),
+                    workspace VARCHAR(255),
+                    doc_name VARCHAR(1024),
+                    content TEXT,
+                    meta JSONB,
+                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    update_time TIMESTAMP,
+	                CONSTRAINT LIGHTRAG_DOC_FULL_PK PRIMARY KEY (workspace, id)
+                    )"""
+    },
+    "LIGHTRAG_DOC_CHUNKS": {
+        "ddl": """CREATE TABLE LIGHTRAG_DOC_CHUNKS (
+                    id VARCHAR(255),
+                    workspace VARCHAR(255),
+                    full_doc_id VARCHAR(256),
+                    chunk_order_index INTEGER,
+                    tokens INTEGER,
+                    content TEXT,
+                    content_vector VECTOR,
+                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    update_time TIMESTAMP,
+	                CONSTRAINT LIGHTRAG_DOC_CHUNKS_PK PRIMARY KEY (workspace, id)
+                    )"""
+    },
+    "LIGHTRAG_VDB_ENTITY": {
+        "ddl": """CREATE TABLE LIGHTRAG_VDB_ENTITY (
+                    id VARCHAR(255),
+                    workspace VARCHAR(255),
+                    entity_name VARCHAR(255),
+                    content TEXT,
+                    content_vector VECTOR,
+                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    update_time TIMESTAMP,
+	                CONSTRAINT LIGHTRAG_VDB_ENTITY_PK PRIMARY KEY (workspace, id)
+                    )"""
+    },
+    "LIGHTRAG_VDB_RELATION": {
+        "ddl": """CREATE TABLE LIGHTRAG_VDB_RELATION (
+                    id VARCHAR(255),
+                    workspace VARCHAR(255),
+                    source_id VARCHAR(256),
+                    target_id VARCHAR(256),
+                    content TEXT,
+                    content_vector VECTOR,
+                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    update_time TIMESTAMP,
+	                CONSTRAINT LIGHTRAG_VDB_RELATION_PK PRIMARY KEY (workspace, id)
+                    )"""
+    },
+    "LIGHTRAG_LLM_CACHE": {
+        "ddl": """CREATE TABLE LIGHTRAG_LLM_CACHE (
+	                workspace varchar(255) NOT NULL,
+	                id varchar(255) NOT NULL,
+	                mode varchar(32) NOT NULL,
+                    original_prompt TEXT,
+                    return_value TEXT,
+                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                    update_time TIMESTAMP,
+	                CONSTRAINT LIGHTRAG_LLM_CACHE_PK PRIMARY KEY (workspace, mode, id)
+                    )"""
+    },
+    "LIGHTRAG_DOC_STATUS": {
+        "ddl": """CREATE TABLE LIGHTRAG_DOC_STATUS (
+	               workspace varchar(255) NOT NULL,
+	               id varchar(255) NOT NULL,
+	               content_summary varchar(255) NULL,
+	               content_length int4 NULL,
+	               chunks_count int4 NULL,
+	               status varchar(64) NULL,
+	               created_at timestamp DEFAULT CURRENT_TIMESTAMP NULL,
+	               updated_at timestamp DEFAULT CURRENT_TIMESTAMP NULL,
+	               CONSTRAINT LIGHTRAG_DOC_STATUS_PK PRIMARY KEY (workspace, id)
+	              )"""
+    },
+}
+
+
+SQL_TEMPLATES = {
+    # SQL for KVStorage
+    "get_by_id_full_docs": """SELECT id, COALESCE(content, '') as content
+                                FROM LIGHTRAG_DOC_FULL WHERE workspace=$1 AND id=$2
+                            """,
+    "get_by_id_text_chunks": """SELECT id, tokens, COALESCE(content, '') as content,
+                                chunk_order_index, full_doc_id
+                                FROM LIGHTRAG_DOC_CHUNKS WHERE workspace=$1 AND id=$2
+                            """,
+    "get_by_id_llm_response_cache": """SELECT id, original_prompt, COALESCE(return_value, '') as "return", mode
+                                FROM LIGHTRAG_LLM_CACHE WHERE workspace=$1 AND mode=$2
+                               """,
+    "get_by_mode_id_llm_response_cache": """SELECT id, original_prompt, COALESCE(return_value, '') as "return", mode
+                           FROM LIGHTRAG_LLM_CACHE WHERE workspace=$1 AND mode=$2 AND id=$3
+                          """,
+    "get_by_ids_full_docs": """SELECT id, COALESCE(content, '') as content
+                                 FROM LIGHTRAG_DOC_FULL WHERE workspace=$1 AND id IN ({ids})
+                            """,
+    "get_by_ids_text_chunks": """SELECT id, tokens, COALESCE(content, '') as content,
+                                  chunk_order_index, full_doc_id
+                                   FROM LIGHTRAG_DOC_CHUNKS WHERE workspace=$1 AND id IN ({ids})
+                                """,
+    "get_by_ids_llm_response_cache": """SELECT id, original_prompt, COALESCE(return_value, '') as "return", mode
+                                 FROM LIGHTRAG_LLM_CACHE WHERE workspace=$1 AND mode= IN ({ids})
+                                """,
+    "filter_keys": "SELECT id FROM {table_name} WHERE workspace=$1 AND id IN ({ids})",
+    "upsert_doc_full": """INSERT INTO LIGHTRAG_DOC_FULL (id, content, workspace)
+                        VALUES ($1, $2, $3)
+                        ON CONFLICT (workspace,id) DO UPDATE
+                           SET content = $2, update_time = CURRENT_TIMESTAMP
+                       """,
+    "upsert_llm_response_cache": """INSERT INTO LIGHTRAG_LLM_CACHE(workspace,id,original_prompt,return_value,mode)
+                                      VALUES ($1, $2, $3, $4, $5)
+                                      ON CONFLICT (workspace,mode,id) DO UPDATE
+                                      SET original_prompt = EXCLUDED.original_prompt,
+                                      return_value=EXCLUDED.return_value,
+                                      mode=EXCLUDED.mode,
+                                      update_time = CURRENT_TIMESTAMP
+                                     """,
+    "upsert_chunk": """INSERT INTO LIGHTRAG_DOC_CHUNKS (workspace, id, tokens,
+                      chunk_order_index, full_doc_id, content, content_vector)
+                      VALUES ($1, $2, $3, $4, $5, $6, $7)
+                      ON CONFLICT (workspace,id) DO UPDATE
+                      SET tokens=EXCLUDED.tokens,
+                      chunk_order_index=EXCLUDED.chunk_order_index,
+                      full_doc_id=EXCLUDED.full_doc_id,
+                      content = EXCLUDED.content,
+                      content_vector=EXCLUDED.content_vector,
+                      update_time = CURRENT_TIMESTAMP
+                     """,
+    "upsert_entity": """INSERT INTO LIGHTRAG_VDB_ENTITY (workspace, id, entity_name, content, content_vector)
+                      VALUES ($1, $2, $3, $4, $5)
+                      ON CONFLICT (workspace,id) DO UPDATE
+                      SET entity_name=EXCLUDED.entity_name,
+                      content=EXCLUDED.content,
+                      content_vector=EXCLUDED.content_vector,
+                      update_time=CURRENT_TIMESTAMP
+                     """,
+    "upsert_relationship": """INSERT INTO LIGHTRAG_VDB_RELATION (workspace, id, source_id,
+                      target_id, content, content_vector)
+                      VALUES ($1, $2, $3, $4, $5, $6)
+                      ON CONFLICT (workspace,id) DO UPDATE
+                      SET source_id=EXCLUDED.source_id,
+                      target_id=EXCLUDED.target_id,
+                      content=EXCLUDED.content,
+                      content_vector=EXCLUDED.content_vector, update_time = CURRENT_TIMESTAMP
+                     """,
+    # SQL for VectorStorage
+    "entities": """SELECT entity_name FROM
+        (SELECT id, entity_name, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance
+        FROM LIGHTRAG_VDB_ENTITY where workspace=$1)
+        WHERE distance>$2 ORDER BY distance DESC  LIMIT $3
+       """,
+    "relationships": """SELECT source_id as src_id, target_id as tgt_id FROM
+        (SELECT id, source_id,target_id, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance
+        FROM LIGHTRAG_VDB_RELATION where workspace=$1)
+        WHERE distance>$2 ORDER BY distance DESC  LIMIT $3
+       """,
+    "chunks": """SELECT id FROM
+        (SELECT id, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance
+        FROM LIGHTRAG_DOC_CHUNKS where workspace=$1)
+        WHERE distance>$2 ORDER BY distance DESC  LIMIT $3
+       """,
+}
diff --git a/test/minirag/minirag/kg/postgres_impl_test.py b/test/minirag/minirag/kg/postgres_impl_test.py
new file mode 100755
index 0000000..c295aed
--- /dev/null
+++ b/test/minirag/minirag/kg/postgres_impl_test.py
@@ -0,0 +1,136 @@
+import asyncio
+import sys
+import os
+import pipmaster as pm
+
+if not pm.is_installed("psycopg-pool"):
+    pm.install("psycopg-pool")
+    pm.install("psycopg[binary,pool]")
+if not pm.is_installed("asyncpg"):
+    pm.install("asyncpg")
+
+import asyncpg
+import psycopg
+from psycopg_pool import AsyncConnectionPool
+from minirag.kg.postgres_impl import PostgreSQLDB, PGGraphStorage
+
+DB = "rag"
+USER = "rag"
+PASSWORD = "rag"
+HOST = "localhost"
+PORT = "15432"
+os.environ["AGE_GRAPH_NAME"] = "dickens"
+
+if sys.platform.startswith("win"):
+    import asyncio.windows_events
+
+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
+
+
+async def get_pool():
+    return await asyncpg.create_pool(
+        f"postgres://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}",
+        min_size=10,
+        max_size=10,
+        max_queries=5000,
+        max_inactive_connection_lifetime=300.0,
+    )
+
+
+async def main1():
+    connection_string = (
+        f"dbname='{DB}' user='{USER}' password='{PASSWORD}' host='{HOST}' port={PORT}"
+    )
+    pool = AsyncConnectionPool(connection_string, open=False)
+    await pool.open()
+
+    try:
+        conn = await pool.getconn(timeout=10)
+        async with conn.cursor() as curs:
+            try:
+                await curs.execute('SET search_path = ag_catalog, "$user", public')
+                await curs.execute("SELECT create_graph('dickens-2')")
+                await conn.commit()
+                print("create_graph success")
+            except (
+                psycopg.errors.InvalidSchemaName,
+                psycopg.errors.UniqueViolation,
+            ):
+                print("create_graph already exists")
+                await conn.rollback()
+    finally:
+        pass
+
+
+db = PostgreSQLDB(
+    config={
+        "host": "localhost",
+        "port": 15432,
+        "user": "rag",
+        "password": "rag",
+        "database": "r1",
+    }
+)
+
+
+async def query_with_age():
+    await db.initdb()
+    graph = PGGraphStorage(
+        namespace="chunk_entity_relation",
+        global_config={},
+        embedding_func=None,
+    )
+    graph.db = db
+    res = await graph.get_node('"A CHRISTMAS CAROL"')
+    print("Node is: ", res)
+    res = await graph.get_edge('"A CHRISTMAS CAROL"', "PROJECT GUTENBERG")
+    print("Edge is: ", res)
+    res = await graph.get_node_edges('"SCROOGE"')
+    print("Node Edges are: ", res)
+
+
+async def create_edge_with_age():
+    await db.initdb()
+    graph = PGGraphStorage(
+        namespace="chunk_entity_relation",
+        global_config={},
+        embedding_func=None,
+    )
+    graph.db = db
+    await graph.upsert_node('"THE CRATCHITS"', {"hello": "world"})
+    await graph.upsert_node('"THE GIRLS"', {"world": "hello"})
+    await graph.upsert_edge(
+        '"THE CRATCHITS"',
+        '"THE GIRLS"',
+        edge_data={
+            "weight": 7.0,
+            "description": '"The girls are part of the Cratchit family, contributing to their collective efforts and shared experiences.',
+            "keywords": '"family, collective effort"',
+            "source_id": "chunk-1d4b58de5429cd1261370c231c8673e8",
+        },
+    )
+    res = await graph.get_edge("THE CRATCHITS", '"THE GIRLS"')
+    print("Edge is: ", res)
+
+
+async def main():
+    pool = await get_pool()
+    sql = r"SELECT * FROM ag_catalog.cypher('dickens', $$ MATCH (n:帅哥) RETURN n $$) AS (n ag_catalog.agtype)"
+    # cypher = "MATCH (n:how_are_you_doing) RETURN n"
+    async with pool.acquire() as conn:
+        try:
+            await conn.execute(
+                """SET search_path = ag_catalog, "$user", public;select create_graph('dickens')"""
+            )
+        except asyncpg.exceptions.InvalidSchemaNameError:
+            print("create_graph already exists")
+        # stmt = await conn.prepare(sql)
+        row = await conn.fetch(sql)
+        print("row is: ", row)
+
+        row = await conn.fetchrow("select '100'::int + 200 as result")
+        print(row)  # <Record result=300>
+
+
+if __name__ == "__main__":
+    asyncio.run(query_with_age())
diff --git a/test/minirag/minirag/kg/redis_impl.py b/test/minirag/minirag/kg/redis_impl.py
new file mode 100755
index 0000000..89c1daf
--- /dev/null
+++ b/test/minirag/minirag/kg/redis_impl.py
@@ -0,0 +1,72 @@
+import os
+from tqdm.asyncio import tqdm as tqdm_async
+from dataclasses import dataclass
+import pipmaster as pm
+
+if not pm.is_installed("redis"):
+    pm.install("redis")
+
+# aioredis is a depricated library, replaced with redis
+from redis.asyncio import Redis
+from minirag.utils import logger
+from minirag.base import BaseKVStorage
+import json
+import copy
+from minirag.utils import merge_tuples
+
+
+@dataclass
+class RedisKVStorage(BaseKVStorage):
+    def __post_init__(self):
+        redis_url = os.environ.get("REDIS_URI", "redis://localhost:6379")
+        self._redis = Redis.from_url(redis_url, decode_responses=True)
+        logger.info(f"Use Redis as KV {self.namespace}")
+
+    async def all_keys(self) -> list[str]:
+        keys = await self._redis.keys(f"{self.namespace}:*")
+        return [key.split(":", 1)[-1] for key in keys]
+
+    async def get_by_id(self, id):
+        data = await self._redis.get(f"{self.namespace}:{id}")
+        return json.loads(data) if data else None
+
+    async def get_by_ids(self, ids, fields=None):
+        pipe = self._redis.pipeline()
+        for id in ids:
+            pipe.get(f"{self.namespace}:{id}")
+        results = await pipe.execute()
+
+        if fields:
+            # Filter fields if specified
+            return [
+                {field: value.get(field) for field in fields if field in value}
+                if (value := json.loads(result))
+                else None
+                for result in results
+            ]
+
+        return [json.loads(result) if result else None for result in results]
+
+    async def filter_keys(self, data: list[str]) -> set[str]:
+        pipe = self._redis.pipeline()
+        for key in data:
+            pipe.exists(f"{self.namespace}:{key}")
+        results = await pipe.execute()
+
+        existing_ids = {data[i] for i, exists in enumerate(results) if exists}
+        return set(data) - existing_ids
+
+    async def upsert(self, data: dict[str, dict]):
+        pipe = self._redis.pipeline()
+        for k, v in tqdm_async(data.items(), desc="Upserting"):
+            pipe.set(f"{self.namespace}:{k}", json.dumps(v))
+        await pipe.execute()
+
+        for k in data:
+            data[k]["_id"] = k
+        return data
+
+    async def drop(self):
+        keys = await self._redis.keys(f"{self.namespace}:*")
+        if keys:
+            await self._redis.delete(*keys)
diff --git a/test/minirag/minirag/llm.py b/test/minirag/minirag/llm.py
new file mode 100755
index 0000000..1873f75
--- /dev/null
+++ b/test/minirag/minirag/llm.py
@@ -0,0 +1,95 @@
+from typing import List, Dict, Callable, Any
+from pydantic import BaseModel, Field
+
+
+class Model(BaseModel):
+    """
+    This is a Pydantic model class named 'Model' that is used to define a custom language model.
+
+    Attributes:
+        gen_func (Callable[[Any], str]): A callable function that generates the response from the language model.
+            The function should take any argument and return a string.
+        kwargs (Dict[str, Any]): A dictionary that contains the arguments to pass to the callable function.
+            This could include parameters such as the model name, API key, etc.
+
+    Example usage:
+        Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_1"]})
+
+    In this example, 'openai_complete_if_cache' is the callable function that generates the response from the OpenAI model.
+    The 'kwargs' dictionary contains the model name and API key to be passed to the function.
+    """
+
+    gen_func: Callable[[Any], str] = Field(
+        ...,
+        description="A function that generates the response from the llm. The response must be a string",
+    )
+    kwargs: Dict[str, Any] = Field(
+        ...,
+        description="The arguments to pass to the callable function. Eg. the api key, model name, etc",
+    )
+
+    class Config:
+        arbitrary_types_allowed = True
+
+
+class MultiModel:
+    """
+    Distributes the load across multiple language models. Useful for circumventing low rate limits with certain api providers especially if you are on the free tier.
+    Could also be used for spliting across diffrent models or providers.
+
+    Attributes:
+        models (List[Model]): A list of language models to be used.
+
+    Usage example:
+        ```python
+        models = [
+            Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_1"]}),
+            Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_2"]}),
+            Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_3"]}),
+            Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_4"]}),
+            Model(gen_func=openai_complete_if_cache, kwargs={"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY_5"]}),
+        ]
+        multi_model = MultiModel(models)
+        rag = LightRAG(
+            llm_model_func=multi_model.llm_model_func
+            / ..other args
+            )
+        ```
+    """
+
+    def __init__(self, models: List[Model]):
+        self._models = models
+        self._current_model = 0
+
+    def _next_model(self):
+        self._current_model = (self._current_model + 1) % len(self._models)
+        return self._models[self._current_model]
+
+    async def llm_model_func(
+        self, prompt, system_prompt=None, history_messages=[], **kwargs
+    ) -> str:
+        kwargs.pop("model", None)  # stop from overwriting the custom model name
+        kwargs.pop("keyword_extraction", None)
+        kwargs.pop("mode", None)
+        next_model = self._next_model()
+        args = dict(
+            prompt=prompt,
+            system_prompt=system_prompt,
+            history_messages=history_messages,
+            **kwargs,
+            **next_model.kwargs,
+        )
+
+        return await next_model.gen_func(**args)
+
+
+if __name__ == "__main__":
+    import asyncio
+
+    async def main():
+        from minirag.llm.openai import gpt_4o_mini_complete
+
+        result = await gpt_4o_mini_complete("How are you?")
+        print(result)
+
+    asyncio.run(main())
diff --git a/test/minirag/minirag/llm/__init__.py b/test/minirag/minirag/llm/__init__.py
new file mode 100755
index 0000000..3295164
--- /dev/null
+++ b/test/minirag/minirag/llm/__init__.py
@@ -0,0 +1,7 @@
+from minirag.llm.openai import (
+    gpt_4o_mini_complete,
+)
+from minirag.llm.hf import (
+    hf_embed,
+    hf_model_complete
+)
\ No newline at end of file
diff --git a/test/minirag/minirag/llm/azure_openai.py b/test/minirag/minirag/llm/azure_openai.py
new file mode 100755
index 0000000..eb9c49c
--- /dev/null
+++ b/test/minirag/minirag/llm/azure_openai.py
@@ -0,0 +1,189 @@
+"""
+Azure OpenAI LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with aure openai's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - openai
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.azure_openai import azure_openai_model_complete, azure_openai_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+
+import os
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("openai"):
+    pm.install("openai")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+from openai import (
+    AsyncAzureOpenAI,
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.utils import (
+    wrap_embedding_func_with_attrs,
+    locate_json_string_body_from_string,
+    safe_unicode_decode,
+)
+
+import numpy as np
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APIConnectionError)
+    ),
+)
+async def azure_openai_complete_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    base_url=None,
+    api_key=None,
+    api_version=None,
+    **kwargs,
+):
+    if api_key:
+        os.environ["AZURE_OPENAI_API_KEY"] = api_key
+    if base_url:
+        os.environ["AZURE_OPENAI_ENDPOINT"] = base_url
+    if api_version:
+        os.environ["AZURE_OPENAI_API_VERSION"] = api_version
+
+    openai_async_client = AsyncAzureOpenAI(
+        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
+        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
+        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
+    )
+    kwargs.pop("hashing_kv", None)
+    messages = []
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+    messages.extend(history_messages)
+    if prompt is not None:
+        messages.append({"role": "user", "content": prompt})
+
+    if "response_format" in kwargs:
+        response = await openai_async_client.beta.chat.completions.parse(
+            model=model, messages=messages, **kwargs
+        )
+    else:
+        response = await openai_async_client.chat.completions.create(
+            model=model, messages=messages, **kwargs
+        )
+
+    if hasattr(response, "__aiter__"):
+
+        async def inner():
+            async for chunk in response:
+                if len(chunk.choices) == 0:
+                    continue
+                content = chunk.choices[0].delta.content
+                if content is None:
+                    continue
+                if r"\u" in content:
+                    content = safe_unicode_decode(content.encode("utf-8"))
+                yield content
+
+        return inner()
+    else:
+        content = response.choices[0].message.content
+        if r"\u" in content:
+            content = safe_unicode_decode(content.encode("utf-8"))
+        return content
+
+
+async def azure_openai_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    result = await azure_openai_complete_if_cache(
+        os.getenv("LLM_MODEL", "gpt-4o-mini"),
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+    if keyword_extraction:  # TODO: use JSON API
+        return locate_json_string_body_from_string(result)
+    return result
+
+
+@wrap_embedding_func_with_attrs(embedding_dim=1536, max_token_size=8191)
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def azure_openai_embed(
+    texts: list[str],
+    model: str = "text-embedding-3-small",
+    base_url: str = None,
+    api_key: str = None,
+    api_version: str = None,
+) -> np.ndarray:
+    if api_key:
+        os.environ["AZURE_OPENAI_API_KEY"] = api_key
+    if base_url:
+        os.environ["AZURE_OPENAI_ENDPOINT"] = base_url
+    if api_version:
+        os.environ["AZURE_OPENAI_API_VERSION"] = api_version
+
+    openai_async_client = AsyncAzureOpenAI(
+        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
+        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
+        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
+    )
+
+    response = await openai_async_client.embeddings.create(
+        model=model, input=texts, encoding_format="float"
+    )
+    return np.array([dp.embedding for dp in response.data])
diff --git a/test/minirag/minirag/llm/bedrock.py b/test/minirag/minirag/llm/bedrock.py
new file mode 100755
index 0000000..8b7b12f
--- /dev/null
+++ b/test/minirag/minirag/llm/bedrock.py
@@ -0,0 +1,225 @@
+"""
+Bedrock LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with Bedrock's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - aioboto3, tenacity
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.bebrock import bebrock_model_complete, bebrock_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+
+import copy
+import os
+import json
+
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+if not pm.is_installed("aioboto3"):
+    pm.install("aioboto3")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+import aioboto3
+import numpy as np
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.utils import (
+    locate_json_string_body_from_string,
+)
+
+
+class BedrockError(Exception):
+    """Generic error for issues related to Amazon Bedrock"""
+
+
+@retry(
+    stop=stop_after_attempt(5),
+    wait=wait_exponential(multiplier=1, max=60),
+    retry=retry_if_exception_type((BedrockError)),
+)
+async def bedrock_complete_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    aws_access_key_id=None,
+    aws_secret_access_key=None,
+    aws_session_token=None,
+    **kwargs,
+) -> str:
+    os.environ["AWS_ACCESS_KEY_ID"] = os.environ.get(
+        "AWS_ACCESS_KEY_ID", aws_access_key_id
+    )
+    os.environ["AWS_SECRET_ACCESS_KEY"] = os.environ.get(
+        "AWS_SECRET_ACCESS_KEY", aws_secret_access_key
+    )
+    os.environ["AWS_SESSION_TOKEN"] = os.environ.get(
+        "AWS_SESSION_TOKEN", aws_session_token
+    )
+    kwargs.pop("hashing_kv", None)
+    # Fix message history format
+    messages = []
+    for history_message in history_messages:
+        message = copy.copy(history_message)
+        message["content"] = [{"text": message["content"]}]
+        messages.append(message)
+
+    # Add user prompt
+    messages.append({"role": "user", "content": [{"text": prompt}]})
+
+    # Initialize Converse API arguments
+    args = {"modelId": model, "messages": messages}
+
+    # Define system prompt
+    if system_prompt:
+        args["system"] = [{"text": system_prompt}]
+
+    # Map and set up inference parameters
+    inference_params_map = {
+        "max_tokens": "maxTokens",
+        "top_p": "topP",
+        "stop_sequences": "stopSequences",
+    }
+    if inference_params := list(
+        set(kwargs) & set(["max_tokens", "temperature", "top_p", "stop_sequences"])
+    ):
+        args["inferenceConfig"] = {}
+        for param in inference_params:
+            args["inferenceConfig"][inference_params_map.get(param, param)] = (
+                kwargs.pop(param)
+            )
+
+    # Call model via Converse API
+    session = aioboto3.Session()
+    async with session.client("bedrock-runtime") as bedrock_async_client:
+        try:
+            response = await bedrock_async_client.converse(**args, **kwargs)
+        except Exception as e:
+            raise BedrockError(e)
+
+    return response["output"]["message"]["content"][0]["text"]
+
+
+async def bedrock_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    result = await bedrock_complete_if_cache(
+        "anthropic.claude-3-haiku-20240307-v1:0",
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+    if keyword_extraction:  # TODO: use JSON API
+        return locate_json_string_body_from_string(result)
+    return result
+
+
+# @wrap_embedding_func_with_attrs(embedding_dim=1024, max_token_size=8192)
+# @retry(
+#     stop=stop_after_attempt(3),
+#     wait=wait_exponential(multiplier=1, min=4, max=10),
+#     retry=retry_if_exception_type((RateLimitError, APIConnectionError, Timeout)),  # TODO: fix exceptions
+# )
+async def bedrock_embed(
+    texts: list[str],
+    model: str = "amazon.titan-embed-text-v2:0",
+    aws_access_key_id=None,
+    aws_secret_access_key=None,
+    aws_session_token=None,
+) -> np.ndarray:
+    os.environ["AWS_ACCESS_KEY_ID"] = os.environ.get(
+        "AWS_ACCESS_KEY_ID", aws_access_key_id
+    )
+    os.environ["AWS_SECRET_ACCESS_KEY"] = os.environ.get(
+        "AWS_SECRET_ACCESS_KEY", aws_secret_access_key
+    )
+    os.environ["AWS_SESSION_TOKEN"] = os.environ.get(
+        "AWS_SESSION_TOKEN", aws_session_token
+    )
+
+    session = aioboto3.Session()
+    async with session.client("bedrock-runtime") as bedrock_async_client:
+        if (model_provider := model.split(".")[0]) == "amazon":
+            embed_texts = []
+            for text in texts:
+                if "v2" in model:
+                    body = json.dumps(
+                        {
+                            "inputText": text,
+                            # 'dimensions': embedding_dim,
+                            "embeddingTypes": ["float"],
+                        }
+                    )
+                elif "v1" in model:
+                    body = json.dumps({"inputText": text})
+                else:
+                    raise ValueError(f"Model {model} is not supported!")
+
+                response = await bedrock_async_client.invoke_model(
+                    modelId=model,
+                    body=body,
+                    accept="application/json",
+                    contentType="application/json",
+                )
+
+                response_body = await response.get("body").json()
+
+                embed_texts.append(response_body["embedding"])
+        elif model_provider == "cohere":
+            body = json.dumps(
+                {"texts": texts, "input_type": "search_document", "truncate": "NONE"}
+            )
+
+            response = await bedrock_async_client.invoke_model(
+                model=model,
+                body=body,
+                accept="application/json",
+                contentType="application/json",
+            )
+
+            response_body = json.loads(response.get("body").read())
+
+            embed_texts = response_body["embeddings"]
+        else:
+            raise ValueError(f"Model provider '{model_provider}' is not supported!")
+
+        return np.array(embed_texts)
diff --git a/test/minirag/minirag/llm/hf.py b/test/minirag/minirag/llm/hf.py
new file mode 100755
index 0000000..aed137d
--- /dev/null
+++ b/test/minirag/minirag/llm/hf.py
@@ -0,0 +1,188 @@
+"""
+Hugging face LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with Hugging face's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - transformers
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.hf import hf_model_complete, hf_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import copy
+import os
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("transformers"):
+    pm.install("transformers")
+if not pm.is_installed("torch"):
+    pm.install("torch")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+from transformers import AutoTokenizer, AutoModelForCausalLM
+from functools import lru_cache
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+from minirag.exceptions import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from minirag.utils import (
+    locate_json_string_body_from_string,
+)
+import torch
+import numpy as np
+
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+
+@lru_cache(maxsize=1)
+def initialize_hf_model(model_name):
+    hf_tokenizer = AutoTokenizer.from_pretrained(
+        model_name, device_map="auto", trust_remote_code=True
+    )
+    hf_model = AutoModelForCausalLM.from_pretrained(
+        model_name, device_map="auto", trust_remote_code=True
+    )
+    if hf_tokenizer.pad_token is None:
+        hf_tokenizer.pad_token = hf_tokenizer.eos_token
+
+    return hf_model, hf_tokenizer
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def hf_model_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    **kwargs,
+) -> str:
+    model_name = model
+    hf_model, hf_tokenizer = initialize_hf_model(model_name)
+    messages = []
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+    messages.extend(history_messages)
+    messages.append({"role": "user", "content": prompt})
+    kwargs.pop("hashing_kv", None)
+    input_prompt = ""
+    try:
+        input_prompt = hf_tokenizer.apply_chat_template(
+            messages, tokenize=False, add_generation_prompt=True
+        )
+    except Exception:
+        try:
+            ori_message = copy.deepcopy(messages)
+            if messages[0]["role"] == "system":
+                messages[1]["content"] = (
+                    "<system>"
+                    + messages[0]["content"]
+                    + "</system>\n"
+                    + messages[1]["content"]
+                )
+                messages = messages[1:]
+                input_prompt = hf_tokenizer.apply_chat_template(
+                    messages, tokenize=False, add_generation_prompt=True
+                )
+        except Exception:
+            len_message = len(ori_message)
+            for msgid in range(len_message):
+                input_prompt = (
+                    input_prompt
+                    + "<"
+                    + ori_message[msgid]["role"]
+                    + ">"
+                    + ori_message[msgid]["content"]
+                    + "</"
+                    + ori_message[msgid]["role"]
+                    + ">\n"
+                )
+
+    input_ids = hf_tokenizer(
+        input_prompt, return_tensors="pt", padding=True, truncation=True
+    ).to("cuda")
+    inputs = {k: v.to(hf_model.device) for k, v in input_ids.items()}
+    output = hf_model.generate(
+        **input_ids, max_new_tokens=512, num_return_sequences=1, early_stopping=True
+    )
+    response_text = hf_tokenizer.decode(
+        output[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
+    )
+
+    return response_text
+
+
+async def hf_model_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    model_name = kwargs["hashing_kv"].global_config["llm_model_name"]
+    result = await hf_model_if_cache(
+        model_name,
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+    if keyword_extraction:  # TODO: use JSON API
+        return locate_json_string_body_from_string(result)
+    return result
+
+
+async def hf_embed(texts: list[str], tokenizer, embed_model) -> np.ndarray:
+    device = next(embed_model.parameters()).device
+    input_ids = tokenizer(
+        texts, return_tensors="pt", padding=True, truncation=True
+    ).input_ids.to(device)
+    with torch.no_grad():
+        outputs = embed_model(input_ids)
+        embeddings = outputs.last_hidden_state.mean(dim=1)
+    if embeddings.dtype == torch.bfloat16:
+        return embeddings.detach().to(torch.float32).cpu().numpy()
+    else:
+        return embeddings.detach().cpu().numpy()
diff --git a/test/minirag/minirag/llm/jina.py b/test/minirag/minirag/llm/jina.py
new file mode 100755
index 0000000..62404dd
--- /dev/null
+++ b/test/minirag/minirag/llm/jina.py
@@ -0,0 +1,86 @@
+"""
+Jina Embedding Interface Module
+==========================
+
+This module provides interfaces for interacting with jina system,
+including embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added embedding generation
+
+Dependencies:
+    - tenacity
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.jina import jina_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import os
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("lmdeploy"):
+    pm.install("lmdeploy")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+
+import numpy as np
+import aiohttp
+
+
+async def fetch_data(url, headers, data):
+    async with aiohttp.ClientSession() as session:
+        async with session.post(url, headers=headers, json=data) as response:
+            response_json = await response.json()
+            data_list = response_json.get("data", [])
+            return data_list
+
+
+async def jina_embed(
+    texts: list[str],
+    dimensions: int = 1024,
+    late_chunking: bool = False,
+    base_url: str = None,
+    api_key: str = None,
+) -> np.ndarray:
+    if api_key:
+        os.environ["JINA_API_KEY"] = api_key
+    url = "https://api.jina.ai/v1/embeddings" if not base_url else base_url
+    headers = {
+        "Content-Type": "application/json",
+        "Authorization": f"Bearer {os.environ['JINA_API_KEY']}",
+    }
+    data = {
+        "model": "jina-embeddings-v3",
+        "normalized": True,
+        "embedding_type": "float",
+        "dimensions": f"{dimensions}",
+        "late_chunking": late_chunking,
+        "input": texts,
+    }
+    data_list = await fetch_data(url, headers, data)
+    return np.array([dp["embedding"] for dp in data_list])
diff --git a/test/minirag/minirag/llm/lmdeploy.py b/test/minirag/minirag/llm/lmdeploy.py
new file mode 100755
index 0000000..ab39a27
--- /dev/null
+++ b/test/minirag/minirag/llm/lmdeploy.py
@@ -0,0 +1,191 @@
+"""
+LMDeploy LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with LMDeploy's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - tenacity
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.lmdeploy import lmdeploy_model_complete, lmdeploy_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("lmdeploy"):
+    pm.install("lmdeploy[all]")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+from minirag.exceptions import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+
+from functools import lru_cache
+
+
+@lru_cache(maxsize=1)
+def initialize_lmdeploy_pipeline(
+    model,
+    tp=1,
+    chat_template=None,
+    log_level="WARNING",
+    model_format="hf",
+    quant_policy=0,
+):
+    from lmdeploy import pipeline, ChatTemplateConfig, TurbomindEngineConfig
+
+    lmdeploy_pipe = pipeline(
+        model_path=model,
+        backend_config=TurbomindEngineConfig(
+            tp=tp, model_format=model_format, quant_policy=quant_policy
+        ),
+        chat_template_config=(
+            ChatTemplateConfig(model_name=chat_template) if chat_template else None
+        ),
+        log_level="WARNING",
+    )
+    return lmdeploy_pipe
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def lmdeploy_model_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    chat_template=None,
+    model_format="hf",
+    quant_policy=0,
+    **kwargs,
+) -> str:
+    """
+    Args:
+        model (str): The path to the model.
+            It could be one of the following options:
+                    - i) A local directory path of a turbomind model which is
+                        converted by `lmdeploy convert` command or download
+                        from ii) and iii).
+                    - ii) The model_id of a lmdeploy-quantized model hosted
+                        inside a model repo on huggingface.co, such as
+                        "InternLM/internlm-chat-20b-4bit",
+                        "lmdeploy/llama2-chat-70b-4bit", etc.
+                    - iii) The model_id of a model hosted inside a model repo
+                        on huggingface.co, such as "internlm/internlm-chat-7b",
+                        "Qwen/Qwen-7B-Chat ", "baichuan-inc/Baichuan2-7B-Chat"
+                        and so on.
+        chat_template (str): needed when model is a pytorch model on
+            huggingface.co, such as "internlm-chat-7b",
+            "Qwen-7B-Chat ", "Baichuan2-7B-Chat" and so on,
+            and when the model name of local path did not match the original model name in HF.
+        tp (int): tensor parallel
+        prompt (Union[str, List[str]]): input texts to be completed.
+        do_preprocess (bool): whether pre-process the messages. Default to
+            True, which means chat_template will be applied.
+        skip_special_tokens (bool): Whether or not to remove special tokens
+            in the decoding. Default to be True.
+        do_sample (bool): Whether or not to use sampling, use greedy decoding otherwise.
+            Default to be False, which means greedy decoding will be applied.
+    """
+    try:
+        import lmdeploy
+        from lmdeploy import version_info, GenerationConfig
+    except Exception:
+        raise ImportError("Please install lmdeploy before initialize lmdeploy backend.")
+    kwargs.pop("hashing_kv", None)
+    kwargs.pop("response_format", None)
+    max_new_tokens = kwargs.pop("max_tokens", 512)
+    tp = kwargs.pop("tp", 1)
+    skip_special_tokens = kwargs.pop("skip_special_tokens", True)
+    do_preprocess = kwargs.pop("do_preprocess", True)
+    do_sample = kwargs.pop("do_sample", False)
+    gen_params = kwargs
+
+    version = version_info
+    if do_sample is not None and version < (0, 6, 0):
+        raise RuntimeError(
+            "`do_sample` parameter is not supported by lmdeploy until "
+            f"v0.6.0, but currently using lmdeloy {lmdeploy.__version__}"
+        )
+    else:
+        do_sample = True
+        gen_params.update(do_sample=do_sample)
+
+    lmdeploy_pipe = initialize_lmdeploy_pipeline(
+        model=model,
+        tp=tp,
+        chat_template=chat_template,
+        model_format=model_format,
+        quant_policy=quant_policy,
+        log_level="WARNING",
+    )
+
+    messages = []
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+
+    messages.extend(history_messages)
+    messages.append({"role": "user", "content": prompt})
+
+    gen_config = GenerationConfig(
+        skip_special_tokens=skip_special_tokens,
+        max_new_tokens=max_new_tokens,
+        **gen_params,
+    )
+
+    response = ""
+    async for res in lmdeploy_pipe.generate(
+        messages,
+        gen_config=gen_config,
+        do_preprocess=do_preprocess,
+        stream_response=False,
+        session_id=1,
+    ):
+        response += res.response
+    return response
diff --git a/test/minirag/minirag/llm/lollms.py b/test/minirag/minirag/llm/lollms.py
new file mode 100755
index 0000000..d04af90
--- /dev/null
+++ b/test/minirag/minirag/llm/lollms.py
@@ -0,0 +1,224 @@
+"""
+LoLLMs (Lord of Large Language Models) Interface Module
+=====================================================
+
+This module provides the official interface for interacting with LoLLMs (Lord of Large Language and multimodal Systems),
+a unified framework for AI model interaction and deployment.
+
+LoLLMs is designed as a "one tool to rule them all" solution, providing seamless integration
+with various AI models while maintaining high performance and user-friendly interfaces.
+
+Author: ParisNeo
+Created: 2024-01-24
+License: Apache 2.0
+
+Copyright (c) 2024 ParisNeo
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Version: 2.0.0
+
+Change Log:
+- 2.0.0 (2024-01-24):
+    * Added async support for model inference
+    * Implemented streaming capabilities
+    * Added embedding generation functionality
+    * Enhanced parameter handling
+    * Improved error handling and timeout management
+
+Dependencies:
+    - aiohttp
+    - numpy
+    - Python >= 3.10
+
+Features:
+    - Async text generation with streaming support
+    - Embedding generation
+    - Configurable model parameters
+    - System prompt and chat history support
+    - Timeout handling
+    - API key authentication
+
+Usage:
+    from llm_interfaces.lollms import lollms_model_complete, lollms_embed
+
+Project Repository: https://github.com/ParisNeo/lollms
+Documentation: https://github.com/ParisNeo/lollms/docs
+"""
+
+__version__ = "1.0.0"
+__author__ = "ParisNeo"
+__status__ = "Production"
+__project_url__ = "https://github.com/ParisNeo/lollms"
+__doc_url__ = "https://github.com/ParisNeo/lollms/docs"
+import sys
+
+if sys.version_info < (3, 9):
+    from typing import AsyncIterator
+else:
+    from collections.abc import AsyncIterator
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+if not pm.is_installed("aiohttp"):
+    pm.install("aiohttp")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+import aiohttp
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.exceptions import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+
+from typing import Union, List
+import numpy as np
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def lollms_model_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    base_url="http://localhost:9600",
+    **kwargs,
+) -> Union[str, AsyncIterator[str]]:
+    """Client implementation for lollms generation."""
+
+    stream = True if kwargs.get("stream") else False
+    api_key = kwargs.pop("api_key", None)
+    headers = (
+        {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
+        if api_key
+        else {"Content-Type": "application/json"}
+    )
+
+    # Extract lollms specific parameters
+    request_data = {
+        "prompt": prompt,
+        "model_name": model,
+        "personality": kwargs.get("personality", -1),
+        "n_predict": kwargs.get("n_predict", None),
+        "stream": stream,
+        "temperature": kwargs.get("temperature", 0.1),
+        "top_k": kwargs.get("top_k", 50),
+        "top_p": kwargs.get("top_p", 0.95),
+        "repeat_penalty": kwargs.get("repeat_penalty", 0.8),
+        "repeat_last_n": kwargs.get("repeat_last_n", 40),
+        "seed": kwargs.get("seed", None),
+        "n_threads": kwargs.get("n_threads", 8),
+    }
+
+    # Prepare the full prompt including history
+    full_prompt = ""
+    if system_prompt:
+        full_prompt += f"{system_prompt}\n"
+    for msg in history_messages:
+        full_prompt += f"{msg['role']}: {msg['content']}\n"
+    full_prompt += prompt
+
+    request_data["prompt"] = full_prompt
+    timeout = aiohttp.ClientTimeout(total=kwargs.get("timeout", None))
+
+    async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
+        if stream:
+
+            async def inner():
+                async with session.post(
+                    f"{base_url}/lollms_generate", json=request_data
+                ) as response:
+                    async for line in response.content:
+                        yield line.decode().strip()
+
+            return inner()
+        else:
+            async with session.post(
+                f"{base_url}/lollms_generate", json=request_data
+            ) as response:
+                return await response.text()
+
+
+async def lollms_model_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> Union[str, AsyncIterator[str]]:
+    """Complete function for lollms model generation."""
+
+    # Extract and remove keyword_extraction from kwargs if present
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+
+    # Get model name from config
+    model_name = kwargs["hashing_kv"].global_config["llm_model_name"]
+
+    # If keyword extraction is needed, we might need to modify the prompt
+    # or add specific parameters for JSON output (if lollms supports it)
+    if keyword_extraction:
+        # Note: You might need to adjust this based on how lollms handles structured output
+        pass
+
+    return await lollms_model_if_cache(
+        model_name,
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+
+
+async def lollms_embed(
+    texts: List[str], embed_model=None, base_url="http://localhost:9600", **kwargs
+) -> np.ndarray:
+    """
+    Generate embeddings for a list of texts using lollms server.
+
+    Args:
+        texts: List of strings to embed
+        embed_model: Model name (not used directly as lollms uses configured vectorizer)
+        base_url: URL of the lollms server
+        **kwargs: Additional arguments passed to the request
+
+    Returns:
+        np.ndarray: Array of embeddings
+    """
+    api_key = kwargs.pop("api_key", None)
+    headers = (
+        {"Content-Type": "application/json", "Authorization": api_key}
+        if api_key
+        else {"Content-Type": "application/json"}
+    )
+    async with aiohttp.ClientSession(headers=headers) as session:
+        embeddings = []
+        for text in texts:
+            request_data = {"text": text}
+
+            async with session.post(
+                f"{base_url}/lollms_embed",
+                json=request_data,
+            ) as response:
+                result = await response.json()
+                embeddings.append(result["vector"])
+
+        return np.array(embeddings)
diff --git a/test/minirag/minirag/llm/nvidia_openai.py b/test/minirag/minirag/llm/nvidia_openai.py
new file mode 100755
index 0000000..4fdf876
--- /dev/null
+++ b/test/minirag/minirag/llm/nvidia_openai.py
@@ -0,0 +1,108 @@
+"""
+OpenAI LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with openai's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - openai
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.nvidia_openai import nvidia_openai_model_complete, nvidia_openai_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+
+import sys
+import os
+
+if sys.version_info < (3, 9):
+    pass
+else:
+    pass
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("openai"):
+    pm.install("openai")
+
+from openai import (
+    AsyncOpenAI,
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.utils import (
+    wrap_embedding_func_with_attrs,
+)
+
+
+import numpy as np
+
+
+@wrap_embedding_func_with_attrs(embedding_dim=2048, max_token_size=512)
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=60),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def nvidia_openai_embed(
+    texts: list[str],
+    model: str = "nvidia/llama-3.2-nv-embedqa-1b-v1",
+    # refer to https://build.nvidia.com/nim?filters=usecase%3Ausecase_text_to_embedding
+    base_url: str = "https://integrate.api.nvidia.com/v1",
+    api_key: str = None,
+    input_type: str = "passage",  # query for retrieval, passage for embedding
+    trunc: str = "NONE",  # NONE or START or END
+    encode: str = "float",  # float or base64
+) -> np.ndarray:
+    if api_key:
+        os.environ["OPENAI_API_KEY"] = api_key
+
+    openai_async_client = (
+        AsyncOpenAI() if base_url is None else AsyncOpenAI(base_url=base_url)
+    )
+    response = await openai_async_client.embeddings.create(
+        model=model,
+        input=texts,
+        encoding_format=encode,
+        extra_body={"input_type": input_type, "truncate": trunc},
+    )
+    return np.array([dp.embedding for dp in response.data])
diff --git a/test/minirag/minirag/llm/ollama.py b/test/minirag/minirag/llm/ollama.py
new file mode 100755
index 0000000..3c2f9e8
--- /dev/null
+++ b/test/minirag/minirag/llm/ollama.py
@@ -0,0 +1,158 @@
+"""
+Ollama LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with Ollama's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - ollama
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.ollama_interface import ollama_model_complete, ollama_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import sys
+
+if sys.version_info < (3, 9):
+    from typing import AsyncIterator
+else:
+    from collections.abc import AsyncIterator
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("ollama"):
+    pm.install("ollama")
+if not pm.is_installed("tenacity"):
+    pm.install("tenacity")
+
+import ollama
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+from minirag.exceptions import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+import numpy as np
+from typing import Union
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def ollama_model_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    **kwargs,
+) -> Union[str, AsyncIterator[str]]:
+    stream = True if kwargs.get("stream") else False
+    kwargs.pop("max_tokens", None)
+    # kwargs.pop("response_format", None) # allow json
+    host = kwargs.pop("host", None)
+    timeout = kwargs.pop("timeout", None)
+    kwargs.pop("hashing_kv", None)
+    api_key = kwargs.pop("api_key", None)
+    headers = (
+        {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
+        if api_key
+        else {"Content-Type": "application/json"}
+    )
+    ollama_client = ollama.AsyncClient(host=host, timeout=timeout, headers=headers)
+    messages = []
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+    messages.extend(history_messages)
+    messages.append({"role": "user", "content": prompt})
+
+    response = await ollama_client.chat(model=model, messages=messages, **kwargs)
+    if stream:
+        """cannot cache stream response"""
+
+        async def inner():
+            async for chunk in response:
+                yield chunk["message"]["content"]
+
+        return inner()
+    else:
+        return response["message"]["content"]
+
+
+async def ollama_model_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> Union[str, AsyncIterator[str]]:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    if keyword_extraction:
+        kwargs["format"] = "json"
+    model_name = kwargs["hashing_kv"].global_config["llm_model_name"]
+    return await ollama_model_if_cache(
+        model_name,
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+
+
+async def ollama_embedding(texts: list[str], embed_model, **kwargs) -> np.ndarray:
+    """
+    Deprecated in favor of `embed`.
+    """
+    embed_text = []
+    ollama_client = ollama.Client(**kwargs)
+    for text in texts:
+        data = ollama_client.embeddings(model=embed_model, prompt=text)
+        embed_text.append(data["embedding"])
+
+    return embed_text
+
+
+async def ollama_embed(texts: list[str], embed_model, **kwargs) -> np.ndarray:
+    api_key = kwargs.pop("api_key", None)
+    headers = (
+        {"Content-Type": "application/json", "Authorization": api_key}
+        if api_key
+        else {"Content-Type": "application/json"}
+    )
+    kwargs["headers"] = headers
+    ollama_client = ollama.Client(**kwargs)
+    data = ollama_client.embed(model=embed_model, input=texts)
+    return data["embeddings"]
diff --git a/test/minirag/minirag/llm/openai.py b/test/minirag/minirag/llm/openai.py
new file mode 100755
index 0000000..cdb09b8
--- /dev/null
+++ b/test/minirag/minirag/llm/openai.py
@@ -0,0 +1,266 @@
+"""
+OpenAI LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with openai's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - openai
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.openai import openai_model_complete, openai_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+
+import sys
+import os
+
+if sys.version_info < (3, 9):
+    from typing import AsyncIterator
+else:
+    from collections.abc import AsyncIterator
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("openai"):
+    pm.install("openai")
+
+from openai import (
+    AsyncOpenAI,
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+from minirag.utils import (
+    wrap_embedding_func_with_attrs,
+    locate_json_string_body_from_string,
+    safe_unicode_decode,
+    logger,
+)
+from pydantic import BaseModel
+from typing import List
+import numpy as np
+from typing import Union
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+
+class GPTKeywordExtractionFormat(BaseModel):
+    high_level_keywords: List[str]
+    low_level_keywords: List[str]
+
+async def openai_complete_if_cache(
+    model,
+    prompt,
+    system_prompt=None,
+    history_messages=[],
+    base_url=None,
+    api_key=None,
+    **kwargs,
+) -> str:
+    if api_key:
+        os.environ["OPENAI_API_KEY"] = api_key
+    if base_url==None:
+        base_url = os.environ["OPENAI_API_BASE"]
+    openai_async_client = (
+        AsyncOpenAI() if base_url is None else AsyncOpenAI(base_url=base_url)
+    )
+    kwargs.pop("hashing_kv", None)
+    kwargs.pop("keyword_extraction", None)
+    messages = []
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+    messages.extend(history_messages)
+    messages.append({"role": "user", "content": prompt})
+
+    # 添加日志输出
+    logger.debug("===== Query Input to LLM =====")
+    logger.debug(f"Query: {prompt}")
+    logger.debug(f"System prompt: {system_prompt}")
+    logger.debug("Full context:")
+    if "response_format" in kwargs:
+        response = await openai_async_client.beta.chat.completions.parse(
+            model=model, messages=messages, **kwargs
+        )
+    else:
+        response = await openai_async_client.chat.completions.create(
+            model=model, messages=messages, **kwargs
+        )
+
+    if hasattr(response, "__aiter__"):
+
+        async def inner():
+            async for chunk in response:
+                content = chunk.choices[0].delta.content
+                if content is None:
+                    continue
+                if r"\u" in content:
+                    content = safe_unicode_decode(content.encode("utf-8"))
+                yield content
+
+        return inner()
+    else:
+        if not response or not hasattr(response, "choices") or not response.choices:
+            logger.error("No valid choices returned. Full response: %s", response)
+            return ""  # or raise a more specific exception
+        content = response.choices[0].message.content
+        if content is None:
+            logger.error("The message content is None. Full response: %s", response)
+            return ""
+        if r"\u" in content:
+            content = safe_unicode_decode(content.encode("utf-8"))
+        return content
+
+
+async def openai_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> Union[str, AsyncIterator[str]]:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    if keyword_extraction:
+        kwargs["response_format"] = "json"
+    model_name = kwargs["hashing_kv"].global_config["llm_model_name"]
+    return await openai_complete_if_cache(
+        model_name,
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+
+
+async def gpt_4o_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    if keyword_extraction:
+        kwargs["response_format"] = GPTKeywordExtractionFormat
+    return await openai_complete_if_cache(
+        "gpt-4o",
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+
+
+async def gpt_4o_mini_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    if keyword_extraction:
+        kwargs["response_format"] = GPTKeywordExtractionFormat
+    return await openai_complete_if_cache(
+        "gpt-4o-mini",
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        **kwargs,
+    )
+
+
+async def nvidia_openai_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+) -> str:
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    result = await openai_complete_if_cache(
+        "nvidia/llama-3.1-nemotron-70b-instruct",  # context length 128k
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        base_url="https://integrate.api.nvidia.com/v1",
+        **kwargs,
+    )
+    if keyword_extraction:  # TODO: use JSON API
+        return locate_json_string_body_from_string(result)
+    return result
+
+async def openrouter_openai_complete(
+    prompt, 
+    system_prompt=None, 
+    history_messages=[], 
+    keyword_extraction=False, 
+    api_key: str = None, 
+    **kwargs,
+) -> str:
+    if api_key:
+        os.environ["OPENROUTER_API_KEY"] = api_key
+
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+    result = await openai_complete_if_cache(
+        "google/gemini-2.0-flash-001",  # change accordingly
+        prompt,
+        system_prompt=system_prompt,
+        history_messages=history_messages,
+        base_url="https://openrouter.ai/api/v1",
+        api_key=api_key,
+        **kwargs,
+    )
+    if keyword_extraction:  # TODO: use JSON API
+        return locate_json_string_body_from_string(result)
+    return result
+
+@wrap_embedding_func_with_attrs(embedding_dim=1536, max_token_size=8192)
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=60),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def openai_embed(
+    texts: list[str],
+    model: str = "text-embedding-3-small",
+    base_url: str = None,
+    api_key: str = None,
+) -> np.ndarray:
+    if api_key:
+        os.environ["OPENAI_API_KEY"] = api_key
+
+    openai_async_client = (
+        AsyncOpenAI() if base_url is None else AsyncOpenAI(base_url=base_url)
+    )
+    response = await openai_async_client.embeddings.create(
+        model=model, input=texts, encoding_format="float"
+    )
+    return np.array([dp.embedding for dp in response.data])
diff --git a/test/minirag/minirag/llm/siliconcloud.py b/test/minirag/minirag/llm/siliconcloud.py
new file mode 100755
index 0000000..4aaaf7e
--- /dev/null
+++ b/test/minirag/minirag/llm/siliconcloud.py
@@ -0,0 +1,109 @@
+"""
+SiliconCloud Embedding Interface Module
+==========================
+
+This module provides interfaces for interacting with SiliconCloud system,
+including embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added embedding generation
+
+Dependencies:
+    - tenacity
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.siliconcloud import siliconcloud_model_complete, siliconcloud_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import sys
+
+if sys.version_info < (3, 9):
+    pass
+else:
+    pass
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("lmdeploy"):
+    pm.install("lmdeploy")
+
+from openai import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+
+import numpy as np
+import aiohttp
+import base64
+import struct
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=60),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def siliconcloud_embedding(
+    texts: list[str],
+    model: str = "netease-youdao/bce-embedding-base_v1",
+    base_url: str = "https://api.siliconflow.cn/v1/embeddings",
+    max_token_size: int = 512,
+    api_key: str = None,
+) -> np.ndarray:
+    if api_key and not api_key.startswith("Bearer "):
+        api_key = "Bearer " + api_key
+
+    headers = {"Authorization": api_key, "Content-Type": "application/json"}
+
+    truncate_texts = [text[0:max_token_size] for text in texts]
+
+    payload = {"model": model, "input": truncate_texts, "encoding_format": "base64"}
+
+    base64_strings = []
+    async with aiohttp.ClientSession() as session:
+        async with session.post(base_url, headers=headers, json=payload) as response:
+            content = await response.json()
+            if "code" in content:
+                raise ValueError(content)
+            base64_strings = [item["embedding"] for item in content["data"]]
+
+    embeddings = []
+    for string in base64_strings:
+        decode_bytes = base64.b64decode(string)
+        n = len(decode_bytes) // 4
+        float_array = struct.unpack("<" + "f" * n, decode_bytes)
+        embeddings.append(float_array)
+    return np.array(embeddings)
diff --git a/test/minirag/minirag/llm/zhipu.py b/test/minirag/minirag/llm/zhipu.py
new file mode 100755
index 0000000..e1be7bf
--- /dev/null
+++ b/test/minirag/minirag/llm/zhipu.py
@@ -0,0 +1,246 @@
+"""
+Zhipu LLM Interface Module
+==========================
+
+This module provides interfaces for interacting with LMDeploy's language models,
+including text generation and embedding capabilities.
+
+Author: Lightrag team
+Created: 2024-01-24
+License: MIT License
+
+Copyright (c) 2024 Lightrag
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+Version: 1.0.0
+
+Change Log:
+- 1.0.0 (2024-01-24): Initial release
+    * Added async chat completion support
+    * Added embedding generation
+    * Added stream response capability
+
+Dependencies:
+    - tenacity
+    - numpy
+    - pipmaster
+    - Python >= 3.10
+
+Usage:
+    from llm_interfaces.zhipu import zhipu_model_complete, zhipu_embed
+"""
+
+__version__ = "1.0.0"
+__author__ = "lightrag Team"
+__status__ = "Production"
+
+import sys
+import re
+import json
+
+if sys.version_info < (3, 9):
+    pass
+else:
+    pass
+import pipmaster as pm  # Pipmaster for dynamic library install
+
+# install specific modules
+if not pm.is_installed("zhipuai"):
+    pm.install("zhipuai")
+
+from openai import (
+    APIConnectionError,
+    RateLimitError,
+    APITimeoutError,
+)
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_exponential,
+    retry_if_exception_type,
+)
+
+from minirag.utils import (
+    wrap_embedding_func_with_attrs,
+    logger,
+)
+
+from minirag.types import GPTKeywordExtractionFormat
+
+import numpy as np
+from typing import Union, List, Optional, Dict
+
+
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=10),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def zhipu_complete_if_cache(
+    prompt: Union[str, List[Dict[str, str]]],
+    model: str = "glm-4-flashx",  # The most cost/performance balance model in glm-4 series
+    api_key: Optional[str] = None,
+    system_prompt: Optional[str] = None,
+    history_messages: List[Dict[str, str]] = [],
+    **kwargs,
+) -> str:
+    # dynamically load ZhipuAI
+    try:
+        from zhipuai import ZhipuAI
+    except ImportError:
+        raise ImportError("Please install zhipuai before initialize zhipuai backend.")
+
+    if api_key:
+        client = ZhipuAI(api_key=api_key)
+    else:
+        # please set ZHIPUAI_API_KEY in your environment
+        # os.environ["ZHIPUAI_API_KEY"]
+        client = ZhipuAI()
+
+    messages = []
+
+    if not system_prompt:
+        system_prompt = "You are a helpful assistant. Note that sensitive words in the content should be replaced with ***"
+
+    # Add system prompt if provided
+    if system_prompt:
+        messages.append({"role": "system", "content": system_prompt})
+    messages.extend(history_messages)
+    messages.append({"role": "user", "content": prompt})
+
+    # Add debug logging
+    logger.debug("===== Query Input to LLM =====")
+    logger.debug(f"Query: {prompt}")
+    logger.debug(f"System prompt: {system_prompt}")
+
+    # Remove unsupported kwargs
+    kwargs = {
+        k: v for k, v in kwargs.items() if k not in ["hashing_kv", "keyword_extraction"]
+    }
+
+    response = client.chat.completions.create(model=model, messages=messages, **kwargs)
+
+    return response.choices[0].message.content
+
+
+async def zhipu_complete(
+    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
+):
+    # Pop keyword_extraction from kwargs to avoid passing it to zhipu_complete_if_cache
+    keyword_extraction = kwargs.pop("keyword_extraction", None)
+
+    if keyword_extraction:
+        # Add a system prompt to guide the model to return JSON format
+        extraction_prompt = """You are a helpful assistant that extracts keywords from text.
+        Please analyze the content and extract two types of keywords:
+        1. High-level keywords: Important concepts and main themes
+        2. Low-level keywords: Specific details and supporting elements
+
+        Return your response in this exact JSON format:
+        {
+            "high_level_keywords": ["keyword1", "keyword2"],
+            "low_level_keywords": ["keyword1", "keyword2", "keyword3"]
+        }
+
+        Only return the JSON, no other text."""
+
+        # Combine with existing system prompt if any
+        if system_prompt:
+            system_prompt = f"{system_prompt}\n\n{extraction_prompt}"
+        else:
+            system_prompt = extraction_prompt
+
+        try:
+            response = await zhipu_complete_if_cache(
+                prompt=prompt,
+                system_prompt=system_prompt,
+                history_messages=history_messages,
+                **kwargs,
+            )
+
+            # Try to parse as JSON
+            try:
+                data = json.loads(response)
+                return GPTKeywordExtractionFormat(
+                    high_level_keywords=data.get("high_level_keywords", []),
+                    low_level_keywords=data.get("low_level_keywords", []),
+                )
+            except json.JSONDecodeError:
+                # If direct JSON parsing fails, try to extract JSON from text
+                match = re.search(r"\{[\s\S]*\}", response)
+                if match:
+                    try:
+                        data = json.loads(match.group())
+                        return GPTKeywordExtractionFormat(
+                            high_level_keywords=data.get("high_level_keywords", []),
+                            low_level_keywords=data.get("low_level_keywords", []),
+                        )
+                    except json.JSONDecodeError:
+                        pass
+
+                # If all parsing fails, log warning and return empty format
+                logger.warning(
+                    f"Failed to parse keyword extraction response: {response}"
+                )
+                return GPTKeywordExtractionFormat(
+                    high_level_keywords=[], low_level_keywords=[]
+                )
+        except Exception as e:
+            logger.error(f"Error during keyword extraction: {str(e)}")
+            return GPTKeywordExtractionFormat(
+                high_level_keywords=[], low_level_keywords=[]
+            )
+    else:
+        # For non-keyword-extraction, just return the raw response string
+        return await zhipu_complete_if_cache(
+            prompt=prompt,
+            system_prompt=system_prompt,
+            history_messages=history_messages,
+            **kwargs,
+        )
+
+
+@wrap_embedding_func_with_attrs(embedding_dim=1024, max_token_size=8192)
+@retry(
+    stop=stop_after_attempt(3),
+    wait=wait_exponential(multiplier=1, min=4, max=60),
+    retry=retry_if_exception_type(
+        (RateLimitError, APIConnectionError, APITimeoutError)
+    ),
+)
+async def zhipu_embedding(
+    texts: list[str], model: str = "embedding-3", api_key: str = None, **kwargs
+) -> np.ndarray:
+    # dynamically load ZhipuAI
+    try:
+        from zhipuai import ZhipuAI
+    except ImportError:
+        raise ImportError("Please install zhipuai before initialize zhipuai backend.")
+    if api_key:
+        client = ZhipuAI(api_key=api_key)
+    else:
+        # please set ZHIPUAI_API_KEY in your environment
+        # os.environ["ZHIPUAI_API_KEY"]
+        client = ZhipuAI()
+
+    # Convert single text to list if needed
+    if isinstance(texts, str):
+        texts = [texts]
+
+    embeddings = []
+    for text in texts:
+        try:
+            response = client.embeddings.create(model=model, input=[text], **kwargs)
+            embeddings.append(response.data[0].embedding)
+        except Exception as e:
+            raise Exception(f"Error calling ChatGLM Embedding API: {str(e)}")
+
+    return np.array(embeddings)
diff --git a/test/minirag/minirag/minirag.py b/test/minirag/minirag/minirag.py
new file mode 100755
index 0000000..ce5223e
--- /dev/null
+++ b/test/minirag/minirag/minirag.py
@@ -0,0 +1,607 @@
+import asyncio
+import os
+from dataclasses import asdict, dataclass, field
+from datetime import datetime
+from functools import partial
+from typing import Type, cast, Any
+from dotenv import load_dotenv
+
+
+from .operate import (
+    chunking_by_token_size,
+    extract_entities,
+    hybrid_query,
+    minirag_query,
+    naive_query,
+)
+
+from .utils import (
+    EmbeddingFunc,
+    compute_mdhash_id,
+    limit_async_func_call,
+    convert_response_to_json,
+    logger,
+    clean_text,
+    get_content_summary,
+    set_logger,
+    logger,
+)
+from .base import (
+    BaseGraphStorage,
+    BaseKVStorage,
+    BaseVectorStorage,
+    StorageNameSpace,
+    QueryParam,
+    DocStatus,
+)
+
+
+STORAGES = {
+    "NetworkXStorage": ".kg.networkx_impl",
+    "JsonKVStorage": ".kg.json_kv_impl",
+    "NanoVectorDBStorage": ".kg.nano_vector_db_impl",
+    "JsonDocStatusStorage": ".kg.jsondocstatus_impl",
+    "Neo4JStorage": ".kg.neo4j_impl",
+    "OracleKVStorage": ".kg.oracle_impl",
+    "OracleGraphStorage": ".kg.oracle_impl",
+    "OracleVectorDBStorage": ".kg.oracle_impl",
+    "MilvusVectorDBStorge": ".kg.milvus_impl",
+    "MongoKVStorage": ".kg.mongo_impl",
+    "MongoGraphStorage": ".kg.mongo_impl",
+    "RedisKVStorage": ".kg.redis_impl",
+    "ChromaVectorDBStorage": ".kg.chroma_impl",
+    "TiDBKVStorage": ".kg.tidb_impl",
+    "TiDBVectorDBStorage": ".kg.tidb_impl",
+    "TiDBGraphStorage": ".kg.tidb_impl",
+    "PGKVStorage": ".kg.postgres_impl",
+    "PGVectorStorage": ".kg.postgres_impl",
+    "AGEStorage": ".kg.age_impl",
+    "PGGraphStorage": ".kg.postgres_impl",
+    "GremlinStorage": ".kg.gremlin_impl",
+    "PGDocStatusStorage": ".kg.postgres_impl",
+}
+
+# future KG integrations
+
+# from .kg.ArangoDB_impl import (
+#     GraphStorage as ArangoDBStorage
+# )
+
+load_dotenv(dotenv_path=".env", override=False)
+
+
+def lazy_external_import(module_name: str, class_name: str):
+    """Lazily import a class from an external module based on the package of the caller."""
+
+    # Get the caller's module and package
+    import inspect
+
+    caller_frame = inspect.currentframe().f_back
+    module = inspect.getmodule(caller_frame)
+    package = module.__package__ if module else None
+
+    def import_class(*args, **kwargs):
+        import importlib
+
+        module = importlib.import_module(module_name, package=package)
+        cls = getattr(module, class_name)
+        return cls(*args, **kwargs)
+
+    return import_class
+
+
+def always_get_an_event_loop() -> asyncio.AbstractEventLoop:
+    """
+    Ensure that there is always an event loop available.
+
+    This function tries to get the current event loop. If the current event loop is closed or does not exist,
+    it creates a new event loop and sets it as the current event loop.
+
+    Returns:
+        asyncio.AbstractEventLoop: The current or newly created event loop.
+    """
+    try:
+        # Try to get the current event loop
+        current_loop = asyncio.get_event_loop()
+        if current_loop.is_closed():
+            raise RuntimeError("Event loop is closed.")
+        return current_loop
+
+    except RuntimeError:
+        # If no event loop exists or it is closed, create a new one
+        logger.info("Creating a new event loop in main thread.")
+        new_loop = asyncio.new_event_loop()
+        asyncio.set_event_loop(new_loop)
+        return new_loop
+
+
+@dataclass
+class MiniRAG:
+    working_dir: str = field(
+        default_factory=lambda: f"./minirag_cache_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}"
+    )
+
+    # RAGmode: str = 'minirag'
+
+    kv_storage: str = field(default="JsonKVStorage")
+    vector_storage: str = field(default="NanoVectorDBStorage")
+    graph_storage: str = field(default="NetworkXStorage")
+
+    current_log_level = logger.level
+    log_level: str = field(default=current_log_level)
+
+    # text chunking
+    chunk_token_size: int = 1200
+    chunk_overlap_token_size: int = 100
+    tiktoken_model_name: str = "gpt-4o-mini"
+
+    # entity extraction
+    entity_extract_max_gleaning: int = 1
+    entity_summary_to_max_tokens: int = 500
+
+    # node embedding
+    node_embedding_algorithm: str = "node2vec"
+    node2vec_params: dict = field(
+        default_factory=lambda: {
+            "dimensions": 1536,
+            "num_walks": 10,
+            "walk_length": 40,
+            "window_size": 2,
+            "iterations": 3,
+            "random_seed": 3,
+        }
+    )
+
+    embedding_func: EmbeddingFunc = None
+    embedding_batch_num: int = 32
+    embedding_func_max_async: int = 16
+
+    # LLM
+    llm_model_func: callable = None
+    llm_model_name: str = (
+        "meta-llama/Llama-3.2-1B-Instruct"  #'meta-llama/Llama-3.2-1B'#'google/gemma-2-2b-it'
+    )
+    llm_model_max_token_size: int = 32768
+    llm_model_max_async: int = 16
+    llm_model_kwargs: dict = field(default_factory=dict)
+
+    # storage
+    vector_db_storage_cls_kwargs: dict = field(default_factory=dict)
+
+    enable_llm_cache: bool = True
+
+    # extension
+    addon_params: dict = field(default_factory=dict)
+    convert_response_to_json_func: callable = convert_response_to_json
+
+    # Add new field for document status storage type
+    doc_status_storage: str = field(default="JsonDocStatusStorage")
+
+    # Custom Chunking Function
+    chunking_func: callable = chunking_by_token_size
+    chunking_func_kwargs: dict = field(default_factory=dict)
+
+    max_parallel_insert: int = field(default=int(os.getenv("MAX_PARALLEL_INSERT", 2)))
+
+    def __post_init__(self):
+        log_file = os.path.join(self.working_dir, "minirag.log")
+        set_logger(log_file)
+        logger.setLevel(self.log_level)
+
+        logger.info(f"Logger initialized for working directory: {self.working_dir}")
+        if not os.path.exists(self.working_dir):
+            logger.info(f"Creating working directory {self.working_dir}")
+            os.makedirs(self.working_dir)
+
+        # show config
+        global_config = asdict(self)
+        _print_config = ",\n  ".join([f"{k} = {v}" for k, v in global_config.items()])
+        logger.debug(f"MiniRAG init with param:\n  {_print_config}\n")
+
+        # @TODO: should move all storage setup here to leverage initial start params attached to self.
+
+        self.key_string_value_json_storage_cls: Type[BaseKVStorage] = (
+            self._get_storage_class(self.kv_storage)
+        )
+        self.vector_db_storage_cls: Type[BaseVectorStorage] = self._get_storage_class(
+            self.vector_storage
+        )
+        self.graph_storage_cls: Type[BaseGraphStorage] = self._get_storage_class(
+            self.graph_storage
+        )
+
+        self.key_string_value_json_storage_cls = partial(
+            self.key_string_value_json_storage_cls, global_config=global_config
+        )
+
+        self.vector_db_storage_cls = partial(
+            self.vector_db_storage_cls, global_config=global_config
+        )
+
+        self.graph_storage_cls = partial(
+            self.graph_storage_cls, global_config=global_config
+        )
+        self.json_doc_status_storage = self.key_string_value_json_storage_cls(
+            namespace="json_doc_status_storage",
+            embedding_func=None,
+        )
+
+        if not os.path.exists(self.working_dir):
+            logger.info(f"Creating working directory {self.working_dir}")
+            os.makedirs(self.working_dir)
+
+        self.llm_response_cache = (
+            self.key_string_value_json_storage_cls(
+                namespace="llm_response_cache",
+                global_config=asdict(self),
+                embedding_func=None,
+            )
+            if self.enable_llm_cache
+            else None
+        )
+
+        self.embedding_func = limit_async_func_call(self.embedding_func_max_async)(
+            self.embedding_func
+        )
+
+        ####
+        # add embedding func by walter
+        ####
+        self.full_docs = self.key_string_value_json_storage_cls(
+            namespace="full_docs",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+        )
+        self.text_chunks = self.key_string_value_json_storage_cls(
+            namespace="text_chunks",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+        )
+        self.chunk_entity_relation_graph = self.graph_storage_cls(
+            namespace="chunk_entity_relation",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+        )
+        ####
+        # add embedding func by walter over
+        ####
+
+        self.entities_vdb = self.vector_db_storage_cls(
+            namespace="entities",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+            meta_fields={"entity_name"},
+        )
+        global_config = asdict(self)
+
+        self.entity_name_vdb = self.vector_db_storage_cls(
+            namespace="entities_name",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+            meta_fields={"entity_name"},
+        )
+
+        self.relationships_vdb = self.vector_db_storage_cls(
+            namespace="relationships",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+            meta_fields={"src_id", "tgt_id"},
+        )
+        self.chunks_vdb = self.vector_db_storage_cls(
+            namespace="chunks",
+            global_config=asdict(self),
+            embedding_func=self.embedding_func,
+        )
+
+        self.llm_model_func = limit_async_func_call(self.llm_model_max_async)(
+            partial(
+                self.llm_model_func,
+                hashing_kv=self.llm_response_cache,
+                **self.llm_model_kwargs,
+            )
+        )
+        # Initialize document status storage
+        self.doc_status_storage_cls = self._get_storage_class(self.doc_status_storage)
+        self.doc_status = self.doc_status_storage_cls(
+            namespace="doc_status",
+            global_config=global_config,
+            embedding_func=None,
+        )
+
+    def _get_storage_class(self, storage_name: str) -> dict:
+        import_path = STORAGES[storage_name]
+        storage_class = lazy_external_import(import_path, storage_name)
+        return storage_class
+
+    def set_storage_client(self, db_client):
+        # Now only tested on Oracle Database
+        for storage in [
+            self.vector_db_storage_cls,
+            self.graph_storage_cls,
+            self.doc_status,
+            self.full_docs,
+            self.text_chunks,
+            self.llm_response_cache,
+            self.key_string_value_json_storage_cls,
+            self.chunks_vdb,
+            self.relationships_vdb,
+            self.entities_vdb,
+            self.graph_storage_cls,
+            self.chunk_entity_relation_graph,
+            self.llm_response_cache,
+        ]:
+            # set client
+            storage.db = db_client
+
+    def insert(self, string_or_strings):
+        loop = always_get_an_event_loop()
+        return loop.run_until_complete(self.ainsert(string_or_strings))
+
+    async def ainsert(
+        self,
+        input: str | list[str],
+        split_by_character: str | None = None,
+        split_by_character_only: bool = False,
+        ids: str | list[str] | None = None,
+    ) -> None:
+        if isinstance(input, str):
+            input = [input]
+        if isinstance(ids, str):
+            ids = [ids]
+
+        await self.apipeline_enqueue_documents(input, ids)
+        await self.apipeline_process_enqueue_documents(
+            split_by_character, split_by_character_only
+        )
+
+        # Perform additional entity extraction as per original ainsert logic
+        inserting_chunks = {
+            compute_mdhash_id(dp["content"], prefix="chunk-"): {
+                **dp,
+                "full_doc_id": doc_id,
+            }
+            for doc_id, status_doc in (
+                await self.doc_status.get_docs_by_status(DocStatus.PROCESSED)
+            ).items()
+            for dp in self.chunking_func(
+                status_doc.content,
+                self.chunk_overlap_token_size,
+                self.chunk_token_size,
+                self.tiktoken_model_name,
+            )
+        }
+
+        if inserting_chunks:
+            logger.info("Performing entity extraction on newly processed chunks")
+            await extract_entities(
+                inserting_chunks,
+                knowledge_graph_inst=self.chunk_entity_relation_graph,
+                entity_vdb=self.entities_vdb,
+                entity_name_vdb=self.entity_name_vdb,
+                relationships_vdb=self.relationships_vdb,
+                global_config=asdict(self),
+            )
+ 
+        await self._insert_done()
+
+    async def apipeline_enqueue_documents(
+        self, input: str | list[str], ids: list[str] | None = None
+    ) -> None:
+        """
+        Pipeline for Processing Documents
+
+        1. Validate ids if provided or generate MD5 hash IDs
+        2. Remove duplicate contents
+        3. Generate document initial status
+        4. Filter out already processed documents
+        5. Enqueue document in status
+        """
+        if isinstance(input, str):
+            input = [input]
+        if isinstance(ids, str):
+            ids = [ids]
+
+        if ids is not None:
+            if len(ids) != len(input):
+                raise ValueError("Number of IDs must match the number of documents")
+            if len(ids) != len(set(ids)):
+                raise ValueError("IDs must be unique")
+            contents = {id_: doc for id_, doc in zip(ids, input)}
+        else:
+            input = list(set(clean_text(doc) for doc in input))
+            contents = {compute_mdhash_id(doc, prefix="doc-"): doc for doc in input}
+
+        unique_contents = {
+            id_: content
+            for content, id_ in {
+                content: id_ for id_, content in contents.items()
+            }.items()
+        }
+        new_docs: dict[str, Any] = {
+            id_: {
+                "content": content,
+                "content_summary": get_content_summary(content),
+                "content_length": len(content),
+                "status": DocStatus.PENDING,
+                "created_at": datetime.now().isoformat(),
+                "updated_at": datetime.now().isoformat(),
+            }
+            for id_, content in unique_contents.items()
+        }
+
+        all_new_doc_ids = set(new_docs.keys())
+        unique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)
+
+        new_docs = {
+            doc_id: new_docs[doc_id]
+            for doc_id in unique_new_doc_ids
+            if doc_id in new_docs
+        }
+        if not new_docs:
+            logger.info("No new unique documents were found.")
+            return
+
+        await self.doc_status.upsert(new_docs)
+        logger.info(f"Stored {len(new_docs)} new unique documents")
+
+    async def apipeline_process_enqueue_documents(
+        self,
+        split_by_character: str | None = None,
+        split_by_character_only: bool = False,
+    ) -> None:
+        """
+        Process pending documents by splitting them into chunks, processing
+        each chunk for entity and relation extraction, and updating the
+        document status.
+        """
+        processing_docs, failed_docs, pending_docs = await asyncio.gather(
+            self.doc_status.get_docs_by_status(DocStatus.PROCESSING),
+            self.doc_status.get_docs_by_status(DocStatus.FAILED),
+            self.doc_status.get_docs_by_status(DocStatus.PENDING),
+        )
+
+        to_process_docs: dict[str, Any] = {
+            **processing_docs,
+            **failed_docs,
+            **pending_docs,
+        }
+        if not to_process_docs:
+            logger.info("No documents to process")
+            return
+
+        docs_batches = [
+            list(to_process_docs.items())[i : i + self.max_parallel_insert]
+            for i in range(0, len(to_process_docs), self.max_parallel_insert)
+        ]
+        logger.info(f"Number of batches to process: {len(docs_batches)}")
+
+        for batch_idx, docs_batch in enumerate(docs_batches):
+            for doc_id, status_doc in docs_batch:
+                chunks = {
+                    compute_mdhash_id(dp["content"], prefix="chunk-"): {
+                        **dp,
+                        "full_doc_id": doc_id,
+                    }
+                    for dp in self.chunking_func(
+                        status_doc.content,
+                        self.chunk_overlap_token_size,
+                        self.chunk_token_size,
+                        self.tiktoken_model_name,
+                    )
+                }
+                await asyncio.gather(
+                    self.chunks_vdb.upsert(chunks),
+                    self.full_docs.upsert({doc_id: {"content": status_doc.content}}),
+                    self.text_chunks.upsert(chunks),
+                )
+                await self.doc_status.upsert(
+                    {
+                        doc_id: {
+                            "status": DocStatus.PROCESSED,
+                            "chunks_count": len(chunks),
+                            "content": status_doc.content,
+                            "content_summary": status_doc.content_summary,
+                            "content_length": status_doc.content_length,
+                            "created_at": status_doc.created_at,
+                            "updated_at": datetime.now().isoformat(),
+                        }
+                    }
+                )
+        logger.info("Document processing pipeline completed")
+
+    async def _insert_done(self):
+        tasks = []
+        for storage_inst in [
+            self.full_docs,
+            self.text_chunks,
+            self.llm_response_cache,
+            self.entities_vdb,
+            self.entity_name_vdb,
+            self.relationships_vdb,
+            self.chunks_vdb,
+            self.chunk_entity_relation_graph,
+        ]:
+            if storage_inst is None:
+                continue
+            tasks.append(cast(StorageNameSpace, storage_inst).index_done_callback())
+        await asyncio.gather(*tasks)
+
+    def query(self, query: str, param: QueryParam = QueryParam()):
+        loop = always_get_an_event_loop()
+        return loop.run_until_complete(self.aquery(query, param))
+
+    async def aquery(self, query: str, param: QueryParam = QueryParam()):
+        if param.mode == "light":
+            response = await hybrid_query(
+                query,
+                self.chunk_entity_relation_graph,
+                self.entities_vdb,
+                self.relationships_vdb,
+                self.text_chunks,
+                param,
+                asdict(self),
+            )
+        elif param.mode == "mini":
+            response = await minirag_query(
+                query,
+                self.chunk_entity_relation_graph,
+                self.entities_vdb,
+                self.entity_name_vdb,
+                self.relationships_vdb,
+                self.chunks_vdb,
+                self.text_chunks,
+                self.embedding_func,
+                param,
+                asdict(self),
+            )
+        elif param.mode == "naive":
+            response = await naive_query(
+                query,
+                self.chunks_vdb,
+                self.text_chunks,
+                param,
+                asdict(self),
+            )
+        else:
+            raise ValueError(f"Unknown mode {param.mode}")
+        await self._query_done()
+        return response
+
+    async def _query_done(self):
+        tasks = []
+        for storage_inst in [self.llm_response_cache]:
+            if storage_inst is None:
+                continue
+            tasks.append(cast(StorageNameSpace, storage_inst).index_done_callback())
+        await asyncio.gather(*tasks)
+
+    def delete_by_entity(self, entity_name: str):
+        loop = always_get_an_event_loop()
+        return loop.run_until_complete(self.adelete_by_entity(entity_name))
+
+    async def adelete_by_entity(self, entity_name: str):
+        entity_name = f'"{entity_name.upper()}"'
+
+        try:
+            await self.entities_vdb.delete_entity(entity_name)
+            await self.relationships_vdb.delete_relation(entity_name)
+            await self.chunk_entity_relation_graph.delete_node(entity_name)
+
+            logger.info(
+                f"Entity '{entity_name}' and its relationships have been deleted."
+            )
+            await self._delete_by_entity_done()
+        except Exception as e:
+            logger.error(f"Error while deleting entity '{entity_name}': {e}")
+
+    async def _delete_by_entity_done(self):
+        tasks = []
+        for storage_inst in [
+            self.entities_vdb,
+            self.relationships_vdb,
+            self.chunk_entity_relation_graph,
+        ]:
+            if storage_inst is None:
+                continue
+            tasks.append(cast(StorageNameSpace, storage_inst).index_done_callback())
+        await asyncio.gather(*tasks)
diff --git a/test/minirag/minirag/operate.py b/test/minirag/minirag/operate.py
new file mode 100755
index 0000000..b938dc3
--- /dev/null
+++ b/test/minirag/minirag/operate.py
@@ -0,0 +1,1479 @@
+import asyncio
+import json
+import re
+from typing import Union
+from collections import Counter, defaultdict
+import warnings
+import json_repair # type: ignore
+
+from .utils import (
+    list_of_list_to_csv,
+    truncate_list_by_token_size,
+    split_string_by_multi_markers,
+    logger,
+    locate_json_string_body_from_string,
+    process_combine_contexts,
+    clean_str,
+    edge_vote_path,
+    encode_string_by_tiktoken,
+    decode_tokens_by_tiktoken,
+    is_float_regex,
+    pack_user_ass_to_openai_messages,
+    compute_mdhash_id,
+    calculate_similarity,
+    cal_path_score_list,
+)
+from .base import (
+    BaseGraphStorage,
+    BaseKVStorage,
+    BaseVectorStorage,
+    TextChunkSchema,
+    QueryParam,
+)
+from .prompt import GRAPH_FIELD_SEP, PROMPTS
+
+
+def chunking_by_token_size(
+    content: str, overlap_token_size=128, max_token_size=1024, tiktoken_model="gpt-4o"
+):
+    tokens = encode_string_by_tiktoken(content, model_name=tiktoken_model)
+    results = []
+    for index, start in enumerate(
+        range(0, len(tokens), max_token_size - overlap_token_size)
+    ):
+        chunk_content = decode_tokens_by_tiktoken(
+            tokens[start : start + max_token_size], model_name=tiktoken_model
+        )
+        results.append(
+            {
+                "tokens": min(max_token_size, len(tokens) - start),
+                "content": chunk_content.strip(),
+                "chunk_order_index": index,
+            }
+        )
+    return results
+
+
+async def _handle_entity_relation_summary(
+    entity_or_relation_name: str,
+    description: str,
+    global_config: dict,
+) -> str:
+    tiktoken_model_name = global_config["tiktoken_model_name"]
+    summary_max_tokens = global_config["entity_summary_to_max_tokens"]
+
+    tokens = encode_string_by_tiktoken(description, model_name=tiktoken_model_name)
+    if len(tokens) < summary_max_tokens:  # No need for summary
+        return description
+
+
+async def _handle_single_entity_extraction(
+    record_attributes: list[str],
+    chunk_key: str,
+):
+    if len(record_attributes) < 4 or record_attributes[0] != '"entity"':
+        return None
+    # add this record as a node in the G
+    entity_name = clean_str(record_attributes[1].upper())
+    if not entity_name.strip():
+        return None
+    entity_type = clean_str(record_attributes[2].upper())
+    entity_description = clean_str(record_attributes[3])
+    entity_source_id = chunk_key
+    return dict(
+        entity_name=entity_name,
+        entity_type=entity_type,
+        description=entity_description,
+        source_id=entity_source_id,
+    )
+
+
+async def _handle_single_relationship_extraction(
+    record_attributes: list[str],
+    chunk_key: str,
+):
+    if len(record_attributes) < 5 or record_attributes[0] != '"relationship"':
+        return None
+    # add this record as edge
+    source = clean_str(record_attributes[1].upper())
+    target = clean_str(record_attributes[2].upper())
+    edge_description = clean_str(record_attributes[3])
+
+    edge_keywords = clean_str(record_attributes[4])
+    edge_source_id = chunk_key
+    weight = (
+        float(record_attributes[-1]) if is_float_regex(record_attributes[-1]) else 1.0
+    )
+    return dict(
+        src_id=source,
+        tgt_id=target,
+        weight=weight,
+        description=edge_description,
+        keywords=edge_keywords,
+        source_id=edge_source_id,
+    )
+
+
+async def _merge_nodes_then_upsert(
+    entity_name: str,
+    nodes_data: list[dict],
+    knowledge_graph_inst: BaseGraphStorage,
+    global_config: dict,
+):
+    already_entitiy_types = []
+    already_source_ids = []
+    already_description = []
+
+    already_node = await knowledge_graph_inst.get_node(entity_name)
+    if already_node is not None:
+        already_entitiy_types.append(already_node["entity_type"])
+        already_source_ids.extend(
+            split_string_by_multi_markers(already_node["source_id"], [GRAPH_FIELD_SEP])
+        )
+        already_description.append(already_node["description"])
+
+    entity_type = sorted(
+        Counter(
+            [dp["entity_type"] for dp in nodes_data] + already_entitiy_types
+        ).items(),
+        key=lambda x: x[1],
+        reverse=True,
+    )[0][0]
+
+    description = GRAPH_FIELD_SEP.join(
+        sorted(set([dp["description"] for dp in nodes_data] + already_description))
+    )
+    source_id = GRAPH_FIELD_SEP.join(
+        set([dp["source_id"] for dp in nodes_data] + already_source_ids)
+    )
+
+    # description = await _handle_entity_relation_summary(
+    #     entity_name, description, global_config
+    # )
+    node_data = dict(
+        entity_type=entity_type,
+        description=description,
+        source_id=source_id,
+    )
+    await knowledge_graph_inst.upsert_node(
+        entity_name,
+        node_data=node_data,
+    )
+    node_data["entity_name"] = entity_name
+    return node_data
+
+
+async def _merge_edges_then_upsert(
+    src_id: str,
+    tgt_id: str,
+    edges_data: list[dict],
+    knowledge_graph_inst: BaseGraphStorage,
+    global_config: dict,
+):
+    already_weights = []
+    already_source_ids = []
+    already_description = []
+    already_keywords = []
+
+    if await knowledge_graph_inst.has_edge(src_id, tgt_id):
+        already_edge = await knowledge_graph_inst.get_edge(src_id, tgt_id)
+        already_weights.append(already_edge["weight"])
+        already_source_ids.extend(
+            split_string_by_multi_markers(already_edge["source_id"], [GRAPH_FIELD_SEP])
+        )
+        already_description.append(already_edge["description"])
+        already_keywords.extend(
+            split_string_by_multi_markers(already_edge["keywords"], [GRAPH_FIELD_SEP])
+        )
+
+    weight = sum([dp["weight"] for dp in edges_data] + already_weights)
+    description = GRAPH_FIELD_SEP.join(
+        sorted(set([dp["description"] for dp in edges_data] + already_description))
+    )
+    keywords = GRAPH_FIELD_SEP.join(
+        sorted(set([dp["keywords"] for dp in edges_data] + already_keywords))
+    )
+    source_id = GRAPH_FIELD_SEP.join(
+        set([dp["source_id"] for dp in edges_data] + already_source_ids)
+    )
+    for need_insert_id in [src_id, tgt_id]:
+        if not (await knowledge_graph_inst.has_node(need_insert_id)):
+            await knowledge_graph_inst.upsert_node(
+                need_insert_id,
+                node_data={
+                    "source_id": source_id,
+                    "description": description,
+                    "entity_type": '"UNKNOWN"',
+                },
+            )
+    # description = await _handle_entity_relation_summary(
+    #     (src_id, tgt_id), description, global_config
+    # )
+    await knowledge_graph_inst.upsert_edge(
+        src_id,
+        tgt_id,
+        edge_data=dict(
+            weight=weight,
+            description=description,
+            keywords=keywords,
+            source_id=source_id,
+        ),
+    )
+
+    edge_data = dict(
+        src_id=src_id,
+        tgt_id=tgt_id,
+        description=description,
+        keywords=keywords,
+    )
+
+    return edge_data
+
+
+async def extract_entities(
+    chunks: dict[str, TextChunkSchema],
+    knowledge_graph_inst: BaseGraphStorage,
+    entity_vdb: BaseVectorStorage,
+    entity_name_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    global_config: dict,
+) -> Union[BaseGraphStorage, None]:
+    use_llm_func: callable = global_config["llm_model_func"]
+    entity_extract_max_gleaning = global_config["entity_extract_max_gleaning"]
+
+    ordered_chunks = list(chunks.items())
+    # if global_config['RAGmode'] == 'minirag':
+    #     # entity_extract_prompt = PROMPTS["entity_extraction_noDes"]
+    #     entity_extract_prompt = PROMPTS["entity_extraction"]
+    # else:
+    entity_extract_prompt = PROMPTS["entity_extraction"]
+
+    context_base = dict(
+        tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
+        record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
+        completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
+        entity_types=",".join(PROMPTS["DEFAULT_ENTITY_TYPES"]),
+    )
+    continue_prompt = PROMPTS["entiti_continue_extraction"]
+
+    if_loop_prompt = PROMPTS["entiti_if_loop_extraction"]
+
+    already_processed = 0
+    already_entities = 0
+    already_relations = 0
+
+    async def _process_single_content(chunk_key_dp: tuple[str, TextChunkSchema]):
+        nonlocal already_processed, already_entities, already_relations
+        chunk_key = chunk_key_dp[0]
+        chunk_dp = chunk_key_dp[1]
+        content = chunk_dp["content"]
+        hint_prompt = entity_extract_prompt.format(**context_base, input_text=content)
+        final_result = await use_llm_func(hint_prompt)
+
+        history = pack_user_ass_to_openai_messages(hint_prompt, final_result)
+        for now_glean_index in range(entity_extract_max_gleaning):
+            glean_result = await use_llm_func(continue_prompt, history_messages=history)
+
+            history += pack_user_ass_to_openai_messages(continue_prompt, glean_result)
+            final_result += glean_result
+            if now_glean_index == entity_extract_max_gleaning - 1:
+                break
+
+            if_loop_result: str = await use_llm_func(
+                if_loop_prompt, history_messages=history
+            )
+            if_loop_result = if_loop_result.strip().strip('"').strip("'").lower()
+            if if_loop_result != "yes":
+                break
+
+        records = split_string_by_multi_markers(
+            final_result,
+            [context_base["record_delimiter"], context_base["completion_delimiter"]],
+        )
+
+        maybe_nodes = defaultdict(list)
+        maybe_edges = defaultdict(list)
+        for record in records:
+            record = re.search(r"\((.*)\)", record)
+            if record is None:
+                continue
+            record = record.group(1)
+            record_attributes = split_string_by_multi_markers(
+                record, [context_base["tuple_delimiter"]]
+            )
+            if_entities = await _handle_single_entity_extraction(
+                record_attributes, chunk_key
+            )
+            if if_entities is not None:
+                maybe_nodes[if_entities["entity_name"]].append(if_entities)
+                continue
+
+            if_relation = await _handle_single_relationship_extraction(
+                record_attributes, chunk_key
+            )
+            if if_relation is not None:
+                maybe_edges[(if_relation["src_id"], if_relation["tgt_id"])].append(
+                    if_relation
+                )
+        already_processed += 1
+        already_entities += len(maybe_nodes)
+        already_relations += len(maybe_edges)
+        now_ticks = PROMPTS["process_tickers"][
+            already_processed % len(PROMPTS["process_tickers"])
+        ]
+        print(
+            f"{now_ticks} Processed {already_processed} chunks, {already_entities} entities(duplicated), {already_relations} relations(duplicated)\r",
+            end="",
+            flush=True,
+        )
+        return dict(maybe_nodes), dict(maybe_edges)
+
+    # use_llm_func is wrapped in ascynio.Semaphore, limiting max_async callings
+    results = await asyncio.gather(
+        *[_process_single_content(c) for c in ordered_chunks]
+    )
+    print()  # clear the progress bar
+    maybe_nodes = defaultdict(list)
+    maybe_edges = defaultdict(list)
+    for m_nodes, m_edges in results:
+        for k, v in m_nodes.items():
+            maybe_nodes[k].extend(v)
+        for k, v in m_edges.items():
+            maybe_edges[tuple(sorted(k))].extend(v)
+    all_entities_data = await asyncio.gather(
+        *[
+            _merge_nodes_then_upsert(k, v, knowledge_graph_inst, global_config)
+            for k, v in maybe_nodes.items()
+        ]
+    )
+    all_relationships_data = await asyncio.gather(
+        *[
+            _merge_edges_then_upsert(k[0], k[1], v, knowledge_graph_inst, global_config)
+            for k, v in maybe_edges.items()
+        ]
+    )
+    if not len(all_entities_data):
+        logger.warning("Didn't extract any entities, maybe your LLM is not working")
+        return None
+    if not len(all_relationships_data):
+        logger.warning(
+            "Didn't extract any relationships, maybe your LLM is not working"
+        )
+        return None
+
+    if entity_vdb is not None:
+        data_for_vdb = {
+            compute_mdhash_id(dp["entity_name"], prefix="ent-"): {
+                "content": dp["entity_name"] + dp["description"],
+                "entity_name": dp["entity_name"],
+            }
+            for dp in all_entities_data
+        }
+        await entity_vdb.upsert(data_for_vdb)
+    if entity_vdb is not None:
+        data_for_vdb = {
+            compute_mdhash_id(dp["entity_name"], prefix="ent-"): {
+                "content": dp["entity_name"] + " " + dp["description"],
+                "entity_name": dp["entity_name"],
+            }
+            for dp in all_entities_data
+        }
+        await entity_vdb.upsert(data_for_vdb)
+
+    if entity_name_vdb is not None:
+        data_for_vdb = {
+            compute_mdhash_id(dp["entity_name"], prefix="Ename-"): {
+                "content": dp["entity_name"],
+                "entity_name": dp["entity_name"],
+            }
+            for dp in all_entities_data
+        }
+        await entity_name_vdb.upsert(data_for_vdb)
+
+    if relationships_vdb is not None:
+        data_for_vdb = {
+            compute_mdhash_id(dp["src_id"] + dp["tgt_id"], prefix="rel-"): {
+                "src_id": dp["src_id"],
+                "tgt_id": dp["tgt_id"],
+                "content": dp["keywords"]
+                + " " + dp["src_id"]
+                + " " + dp["tgt_id"]
+                + " " + dp["description"],
+            }
+            for dp in all_relationships_data
+        }
+
+        await relationships_vdb.upsert(data_for_vdb)
+
+    return knowledge_graph_inst
+
+
+async def local_query(
+    query,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+    global_config: dict,
+) -> str:
+    context = None
+    use_model_func = global_config["llm_model_func"]
+
+    kw_prompt_temp = PROMPTS["keywords_extraction"]
+    kw_prompt = kw_prompt_temp.format(query=query)
+    result = await use_model_func(kw_prompt)
+    json_text = locate_json_string_body_from_string(result)
+
+    try:
+        keywords_data = json.loads(json_text)
+        keywords = keywords_data.get("low_level_keywords", [])
+        keywords = ", ".join(keywords)
+    except json.JSONDecodeError:
+        try:
+            result = (
+                result.replace(kw_prompt[:-1], "")
+                .replace("user", "")
+                .replace("model", "")
+                .strip()
+            )
+            result = "{" + result.split("{")[1].split("}")[0] + "}"
+
+            keywords_data = json.loads(result)
+            keywords = keywords_data.get("low_level_keywords", [])
+            keywords = ", ".join(keywords)
+        # Handle parsing error
+        except json.JSONDecodeError as e:
+            print(f"JSON parsing error: {e}")
+            return PROMPTS["fail_response"]
+    if keywords:
+        context = await _build_local_query_context(
+            keywords,
+            knowledge_graph_inst,
+            entities_vdb,
+            text_chunks_db,
+            query_param,
+        )
+    if query_param.only_need_context:
+        return context
+    if context is None:
+        return PROMPTS["fail_response"]
+    sys_prompt_temp = PROMPTS["rag_response"]
+    sys_prompt = sys_prompt_temp.format(
+        context_data=context, response_type=query_param.response_type
+    )
+    response = await use_model_func(
+        query,
+        system_prompt=sys_prompt,
+    )
+    if len(response) > len(sys_prompt):
+        response = (
+            response.replace(sys_prompt, "")
+            .replace("user", "")
+            .replace("model", "")
+            .replace(query, "")
+            .replace("<system>", "")
+            .replace("</system>", "")
+            .strip()
+        )
+
+    return response
+
+
+async def _build_local_query_context(
+    query,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+):
+    results = await entities_vdb.query(query, top_k=query_param.top_k)
+
+    if not len(results):
+        return None
+    node_datas = await asyncio.gather(
+        *[knowledge_graph_inst.get_node(r["entity_name"]) for r in results]
+    )
+    if not all([n is not None for n in node_datas]):
+        logger.warning("Some nodes are missing, maybe the storage is damaged")
+    node_degrees = await asyncio.gather(
+        *[knowledge_graph_inst.node_degree(r["entity_name"]) for r in results]
+    )
+    node_datas = [
+        {**n, "entity_name": k["entity_name"], "rank": d}
+        for k, n, d in zip(results, node_datas, node_degrees)
+        if n is not None
+    ]  # what is this text_chunks_db doing.  dont remember it in airvx.  check the diagram.
+    use_text_units = await _find_most_related_text_unit_from_entities(
+        node_datas, query_param, text_chunks_db, knowledge_graph_inst
+    )
+    use_relations = await _find_most_related_edges_from_entities(
+        node_datas, query_param, knowledge_graph_inst
+    )
+    logger.info(
+        f"Local query uses {len(node_datas)} entites, {len(use_relations)} relations, {len(use_text_units)} text units"
+    )
+    entites_section_list = [["id", "entity", "type", "description", "rank"]]
+    for i, n in enumerate(node_datas):
+        entites_section_list.append(
+            [
+                i,
+                n["entity_name"],
+                n.get("entity_type", "UNKNOWN"),
+                n.get("description", "UNKNOWN"),
+                n["rank"],
+            ]
+        )
+    entities_context = list_of_list_to_csv(entites_section_list)
+
+    relations_section_list = [
+        ["id", "source", "target", "description", "keywords", "weight", "rank"]
+    ]
+    for i, e in enumerate(use_relations):
+        relations_section_list.append(
+            [
+                i,
+                e["src_tgt"][0],
+                e["src_tgt"][1],
+                e["description"],
+                e["keywords"],
+                e["weight"],
+                e["rank"],
+            ]
+        )
+    relations_context = list_of_list_to_csv(relations_section_list)
+
+    text_units_section_list = [["id", "content"]]
+    for i, t in enumerate(use_text_units):
+        text_units_section_list.append([i, t["content"]])
+    text_units_context = list_of_list_to_csv(text_units_section_list)
+    return f"""
+-----Entities-----
+```csv
+{entities_context}
+```
+-----Relationships-----
+```csv
+{relations_context}
+```
+-----Sources-----
+```csv
+{text_units_context}
+```
+"""
+
+
+async def _find_most_related_text_unit_from_entities(
+    node_datas: list[dict],
+    query_param: QueryParam,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    knowledge_graph_inst: BaseGraphStorage,
+):
+    text_units = [
+        split_string_by_multi_markers(dp["source_id"], [GRAPH_FIELD_SEP])
+        for dp in node_datas
+    ]
+    edges = await asyncio.gather(
+        *[knowledge_graph_inst.get_node_edges(dp["entity_name"]) for dp in node_datas]
+    )
+    all_one_hop_nodes = set()
+    for this_edges in edges:
+        if not this_edges:
+            continue
+        all_one_hop_nodes.update([e[1] for e in this_edges])
+
+    all_one_hop_nodes = list(all_one_hop_nodes)
+    all_one_hop_nodes_data = await asyncio.gather(
+        *[knowledge_graph_inst.get_node(e) for e in all_one_hop_nodes]
+    )
+
+    # Add null check for node data
+    all_one_hop_text_units_lookup = {
+        k: set(split_string_by_multi_markers(v["source_id"], [GRAPH_FIELD_SEP]))
+        for k, v in zip(all_one_hop_nodes, all_one_hop_nodes_data)
+        if v is not None and "source_id" in v  # Add source_id check
+    }
+
+    all_text_units_lookup = {}
+    for index, (this_text_units, this_edges) in enumerate(zip(text_units, edges)):
+        for c_id in this_text_units:
+            if c_id in all_text_units_lookup:
+                continue
+            relation_counts = 0
+            if this_edges:  # Add check for None edges
+                for e in this_edges:
+                    if (
+                        e[1] in all_one_hop_text_units_lookup
+                        and c_id in all_one_hop_text_units_lookup[e[1]]
+                    ):
+                        relation_counts += 1
+
+            chunk_data = await text_chunks_db.get_by_id(c_id)
+            if chunk_data is not None and "content" in chunk_data:  # Add content check
+                all_text_units_lookup[c_id] = {
+                    "data": chunk_data,
+                    "order": index,
+                    "relation_counts": relation_counts,
+                }
+
+    # Filter out None values and ensure data has content
+    all_text_units = [
+        {"id": k, **v}
+        for k, v in all_text_units_lookup.items()
+        if v is not None and v.get("data") is not None and "content" in v["data"]
+    ]
+
+    if not all_text_units:
+        logger.warning("No valid text units found")
+        return []
+
+    all_text_units = sorted(
+        all_text_units, key=lambda x: (x["order"], -x["relation_counts"])
+    )
+
+    all_text_units = truncate_list_by_token_size(
+        all_text_units,
+        key=lambda x: x["data"]["content"],
+        max_token_size=query_param.max_token_for_text_unit,
+    )
+
+    all_text_units = [t["data"] for t in all_text_units]
+    return all_text_units
+
+
+async def _find_most_related_edges_from_entities(
+    node_datas: list[dict],
+    query_param: QueryParam,
+    knowledge_graph_inst: BaseGraphStorage,
+):
+    all_related_edges = await asyncio.gather(
+        *[knowledge_graph_inst.get_node_edges(dp["entity_name"]) for dp in node_datas]
+    )
+    all_edges = set()
+    for this_edges in all_related_edges:
+        all_edges.update([tuple(sorted(e)) for e in this_edges])
+    all_edges = list(all_edges)
+    all_edges_pack = await asyncio.gather(
+        *[knowledge_graph_inst.get_edge(e[0], e[1]) for e in all_edges]
+    )
+    all_edges_degree = await asyncio.gather(
+        *[knowledge_graph_inst.edge_degree(e[0], e[1]) for e in all_edges]
+    )
+    all_edges_data = [
+        {"src_tgt": k, "rank": d, **v}
+        for k, v, d in zip(all_edges, all_edges_pack, all_edges_degree)
+        if v is not None
+    ]
+    all_edges_data = sorted(
+        all_edges_data, key=lambda x: (x["rank"], x["weight"]), reverse=True
+    )
+    all_edges_data = truncate_list_by_token_size(
+        all_edges_data,
+        key=lambda x: x["description"],
+        max_token_size=query_param.max_token_for_global_context,
+    )
+    return all_edges_data
+
+
+async def global_query(
+    query,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+    global_config: dict,
+) -> str:
+    context = None
+    use_model_func = global_config["llm_model_func"]
+
+    kw_prompt_temp = PROMPTS["keywords_extraction"]
+    kw_prompt = kw_prompt_temp.format(query=query)
+    result = await use_model_func(kw_prompt)
+    json_text = locate_json_string_body_from_string(result)
+
+    try:
+        keywords_data = json.loads(json_text)
+        keywords = keywords_data.get("high_level_keywords", [])
+        keywords = ", ".join(keywords)
+    except json.JSONDecodeError:
+        try:
+            result = (
+                result.replace(kw_prompt[:-1], "")
+                .replace("user", "")
+                .replace("model", "")
+                .strip()
+            )
+            result = "{" + result.split("{")[1].split("}")[0] + "}"
+
+            keywords_data = json.loads(result)
+            keywords = keywords_data.get("high_level_keywords", [])
+            keywords = ", ".join(keywords)
+
+        except json.JSONDecodeError as e:
+            # Handle parsing error
+            print(f"JSON parsing error: {e}")
+            return PROMPTS["fail_response"]
+    if keywords:
+        context = await _build_global_query_context(
+            keywords,
+            knowledge_graph_inst,
+            entities_vdb,
+            relationships_vdb,
+            text_chunks_db,
+            query_param,
+        )
+
+    if query_param.only_need_context:
+        return context
+    if context is None:
+        return PROMPTS["fail_response"]
+
+    sys_prompt_temp = PROMPTS["rag_response"]
+    sys_prompt = sys_prompt_temp.format(
+        context_data=context, response_type=query_param.response_type
+    )
+    response = await use_model_func(
+        query,
+        system_prompt=sys_prompt,
+    )
+    if len(response) > len(sys_prompt):
+        response = (
+            response.replace(sys_prompt, "")
+            .replace("user", "")
+            .replace("model", "")
+            .replace(query, "")
+            .replace("<system>", "")
+            .replace("</system>", "")
+            .strip()
+        )
+
+    return response
+
+
+async def _build_global_query_context(
+    keywords,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+):
+    results = await relationships_vdb.query(keywords, top_k=query_param.top_k)
+
+    if not len(results):
+        return None
+
+    edge_datas = await asyncio.gather(
+        *[knowledge_graph_inst.get_edge(r["src_id"], r["tgt_id"]) for r in results]
+    )
+
+    if not all([n is not None for n in edge_datas]):
+        logger.warning("Some edges are missing, maybe the storage is damaged")
+    edge_degree = await asyncio.gather(
+        *[knowledge_graph_inst.edge_degree(r["src_id"], r["tgt_id"]) for r in results]
+    )
+    edge_datas = [
+        {"src_id": k["src_id"], "tgt_id": k["tgt_id"], "rank": d, **v}
+        for k, v, d in zip(results, edge_datas, edge_degree)
+        if v is not None
+    ]
+    edge_datas = sorted(
+        edge_datas, key=lambda x: (x["rank"], x["weight"]), reverse=True
+    )
+    edge_datas = truncate_list_by_token_size(
+        edge_datas,
+        key=lambda x: x["description"],
+        max_token_size=query_param.max_token_for_global_context,
+    )
+
+    use_entities = await _find_most_related_entities_from_relationships(
+        edge_datas, query_param, knowledge_graph_inst
+    )
+    use_text_units = await _find_related_text_unit_from_relationships(
+        edge_datas, query_param, text_chunks_db, knowledge_graph_inst
+    )
+    logger.info(
+        f"Global query uses {len(use_entities)} entites, {len(edge_datas)} relations, {len(use_text_units)} text units"
+    )
+    relations_section_list = [
+        ["id", "source", "target", "description", "keywords", "weight", "rank"]
+    ]
+    for i, e in enumerate(edge_datas):
+        relations_section_list.append(
+            [
+                i,
+                e["src_id"],
+                e["tgt_id"],
+                e["description"],
+                e["keywords"],
+                e["weight"],
+                e["rank"],
+            ]
+        )
+    relations_context = list_of_list_to_csv(relations_section_list)
+
+    entites_section_list = [["id", "entity", "type", "description", "rank"]]
+    for i, n in enumerate(use_entities):
+        entites_section_list.append(
+            [
+                i,
+                n["entity_name"],
+                n.get("entity_type", "UNKNOWN"),
+                n.get("description", "UNKNOWN"),
+                n["rank"],
+            ]
+        )
+    entities_context = list_of_list_to_csv(entites_section_list)
+
+    text_units_section_list = [["id", "content"]]
+    for i, t in enumerate(use_text_units):
+        text_units_section_list.append([i, t["content"]])
+    text_units_context = list_of_list_to_csv(text_units_section_list)
+
+    return f"""
+-----Entities-----
+```csv
+{entities_context}
+```
+-----Relationships-----
+```csv
+{relations_context}
+```
+-----Sources-----
+```csv
+{text_units_context}
+```
+"""
+
+
+async def _find_most_related_entities_from_relationships(
+    edge_datas: list[dict],
+    query_param: QueryParam,
+    knowledge_graph_inst: BaseGraphStorage,
+):
+    entity_names = set()
+    for e in edge_datas:
+        entity_names.add(e["src_id"])
+        entity_names.add(e["tgt_id"])
+
+    node_datas = await asyncio.gather(
+        *[knowledge_graph_inst.get_node(entity_name) for entity_name in entity_names]
+    )
+
+    node_degrees = await asyncio.gather(
+        *[knowledge_graph_inst.node_degree(entity_name) for entity_name in entity_names]
+    )
+    node_datas = [
+        {**n, "entity_name": k, "rank": d}
+        for k, n, d in zip(entity_names, node_datas, node_degrees)
+    ]
+
+    node_datas = truncate_list_by_token_size(
+        node_datas,
+        key=lambda x: x["description"],
+        max_token_size=query_param.max_token_for_local_context,
+    )
+
+    return node_datas
+
+
+async def _find_related_text_unit_from_relationships(
+    edge_datas: list[dict],
+    query_param: QueryParam,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    knowledge_graph_inst: BaseGraphStorage,
+):
+    text_units = [
+        split_string_by_multi_markers(dp["source_id"], [GRAPH_FIELD_SEP])
+        for dp in edge_datas
+    ]
+
+    all_text_units_lookup = {}
+
+    for index, unit_list in enumerate(text_units):
+        for c_id in unit_list:
+            if c_id not in all_text_units_lookup:
+                all_text_units_lookup[c_id] = {
+                    "data": await text_chunks_db.get_by_id(c_id),
+                    "order": index,
+                }
+
+    if any([v is None for v in all_text_units_lookup.values()]):
+        logger.warning("Text chunks are missing, maybe the storage is damaged")
+    all_text_units = [
+        {"id": k, **v} for k, v in all_text_units_lookup.items() if v is not None
+    ]
+    all_text_units = sorted(all_text_units, key=lambda x: x["order"])
+    all_text_units = truncate_list_by_token_size(
+        all_text_units,
+        key=lambda x: x["data"]["content"],
+        max_token_size=query_param.max_token_for_text_unit,
+    )
+    all_text_units: list[TextChunkSchema] = [t["data"] for t in all_text_units]
+
+    return all_text_units
+
+
+async def hybrid_query(
+    query,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+    global_config: dict,
+) -> str:
+    low_level_context = None
+    high_level_context = None
+    use_model_func = global_config["llm_model_func"]
+
+    kw_prompt_temp = PROMPTS["keywords_extraction"]
+    kw_prompt = kw_prompt_temp.format(query=query)
+
+    result = await use_model_func(kw_prompt)
+    json_text = locate_json_string_body_from_string(result)
+    try:
+        keywords_data = json.loads(json_text)
+        hl_keywords = keywords_data.get("high_level_keywords", [])
+        ll_keywords = keywords_data.get("low_level_keywords", [])
+        hl_keywords = ", ".join(hl_keywords)
+        ll_keywords = ", ".join(ll_keywords)
+    except json.JSONDecodeError:
+        try:
+            result = (
+                result.replace(kw_prompt[:-1], "")
+                .replace("user", "")
+                .replace("model", "")
+                .strip()
+            )
+            result = "{" + result.split("{")[1].split("}")[0] + "}"
+            keywords_data = json.loads(result)
+            hl_keywords = keywords_data.get("high_level_keywords", [])
+            ll_keywords = keywords_data.get("low_level_keywords", [])
+            hl_keywords = ", ".join(hl_keywords)
+            ll_keywords = ", ".join(ll_keywords)
+        # Handle parsing error
+        except json.JSONDecodeError as e:
+            print(f"JSON parsing error: {e}")
+            return PROMPTS["fail_response"]
+    if ll_keywords:
+        low_level_context = await _build_local_query_context(
+            ll_keywords,
+            knowledge_graph_inst,
+            entities_vdb,
+            text_chunks_db,
+            query_param,
+        )
+
+    if hl_keywords:
+        high_level_context = await _build_global_query_context(
+            hl_keywords,
+            knowledge_graph_inst,
+            entities_vdb,
+            relationships_vdb,
+            text_chunks_db,
+            query_param,
+        )
+
+    context = combine_contexts(high_level_context, low_level_context)
+
+    if query_param.only_need_context:
+        return context
+    if context is None:
+        return PROMPTS["fail_response"]
+
+    sys_prompt_temp = PROMPTS["rag_response"]
+    sys_prompt = sys_prompt_temp.format(
+        context_data=context, response_type=query_param.response_type
+    )
+    response = await use_model_func(
+        query,
+        system_prompt=sys_prompt,
+    )
+    if len(response) > len(sys_prompt):
+        response = (
+            response.replace(sys_prompt, "")
+            .replace("user", "")
+            .replace("model", "")
+            .replace(query, "")
+            .replace("<system>", "")
+            .replace("</system>", "")
+            .strip()
+        )
+    return response
+
+
+def combine_contexts(high_level_context, low_level_context):
+    # Function to extract entities, relationships, and sources from context strings
+
+    def extract_sections(context):
+        entities_match = re.search(
+            r"-----Entities-----\s*```csv\s*(.*?)\s*```", context, re.DOTALL
+        )
+        relationships_match = re.search(
+            r"-----Relationships-----\s*```csv\s*(.*?)\s*```", context, re.DOTALL
+        )
+        sources_match = re.search(
+            r"-----Sources-----\s*```csv\s*(.*?)\s*```", context, re.DOTALL
+        )
+
+        entities = entities_match.group(1) if entities_match else ""
+        relationships = relationships_match.group(1) if relationships_match else ""
+        sources = sources_match.group(1) if sources_match else ""
+
+        return entities, relationships, sources
+
+    # Extract sections from both contexts
+
+    if high_level_context is None:
+        warnings.warn(
+            "High Level context is None. Return empty High entity/relationship/source"
+        )
+        hl_entities, hl_relationships, hl_sources = "", "", ""
+    else:
+        hl_entities, hl_relationships, hl_sources = extract_sections(high_level_context)
+
+    if low_level_context is None:
+        warnings.warn(
+            "Low Level context is None. Return empty Low entity/relationship/source"
+        )
+        ll_entities, ll_relationships, ll_sources = "", "", ""
+    else:
+        ll_entities, ll_relationships, ll_sources = extract_sections(low_level_context)
+
+    # Combine and deduplicate the entities
+
+    combined_entities = process_combine_contexts(hl_entities, ll_entities)
+    combined_entities = chunking_by_token_size(combined_entities, max_token_size=2000)
+    # Combine and deduplicate the relationships
+    combined_relationships = process_combine_contexts(
+        hl_relationships, ll_relationships
+    )
+    combined_relationships = chunking_by_token_size(
+        combined_relationships, max_token_size=2000
+    )
+    # Combine and deduplicate the sources
+    combined_sources = process_combine_contexts(hl_sources, ll_sources)
+    combined_sources = chunking_by_token_size(combined_sources, max_token_size=2000)
+    # Format the combined context
+    return f"""
+-----Entities-----
+```csv
+{combined_entities}
+```
+-----Relationships-----
+```csv
+{combined_relationships}
+```
+-----Sources-----
+```csv
+{combined_sources}
+```
+"""
+
+
+async def naive_query(
+    query,
+    chunks_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    query_param: QueryParam,
+    global_config: dict,
+):
+    use_model_func = global_config["llm_model_func"]
+    results = await chunks_vdb.query(query, top_k=query_param.top_k)
+    if not len(results):
+        return PROMPTS["fail_response"]
+    chunks_ids = [r["id"] for r in results]
+
+    chunks = await text_chunks_db.get_by_ids(chunks_ids)
+
+    maybe_trun_chunks = truncate_list_by_token_size(
+        chunks,
+        key=lambda x: x["content"],
+        max_token_size=query_param.max_token_for_text_unit,
+    )
+    logger.info(f"Truncate {len(chunks)} to {len(maybe_trun_chunks)} chunks")
+    section = "--New Chunk--\n".join([c["content"] for c in maybe_trun_chunks])
+    if query_param.only_need_context:
+        return section
+    sys_prompt_temp = PROMPTS["naive_rag_response"]
+    sys_prompt = sys_prompt_temp.format(
+        content_data=section, response_type=query_param.response_type
+    )
+    response = await use_model_func(
+        query,
+        system_prompt=sys_prompt,
+    )
+
+    if len(response) > len(sys_prompt):
+        response = (
+            response[len(sys_prompt) :]
+            .replace(sys_prompt, "")
+            .replace("user", "")
+            .replace("model", "")
+            .replace(query, "")
+            .replace("<system>", "")
+            .replace("</system>", "")
+            .strip()
+        )
+
+    return response
+
+
+async def path2chunk(
+    scored_edged_reasoning_path, knowledge_graph_inst, pairs_append, query, max_chunks=5
+):
+    already_node = {}
+    for k, v in scored_edged_reasoning_path.items():
+        node_chunk_id = None
+
+        for pathtuple, scorelist in v["Path"].items():
+            if pathtuple in pairs_append:
+                use_edge = pairs_append[pathtuple]
+                edge_datas = []
+                edge_datas = await asyncio.gather(
+                    *[knowledge_graph_inst.get_edge(r[0], r[1]) for r in use_edge]
+                )
+                text_units = [
+                    split_string_by_multi_markers(dp["source_id"], [GRAPH_FIELD_SEP])
+                    for dp in edge_datas  # chunk ID
+                ][0]
+
+            else:
+                use_edge = []
+                text_units = []
+
+            node_datas = await asyncio.gather(
+                *[knowledge_graph_inst.get_node(pathtuple[0])]
+            )
+            for dp in node_datas:
+                text_units_node = split_string_by_multi_markers(
+                    dp["source_id"], [GRAPH_FIELD_SEP]
+                )
+                text_units = text_units + text_units_node
+
+            node_datas = await asyncio.gather(
+                *[knowledge_graph_inst.get_node(ents) for ents in pathtuple[1:]]
+            )
+            if query is not None:
+                for dp in node_datas:
+                    text_units_node = split_string_by_multi_markers(
+                        dp["source_id"], [GRAPH_FIELD_SEP]
+                    )
+                    descriptionlist_node = split_string_by_multi_markers(
+                        dp["description"], [GRAPH_FIELD_SEP]
+                    )
+                    if descriptionlist_node[0] not in already_node.keys():
+                        already_node[descriptionlist_node[0]] = None
+
+                        if len(text_units_node) == len(descriptionlist_node):
+                            if len(text_units_node) > 5:
+                                max_ids = int(max(5, len(text_units_node) / 2))
+                                should_consider_idx = calculate_similarity(
+                                    descriptionlist_node, query, k=max_ids
+                                )
+                                text_units_node = [
+                                    text_units_node[i] for i in should_consider_idx
+                                ]
+                                already_node[descriptionlist_node[0]] = text_units_node
+                    else:
+                        text_units_node = already_node[descriptionlist_node[0]]
+                    if text_units_node is not None:
+                        text_units = text_units + text_units_node
+
+            count_dict = Counter(text_units)
+            total_score = scorelist[0] + scorelist[1] + 1
+            for key, value in count_dict.items():
+                count_dict[key] = value * total_score
+            if node_chunk_id is None:
+                node_chunk_id = count_dict
+            else:
+                node_chunk_id = node_chunk_id + count_dict
+        v["Path"] = []
+        if node_chunk_id is None:
+            node_datas = await asyncio.gather(*[knowledge_graph_inst.get_node(k)])
+            for dp in node_datas:
+                text_units_node = split_string_by_multi_markers(
+                    dp["source_id"], [GRAPH_FIELD_SEP]
+                )
+                count_dict = Counter(text_units_node)
+
+            for id in count_dict.most_common(max_chunks):
+                v["Path"].append(id[0])
+            # v['Path'] = count_dict.most_common(max_chunks)#[]
+        else:
+            for id in count_dict.most_common(max_chunks):
+                v["Path"].append(id[0])
+            # v['Path'] = node_chunk_id.most_common(max_chunks)
+    return scored_edged_reasoning_path
+
+
+def scorednode2chunk(input_dict, values_dict):
+    for key, value_list in input_dict.items():
+        input_dict[key] = [
+            values_dict.get(val, None) for val in value_list if val in values_dict
+        ]
+        input_dict[key] = [val for val in input_dict[key] if val is not None]
+
+
+def kwd2chunk(ent_from_query_dict, chunks_ids, chunk_nums):
+    final_chunk = Counter()
+    final_chunk_id = []
+    for key, list_of_dicts in ent_from_query_dict.items():
+        total_id_scores = Counter()
+        id_scores_list = []
+        id_scores = {}
+        for d in list_of_dicts:
+            if d == list_of_dicts[0]:
+                score = d["Score"] * 2
+            else:
+                score = d["Score"]
+            path = d["Path"]
+
+            for id in path:
+                if id == path[0] and id in chunks_ids:
+                    score = score * 10
+                if id in id_scores:
+                    id_scores[id] += score
+                else:
+                    id_scores[id] = score
+        id_scores_list.append(id_scores)
+
+        for scores in id_scores_list:
+            total_id_scores.update(scores)
+        final_chunk = final_chunk + total_id_scores  # .most_common(3)
+
+    for i in final_chunk.most_common(chunk_nums):
+        final_chunk_id.append(i[0])
+    return final_chunk_id
+
+
+async def _build_mini_query_context(
+    ent_from_query,
+    type_keywords,
+    originalquery,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    entity_name_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    chunks_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    embedder,
+    query_param: QueryParam,
+):
+    imp_ents = []
+    nodes_from_query_list = []
+    ent_from_query_dict = {}
+
+    for ent in ent_from_query:
+        ent_from_query_dict[ent] = []
+        results_node = await entity_name_vdb.query(ent, top_k=query_param.top_k)
+
+        nodes_from_query_list.append(results_node)
+        ent_from_query_dict[ent] = [e["entity_name"] for e in results_node]
+
+    candidate_reasoning_path = {}
+
+    for results_node_list in nodes_from_query_list:
+        candidate_reasoning_path_new = {
+            key["entity_name"]: {"Score": key["distance"], "Path": []}
+            for key in results_node_list
+        }
+
+        candidate_reasoning_path = {
+            **candidate_reasoning_path,
+            **candidate_reasoning_path_new,
+        }
+    for key in candidate_reasoning_path.keys():
+        candidate_reasoning_path[key][
+            "Path"
+        ] = await knowledge_graph_inst.get_neighbors_within_k_hops(key, 2)
+        imp_ents.append(key)
+
+    short_path_entries = {
+        name: entry
+        for name, entry in candidate_reasoning_path.items()
+        if len(entry["Path"]) < 1
+    }
+    sorted_short_path_entries = sorted(
+        short_path_entries.items(), key=lambda x: x[1]["Score"], reverse=True
+    )
+    save_p = max(1, int(len(sorted_short_path_entries) * 0.2))
+    top_short_path_entries = sorted_short_path_entries[:save_p]
+    top_short_path_dict = {name: entry for name, entry in top_short_path_entries}
+    long_path_entries = {
+        name: entry
+        for name, entry in candidate_reasoning_path.items()
+        if len(entry["Path"]) >= 1
+    }
+    candidate_reasoning_path = {**long_path_entries, **top_short_path_dict}
+    node_datas_from_type = await knowledge_graph_inst.get_node_from_types(
+        type_keywords
+    )  # entity_type, description,...
+
+    maybe_answer_list = [n["entity_name"] for n in node_datas_from_type]
+    imp_ents = imp_ents + maybe_answer_list
+    scored_reasoning_path = cal_path_score_list(
+        candidate_reasoning_path, maybe_answer_list
+    )
+
+    results_edge = await relationships_vdb.query(
+        originalquery, top_k=len(ent_from_query) * query_param.top_k
+    )
+    goodedge = []
+    badedge = []
+    for item in results_edge:
+        if item["src_id"] in imp_ents or item["tgt_id"] in imp_ents:
+            goodedge.append(item)
+        else:
+            badedge.append(item)
+    scored_edged_reasoning_path, pairs_append = edge_vote_path(
+        scored_reasoning_path, goodedge
+    )
+    scored_edged_reasoning_path = await path2chunk(
+        scored_edged_reasoning_path,
+        knowledge_graph_inst,
+        pairs_append,
+        originalquery,
+        max_chunks=3,
+    )
+
+    entites_section_list = []
+    node_datas = await asyncio.gather(
+        *[
+            knowledge_graph_inst.get_node(entity_name)
+            for entity_name in scored_edged_reasoning_path.keys()
+        ]
+    )
+    node_datas = [
+        {**n, "entity_name": k, "Score": scored_edged_reasoning_path[k]["Score"]}
+        for k, n in zip(scored_edged_reasoning_path.keys(), node_datas)
+    ]
+    for i, n in enumerate(node_datas):
+        entites_section_list.append(
+            [
+                n["entity_name"],
+                n["Score"],
+                n.get("description", "UNKNOWN"),
+            ]
+        )
+    entites_section_list = sorted(
+        entites_section_list, key=lambda x: x[1], reverse=True
+    )
+    entites_section_list = truncate_list_by_token_size(
+        entites_section_list,
+        key=lambda x: x[2],
+        max_token_size=query_param.max_token_for_node_context,
+    )
+
+    entites_section_list.insert(0, ["entity", "score", "description"])
+    entities_context = list_of_list_to_csv(entites_section_list)
+
+    scorednode2chunk(ent_from_query_dict, scored_edged_reasoning_path)
+
+    results = await chunks_vdb.query(originalquery, top_k=int(query_param.top_k / 2))
+    chunks_ids = [r["id"] for r in results]
+    final_chunk_id = kwd2chunk(
+        ent_from_query_dict, chunks_ids, chunk_nums=int(query_param.top_k / 2)
+    )
+
+    if not len(results_node):
+        return None
+
+    if not len(results_edge):
+        return None
+
+    use_text_units = await asyncio.gather(
+        *[text_chunks_db.get_by_id(id) for id in final_chunk_id]
+    )
+    text_units_section_list = [["id", "content"]]
+
+    for i, t in enumerate(use_text_units):
+        if t is not None:
+            text_units_section_list.append([i, t["content"]])
+    text_units_context = list_of_list_to_csv(text_units_section_list)
+
+    return f"""
+-----Entities-----
+```csv
+{entities_context}
+```
+-----Sources-----
+```csv
+{text_units_context}
+```
+"""
+
+
+async def minirag_query(  # MiniRAG
+    query,
+    knowledge_graph_inst: BaseGraphStorage,
+    entities_vdb: BaseVectorStorage,
+    entity_name_vdb: BaseVectorStorage,
+    relationships_vdb: BaseVectorStorage,
+    chunks_vdb: BaseVectorStorage,
+    text_chunks_db: BaseKVStorage[TextChunkSchema],
+    embedder,
+    query_param: QueryParam,
+    global_config: dict,
+) -> str:
+    use_model_func = global_config["llm_model_func"]
+    kw_prompt_temp = PROMPTS["minirag_query2kwd"]
+    TYPE_POOL, TYPE_POOL_w_CASE = await knowledge_graph_inst.get_types()
+    kw_prompt = kw_prompt_temp.format(query=query, TYPE_POOL=TYPE_POOL)
+    result = await use_model_func(kw_prompt)
+
+    try:
+        keywords_data = json_repair.loads(result)
+
+        type_keywords = keywords_data.get("answer_type_keywords", [])
+        entities_from_query = keywords_data.get("entities_from_query", [])[:5]
+
+    except json.JSONDecodeError:
+        try:
+            result = (
+                result.replace(kw_prompt[:-1], "")
+                .replace("user", "")
+                .replace("model", "")
+                .strip()
+            )
+            result = "{" + result.split("{")[1].split("}")[0] + "}"
+            keywords_data = json_repair.loads(result)
+            type_keywords = keywords_data.get("answer_type_keywords", [])
+            entities_from_query = keywords_data.get("entities_from_query", [])[:5]
+
+        # Handle parsing error
+        except Exception as e:
+            print(f"JSON parsing error: {e}")
+            return PROMPTS["fail_response"]
+
+    context = await _build_mini_query_context(
+        entities_from_query,
+        type_keywords,
+        query,
+        knowledge_graph_inst,
+        entities_vdb,
+        entity_name_vdb,
+        relationships_vdb,
+        chunks_vdb,
+        text_chunks_db,
+        embedder,
+        query_param,
+    )
+
+    if query_param.only_need_context:
+        return context
+    if context is None:
+        return PROMPTS["fail_response"]
+
+    sys_prompt_temp = PROMPTS["rag_response"]
+    sys_prompt = sys_prompt_temp.format(
+        context_data=context, response_type=query_param.response_type
+    )
+    response = await use_model_func(
+        query,
+        system_prompt=sys_prompt,
+    )
+
+    return response
diff --git a/test/minirag/minirag/prompt.py b/test/minirag/minirag/prompt.py
new file mode 100755
index 0000000..e7d3261
--- /dev/null
+++ b/test/minirag/minirag/prompt.py
@@ -0,0 +1,413 @@
+GRAPH_FIELD_SEP = "<SEP>"
+
+PROMPTS = {}
+
+PROMPTS["DEFAULT_TUPLE_DELIMITER"] = "<|>"
+PROMPTS["DEFAULT_RECORD_DELIMITER"] = "##"
+PROMPTS["DEFAULT_COMPLETION_DELIMITER"] = "<|COMPLETE|>"
+PROMPTS["process_tickers"] = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
+
+PROMPTS["DEFAULT_ENTITY_TYPES"] = ["organization", "person", "location", "event"]
+
+
+PROMPTS["entity_extraction"] = """-Goal-
+Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.
+
+-Steps-
+1. Identify all entities. For each identified entity, extract the following information:
+- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.
+- entity_type: One of the following types: [{entity_types}]
+- entity_description: Comprehensive description of the entity's attributes and activities
+Format each entity as ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>
+
+2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.
+For each pair of related entities, extract the following information:
+- source_entity: name of the source entity, as identified in step 1
+- target_entity: name of the target entity, as identified in step 1
+- relationship_description: explanation as to why you think the source entity and the target entity are related to each other
+- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity
+- relationship_keywords: one or more high-level key words that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details
+Format each relationship as ("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_keywords>{tuple_delimiter}<relationship_strength>)
+
+3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the overarching ideas present in the document.
+Format the content-level key words as ("content_keywords"{tuple_delimiter}<high_level_keywords>)
+
+4. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.
+
+5. When finished, output {completion_delimiter}
+
+######################
+-Examples-
+######################
+Example 1:
+
+Entity_types: [person, technology, mission, organization, location]
+Text:
+while Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.
+
+Then Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. “If this tech can be understood..." Taylor said, their voice quieter, "It could change the game for us. For all of us.”
+
+The underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.
+
+It was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths
+################
+Output:
+("entity"{tuple_delimiter}"Alex"{tuple_delimiter}"person"{tuple_delimiter}"Alex is a character who experiences frustration and is observant of the dynamics among other characters."){record_delimiter}
+("entity"{tuple_delimiter}"Taylor"{tuple_delimiter}"person"{tuple_delimiter}"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective."){record_delimiter}
+("entity"{tuple_delimiter}"Jordan"{tuple_delimiter}"person"{tuple_delimiter}"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device."){record_delimiter}
+("entity"{tuple_delimiter}"Cruz"{tuple_delimiter}"person"{tuple_delimiter}"Cruz is associated with a vision of control and order, influencing the dynamics among other characters."){record_delimiter}
+("entity"{tuple_delimiter}"The Device"{tuple_delimiter}"technology"{tuple_delimiter}"The Device is central to the story, with potential game-changing implications, and is revered by Taylor."){record_delimiter}
+("relationship"{tuple_delimiter}"Alex"{tuple_delimiter}"Taylor"{tuple_delimiter}"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device."{tuple_delimiter}"power dynamics, perspective shift"{tuple_delimiter}7){record_delimiter}
+("relationship"{tuple_delimiter}"Alex"{tuple_delimiter}"Jordan"{tuple_delimiter}"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision."{tuple_delimiter}"shared goals, rebellion"{tuple_delimiter}6){record_delimiter}
+("relationship"{tuple_delimiter}"Taylor"{tuple_delimiter}"Jordan"{tuple_delimiter}"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce."{tuple_delimiter}"conflict resolution, mutual respect"{tuple_delimiter}8){record_delimiter}
+("relationship"{tuple_delimiter}"Jordan"{tuple_delimiter}"Cruz"{tuple_delimiter}"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order."{tuple_delimiter}"ideological conflict, rebellion"{tuple_delimiter}5){record_delimiter}
+("relationship"{tuple_delimiter}"Taylor"{tuple_delimiter}"The Device"{tuple_delimiter}"Taylor shows reverence towards the device, indicating its importance and potential impact."{tuple_delimiter}"reverence, technological significance"{tuple_delimiter}9){record_delimiter}
+("content_keywords"{tuple_delimiter}"power dynamics, ideological conflict, discovery, rebellion"){completion_delimiter}
+#############################
+Example 2:
+
+Entity_types: [person, technology, mission, organization, location]
+Text:
+They were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols—it demanded a new perspective, a new resolve.
+
+Tension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.
+
+Their connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence— the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly
+#############
+Output:
+("entity"{tuple_delimiter}"Washington"{tuple_delimiter}"location"{tuple_delimiter}"Washington is a location where communications are being received, indicating its importance in the decision-making process."){record_delimiter}
+("entity"{tuple_delimiter}"Operation: Dulce"{tuple_delimiter}"mission"{tuple_delimiter}"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities."){record_delimiter}
+("entity"{tuple_delimiter}"The team"{tuple_delimiter}"organization"{tuple_delimiter}"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role."){record_delimiter}
+("relationship"{tuple_delimiter}"The team"{tuple_delimiter}"Washington"{tuple_delimiter}"The team receives communications from Washington, which influences their decision-making process."{tuple_delimiter}"decision-making, external influence"{tuple_delimiter}7){record_delimiter}
+("relationship"{tuple_delimiter}"The team"{tuple_delimiter}"Operation: Dulce"{tuple_delimiter}"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities."{tuple_delimiter}"mission evolution, active participation"{tuple_delimiter}9){completion_delimiter}
+("content_keywords"{tuple_delimiter}"mission evolution, decision-making, active participation, cosmic significance"){completion_delimiter}
+#############################
+Example 3:
+
+Entity_types: [person, role, technology, organization, event, location, concept]
+Text:
+their voice slicing through the buzz of activity. "Control may be an illusion when facing an intelligence that literally writes its own rules," they stated stoically, casting a watchful eye over the flurry of data.
+
+"It's like it's learning to communicate," offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. "This gives talking to strangers' a whole new meaning."
+
+Alex surveyed his team—each face a study in concentration, determination, and not a small measure of trepidation. "This might well be our first contact," he acknowledged, "And we need to be ready for whatever answers back."
+
+Together, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable—a collective introspection about their role in this grand cosmic play, one that could rewrite human history.
+
+The encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation
+#############
+Output:
+("entity"{tuple_delimiter}"Sam Rivera"{tuple_delimiter}"person"{tuple_delimiter}"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety."){record_delimiter}
+("entity"{tuple_delimiter}"Alex"{tuple_delimiter}"person"{tuple_delimiter}"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task."){record_delimiter}
+("entity"{tuple_delimiter}"Control"{tuple_delimiter}"concept"{tuple_delimiter}"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules."){record_delimiter}
+("entity"{tuple_delimiter}"Intelligence"{tuple_delimiter}"concept"{tuple_delimiter}"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate."){record_delimiter}
+("entity"{tuple_delimiter}"First Contact"{tuple_delimiter}"event"{tuple_delimiter}"First Contact is the potential initial communication between humanity and an unknown intelligence."){record_delimiter}
+("entity"{tuple_delimiter}"Humanity's Response"{tuple_delimiter}"event"{tuple_delimiter}"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence."){record_delimiter}
+("relationship"{tuple_delimiter}"Sam Rivera"{tuple_delimiter}"Intelligence"{tuple_delimiter}"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence."{tuple_delimiter}"communication, learning process"{tuple_delimiter}9){record_delimiter}
+("relationship"{tuple_delimiter}"Alex"{tuple_delimiter}"First Contact"{tuple_delimiter}"Alex leads the team that might be making the First Contact with the unknown intelligence."{tuple_delimiter}"leadership, exploration"{tuple_delimiter}10){record_delimiter}
+("relationship"{tuple_delimiter}"Alex"{tuple_delimiter}"Humanity's Response"{tuple_delimiter}"Alex and his team are the key figures in Humanity's Response to the unknown intelligence."{tuple_delimiter}"collective action, cosmic significance"{tuple_delimiter}8){record_delimiter}
+("relationship"{tuple_delimiter}"Control"{tuple_delimiter}"Intelligence"{tuple_delimiter}"The concept of Control is challenged by the Intelligence that writes its own rules."{tuple_delimiter}"power dynamics, autonomy"{tuple_delimiter}7){record_delimiter}
+("content_keywords"{tuple_delimiter}"first contact, control, communication, cosmic significance"){completion_delimiter}
+#############################
+-Real Data-
+######################
+Entity_types: {entity_types}
+Text: {input_text}
+######################
+Output:
+"""
+
+
+PROMPTS[
+    "summarize_entity_descriptions"
+] = """You are a helpful assistant responsible for generating a comprehensive summary of the data provided below.
+Given one or two entities, and a list of descriptions, all related to the same entity or group of entities.
+Please concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.
+If the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.
+Make sure it is written in third person, and include the entity names so we the have full context.
+
+#######
+-Data-
+Entities: {entity_name}
+Description List: {description_list}
+#######
+Output:
+"""
+
+PROMPTS[
+    "entiti_continue_extraction"
+] = """MANY entities were missed in the last extraction.  Add them below using the same format:
+"""
+
+
+PROMPTS[
+    "entiti_continue_extraction_mini"
+] = """MANY entities were missed in the last extraction.
+After summarizing with all the information previously extracted, compared to the original text, it was noticed that the following information was mainly omitted:
+{omit}
+
+The types of entities that need to be added can be obtained from Entity_types,
+or you can add them yourself.
+
+Entity_types: {entity_types}
+
+
+Add them below using the same format:
+"""
+
+
+PROMPTS["minirag_query2kwd"] = """---Role---
+
+You are a helpful assistant tasked with identifying both answer-type and low-level keywords in the user's query.
+
+---Goal---
+
+Given the query, list both answer-type and low-level keywords.
+answer_type_keywords focus on the type of the answer to the certain query, while low-level keywords focus on specific entities, details, or concrete terms.
+The answer_type_keywords must be selected from Answer type pool.
+This pool is in the form of a dictionary, where the key represents the Type you should choose from and the value represents the example samples.
+
+---Instructions---
+
+- Output the keywords in JSON format.
+- The JSON should have three keys:
+  - "answer_type_keywords" for the types of the answer. In this list, the types with the highest likelihood should be placed at the forefront. No more than 3.
+  - "entities_from_query" for specific entities or details. It must be extracted from the query.
+######################
+-Examples-
+######################
+Example 1:
+
+Query: "How does international trade influence global economic stability?"
+Answer type pool: {{
+ 'PERSONAL LIFE': ['FAMILY TIME', 'HOME MAINTENANCE'],
+ 'STRATEGY': ['MARKETING PLAN', 'BUSINESS EXPANSION'],
+ 'SERVICE FACILITATION': ['ONLINE SUPPORT', 'CUSTOMER SERVICE TRAINING'],
+ 'PERSON': ['JANE DOE', 'JOHN SMITH'],
+ 'FOOD': ['PASTA', 'SUSHI'],
+ 'EMOTION': ['HAPPINESS', 'ANGER'],
+ 'PERSONAL EXPERIENCE': ['TRAVEL ABROAD', 'STUDYING ABROAD'],
+ 'INTERACTION': ['TEAM MEETING', 'NETWORKING EVENT'],
+ 'BEVERAGE': ['COFFEE', 'TEA'],
+ 'PLAN': ['ANNUAL BUDGET', 'PROJECT TIMELINE'],
+ 'GEO': ['NEW YORK CITY', 'SOUTH AFRICA'],
+ 'GEAR': ['CAMPING TENT', 'CYCLING HELMET'],
+ 'EMOJI': ['🎉', '🚀'],
+ 'BEHAVIOR': ['POSITIVE FEEDBACK', 'NEGATIVE CRITICISM'],
+ 'TONE': ['FORMAL', 'INFORMAL'],
+ 'LOCATION': ['DOWNTOWN', 'SUBURBS']
+}}
+################
+Output:
+{{
+  "answer_type_keywords": ["STRATEGY","PERSONAL LIFE"],
+  "entities_from_query": ["Trade agreements", "Tariffs", "Currency exchange", "Imports", "Exports"]
+}}
+#############################
+Example 2:
+
+Query: "When was SpaceX's first rocket launch?"
+Answer type pool: {{
+ 'DATE AND TIME': ['2023-10-10 10:00', 'THIS AFTERNOON'],
+ 'ORGANIZATION': ['GLOBAL INITIATIVES CORPORATION', 'LOCAL COMMUNITY CENTER'],
+ 'PERSONAL LIFE': ['DAILY EXERCISE ROUTINE', 'FAMILY VACATION PLANNING'],
+ 'STRATEGY': ['NEW PRODUCT LAUNCH', 'YEAR-END SALES BOOST'],
+ 'SERVICE FACILITATION': ['REMOTE IT SUPPORT', 'ON-SITE TRAINING SESSIONS'],
+ 'PERSON': ['ALEXANDER HAMILTON', 'MARIA CURIE'],
+ 'FOOD': ['GRILLED SALMON', 'VEGETARIAN BURRITO'],
+ 'EMOTION': ['EXCITEMENT', 'DISAPPOINTMENT'],
+ 'PERSONAL EXPERIENCE': ['BIRTHDAY CELEBRATION', 'FIRST MARATHON'],
+ 'INTERACTION': ['OFFICE WATER COOLER CHAT', 'ONLINE FORUM DEBATE'],
+ 'BEVERAGE': ['ICED COFFEE', 'GREEN SMOOTHIE'],
+ 'PLAN': ['WEEKLY MEETING SCHEDULE', 'MONTHLY BUDGET OVERVIEW'],
+ 'GEO': ['MOUNT EVEREST BASE CAMP', 'THE GREAT BARRIER REEF'],
+ 'GEAR': ['PROFESSIONAL CAMERA EQUIPMENT', 'OUTDOOR HIKING GEAR'],
+ 'EMOJI': ['📅', '⏰'],
+ 'BEHAVIOR': ['PUNCTUALITY', 'HONESTY'],
+ 'TONE': ['CONFIDENTIAL', 'SATIRICAL'],
+ 'LOCATION': ['CENTRAL PARK', 'DOWNTOWN LIBRARY']
+}}
+
+################
+Output:
+{{
+  "answer_type_keywords": ["DATE AND TIME", "ORGANIZATION", "PLAN"],
+  "entities_from_query": ["SpaceX", "Rocket launch", "Aerospace", "Power Recovery"]
+
+}}
+#############################
+Example 3:
+
+Query: "What is the role of education in reducing poverty?"
+Answer type pool: {{
+ 'PERSONAL LIFE': ['MANAGING WORK-LIFE BALANCE', 'HOME IMPROVEMENT PROJECTS'],
+ 'STRATEGY': ['MARKETING STRATEGIES FOR Q4', 'EXPANDING INTO NEW MARKETS'],
+ 'SERVICE FACILITATION': ['CUSTOMER SATISFACTION SURVEYS', 'STAFF RETENTION PROGRAMS'],
+ 'PERSON': ['ALBERT EINSTEIN', 'MARIA CALLAS'],
+ 'FOOD': ['PAN-FRIED STEAK', 'POACHED EGGS'],
+ 'EMOTION': ['OVERWHELM', 'CONTENTMENT'],
+ 'PERSONAL EXPERIENCE': ['LIVING ABROAD', 'STARTING A NEW JOB'],
+ 'INTERACTION': ['SOCIAL MEDIA ENGAGEMENT', 'PUBLIC SPEAKING'],
+ 'BEVERAGE': ['CAPPUCCINO', 'MATCHA LATTE'],
+ 'PLAN': ['ANNUAL FITNESS GOALS', 'QUARTERLY BUSINESS REVIEW'],
+ 'GEO': ['THE AMAZON RAINFOREST', 'THE GRAND CANYON'],
+ 'GEAR': ['SURFING ESSENTIALS', 'CYCLING ACCESSORIES'],
+ 'EMOJI': ['💻', '📱'],
+ 'BEHAVIOR': ['TEAMWORK', 'LEADERSHIP'],
+ 'TONE': ['FORMAL MEETING', 'CASUAL CONVERSATION'],
+ 'LOCATION': ['URBAN CITY CENTER', 'RURAL COUNTRYSIDE']
+}}
+
+################
+Output:
+{{
+  "answer_type_keywords": ["STRATEGY", "PERSON"],
+  "entities_from_query": ["School access", "Literacy rates", "Job training", "Income inequality"]
+}}
+#############################
+Example 4:
+
+Query: "Where is the capital of the United States?"
+Answer type pool: {{
+ 'ORGANIZATION': ['GREENPEACE', 'RED CROSS'],
+ 'PERSONAL LIFE': ['DAILY WORKOUT', 'HOME COOKING'],
+ 'STRATEGY': ['FINANCIAL INVESTMENT', 'BUSINESS EXPANSION'],
+ 'SERVICE FACILITATION': ['ONLINE SUPPORT', 'CUSTOMER SERVICE TRAINING'],
+ 'PERSON': ['ALBERTA SMITH', 'BENJAMIN JONES'],
+ 'FOOD': ['PASTA CARBONARA', 'SUSHI PLATTER'],
+ 'EMOTION': ['HAPPINESS', 'SADNESS'],
+ 'PERSONAL EXPERIENCE': ['TRAVEL ADVENTURE', 'BOOK CLUB'],
+ 'INTERACTION': ['TEAM BUILDING', 'NETWORKING MEETUP'],
+ 'BEVERAGE': ['LATTE', 'GREEN TEA'],
+ 'PLAN': ['WEIGHT LOSS', 'CAREER DEVELOPMENT'],
+ 'GEO': ['PARIS', 'NEW YORK'],
+ 'GEAR': ['CAMERA', 'HEADPHONES'],
+ 'EMOJI': ['🏢', '🌍'],
+ 'BEHAVIOR': ['POSITIVE THINKING', 'STRESS MANAGEMENT'],
+ 'TONE': ['FRIENDLY', 'PROFESSIONAL'],
+ 'LOCATION': ['DOWNTOWN', 'SUBURBS']
+}}
+################
+Output:
+{{
+  "answer_type_keywords": ["LOCATION"],
+  "entities_from_query": ["capital of the United States", "Washington", "New York"]
+}}
+#############################
+
+-Real Data-
+######################
+Query: {query}
+Answer type pool:{TYPE_POOL}
+######################
+Output:
+
+"""
+
+
+PROMPTS[
+    "entiti_if_loop_extraction"
+] = """It appears some entities may have still been missed.  Answer YES | NO if there are still entities that need to be added.
+"""
+
+PROMPTS["fail_response"] = "Sorry, I'm not able to provide an answer to that question."
+
+PROMPTS["rag_response"] = """---Role---
+
+You are a helpful assistant responding to questions about data in the tables provided.
+
+
+---Goal---
+
+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.
+If you don't know the answer, just say so. Do not make anything up.
+Do not include information where the supporting evidence for it is not provided.
+
+---Target response length and format---
+
+{response_type}
+
+---Data tables---
+
+{context_data}
+
+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.
+"""
+
+PROMPTS["keywords_extraction"] = """---Role---
+
+You are a helpful assistant tasked with identifying both high-level and low-level keywords in the user's query.
+
+---Goal---
+
+Given the query, list both high-level and low-level keywords. High-level keywords focus on overarching concepts or themes, while low-level keywords focus on specific entities, details, or concrete terms.
+
+---Instructions---
+
+- Output the keywords in JSON format.
+- The JSON should have two keys:
+  - "high_level_keywords" for overarching concepts or themes.
+  - "low_level_keywords" for specific entities or details.
+
+######################
+-Examples-
+######################
+Example 1:
+
+Query: "How does international trade influence global economic stability?"
+################
+Output:
+{{
+  "high_level_keywords": ["International trade", "Global economic stability", "Economic impact"],
+  "low_level_keywords": ["Trade agreements", "Tariffs", "Currency exchange", "Imports", "Exports"]
+}}
+#############################
+Example 2:
+
+Query: "What are the environmental consequences of deforestation on biodiversity?"
+################
+Output:
+{{
+  "high_level_keywords": ["Environmental consequences", "Deforestation", "Biodiversity loss"],
+  "low_level_keywords": ["Species extinction", "Habitat destruction", "Carbon emissions", "Rainforest", "Ecosystem"]
+}}
+#############################
+Example 3:
+
+Query: "What is the role of education in reducing poverty?"
+################
+Output:
+{{
+  "high_level_keywords": ["Education", "Poverty reduction", "Socioeconomic development"],
+  "low_level_keywords": ["School access", "Literacy rates", "Job training", "Income inequality"]
+}}
+#############################
+-Real Data-
+######################
+Query: {query}
+######################
+Output:
+
+"""
+
+PROMPTS["naive_rag_response"] = """---Role---
+
+You are a helpful assistant responding to questions about documents provided.
+
+
+---Goal---
+
+Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.
+If you don't know the answer, just say so. Do not make anything up.
+Do not include information where the supporting evidence for it is not provided.
+
+---Target response length and format---
+
+{response_type}
+
+---Documents---
+
+{content_data}
+
+Add sections and commentary to the response as appropriate for the length and format. Style the response in markdown.
+"""
diff --git a/test/minirag/minirag/utils.py b/test/minirag/minirag/utils.py
new file mode 100755
index 0000000..d6a47a3
--- /dev/null
+++ b/test/minirag/minirag/utils.py
@@ -0,0 +1,488 @@
+import asyncio
+import html
+import io
+import csv
+import json
+import logging
+import os
+import re
+from dataclasses import dataclass
+from functools import wraps
+from hashlib import md5
+from typing import Any, Union, List
+import xml.etree.ElementTree as ET
+import copy
+import numpy as np
+import tiktoken
+from nltk.metrics import edit_distance
+from rouge import Rouge
+
+ENCODER = None
+
+logger = logging.getLogger("minirag")
+
+
+def set_logger(log_file: str):
+    logger.setLevel(logging.DEBUG)
+
+    file_handler = logging.FileHandler(log_file)
+    file_handler.setLevel(logging.DEBUG)
+
+    formatter = logging.Formatter(
+        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    )
+    file_handler.setFormatter(formatter)
+
+    if not logger.handlers:
+        logger.addHandler(file_handler)
+
+
+@dataclass
+class EmbeddingFunc:
+    embedding_dim: int
+    max_token_size: int
+    func: callable
+
+    async def __call__(self, *args, **kwargs) -> np.ndarray:
+        return await self.func(*args, **kwargs)
+
+
+def compute_mdhash_id(content, prefix: str = ""):
+    return prefix + md5(content.encode()).hexdigest()
+
+
+def compute_args_hash(*args, cache_type: str | None = None) -> str:
+    args_str = "".join([str(arg) for arg in args])
+    if cache_type:
+        args_str = f"{cache_type}:{args_str}"
+    return md5(args_str.encode()).hexdigest()
+
+
+def clean_text(text: str) -> str:
+    """Clean text by removing null bytes (0x00) and whitespace"""
+    return text.strip().replace("\x00", "")
+
+
+def get_content_summary(content: str, max_length: int = 100) -> str:
+    """Get a summary of document content, truncating if necessary"""
+    content = content.strip()
+    return content if len(content) <= max_length else content[:max_length] + "..."
+
+
+def locate_json_string_body_from_string(content: str) -> Union[str, None]:
+    """Locate the JSON string body from a string"""
+    maybe_json_str = re.search(r"{.*}", content, re.DOTALL)
+    if maybe_json_str is not None:
+        return maybe_json_str.group(0)
+    else:
+        return None
+
+
+def convert_response_to_json(response: str) -> dict:
+    json_str = locate_json_string_body_from_string(response)
+    assert json_str is not None, f"Unable to parse JSON from response: {response}"
+    try:
+        data = json.loads(json_str)
+        return data
+    except json.JSONDecodeError as e:
+        logger.error(f"Failed to parse JSON: {json_str}")
+        raise e from None
+
+
+def limit_async_func_call(max_size: int, waitting_time: float = 0.0001):
+    """Add restriction of maximum async calling times for a async func"""
+
+    def final_decro(func):
+        """Not using async.Semaphore to aovid use nest-asyncio"""
+        __current_size = 0
+
+        @wraps(func)
+        async def wait_func(*args, **kwargs):
+            nonlocal __current_size
+            while __current_size >= max_size:
+                await asyncio.sleep(waitting_time)
+            __current_size += 1
+            result = await func(*args, **kwargs)
+            __current_size -= 1
+            return result
+
+        return wait_func
+
+    return final_decro
+
+
+def wrap_embedding_func_with_attrs(**kwargs):
+    """Wrap a function with attributes"""
+
+    def final_decro(func) -> EmbeddingFunc:
+        new_func = EmbeddingFunc(**kwargs, func=func)
+        return new_func
+
+    return final_decro
+
+
+def load_json(file_name):
+    if not os.path.exists(file_name):
+        return None
+    with open(file_name, encoding="utf-8") as f:
+        return json.load(f)
+
+
+def write_json(json_obj, file_name):
+    with open(file_name, "w", encoding="utf-8") as f:
+        json.dump(json_obj, f, indent=2, ensure_ascii=False)
+
+
+def encode_string_by_tiktoken(content: str, model_name: str = "gpt-4o"):
+    global ENCODER
+    if ENCODER is None:
+        ENCODER = tiktoken.encoding_for_model(model_name)
+    tokens = ENCODER.encode(content)
+    return tokens
+
+
+def decode_tokens_by_tiktoken(tokens: list[int], model_name: str = "gpt-4o"):
+    global ENCODER
+    if ENCODER is None:
+        ENCODER = tiktoken.encoding_for_model(model_name)
+    content = ENCODER.decode(tokens)
+    return content
+
+
+def pack_user_ass_to_openai_messages(*args: str):
+    roles = ["user", "assistant"]
+    return [
+        {"role": roles[i % 2], "content": content} for i, content in enumerate(args)
+    ]
+
+
+def split_string_by_multi_markers(content: str, markers: list[str]) -> list[str]:
+    """Split a string by multiple markers"""
+    if not markers:
+        return [content]
+    results = re.split("|".join(re.escape(marker) for marker in markers), content)
+    return [r.strip() for r in results if r.strip()]
+
+
+# Refer the utils functions of the official GraphRAG implementation:
+# https://github.com/microsoft/graphrag
+def clean_str(input: Any) -> str:
+    """Clean an input string by removing HTML escapes, control characters, and other unwanted characters."""
+    # If we get non-string input, just give it back
+    if not isinstance(input, str):
+        return input
+
+    result = html.unescape(input.strip())
+    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python
+    return re.sub(r"[\x00-\x1f\x7f-\x9f]", "", result)
+
+
+def is_float_regex(value):
+    return bool(re.match(r"^[-+]?[0-9]*\.?[0-9]+$", value))
+
+
+def truncate_list_by_token_size(list_data: list, key: callable, max_token_size: int):
+    """Truncate a list of data by token size"""
+    if max_token_size <= 0:
+        return []
+    tokens = 0
+    for i, data in enumerate(list_data):
+        tokens += len(encode_string_by_tiktoken(key(data)))
+        if tokens > max_token_size:
+            return list_data[:i]
+    return list_data
+
+
+def list_of_list_to_csv(data: List[List[str]]) -> str:
+    output = io.StringIO()
+    writer = csv.writer(output)
+    writer.writerows(data)
+    return output.getvalue()
+
+
+def csv_string_to_list(csv_string: str) -> List[List[str]]:
+    output = io.StringIO(csv_string)
+    reader = csv.reader(output)
+    return [row for row in reader]
+
+
+def save_data_to_file(data, file_name):
+    with open(file_name, "w", encoding="utf-8") as f:
+        json.dump(data, f, ensure_ascii=False, indent=4)
+
+
+def xml_to_json(xml_file):
+    try:
+        tree = ET.parse(xml_file)
+        root = tree.getroot()
+
+        # Print the root element's tag and attributes to confirm the file has been correctly loaded
+        print(f"Root element: {root.tag}")
+        print(f"Root attributes: {root.attrib}")
+
+        data = {"nodes": [], "edges": []}
+
+        # Use namespace
+        namespace = {"": "http://graphml.graphdrawing.org/xmlns"}
+
+        for node in root.findall(".//node", namespace):
+            node_data = {
+                "id": node.get("id").strip('"'),
+                "entity_type": (
+                    node.find("./data[@key='d0']", namespace).text.strip('"')
+                    if node.find("./data[@key='d0']", namespace) is not None
+                    else ""
+                ),
+                "description": (
+                    node.find("./data[@key='d1']", namespace).text
+                    if node.find("./data[@key='d1']", namespace) is not None
+                    else ""
+                ),
+                "source_id": (
+                    node.find("./data[@key='d2']", namespace).text
+                    if node.find("./data[@key='d2']", namespace) is not None
+                    else ""
+                ),
+            }
+            data["nodes"].append(node_data)
+
+        for edge in root.findall(".//edge", namespace):
+            edge_data = {
+                "source": edge.get("source").strip('"'),
+                "target": edge.get("target").strip('"'),
+                "weight": (
+                    float(edge.find("./data[@key='d3']", namespace).text)
+                    if edge.find("./data[@key='d3']", namespace) is not None
+                    else 0.0
+                ),
+                "description": (
+                    edge.find("./data[@key='d4']", namespace).text
+                    if edge.find("./data[@key='d4']", namespace) is not None
+                    else ""
+                ),
+                "keywords": (
+                    edge.find("./data[@key='d5']", namespace).text
+                    if edge.find("./data[@key='d5']", namespace) is not None
+                    else ""
+                ),
+                "source_id": (
+                    edge.find("./data[@key='d6']", namespace).text
+                    if edge.find("./data[@key='d6']", namespace) is not None
+                    else ""
+                ),
+            }
+            data["edges"].append(edge_data)
+
+        # Print the number of nodes and edges found
+        print(f"Found {len(data['nodes'])} nodes and {len(data['edges'])} edges")
+
+        return data
+    except ET.ParseError as e:
+        print(f"Error parsing XML file: {e}")
+        return None
+    except Exception as e:
+        print(f"An error occurred: {e}")
+        return None
+
+
+def safe_unicode_decode(content):
+    # Regular expression to find all Unicode escape sequences of the form \uXXXX
+    unicode_escape_pattern = re.compile(r"\\u([0-9a-fA-F]{4})")
+
+    # Function to replace the Unicode escape with the actual character
+    def replace_unicode_escape(match):
+        # Convert the matched hexadecimal value into the actual Unicode character
+        return chr(int(match.group(1), 16))
+
+    # Perform the substitution
+    decoded_content = unicode_escape_pattern.sub(
+        replace_unicode_escape, content.decode("utf-8")
+    )
+
+    return decoded_content
+
+
+def process_combine_contexts(hl, ll):
+    header = None
+    list_hl = csv_string_to_list(hl.strip())
+    list_ll = csv_string_to_list(ll.strip())
+
+    if list_hl:
+        header = list_hl[0]
+        list_hl = list_hl[1:]
+    if list_ll:
+        header = list_ll[0]
+        list_ll = list_ll[1:]
+    if header is None:
+        return ""
+
+    if list_hl:
+        list_hl = [",".join(item[1:]) for item in list_hl if item]
+    if list_ll:
+        list_ll = [",".join(item[1:]) for item in list_ll if item]
+
+    combined_sources_set = set(filter(None, list_hl + list_ll))
+
+    combined_sources = [",\t".join(header)]
+
+    for i, item in enumerate(combined_sources_set, start=1):
+        combined_sources.append(f"{i},\t{item}")
+
+    combined_sources = "\n".join(combined_sources)
+
+    return combined_sources
+
+
+def is_continuous_subsequence(subseq, seq):
+    def find_all_indexes(tup, value):
+        indexes = []
+        start = 0
+        while True:
+            try:
+                index = tup.index(value, start)
+                indexes.append(index)
+                start = index + 1
+            except ValueError:
+                break
+        return indexes
+
+    index_list = find_all_indexes(seq, subseq[0])
+    for idx in index_list:
+        if idx != len(seq) - 1:
+            if seq[idx + 1] == subseq[-1]:
+                return True
+    return False
+
+
+def merge_tuples(list1, list2):
+    result = []
+    for tup in list1:
+        last_element = tup[-1]
+        if last_element in tup[:-1]:
+            result.append(tup)
+        else:
+            matching_tuples = [t for t in list2 if t[0] == last_element]
+
+            already_match_flag = 0
+            for match in matching_tuples:
+                matchh = (match[1], match[0])
+                if is_continuous_subsequence(match, tup) or is_continuous_subsequence(
+                    matchh, tup
+                ):
+                    continue
+
+                already_match_flag = 1
+                merged_tuple = tup + match[1:]
+
+                result.append(merged_tuple)
+
+            if not already_match_flag:
+                result.append(tup)
+    return result
+
+
+def count_elements_in_tuple(tuple_elements, list_elements):
+    sorted_list = sorted(list_elements)
+    tuple_elements = sorted(tuple_elements)
+    count = 0
+    list_index = 0
+
+    for elem in tuple_elements:
+        while list_index < len(sorted_list) and sorted_list[list_index] < elem:
+            list_index += 1
+        if list_index < len(sorted_list) and sorted_list[list_index] == elem:
+            count += 1
+            list_index += 1
+    return count
+
+
+def cal_path_score_list(candidate_reasoning_path, maybe_answer_list):
+    scored_reasoning_path = {}
+    for k, v in candidate_reasoning_path.items():
+        score = v["Score"]
+        paths = v["Path"]
+        scores = {}
+        for p in paths:
+            scores[p] = [count_elements_in_tuple(p, maybe_answer_list)]
+        scored_reasoning_path[k] = {"Score": score, "Path": scores}
+    return scored_reasoning_path
+
+
+def edge_vote_path(path_dict, edge_list):
+    return_dict = copy.deepcopy(path_dict)
+    EDGELIST = []
+    pairs_append = {}
+    for i in edge_list:
+        EDGELIST.append((i["src_id"], i["tgt_id"]))
+    for i in return_dict.items():
+        for j in i[1]["Path"].items():
+            if j[1]:
+                count = 0
+
+                for pairs in EDGELIST:
+                    if is_continuous_subsequence(pairs, j[0]):
+                        count = count + 1
+                        if j[0] not in pairs_append:
+                            pairs_append[j[0]] = [pairs]
+                        else:
+                            pairs_append[j[0]].append(pairs)
+
+                # score
+                j[1].append(count)
+    return return_dict, pairs_append
+
+
+# Caching functions
+
+
+def cosine_similarity(v1, v2):
+    dot_product = np.dot(v1, v2)
+    norm1 = np.linalg.norm(v1)
+    norm2 = np.linalg.norm(v2)
+    return dot_product / (norm1 * norm2)
+
+
+def quantize_embedding(embedding: np.ndarray | list[float], bits: int = 8):
+    embedding = np.array(embedding)
+    min_val = embedding.min()
+    max_val = embedding.max()
+    scale = (2**bits - 1) / (max_val - min_val)
+    quantized = np.round((embedding - min_val) * scale).astype(np.uint8)
+    return quantized, min_val, max_val
+
+
+def dequantize_embedding(
+    quantized: np.ndarray, min_val: float, max_val: float, bits=8
+) -> np.ndarray:
+    scale = (max_val - min_val) / (2**bits - 1)
+    return (quantized * scale + min_val).astype(np.float32)
+
+
+def calculate_similarity(sentences, target, method="levenshtein", n=1, k=1):
+    target_tokens = target.lower().split()
+    similarities_with_index = []
+    if method == "jaccard":
+        for i, sentence in enumerate(sentences):
+            sentence_tokens = sentence.lower().split()
+            intersection = set(sentence_tokens).intersection(set(target_tokens))
+            union = set(sentence_tokens).union(set(target_tokens))
+            jaccard_score = len(intersection) / len(union) if union else 0
+            similarities_with_index.append((i, jaccard_score))
+    elif method == "levenshtein":
+        for i, sentence in enumerate(sentences):
+            distance = edit_distance(target_tokens, sentence.lower().split())
+            similarities_with_index.append(
+                (i, 1 - (distance / max(len(target_tokens), len(sentence.split()))))
+            )
+    elif method == "rouge":
+        rouge = Rouge()
+        for i, sentence in enumerate(sentences):
+            scores = rouge.get_scores(sentence, target)
+            rouge_score = scores[0].get(f"rouge-{n}", {}).get("f", 0)
+            similarities_with_index.append((i, rouge_score))
+    else:
+        raise ValueError(
+            "Unsupported method. Choose 'jaccard', 'levenshtein', or 'rouge'."
+        )
+    similarities_with_index.sort(key=lambda x: x[1], reverse=True)
+    return [index for index, score in similarities_with_index[:k]]
diff --git a/test/minirag/pyproject.toml b/test/minirag/pyproject.toml
new file mode 100755
index 0000000..426b539
--- /dev/null
+++ b/test/minirag/pyproject.toml
@@ -0,0 +1,21 @@
+[build-system]
+requires = ["setuptools>=45", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "minirag-hku"
+version = "0.0.2"
+authors = [
+    {name = "Tianyu Fan"},
+]
+description = "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation"
+readme = "README.md"
+requires-python = ">=3.9"
+classifiers = [
+    "Development Status :: 4 - Beta",
+    "Programming Language :: Python :: 3",
+    "License :: OSI Approved :: MIT License",
+    "Operating System :: OS Independent",
+    "Intended Audience :: Developers",
+    "Topic :: Software Development :: Libraries :: Python Modules",
+]
diff --git a/test/minirag/reproduce/Step_0_index.py b/test/minirag/reproduce/Step_0_index.py
new file mode 100755
index 0000000..97dae26
--- /dev/null
+++ b/test/minirag/reproduce/Step_0_index.py
@@ -0,0 +1,96 @@
+# from huggingface_hub import login
+# your_token = "INPUT YOUR TOKEN HERE"
+# login(your_token)
+
+import sys
+import os
+
+sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+
+
+from minirag import MiniRAG
+from minirag.llm import (
+    gpt_4o_mini_complete,
+    hf_embed,
+)
+from minirag.utils import EmbeddingFunc
+from transformers import AutoModel, AutoTokenizer
+
+EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
+
+import argparse
+
+
+def get_args():
+    parser = argparse.ArgumentParser(description="MiniRAG")
+    parser.add_argument("--model", type=str, default="PHI")
+    parser.add_argument("--outputpath", type=str, default="./logs/Default_output.csv")
+    parser.add_argument("--workingdir", type=str, default="./LiHua-World")
+    parser.add_argument("--datapath", type=str, default="./dataset/LiHua-World/data/")
+    parser.add_argument(
+        "--querypath", type=str, default="./dataset/LiHua-World/qa/query_set.csv"
+    )
+    args = parser.parse_args()
+    return args
+
+
+args = get_args()
+
+
+if args.model == "PHI":
+    LLM_MODEL = "microsoft/Phi-3.5-mini-instruct"
+elif args.model == "GLM":
+    LLM_MODEL = "THUDM/glm-edge-1.5b-chat"
+elif args.model == "MiniCPM":
+    LLM_MODEL = "openbmb/MiniCPM3-4B"
+elif args.model == "qwen":
+    LLM_MODEL = "Qwen/Qwen2.5-3B-Instruct"
+else:
+    print("Invalid model name")
+    exit(1)
+
+WORKING_DIR = args.workingdir
+DATA_PATH = args.datapath
+QUERY_PATH = args.querypath
+OUTPUT_PATH = args.outputpath
+print("USING LLM:", LLM_MODEL)
+print("USING WORKING DIR:", WORKING_DIR)
+
+
+if not os.path.exists(WORKING_DIR):
+    os.mkdir(WORKING_DIR)
+
+rag = MiniRAG(
+    working_dir=WORKING_DIR,
+    # llm_model_func=hf_model_complete,
+    llm_model_func=gpt_4o_mini_complete,
+    llm_model_max_token_size=200,
+    llm_model_name=LLM_MODEL,
+    embedding_func=EmbeddingFunc(
+        embedding_dim=384,
+        max_token_size=1000,
+        func=lambda texts: hf_embed(
+            texts,
+            tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),
+            embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),
+        ),
+    ),
+)
+
+
+# Now indexing
+def find_txt_files(root_path):
+    txt_files = []
+    for root, dirs, files in os.walk(root_path):
+        for file in files:
+            if file.endswith(".txt"):
+                txt_files.append(os.path.join(root, file))
+    return txt_files
+
+
+WEEK_LIST = find_txt_files(DATA_PATH)
+for WEEK in WEEK_LIST:
+    id = WEEK_LIST.index(WEEK)
+    print(f"{id}/{len(WEEK_LIST)}")
+    with open(WEEK) as f:
+        rag.insert(f.read())
diff --git a/test/minirag/reproduce/Step_1_QA.py b/test/minirag/reproduce/Step_1_QA.py
new file mode 100755
index 0000000..3383bc4
--- /dev/null
+++ b/test/minirag/reproduce/Step_1_QA.py
@@ -0,0 +1,134 @@
+# from huggingface_hub import login
+# your_token = "INPUT YOUR TOKEN HERE"
+# login(your_token)
+
+import sys
+import os
+
+sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+
+
+import csv
+from tqdm import trange
+from minirag import MiniRAG, QueryParam
+from minirag.llm import (
+    hf_model_complete,
+    hf_embed,
+)
+from minirag.utils import EmbeddingFunc
+from transformers import AutoModel, AutoTokenizer
+
+EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
+
+import argparse
+
+
+def get_args():
+    parser = argparse.ArgumentParser(description="MiniRAG")
+    parser.add_argument("--model", type=str, default="PHI")
+    parser.add_argument("--outputpath", type=str, default="./logs/Default_output.csv")
+    parser.add_argument("--workingdir", type=str, default="./LiHua-World")
+    parser.add_argument("--datapath", type=str, default="./dataset/LiHua-World/data/")
+    parser.add_argument(
+        "--querypath", type=str, default="./dataset/LiHua-World/qa/query_set.csv"
+    )
+    args = parser.parse_args()
+    return args
+
+
+args = get_args()
+
+
+if args.model == "PHI":
+    LLM_MODEL = "microsoft/Phi-3.5-mini-instruct"
+elif args.model == "GLM":
+    LLM_MODEL = "THUDM/glm-edge-1.5b-chat"
+elif args.model == "MiniCPM":
+    LLM_MODEL = "openbmb/MiniCPM3-4B"
+elif args.model == "qwen":
+    LLM_MODEL = "Qwen/Qwen2.5-3B-Instruct"
+else:
+    print("Invalid model name")
+    exit(1)
+
+WORKING_DIR = args.workingdir
+DATA_PATH = args.datapath
+QUERY_PATH = args.querypath
+OUTPUT_PATH = args.outputpath
+print("USING LLM:", LLM_MODEL)
+print("USING WORKING DIR:", WORKING_DIR)
+
+
+if not os.path.exists(WORKING_DIR):
+    os.mkdir(WORKING_DIR)
+
+rag = MiniRAG(
+    working_dir=WORKING_DIR,
+    llm_model_func=hf_model_complete,
+    # llm_model_func=gpt_4o_mini_complete,
+    llm_model_max_token_size=200,
+    llm_model_name=LLM_MODEL,
+    embedding_func=EmbeddingFunc(
+        embedding_dim=384,
+        max_token_size=1000,
+        func=lambda texts: hf_embed(
+            texts,
+            tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),
+            embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),
+        ),
+    ),
+)
+
+# Now QA
+QUESTION_LIST = []
+GA_LIST = []
+with open(QUERY_PATH, mode="r", encoding="utf-8") as question_file:
+    reader = csv.DictReader(question_file)
+    for row in reader:
+        QUESTION_LIST.append(row["Question"])
+        GA_LIST.append(row["Gold Answer"])
+
+
+def run_experiment(output_path):
+    headers = ["Question", "Gold Answer", "minirag"]
+
+    q_already = []
+    if os.path.exists(output_path):
+        with open(output_path, mode="r", encoding="utf-8") as question_file:
+            reader = csv.DictReader(question_file)
+            for row in reader:
+                q_already.append(row["Question"])
+
+    row_count = len(q_already)
+    print("row_count", row_count)
+
+    with open(output_path, mode="a", newline="", encoding="utf-8") as log_file:
+        writer = csv.writer(log_file)
+        if row_count == 0:
+            writer.writerow(headers)
+
+        for QUESTIONid in trange(row_count, len(QUESTION_LIST)):  #
+            QUESTION = QUESTION_LIST[QUESTIONid]
+            Gold_Answer = GA_LIST[QUESTIONid]
+            print()
+            print("QUESTION", QUESTION)
+            print("Gold_Answer", Gold_Answer)
+
+            try:
+                minirag_answer = (
+                    rag.query(QUESTION, param=QueryParam(mode="mini"))
+                    .replace("\n", "")
+                    .replace("\r", "")
+                )
+            except Exception as e:
+                print("Error in minirag_answer", e)
+                minirag_answer = "Error"
+
+            writer.writerow([QUESTION, Gold_Answer, minirag_answer])
+
+    print(f"Experiment data has been recorded in the file: {output_path}")
+
+
+# if __name__ == "__main__":
+
+run_experiment(OUTPUT_PATH)
diff --git a/test/minirag/requirements.txt b/test/minirag/requirements.txt
new file mode 100755
index 0000000..6769f95
--- /dev/null
+++ b/test/minirag/requirements.txt
@@ -0,0 +1,34 @@
+accelerate
+aiofiles
+aiohttp
+configparser
+graspologic
+json_repair
+httpx
+
+# database packages
+networkx
+nltk
+
+# Basic modules
+numpy
+pipmaster
+pydantic
+
+# File manipulation libraries
+PyPDF2
+python-docx
+python-dotenv
+python-pptx
+rouge
+
+setuptools
+tenacity
+
+
+# LLM packages
+tiktoken
+tqdm
+xxhash
+
+# Extra libraries are installed when needed using pipmaster
diff --git a/test/minirag/setup.py b/test/minirag/setup.py
new file mode 100755
index 0000000..c235547
--- /dev/null
+++ b/test/minirag/setup.py
@@ -0,0 +1,70 @@
+import setuptools
+from pathlib import Path
+
+
+# Reading the long description from README.md
+def read_long_description():
+    try:
+        return Path("README.md").read_text(encoding="utf-8")
+    except FileNotFoundError:
+        return "A description of MiniRAG is currently unavailable."
+
+
+# Reading dependencies from requirements.txt
+def read_requirements():
+    deps = []
+    try:
+        with open("./requirements.txt") as f:
+            deps = [line.strip() for line in f if line.strip()]
+    except FileNotFoundError:
+        print(
+            "Warning: 'requirements.txt' not found. No dependencies will be installed."
+        )
+    return deps
+
+
+def read_api_requirements():
+    api_deps = []
+    try:
+        with open("./minirag/api/requirements.txt") as f:
+            api_deps = [line.strip() for line in f if line.strip()]
+    except FileNotFoundError:
+        print("Warning: API requirements.txt not found.")
+    return api_deps
+
+
+long_description = read_long_description()
+requirements = read_requirements()
+
+
+setuptools.setup(
+    name="minirag-hku",
+    url="https://github.com/HKUDS/MiniRAG",
+    version="0.0.2",
+    author="Tianyu Fan",
+    description="MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
+    long_description=long_description,
+    long_description_content_type="text/markdown",
+    packages=setuptools.find_packages(
+        exclude=("tests*", "docs*")
+    ),
+    classifiers=[
+        "Development Status :: 4 - Beta",
+        "Programming Language :: Python :: 3",
+        "License :: OSI Approved :: MIT License",
+        "Operating System :: OS Independent",
+        "Intended Audience :: Developers",
+        "Topic :: Software Development :: Libraries :: Python Modules",
+    ],
+    python_requires=">=3.9",
+    install_requires=requirements,
+    include_package_data=True,
+    extras_require={
+        "api": read_api_requirements(),
+    },
+    entry_points={
+        "console_scripts": [
+            "minirag-server=minirag.api.minirag_server:main [api]",
+        ],
+    },
+)
\ No newline at end of file
diff --git a/test/org-supertag-bridge-test.el.bak2 b/test/org-supertag-bridge-test.el.bak2
new file mode 100755
index 0000000..ee7a0f1
--- /dev/null
+++ b/test/org-supertag-bridge-test.el.bak2
@@ -0,0 +1,139 @@
+;;; org-supertag-bridge-test.el --- Tests for org-supertag-bridge -*- lexical-binding: t; -*-
+
+;;; Commentary:
+;; This file contains tests for the org-supertag EPC bridge.
+;;
+;; Prerequisites:
+;; 1. Ensure `org-supertag-bridge.el` and `org-supertag-bridge-epc.el` are in your Emacs `load-path`.
+;;    If they are in the project root, you can add to your init.el or at the top of this file:
+;;    (add-to-list 'load-path (expand-file-name "." (file-name-directory (or load-file-name buffer-file-name))))
+;;
+;; 2. Customize `org-supertag-bridge-python-script` if `simtag_bridge.py`'s location
+;;    differs from the default expected by `org-supertag-bridge.el` or this test file's override.
+;;    This file assumes `simtag_bridge.py` is in "simtag/" subdirectory relative to this file.
+;;
+;; 3. Ensure `org-supertag-bridge-data-directory` is set to a writable path.
+;;    This file sets it to a temporary directory for testing.
+;;
+;; 4. IMPORTANT: Apply the suggested correction to `org-supertag-bridge-call-sync` in
+;;    `org-supertag-bridge.el` and fix the prefix for `org-suertag-bridge-deferred-chain`
+;;    in `org-supertag-bridge-epc.el` as mentioned in prior communications.
+;;
+;; How to use:
+;; - Load this file: `M-x load-file RET org-supertag-bridge-test.el RET`
+;; - Run individual test functions: `M-x ostb-test-ping-bridge RET`
+;; - Run the full cycle: `M-x ostb-test-full-cycle RET`
+;; - Check `*org-supertag-bridge-log*` and `*simtag-bridge-py-output*` buffers for logs.
+
+;;; Code:
+
+(require 'org-supertag-bridge)
+
+;;;; Configuration (adjust as needed for your environment)
+
+;; Point to the Python interpreter in the project's virtual environment
+(setq org-supertag-bridge-python-command "/Users/chenyibin/Documents/emacs/package/org-supertag/.venv/bin/python3")
+
+;; Assuming this test file is in the project root, and simtag_bridge.py is in 'simtag/'
+;; This overrides the default in org-supertag-bridge.el for the testing session.
+(setq org-supertag-bridge-python-script
+      (expand-file-name "simtag/simtag_bridge.py"
+                        (file-name-directory (or load-file-name buffer-file-name))))
+
+;; Use a temporary data directory for tests to avoid cluttering the user's default.
+(setq org-supertag-bridge-data-directory
+      (expand-file-name "test-simtag-data" temporary-file-directory))
+
+;; Enable logging for easier debugging during tests.
+(setq org-supertag-bridge-enable-log t)
+(setq org-supertag-bridge-epc-debug t) ; Enable EPC level logging for more detail
+(setq org-supertag-bridge-deferred-debug t) ; Enable deferred logging
+
+;;;; Helper Functions
+
+(defun ostb-test--ensure-started ()
+  "Ensure the bridge is started for testing. Errors if not ready after timeout."
+  (unless (and org-supertag-bridge--python-epc-manager
+               (org-supertag-bridge-epc-live-p org-supertag-bridge--python-epc-manager)
+               org-supertag-bridge--ready-p)
+    (message "Bridge not ready, attempting to start (will wait up to 15s)...")
+    (unless (org-supertag-bridge-ensure-ready 15) ; Wait up to 15s
+      (error "Bridge failed to start or become ready within 15s.")))
+  (message "Bridge is confirmed ready."))
+
+;;;; Test Functions
+
+(defun ostb-test-start-bridge ()
+  "Test: Start the SimTagBridge and ensure it's ready."
+  (interactive)
+  (message "Attempting to start and ensure bridge readiness (timeout 15s)...")
+  (make-directory org-supertag-bridge-data-directory t) ; Ensure data dir exists
+  (if (org-supertag-bridge-ensure-ready 15)
+      (message "SimTagBridge started successfully and is ready.")
+    (error "SimTagBridge failed to start or become ready within 15s timeout.")))
+
+(defun ostb-test-ping-bridge ()
+  "Test: Ping the Python server. Expects 'pong'."
+  (interactive)
+  (ostb-test--ensure-started)
+  (message "Pinging Python server...")
+  (let ((response (org-supertag-bridge-call-sync "ping" nil 5)))
+    (if (equal response "pong")
+        (message "Ping response: %S (Success)" response)
+      (error "Ping test failed. Expected 'pong', got: %S" response))))
+
+(defun ostb-test-echo-bridge (message-to-echo)
+  "Test: Echo a MESSAGE-TO-ECHO via the Python server."
+  (interactive "sEcho message (e.g., Hello Elisp!): ")
+  (ostb-test--ensure-started)
+  (message "Sending echo request with message: '%s'..." message-to-echo)
+  (let ((response (org-supertag-bridge-call-sync "echo" message-to-echo 5)))
+    (if (string-match-p (regexp-quote message-to-echo) response)
+        (message "Echo response: %S (Success)" response)
+      (error "Echo test failed. Expected response to contain '%s', got: %S" message-to-echo response))))
+
+(defun ostb-test-get-status-bridge ()
+  "Test: Get status from the Python server."
+  (interactive)
+  (ostb-test--ensure-started)
+  (message "Requesting status from Python server...")
+  (let ((response (org-supertag-bridge-call-sync "get_status" nil 10)))
+    (message "Get_status raw response: %S" response)
+    (if (and (consp response) (equal (plist-get response :status) "success"))
+        (progn
+          (message "Get_status successful. Full status:")
+          (pp response) ; Pretty print the status
+          response)
+      (error "Get_status test failed. Response format incorrect or status not success: %S" response))))
+
+(defun ostb-test-stop-bridge ()
+  "Test: Stop the SimTagBridge Python process and clean up EPC."
+  (interactive)
+  (message "Attempting to stop SimTagBridge process and EPC connections...")
+  (org-supertag-bridge-kill-process)
+  (message "SimTagBridge kill process initiated. Check logs for confirmation."))
+
+(defun ostb-test-full-cycle ()
+  "Test: Run a full cycle: start, ping, echo, get_status, stop."
+  (interactive)
+  (message "Starting full test cycle for SimTagBridge...")
+  (condition-case-unless-debug err
+      (progn
+        (ostb-test-start-bridge)
+        (sit-for 1) ; Give a moment for Python server to fully initialize and report ready.
+        (ostb-test-ping-bridge)
+        (sit-for 0.5)
+        (ostb-test-echo-bridge "Hello from full cycle test!")
+        (sit-for 0.5)
+        (ostb-test-get-status-bridge)
+        (message "Full test cycle (start, ping, echo, get_status) completed successfully."))
+    (error
+     (message "Error during test cycle: %s" (error-message-string err))
+     (error "Test cycle failed: %s" (error-message-string err))))
+  
+  (message "Now stopping the bridge as part of the full cycle...")
+  (ostb-test-stop-bridge)
+  (message "Full test cycle completed (stop initiated). Check logs for details."))
+
+(provide 'org-supertag-bridge-test)
+;;; org-supertag-bridge-test.el ends here 
